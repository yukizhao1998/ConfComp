Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Child-Issue),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Dependent),Outward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Required),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Reviewers),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Since Version),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
CommitLogStressTest timeout in 3.11,CASSANDRA-14143,13127569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,28/Dec/17 19:20,16/Apr/19 09:29,14/Jul/23 05:55,10/Jan/18 14:48,3.11.2,,,,,,Legacy/Testing,,,,,0,testing,,,,"[~jasobrown] fixed the CommitLogStressTest timeout issue as part of CASSANDRA-13530, but it's only in trunk, it would be better to backport the unittest change to 3.11.
",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 14:48:46 UTC 2018,,,,,,,,,,"0|i3ocnb:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"28/Dec/17 21:25;jay.zhuang;Backport unittest test fix from trunk (CASSANDRA-13530), please review:
| Branch | uTest |
| [14143-3.11|https://github.com/cooldoger/cassandra/tree/14143-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14143-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14143-3.11] |;;;","10/Jan/18 14:48;jasobrown;+1, and committed as sha {{62e46f71903b339d962c4dcb3d2c04991c391a68}} to 3.11 only

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
logs directory for gc.log doesn't exist on first start,CASSANDRA-14142,13127488,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,alexott,alexott,28/Dec/17 08:51,16/Mar/22 11:24,14/Jul/23 05:56,21/Oct/18 09:12,4.0,4.0-alpha1,,,,,Local/Config,,,,,0,lhf,,,,"This was originally reported at https://stackoverflow.com/questions/47976248/gc-log-file-error-when-running-cassandra.

This is very minor problem related to timing of 'logs' directory creation - when Cassandra starts first time, this directory doesn't exist, and created when Cassandra starts to write system.log & debug.log files. But this directory is referenced in the -Xloggc command line parameter of JVM, causing following warning:

{{Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file bin/../logs/gc.log due to No such file or directory}}

The fix is to check existence of this directory in the cassandra-env, and create it.

","Unix & Windows environments, when starting freshly downloaded tarball

",alexott,githubbot,jeromatron,kirktrue,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #183:
URL: https://github.com/apache/cassandra/pull/183


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:24;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,CASSANDRA-9608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 21 09:12:13 UTC 2018,,,,,,,,,,"0|i3oc5b:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"28/Dec/17 08:55;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/183

    fix for CASSANDRA-14142

    On the first start of Cassandra JVM complains about absence of `${CASSANDRA_HOME}/logs/gc.log` file.
    
    This pull request fixes this issue - cassandra-env scripts now check for existence of the 'logs' directory and create it if it doesn't exist.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14142

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/183.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #183
    
----
commit 11a3f90e705c66e2d344b34fe2d31e6327819895
Author: Alex Ott <alexott@...>
Date:   2017-12-28T08:52:55Z

    fix for CASSANDRA-14142
    
    cassandra-env scripts now check for existence of the 'logs' directory and create it if it
    doesn't exist.

----
;;;","16/Oct/18 23:18;kirktrue;There's a patch for this on GitHub. If it needs to be a proper patch, as per the contributor guidelines, I can do that on behalf of Alex, just let me know.


Thanks.;;;","17/Oct/18 07:37;alexott;[~kirktrue] - I can rebase my patch to the trunk. Under the proper patch you mean to generate a diff & attach it? Should I also add the entry to CHANGES.txt? Anything else?;;;","20/Oct/18 00:20;kirktrue;[~alexott] - sorry for the confusion. I was merely trying to ping the reviewers to see if that was the reason it hasn't been reviewed/accepted. According to the contributor guidelines on the web site, submitting a patch and updating {{CHANGES.txt}} is the usual path.;;;","21/Oct/18 09:11;alexott;It looks like that it was fixed with this commit: [https://github.com/alexott/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb] as part of work on CASSANDRA-9608 (Java 11 support) - the patch was very long in the review, so I haven't seen the changes.

I'll mark this bug as fixed.;;;","21/Oct/18 09:12;alexott;It was fixed as part of work on CASSANDRA-9608;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Acquire read lock before accessing CompactionStrategyManager fields,CASSANDRA-14139,13127391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,27/Dec/17 17:46,15/May/20 08:03,14/Jul/23 05:56,10/Jan/18 20:44,3.11.2,4.0,4.0-alpha1,,,,,,,,,0,,,,,"There are a few methods in {{CompactionStrategyManager}} accessing the repaired/unrepaired compaction strategy lists without using the read lock, what could cause issues like the one below:

{noformat}
ERROR [CompactionExecutor:1] 2017-12-22 12:17:12,320 CassandraDaemon.java:141 - Exception in thread Thread[CompactionExecutor:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 0, Size: 1
    at java.util.ArrayList.rangeCheck(ArrayList.java:657)
    at java.util.ArrayList.get(ArrayList.java:433)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.supportsEarlyOpen(CompactionStrategyManager.java:1262)
    at org.apache.cassandra.db.ColumnFamilyStore.supportsEarlyOpen(ColumnFamilyStore.java:558)
    at org.apache.cassandra.io.sstable.SSTableRewriter.construct(SSTableRewriter.java:119)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.<init>(CompactionAwareWriter.java:91)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:57)
    at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:293)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:200)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:90)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:101)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:310)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
    at java.lang.Thread.run(Thread.java:748)
{noformat}",,jeromatron,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14150,,,,,,,,,,,,,,,,,,,"29/Dec/17 18:20;pauloricardomg;3.11-14139-dtest.png;https://issues.apache.org/jira/secure/attachment/12904035/3.11-14139-dtest.png","29/Dec/17 18:20;pauloricardomg;3.11-14139-testall.png;https://issues.apache.org/jira/secure/attachment/12904036/3.11-14139-testall.png","29/Dec/17 18:20;pauloricardomg;trunk-14139-dtest.png;https://issues.apache.org/jira/secure/attachment/12904037/trunk-14139-dtest.png","29/Dec/17 18:20;pauloricardomg;trunk-14139-testall.png;https://issues.apache.org/jira/secure/attachment/12904038/trunk-14139-testall.png",,,,,,,,,,,,,,,,4.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 20:44:08 UTC 2018,,,,,,,,,,"0|i3objr:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"29/Dec/17 18:26;pauloricardomg;It seems we missed grabbing the read lock for some operations on CASSANDRA-13948, what can cause IndexOutOfBounds/NullPointers such the ones above during compaction strategy reload, so this patch basically gets the lock on the few methods were not doing it.

CI looks good. Trunk patch is slightly different because there are some additional methods due to CASSANDRA-9143. Can you take a look [~krummas]? Thanks!

||3.11||trunk||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14139]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14139]|
|[testall|https://issues.apache.org/jira/secure/attachment/12904036/3.11-14139-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12904038/trunk-14139-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12904035/3.11-14139-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12904037/trunk-14139-dtest.png]|
;;;","06/Jan/18 17:58;marcuse;* for the {{supportsEarlyOpen}} method we can instead store the boolean in {{startup}} like we do with {{fanout}} and {{shouldDefragment}}
* in trunk, for {{getRepaired}}, {{getUnrepaired}} and {{getPendingRepairManagers}} we don't need the readlock - we return the list directly (and when refreshing we clear the same list) - to make this safe we would need to copy the list and return the copy, but those methods are only used in tests so I doubt we need it (maybe add a comment though)
;;;","10/Jan/18 13:16;pauloricardomg;Thanks for the review! See follow-up below:

bq. for the supportsEarlyOpen method we can instead store the boolean in startup like we do with fanout and shouldDefragment

good catch, fixed!

bq. in trunk, for getRepaired, getUnrepaired and getPendingRepairManagers we don't need the readlock - we return the list directly (and when refreshing we clear the same list) - to make this safe we would need to copy the list and return the copy, but those methods are only used in tests so I doubt we need it (maybe add a comment though)

even though this is only used in test, for consistency I opted for copying the list and returning a copying, so it is safe in case anybody uses it outside testing.

Updated patch with fixes above, CI looks good. Please let me know what do you think.;;;","10/Jan/18 13:38;marcuse;lgtm, +1;;;","10/Jan/18 20:44;pauloricardomg;Committed as {{fe0ee85c71faada0acb48a65f249575c65bf0972}} to cassandra-3.11 and merged up to master. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes on startup.  Crash Problematic frame: # C  [sigar-amd64-winnt.dll+0x14ed4] using JRE version: Java(TM) SE Runtime Environment (9.0+11),CASSANDRA-14137,13127251,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,p2pxd,p2pxd,26/Dec/17 18:05,16/Apr/19 09:29,14/Jul/23 05:56,27/Dec/17 02:26,3.0.15,3.0.16,,,,,Legacy/Core,Observability/Metrics,,,,0,,,,,"Startup of Cassandra crashes in sigar-amd64-winnt.dll+0x14ed4.
Short term work around: change from Java 9 back to Java 8.

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000010014ed4, pid=7112, tid=1764
#
# JRE version: Java(TM) SE Runtime Environment (9.0+11) (build 9.0.1+11)Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
# Java VM: Java HotSpot(TM) 64-Bit Server VM (9.0.1+11, mixed mode, tiered, compressed oops, concurrent mark sweep gc, windows-amd64)
# Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
#
# No core dump will be written. Minidumps are not enabled by default on client versions of Windows
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
# ","Cassandra 3.0.15
Java 9.0.1+11
Workaround is to use Java 8",KurtG,p2pxd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9608,,,,,,,,,,,,,,,,,,,"26/Dec/17 17:59;p2pxd;hs_err_pid7112.log;https://issues.apache.org/jira/secure/attachment/12903731/hs_err_pid7112.log",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 27 02:25:55 UTC 2017,,,,,,,,,,"0|i3oaon:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"27/Dec/17 02:25;KurtG;Java 9 isn't officially supported yet and will likely only be supported for 4.0 onwards. See CASSANDRA-9608;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstablemetadata incorrect date string for ""EncodingStats minLocalDeletionTime:""",CASSANDRA-14132,13126418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,VincentWhite,VincentWhite,VincentWhite,21/Dec/17 00:52,07/Mar/23 11:52,14/Jul/23 05:56,09/Feb/18 12:51,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"There is a unit mismatch in the outputing of EncodingStats minLocalDeletionTime. EncodingStats.minLocalDeletionTime is stored in seconds but is being interpenetrated as milliseconds when converted to a date string. 

Patch: [Trunk|https://github.com/vincewhite/cassandra/commit/fa9ef1dede3067dffb65042ed4bdca08de042a0e]",,jasobrown,jeromatron,KurtG,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Feb 09 12:51:17 UTC 2018,,,,,,,,,,"0|i3o5k7:",9223372036854775807,,,,,,,,,,,,Low,,5.0,,,,,,,,,,,,,,,,,"09/Feb/18 12:51;jasobrown;+1. committed as sha {{714703a08dfb965df40f4dad6ba83196ff95156f}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError while trying to upgrade 2.2.11 -> 3.11.1,CASSANDRA-14113,13124699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,knbk,xiu,xiu,13/Dec/17 15:24,11/Jul/22 14:54,14/Jul/23 05:56,11/Jul/22 14:53,3.11.14,,,,,,Legacy/Core,,,,,0,supercolumns,,,,"We're trying to upgrade a test cluster from Cassandra 2.2.11 to Cassandra 3.11.1. The tables have been created using thrift and have supercolumns. When I try to run {{nodetool upgradesstables}} I get the following:
{noformat}error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:42)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addCell(LegacyLayout.java:1242)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addAtom(LegacyLayout.java:1185)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.readRow(UnfilteredDeserializer.java:498)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.hasNext(UnfilteredDeserializer.java:472)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.hasNext(UnfilteredDeserializer.java:306)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.computeNext(SSTableSimpleIterator.java:188)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.computeNext(SSTableSimpleIterator.java:140)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:122)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:100)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.utils.MergeIterator$TrivialOneToOne.computeNext(MergeIterator.java:484)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:499)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:359)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:74)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:75)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:233)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:196)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:428)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:315)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

We also tried to upgrade to 3.0.15 instead and had a different error:
{noformat}
ERROR 11:00:40 Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IllegalStateException: [ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, position=0}, ColumnDefinition{name=, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType), kind=REGULAR, position=-1}] is not a subset of []
    at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:532) ~[main/:na]
    at org.apache.cassandra.db.Columns$Serializer.serializedSubsetSize(Columns.java:484) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serializedRowBodySize(UnfilteredSerializer.java:290) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:169) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:114) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex$Builder.add(ColumnIndex.java:144) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:112) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:52) ~[main/:na]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[main/:na]
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125) ~[main/:na]
    at org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter.realAppend(MaxSSTableSizeWriter.java:88) ~[main/:na]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195) ~[main/:na]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89) ~[main/:na]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:424) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:311) ~[main/:na]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_151]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_151]
{noformat}

Attached are a set of sstables that reproduce the issue.",Tables have been created in 2.2.11 using thrift and have supercolumns,Bhanu.M.Gandikota,blerer,brandon.williams,dcapwell,knbk,KurtG,xiu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/21 13:36;knbk;14113-3.0.txt;https://issues.apache.org/jira/secure/attachment/13035569/14113-3.0.txt","13/Dec/17 15:24;xiu;data.tar.gz;https://issues.apache.org/jira/secure/attachment/12901905/data.tar.gz",,,,,,,,,,,,,,,,,,2.0,knbk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 11 14:54:24 UTC 2022,,,,,,,,,,"0|i3nv13:",9223372036854775807,,,,,,,,,blerer,,,Normal,,3.0.15,,,https://github.com/apache/cassandra/commit/dd51df6734d9b8d8dca9c2a22659e74f674060e2,,,,,,,,,Unit tests included,,,,,"13/Dec/17 15:25;xiu;CASSANDRA-11613 might be related to that one.;;;","05/Mar/18 19:45;Bhanu.M.Gandikota;Here there, I've been receiving the exact stack errors when I tried to upgrade from 2.2.5 to 3.11.1 using ""nodetool upgradesstables"". 

I'd appreciate if a solution is provided to this issue. 

Just wanted to let you know that I don't see any other errors/warnings in system.log or in debug.log. Just the upgrades stables fails after running for a while. 

{code:sql}

$ nodetool upgradesstables

WARN  11:28:28,430 Small cdc volume detected at /cdc_raw; setting cdc_total_space_in_mb to 1982.  You can override this in cassandra.yaml

error: null

– StackTrace –

java.lang.AssertionError

at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:43)

at org.apache.cassandra.db.LegacyLayout$CellGrouper.addCell(LegacyLayout.java:1242)

at org.apache.cassandra.db.LegacyLayout$CellGrouper.addAtom(LegacyLayout.java:1185)

at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.readRow(UnfilteredDeserializer.java:495)

at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.hasNext(UnfilteredDeserializer.java:472)

at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.hasNext(UnfilteredDeserializer.java:306)

at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.readStaticRow(SSTableSimpleIterator.java:176)

at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:49)

at org.apache.cassandra.io.sstable.SSTableIdentityIterator.create(SSTableIdentityIterator.java:59)

at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:384)

at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48)

at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:70)

at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:122)

at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:113)

at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:466)

at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)

at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:163)

at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)

at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:233)

at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:196)

at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)

at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)

at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)

at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:428)

at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:315)

at java.util.concurrent.FutureTask.run(FutureTask.java:266)

at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)

at java.lang.Thread.run(Thread.java:745)
{code:java}
 {code};;;","05/Mar/18 19:46;Bhanu.M.Gandikota;FYI, here is my JDK version:

-bash-4.2$ java -version

openjdk version ""1.8.0_151""

OpenJDK Runtime Environment (build 1.8.0_151-b12)

OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)

 

 
{code:java}
 {code};;;","10/Jun/20 03:21;dcapwell;I started looking into this since I hit the same exception but the problem I hit was that a frozen collection was written in a multi-cell column; will file a different JIRA for that.

I downloaded the data and replicated the exception

{code}
select * from ""TDF_DATA"";
{code}

Schema

{code}
CREATE TABLE ""WafercloudData"".""TDF_DATA"" (
    key blob,
    column1 blob,
    column2 blob,
    """" map<blob, blob>,
    value blob,
    PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE
{code}

This produced the following in 3.0

{code}
Caused by: java.lang.IllegalStateException: [ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, position=0}, ColumnDefinition{name=, type=org.apache.cassandra.db.ma
rshal.MapType(org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType), kind=REGULAR, position=-1}] is not a subset of []
        at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:545) ~[]
        at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:477) ~[]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:188) ~[]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:118) ~[]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:106) ~[]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:132) ~[]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:77) ~[]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:325) ~[]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:186) ~[]
{code}

The subset isn't empty but its actually the column with the empty name; the exception is rejecting the column ""key"" since it was only expected non rowKey columns.

Now, this happens after the following steps

1) read the lb using LegacyLayout
2) migrate the data to modern Unfiltered
3) serialize the partition to send over the wire - this is where the exception is happening.;;;","10/Jun/20 03:46;dcapwell;When I comment out the metadata.isCQLTable() check here https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L1434, I am able to read the data.

The issue was that the partition key was written again as a normal column, in 2.1 this was dropped but in 3.0 we only drop if the table is a pure CQL table, which this table is not.;;;","22/Sep/21 12:27;knbk;We ran into this issue while upgrading a supercolumn table from 2.2.13 to 3.11.11. The issue appears to be when the literal value in ""column2"" (the key of the surrogate map) corresponds to a defined column name (i.e. ""key"", ""column1"", ""value"" in the default case). This triggers the exception described in the issue and prevents us from upgrading the table to the 3.X format. I've tracked it down to the following code:

[https://github.com/apache/cassandra/blob/8f4ae7d825d90a18327c5555386f3cdaf414d836/src/java/org/apache/cassandra/db/LegacyLayout.java#L146]

Removing the if-block and always using the compact value column fixes the issue in our case. Disabling the metadata.isCQLTable() check as suggested in the above comment avoids the exception, but doesn't return the data for that row.

Is there any case where a supercolumn table can have a statically defined column as indicated in the code comment? I can't think of any case: it's not possible to add a column to a supercolumn table, and after dropping the compact storage flag it is no longer regarded as a supercolumn table.

Potential patch:
{code:java}
diff --git a/src/java/org/apache/cassandra/db/LegacyLayout.java b/src/java/org/apache/cassandra/db/LegacyLayout.java
index 673aa4f849..780e0a54a1 100644
--- a/src/java/org/apache/cassandra/db/LegacyLayout.java
+++ b/src/java/org/apache/cassandra/db/LegacyLayout.java
@@ -143,14 +143,7 @@ public abstract class LegacyLayout
 
     private static LegacyCellName decodeForSuperColumn(CFMetaData metadata, Clustering clustering, ByteBuffer subcol)
     {
-        ColumnDefinition def = metadata.getColumnDefinition(subcol);
-        if (def != null)
-        {
-            // it's a statically defined subcolumn
-            return new LegacyCellName(clustering, def, null);
-        }
-
-        def = metadata.compactValueColumn();
+        ColumnDefinition def = metadata.compactValueColumn();
         assert def != null && def.type instanceof MapType;
         return new LegacyCellName(clustering, def, subcol);
     }
{code};;;","22/Sep/21 14:12;brandon.williams;ping [~blerer];;;","23/Sep/21 11:08;blerer;I had some quick chat with [~slebresne] regarding statically defined subcolumn. His answer was:
{quote}
When you defined a super column in thrift, it was actually possible to defined stuffs in the column_metadata , and when you did so, this was affecting sub-columns.
Another way to put it is that you could declare a super column family where all, inside a given super column, all of the cells have a value of type text, except when the cell is called, say foo, in which case the type of the value is bigint.
To be clear, I have no clue if anyone ever did that, but the API alllowed it.
This code is supposed to handle this case.
In the sense that, when translated in CQL, we translated this by using the normal map we use for super columns for everything that is text, but the foo one was a normal non-map column (with type bigint).
{quote}

Now he does not know why this code trigger the exception. I would need to look into it.;;;","23/Sep/21 11:11;blerer;[~knbk] Do you have an example of how the supercolumn was created?;;;","23/Sep/21 13:28;knbk;[~blerer] This is the CfDef we used to create the table:
{code:java}
final CfDef container = new CfDef(keyspace, tableName);
container.setColumn_type(SUPER.getValue());
container.setComparator_type(BYTESTYPE.getValue());
container.setSubcomparator_type(BYTESTYPE.getValue());
container.setCompression_options(compressionOptions);
container.setCaching(KEYS_ONLY);
container.setRead_repair_chance(0.0);
{code}
Compression options just sets the DeflateCompressor with no options. We're creating the tables through hector, but I don't think that's adding/changing any options.

The CQL table definition:
{code:java}
cqlsh:Keyspace1> desc table ""Profile"";


CREATE TABLE ""Keyspace1"".""Profile"" (
    key blob,
    column1 blob,
    column2 blob,
    """" map<blob, blob>,
    value blob,
    PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (column1 ASC, column2 ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.DeflateCompressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 0
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = 'NONE';
{code};;;","23/Sep/21 13:43;knbk;I got Cassandra's test suite running with the above patch, and it causes a number of test failures related to sparse supercolumn tables in ThriftIntegrationTest. Changing the if to {{if (def != null && !metadata.isDense())}} seems to fix the test failures, but I can't really oversee the impact of that change. I also don't know if this issue can occur for sparse tables, we only have dense tables.;;;","27/Sep/21 07:53;blerer;[~knbk] Thanks. I will find some time this week to look into the problem. ;;;","14/Oct/21 11:52;knbk;I've looked into the case with column_metadata some more. Seems that on the thrift side, the column_metadata overrides the validation type of specific subcolumns, but it's still possible to add data for dynamic subcolumns, not just the statically defined ones. On CQL's side, however, only the static subcolumns defined in columns_metadata are accessible. Either way, it's these statically defined subcolumns in column_metadata that we need to account for in decodeForSuperColumn().

So I think the solution for this should be:
 * If the table is dense, always return the compact value column with the subcolumn as the collection element.
 * If the table is sparse, check for a statically defined subcolumn. This means it is a regular column, and is not the compact value column (i.e. the internal map column) or super column value column (i.e. the ""value"" column by default). If a statically defined subcolumn is found, return that column, otherwise fall back to the compact value column with collection element.

[~blerer] If you've got some time to take a look, let me know what you think. We've decided to temporarily patch Cassandra during the upgrade for this issue and another one specific to our usecase, so it's not blocking for us, but extra eyes are more than welcome.;;;","25/Oct/21 15:26;blerer;[~knbk] Sorry for the long delay.
Your proposal make sense to me as a super column is considered as sparse if it has some {{column_metadata}}.
Would you be interested in providing a patch for this ticket? ;;;","27/Oct/21 11:57;knbk;[~blerer] Sure, I can find some time to prepare a patch.;;;","03/Dec/21 08:52;blerer;Thanks for the patch [~knbk]. It looks good to me. I am just having some issues with the upgrade tests. Due to some CI instabilities it is difficult to be sure if the patch introduce a problem or not. I will look deeper into it but it might take a bit of time as I got involved in to many things.;;;","02/Mar/22 11:04;knbk;[~blerer] Do you think you can take a look at this anytime soon, or can someone else help with the review?

We've just hit the same issue while inserting data through Thrift. When inserting a subcolumn named ""key"", the check in ThriftValidation.validateColumnData() uses the same method in LegacyLayout to decode the cell name. As this returns the partition key column, it throws an InvalidRequestException with the message ""Cannot add primary key column key to partition update"".

This affects inserts on any 3.11 keyspace, unlike the original issue that only affected queries on a 2.2 SSTable format.

Thrift insert to reproduce the issue (same table as above):
{code:java}
ColumnParent parent = new ColumnParent(tableName).setSuper_column(UTF_8.encode(""Default""));
Column column = new Column(UTF_8.encode(""key"")).setValue(UTF_8.encode("""")).setTimestamp(System.currentTimeMillis());
cassandra.insert(UTF_8.encode(""partition_key""), parent, column, ConsistencyLevel.QUORUM); {code};;;","10/Mar/22 12:42;blerer;Sorry [~knbk]. The patch looked good to me. We just had some issue with CI at that time and it was unclear to me if the patch was causing some regressions or if the problems were coming from somewhere else. Then I got drag into other problems. I will look into it. ;;;","12/May/22 12:32;knbk;Hi [~blerer], is there any improvement in the CI situation? Where would I be able to see the results (if they're still available)? I found [https://ci-cassandra.apache.org/,] but wasn't entirely sure where the results for patches would show up.

It would be great if we could land this patch so we no longer have to run our own patched version. Please let me know if you have time for this, or if it would be better to find someone else for the review.;;;","12/May/22 13:22;brandon.williams;I put [this|https://github.com/driftx/cassandra/tree/CASSANDRA-14113-3.11] through circle [here|https://app.circleci.com/pipelines/github/driftx/cassandra/476/workflows/3d84d20f-c919-4974-b7ed-8960381c0a64/jobs/5539] and unfortunately it looks like it indeed broke some things.;;;","12/May/22 15:40;knbk;Ah sorry, that was my first idea for a patch, but it didn't handle sparse tables correctly. The latest patch is in an attachment to this issue, or you can find it at [https://github.com/knbk/cassandra/tree/cassandra-14133/3.11] if that's easier. That one passed the unit test suite when I ran it locally. I think the issues were in the upgrade tests.;;;","12/May/22 16:09;brandon.williams;I've started a circle run here: https://app.circleci.com/pipelines/github/driftx/cassandra/478/workflows/ffdea584-a280-4d07-932b-36cf5e1dd6b8;;;","13/May/22 13:49;knbk;Thanks. I think the dtest failures are unrelated, right? I can reproduce them on a clean 3.11 branch, and the same tests are failing in other 3.X pipelines.;;;","13/May/22 14:46;brandon.williams;They look unrelated to me.;;;","08/Jun/22 14:04;knbk;What still needs to happen before this can be merged? Is there anything I can do?;;;","08/Jun/22 16:24;brandon.williams;Pinging [~blerer] ;;;","16/Jun/22 09:17;blerer;+1 sorry for loosing track totally. :-(
;;;","11/Jul/22 14:53;blerer;Committed into cassandra-3.11 at dd51df6734d9b8d8dca9c2a22659e74f674060e2;;;","11/Jul/22 14:54;blerer;[~knbk] Sorry for the multiple time where I dropped the ball.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The inspectJvmOptions startup check can trigger some Exception on some JRE versions,CASSANDRA-14112,13124660,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,13/Dec/17 12:59,15/May/20 08:01,14/Jul/23 05:56,14/Dec/17 14:05,2.2.12,3.0.16,3.11.2,4.0,4.0-alpha1,,Legacy/Core,,,,,0,,,,,"[~adelapena] pointed out that the Startup check added by CASSANDRA-13006 can cause some Exception if Cassandra is run on a non GA version.
After investigation it seems that it can also be the case for major versions or some JRE 9 versions. ",,adelapena,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 14 14:05:22 UTC 2017,,,,,,,,,,"0|i3nusf:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,,,,,,,,,,,,,,,,,,"13/Dec/17 13:05;blerer;I pushed a patch for 2.2 [here|https://github.com/apache/cassandra/compare/trunk...blerer:14112-2.2].
[~adelapena] could you review? ;;;","13/Dec/17 13:26;adelapena;Sure thing!;;;","14/Dec/17 11:28;adelapena;The patch looks good to me, and confirmed that it allows the server to start with a couple of non-GA JRE versions, +1. 

I have left a few minor suggestions [at this commit|https://github.com/adelapena/cassandra/commit/c19029370ad973bf7b481446a68dd94ef2492c4b], feel free to merge them if they look good to you.;;;","14/Dec/17 14:05;blerer;Thanks for the review.

Committed into 2.2 at b800f3c768289268193b0dd716be99a33f306dad and merged into 3.0, 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent continuous schema exchange between 3.0 and 3.11 nodes,CASSANDRA-14109,13124420,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,12/Dec/17 17:28,16/Apr/19 09:29,14/Jul/23 05:56,13/Dec/17 13:20,3.11.2,,,,,,Legacy/Coordination,Legacy/Distributed Metadata,,,,0,,,,,"Continuous schema migrations can happen during an upgrade from 3.0.x to 3.x even with versions having the patches for CASSANDRA-13441 and CASSANDRA-13559.

The root cause is the {{cdc}} column, which is included in schema version calculation in {{RowIterators.digest()}} via {{SchemaKeyspace.calculateSchemaDigest()}}.

It is possible to make the schema-version calculation between 3.0 and 3.11 compatible. The idea here is: 3.11 accepts both 3.0 compatible and 3.11 ""native"" schema versions. As long as there is one 3.0 node in the cluster, 3.11 announces a 3.0 compatible schema version (without the {{cdc}} column). When there are no (more) 3.0 nodes in the cluster, announce the ""real"" 3.11 schema version (including the {{cdc}} column). ""Announce"" means announcing via Gossip and storing in {{system.local}}.

The change itself is against 3.11 only. A couple of log messages have been improved and some code regarding schema version checks has been moved into the {{Schema}} class. Those ""side changes"" are carried to trunk. Because of that, the 3.11 and trunk branches are different. The ""real"" change is in the 3.11 branch.

{{NEWS.txt}} for 3.11(only) contains upgrade notes.

||OSS 3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:schema-migration-upgrade-bug-3.11?expand=1]
||OSS trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:schema-migration-upgrade-bug-trunk?expand=1]
||OSS dtest|[branch|https://github.com/riptano/cassandra-dtest/compare/master...snazy:schema-migration-upgrade-bug?expand=1]

We've verified the functionality of the patch by usual CI tests and extensive tests using the new upgrade dtest.",,adelapena,aleksey,jay.zhuang,jeromatron,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 13 13:20:46 UTC 2017,,,,,,,,,,"0|i3ntb3:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Critical,,,,,,,,,,,,,,,,,,,"13/Dec/17 10:46;adelapena;The patch looks good to me, +1. 

As a suggestion that can be addressed during commit, the new methods [{{EndpointState.getSchemaVersion()}}|https://github.com/snazy/cassandra/blob/a73fbda73f9074f189dd580512e09a4a78be9bb9/src/java/org/apache/cassandra/gms/EndpointState.java#L160] and [{{EndpointState.getReleaseVersion()}}|https://github.com/snazy/cassandra/blob/a73fbda73f9074f189dd580512e09a4a78be9bb9/src/java/org/apache/cassandra/gms/EndpointState.java#L168] could be annotated with {{@Nullable}}. ;;;","13/Dec/17 13:20;snazy;Thanks a lot for the review!

Committed the ""upgrade fix"" as [e646e5032b68622f7ec1dd0c53137be08baabed9|https://github.com/apache/cassandra/commit/e646e5032b68622f7ec1dd0c53137be08baabed9] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11] and the code improvements from that as [7a40abb6a5108688fb1b10c375bb751cbb782ea4|https://github.com/apache/cassandra/commit/7a40abb6a5108688fb1b10c375bb751cbb782ea4] to [trunk|https://github.com/apache/cassandra/tree/trunk].
Dtest committed as e67ef2b80a45ae02cc15f4e1a6c57cc68c09b0f8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve commit log chain marker updating,CASSANDRA-14108,13124410,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,12/Dec/17 16:43,15/May/20 08:05,14/Jul/23 05:56,14/Dec/17 03:55,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,,,0,,,,,"CASSANDRA-13987 addressed the commit log behavior change that was introduced with CASSANDRA-3578. After that patch was committed, [~aweisberg] did his own review and found a bug as well as having some concerns about the configuration. He and I discussed offline, and agreed on some improvements. 

Instead of requiring users to configure a deep, dark implementation detail like the commit log chained markers (via {{commitlog_marker_period_in_ms}} in the yaml), we decided it is best to eliminate thew configuration and always update the chained markers (when in periodic mode). 

The bug [~aweisberg] found was when the chained marker update is not a value that evenly divides into the periodic sync mode value, we would not sync in an expected manner. For example if the marker interval is 9 seconds, and the sync interval is 10 seconds, we would update the markers at time9, but we would then sleep for another 9 seconds, and when we wake up at time18, it is then that we flush - 8 seconds later than we should have. 
",,aweisberg,cscotta,jasobrown,jay.zhuang,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14292,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 01 22:41:40 UTC 2019,,,,,,,,,,"0|i3nt8v:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"12/Dec/17 16:43;jasobrown;Branches and tests here:

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-3.0]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-trunk]|
||

I've removed the confusing (and confusingly described) yaml property for setting the {{commitlog_marker_period_in_ms}}. Instead, I've hardcoded the marker interval to 100ms and it is always applied when a) using periodic mode, and b) not using compression or encryption.

I've fixed the bug that @ariel found by quantizing the {{commitlog_sync_period_in_ms}} to a multiple of the marker interval. At the worst, this would change the {{commitlog_sync_period_in_ms}} by up to 50 ms. I don't think anyone will really notice those milliseconds at a practical level, and if they do, they should be using batch or group mode.

I've also refactored {{AbstractCommitLogService}} so we can actually unit test it. The anonymous {{Runnable}} class that had 90% of the functionality of  {{AbstractCommitLogService}} is now a full-fledged, named, and accessible nested class, {{AbstractCommitLogService.SyncRunnable}}. To avoid being bound by actual wall clock timings for correctness testing, {{AbstractCommitLogService.SyncRunnable}} is using the {{Clock}} abstraction; this makes testing sane, safe, and repeatable. {{Clock}} was introduced in CASSANDRA-12016, and committed to 3.10. I've backported it to 3.0, but I feel it's quite safe and doesn't affect anything outside of itself.
;;;","12/Dec/17 16:44;jasobrown;I've assigned [~aweisberg] as reviewer, but it'd be fantastic if [~beobal] could take a look, as well.;;;","12/Dec/17 18:56;aweisberg;I am a little surprised we did this change originally to 3.0 and 3.11 since it's more of an enhancement.

[Extra line break|https://github.com/apache/cassandra/compare/trunk...jasobrown:13987-followup-trunk?expand=1#diff-8e54554f3635ce0e8411435dd5896f4dR81]. It's on all 3 branches.
[Another extra line break.https://github.com/apache/cassandra/compare/trunk...jasobrown:13987-followup-trunk?expand=1#diff-8e54554f3635ce0e8411435dd5896f4dR241]

I like the approach of quantizing. It's simple. Besides the whitespace +1.;;;","13/Dec/17 13:23;samt;Looks good to me, I just have a couple of trivial nits:
* The debug log in the ACLS constructor would be more informative if it included the sync interval. As is, the marker interval it does log will always be {{DEFAULT_MARKER_INTERVAL_MILLIS}}. 
* There's an extra whitespace introduced by this patch in the 3 arg ACLS constructor.
* (3.0 only) The string format specifier for the exception message in ACLS::start should be '%d', not '%f' as we're just printing the value in millis.
;;;","14/Dec/17 03:55;jasobrown;committed as sha {{db788fe860dfd69f06ab97ae35fa67fcf2517b6d}}. Thanks [~aweisberg] and [~beobal].;;;","01/Apr/19 22:41;cscotta;[~jasobrown] Sorry for the assignment change; I hit the key combo by mistake while viewing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
utest failed: DistributionSequenceTest.setSeed() and simpleSequence(),CASSANDRA-14106,13124249,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,12/Dec/17 00:53,15/May/20 08:05,14/Jul/23 05:56,20/Dec/17 00:24,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,"To reproduce:
{noformat}
$ ant stress-test -Dtest.name=DistributionSequenceTest
{noformat}

{noformat}
stress-test:
    [junit] Testsuite: org.apache.cassandra.stress.generate.DistributionSequenceTest
    [junit] Testsuite: org.apache.cassandra.stress.generate.DistributionSequenceTest Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.08 sec
    [junit]
    [junit] Testcase: simpleSequence(org.apache.cassandra.stress.generate.DistributionSequenceTest):    FAILED
    [junit] expected:<5> but was:<4>
    [junit] junit.framework.AssertionFailedError: expected:<5> but was:<4>
    [junit]     at org.apache.cassandra.stress.generate.DistributionSequenceTest.simpleSequence(DistributionSequenceTest.java:37)
    [junit]
    [junit]
    [junit] Testcase: setSeed(org.apache.cassandra.stress.generate.DistributionSequenceTest):   FAILED
    [junit] expected:<5> but was:<4>
    [junit] junit.framework.AssertionFailedError: expected:<5> but was:<4>
    [junit]     at org.apache.cassandra.stress.generate.DistributionSequenceTest.setSeed(DistributionSequenceTest.java:111)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.stress.generate.DistributionSequenceTest FAILED
{noformat}",,bdeggleston,jay.zhuang,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14090,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 00:24:54 UTC 2017,,,,,,,,,,"0|i3ns9b:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"12/Dec/17 05:51;jay.zhuang;cassandra-stress is also failed with exception {{/ by zero}}, (added debug info):

{noformat}
$ tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1)' n=10 cl=ONE no-warmup -rate threads=1

...
java.lang.ArithmeticException: / by zero

	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.decompose(PartitionIterator.java:410)
	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.setLastRow(PartitionIterator.java:347)
	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.reset(PartitionIterator.java:282)
	at org.apache.cassandra.stress.generate.PartitionIterator.reset(PartitionIterator.java:107)
	at org.apache.cassandra.stress.operations.PartitionOperation.reset(PartitionOperation.java:115)
	at org.apache.cassandra.stress.operations.PartitionOperation.ready(PartitionOperation.java:101)
	at org.apache.cassandra.stress.StressAction$StreamOfOperations.nextOp(StressAction.java:352)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:453)

...
FAILURE
java.lang.RuntimeException: Failed to execute stress action
	at org.apache.cassandra.stress.StressAction.run(StressAction.java:99)
	at org.apache.cassandra.stress.Stress.run(Stress.java:143)
	at org.apache.cassandra.stress.Stress.main(Stress.java:62)

Process finished with exit code 1
{noformat};;;","12/Dec/17 07:26;jay.zhuang;Fixed the failed unittest and added an uTest for CASSANDRA-14090, please review:
| Branch | uTest |
| [14106|https://github.com/cooldoger/cassandra/tree/14106] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14106.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14106] |;;;","12/Dec/17 19:12;jjirsa;[~bdeggleston] - you recently touched this for CASSANDRA-14090 - and this undoes your change. Want to weigh in?;;;","12/Dec/17 22:49;bdeggleston;undoing that change is going to break stress. This will no longer work: {{tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1,simple=1)' n=1000 cl=ONE -mode native cql3 -rate threads=100 fixed=1000/s}}

However, as [~jasonstack] pointed out, my change only fixed part of the problem, since I didn't fix the denominator in the return statement. I was just going to just ninja fix that, but it's worth seeing if that would fix this problem and doing that instead.

In the future, we might want to reject any one time static analysis fixes that aren't going to become part of the build process.;;;","12/Dec/17 22:59;jay.zhuang;Hi [~bdeggleston], after changing the {{double d}} to {{float d}}, it won't break stress. In the patch, I also added an unittest to reproduce the issue introduced by {{lgmt.com}} change.

The problem with {{double}} is: in the last round, the value of {{d}} would be {{1.0000...004}} instead of {{1.0}}. Which causes the exception {{OutOfRangeException: 1 out of [0, 1] range}} in cassandra-stress gaussian distribution.;;;","13/Dec/17 01:16;bdeggleston;If stress works properly, then it's ok with me.

edit: actually, taking another look at the code, [~jay.zhuang]'s change is the right thing to do here;;;","13/Dec/17 17:21;bdeggleston;-I was thinking about this some more. I think that, in addition to your changes here, the denominator in the return statement should be changed to 50, since that's the number of iterations that will (and should) actually be performed.-

edit: nevermind [~jay.zhuang] corrected me offline;;;","15/Dec/17 17:32;jay.zhuang;hi [~bdeggleston] would you mind being the reviewer?;;;","20/Dec/17 00:24;bdeggleston;+1, committed to trunk as {{1db54a1266a68d9c765c0fa248277635c4520f6d}}. Thanks [~jay.zhuang]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial log format error,CASSANDRA-14105,13124191,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,11/Dec/17 18:29,15/May/20 08:06,14/Jul/23 05:56,12/Dec/17 01:41,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"The same issue as CASSANDRA-13551
The ""{}"" is not needed for: {{log.error(String, Throwable)}}",,jay.zhuang,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 01:41:01 UTC 2017,,,,,,,,,,"0|i3nrwf:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"11/Dec/17 18:32;jay.zhuang;| Branch | uTest |
| [14105|https://github.com/cooldoger/cassandra/tree/14105] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14105.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14105] |;;;","11/Dec/17 19:49;jjirsa;+1
;;;","12/Dec/17 00:54;jay.zhuang;I created a JIRA for failed utest: CASSANDRA-14106. It's not related to this change.;;;","12/Dec/17 01:41;jjirsa;Thanks! Committed as {{e18a49a2399a3fe667c3c08d7350b7528614f0a6}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index target doesn't correctly recognise non-UTF column names after COMPACT STORAGE drop,CASSANDRA-14104,13124185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,11/Dec/17 17:39,15/May/20 08:07,14/Jul/23 05:56,19/Dec/17 10:02,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,,,0,,,,,"Creating a compact storage table with dynamic composite type, then running {{ALTER TALBE ... DROP COMPACT STORAGE}} and then restarting the node will crash Cassandra node, since the Index Target is fetched using hashmap / strict equality. We need to fall back to linear search when index target can't be found (which should not be happening often).",,ifesdjeen,jasonstack,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14468,,,,,,,,,,,,,"19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12902803/ifesdjeen-14104-3.0-dtest.png","19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12902802/ifesdjeen-14104-3.0-testall.png","19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12902801/ifesdjeen-14104-3.11-dtest.png","19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12902800/ifesdjeen-14104-3.11-testall.png","19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-trunk-dtest.png;https://issues.apache.org/jira/secure/attachment/12902799/ifesdjeen-14104-trunk-dtest.png","19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12902798/ifesdjeen-14104-trunk-testall.png",,,,,,,,,,,,,,6.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 19 10:02:10 UTC 2017,,,,,,,,,,"0|i3nrv3:",9223372036854775807,,,,,,,jasonstack,,jasonstack,,,Normal,,,,,,,,,,,,,,,,,,,"11/Dec/17 18:29;ifesdjeen;Patch:

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:14104-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:14104-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:14104-trunk]|;;;","12/Dec/17 01:59;jasonstack;LGTM, thanks for the fix.
;;;","19/Dec/17 08:52;ifesdjeen;Attaching CI results. 
The failures seem unrelated.;;;","19/Dec/17 10:02;ifesdjeen;Committed to 3.0 as [0521f8dc5d5e05c0530726e9549fa2481726a818|https://github.com/apache/cassandra/commit/0521f8dc5d5e05c0530726e9549fa2481726a818] and merged up to [3.11|https://github.com/apache/cassandra/commit/adc32ac836e90b8c4503030feb76ce031998ad80] and [trunk|https://github.com/apache/cassandra/commit/8764ef2da367fac64adad0821a34e6fa15da13c6].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix potential race during compaction strategy reload,CASSANDRA-14103,13123761,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,marcuse,pauloricardomg,pauloricardomg,08/Dec/17 17:59,02/Apr/23 12:06,14/Jul/23 05:56,16/Sep/20 09:14,3.11.9,4.0,4.0-beta3,,,,Local/Compaction,,,,,0,,,,,"When the compaction strategies are reloaded after disk boundary changes (CASSANDRA-13948), it's possible that a recently finished SSTable is added twice to the compaction strategy: once when the compaction strategies are reloaded due to the disk boundary change ({{maybeReloadDiskBoundarie}}), and another when the {{CompactionStrategyManager}} is processing the {{SSTableAddedNotification}}.

This should be quite unlikely because a compaction must finish as soon as the disk boundary changes, and even if it happens most compaction strategies would not be affected by it since they deduplicate sstables internally, but we should protect against such scenario. 

For more context see [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13948?focusedCommentId=16280448&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16280448] from Marcus.",,bdeggleston,jay.zhuang,jeromatron,KurtG,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14642,,,,,CASSANDRA-15242,CASSANDRA-14826,,,,,,,,,,CASSANDRA-13948,,,,,,,,,,"03/Jan/18 13:05;pauloricardomg;3.11-14103-dtest.png;https://issues.apache.org/jira/secure/attachment/12904398/3.11-14103-dtest.png","03/Jan/18 13:05;pauloricardomg;3.11-14103-testall.png;https://issues.apache.org/jira/secure/attachment/12904399/3.11-14103-testall.png","03/Jan/18 13:05;pauloricardomg;trunk-14103-dtest.png;https://issues.apache.org/jira/secure/attachment/12904400/trunk-14103-dtest.png","03/Jan/18 13:05;pauloricardomg;trunk-14103-testall.png;https://issues.apache.org/jira/secure/attachment/12904401/trunk-14103-testall.png",,,,,,,,,,,,,,,,4.0,marcuse,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 09:14:17 UTC 2020,,,,,,,,,,"0|i3np93:",9223372036854775807,,,,,,,marcuse,,bdeggleston,,,Critical,,3.11.2,,,https://github.com/apache/cassandra/commit/f41ea9fb14936bca4aeea0ab2bf6d55c51f37f6a,,,,,,,,,"new unit tests, cci run",,,,,"08/Dec/17 18:02;pauloricardomg;I thought that a simple way to fix it without messing with the locks is to not fetch the SSTables from the tracker when reloading the compaction strategies, but only when initializing the CompactionStrategyManager, and keep an sstable set in the compaction strategy manager which is used when reloading the compaction strategies.;;;","03/Jan/18 14:38;pauloricardomg;The patch below implements the solution described above of keeping an SSTable set in the {{CompactionStrategyManager}} which is updated when receiving notifications from the tracker, what should prevent double adding of sstables if the strategies are reloaded by some other thread when processing a notification from the tracker. I also added a test to check that the sstables are properly added to the compaction strategies when receiving tracker notifications.

On the trunk patch I also fixed a bad merge from CASSANDRA-14082 ([here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14103#diff-1d4755900f9e76a3cf93810d98189951L706]).

CI looks good:

||3.11||trunk||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14103]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14103]|
|[testall|https://issues.apache.org/jira/secure/attachment/12904399/3.11-14103-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12904400/trunk-14103-dtest.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12904398/3.11-14103-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12904401/trunk-14103-testall.png]|;;;","12/Apr/18 05:55;marcuse;(sorry for the delay on this)

LGTM, just a few minor comments;
* Could we make {{CompactionStrategyManager}} take the initial sstables as a parameter to the constructor instead of calling {{cfs.getSSTables(...CANONICAL..)}} there? Feels it makes it more clear that the tracker has to be populated before we can create the CSM
* Make {{maybeReloadDiskBoundaries}} return {{void}}, the only user of the return value is the test case and that could probably be refactored to check that the boundaries changed instead?;;;","07/Sep/20 09:44;marcuse;Cancelling patch available

It is probably a better idea to make sure LCS correctly handles duplicate notifications instead of introducing a third place where we track sstables.;;;","09/Sep/20 11:25;marcuse;Patch: https://github.com/krummas/cassandra/commits/marcuse/14103
cci: https://app.circleci.com/pipelines/github/krummas/cassandra/502/workflows/3403b209-f0fd-4763-a3fe-6cd81ca9b263

this patch contains 2 commits, one which cleans up {{CompactionStrategyManager}} a bit and makes sure we don't miss any notifications, the other one fixes LCS to correctly handle getting added/removed notifications for sstables already added/removed by {{maybeReloadDiskBoundaries()}}

The second patch here essentially just changes the levels in LCS from lists to (sorted) sets, and the reason is that checking if an sstable already exists in the manifest gets very slow if we have many sstables (like when we are overrun in L0 for example). This also improves startup time which is more important now that we reload the compaction strategies after every range movement. This is a reworked patch from CASSANDRA-14826.;;;","14/Sep/20 23:59;bdeggleston;This looks like if fixes the issue and looks good for the most part. I had some questions about how we react to sstables that are in the wrong levels for whatever reason. AFAICT, misclassified sstables makes leveled compaction less effective, but it doesn't seem like it has the potential to cause data loss, correctness bugs, or outages. What do you think about logging something scary in LeveledGenerations.maybeVerifyLevels and putting the sstable in the right spot instead of throwing an assertion error? If so, we should also correct misplaced sstables in LeveledGenerations.addAll when we check its level against getLevelIfExists, if not, we should throw an exception there.

Also 2 nits:
* the fields added to CompactionStrategyManager (and the 2 existing ones, supportsEarlyOpen and fanout, should be volatile)
* the comment in LeveledGenerations.addAll should be indented.;;;","15/Sep/20 06:53;marcuse;bq. LeveledGenerations.maybeVerifyLevels 
this is only runs in the tests, so we don't throw anything during normal operation ({{private final boolean strictLCSChecksTest = Boolean.getBoolean(Config.PROPERTY_PREFIX + ""test.strict_lcs_checks"");}}). Added an ERROR log when adding sstables if it already exists on the wrong level, then removes it and continues to add it at the sstable-metadata-level

fixed the nits;;;","15/Sep/20 15:07;bdeggleston;ah got it, thanks. +1;;;","16/Sep/20 09:14;marcuse;and committed, test runs:

[3.11|https://app.circleci.com/pipelines/github/krummas/cassandra/515/workflows/49751fc1-ef03-45d2-883f-8e8a7c5298ff] , [trunk|https://app.circleci.com/pipelines/github/krummas/cassandra/509/workflows/a50f71ca-b117-4cbe-b059-60f09b3987fe];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCS ordering of sstables by timestamp is inverted,CASSANDRA-14099,13123263,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,VincentWhite,jjirsa,jjirsa,06/Dec/17 18:21,15/May/20 08:36,14/Jul/23 05:56,15/Oct/19 01:40,3.11.4,4.0,4.0-alpha2,,,,Local/Compaction,,,,,0,,,,,"In CASSANDRA-14010 we discovered that CASSANDRA-13776 broke sstable ordering by timestamp (inverted it accidentally). Investigating that revealed that the comparator was expecting newest-to-oldest for read command, but LCS expects oldest-to-newest.

",,bdeggleston,blambov,jasonstack,jeromatron,jjirsa,jjordan,KurtG,rha,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14870,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 13:39:20 UTC 2018,,,,,,,,,,"0|i3nm6n:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"13/Dec/17 04:06;VincentWhite;I've created a patch that splits in this comparator in two as a way to maybe help avoid this confusion in the future. Now that this is split into two I'm not sure if a unit test for ageSortedSSTables (or the comparators themselves) would be required? I have included a unit test for ageSortedSSTables on my 3.0.x branch, not sure if it's worth making ageSortedSStable() Public just for this but I didn't see anywhere else where its behaviour was visible.

[3.0 patch | https://github.com/vincewhite/cassandra/commits/14099_timestamp_comparators_30]
[3.0 utest | https://github.com/vincewhite/cassandra/commit/5ab1ff36a28b41039bd93de7d47b4131e1c2dfaa]
[3.x patch | https://github.com/vincewhite/cassandra/commits/14099_timestamp_comparators_311]
[trunk patch | https://github.com/vincewhite/cassandra/commits/14099_timestamp_comparators_trunk]

;;;","06/Mar/18 03:49;KurtG;I'd say there's not really a need to create an ""ascending"" and ""descending"". Ascending can be acquired by simply calling {{maxTimestampComparator.reversed()}} and this should be satisfactory for anything that needs it.

I'd just change {{ageSortedSSTables}} to specify {{reversed()}} on the comparator. Would save us storing 2 copies of the comparator needlessly and changing the existing {{maxTimestampComparator}}.;;;","13/Nov/18 13:39;blambov;The 3.11 and trunk patches with test have been committed as part of CASSANDRA-14870.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.11.1 Repair Causes Out of Memory,CASSANDRA-14096,13122803,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,molsson,serhatd,serhatd,05/Dec/17 08:46,15/May/20 08:05,14/Jul/23 05:56,14/Feb/19 21:29,3.0.19,3.11.5,4.0,4.0-alpha1,,,Consistency/Repair,,,,,0,,,,,"Number of nodes: 9
System resources: 8 Core, 16GB RAM
Replication factor: 3
Number of vnodes: 256

We get out of memory errors while repairing (incremental or full) our keyspace. I had tried to increase node's memory from 16GB to 32GB but result did not change. Repairing tables one by one in our keyspace was not completed successfully for all tables too. 

Only subrange repair with cassandra-reaper worked for me.

Here is the output of heap utils before oom:

{code}

ERROR [MessagingService-Incoming-/192.168.199.121] 2017-12-05 11:38:08,121 JVMStabilityInspector.java:142 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.gms.GossipDigestSerializationHelper.deserialize(GossipDigestSyn.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:95) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1]
{code}

{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      31105265     1493052720  org.apache.cassandra.utils.MerkleTree$Inner
   2:      31134570      996306240  org.apache.cassandra.utils.MerkleTree$Leaf
   3:      31195121      748682904  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   4:      22885384      667447608  [B
   5:        214550       18357360  [C
   6:        364637       17502576  java.nio.HeapByteBuffer
   7:         46525        9566496  [J
   8:        111024        5306976  [Ljava.lang.Object;
   9:        132674        5306960  org.apache.cassandra.db.rows.BufferCell
  10:        210309        5047416  java.lang.String
  11:         59984        3838976  org.apache.cassandra.utils.btree.BTreeSearchIterator
  12:        101181        3237792  java.util.HashMap$Node
  13:         27158        2719216  [I
  14:         60181        2407240  java.util.TreeMap$Entry
  15:         65998        2111936  org.apache.cassandra.db.rows.BTreeRow
  16:         62387        2023784  [Ljava.nio.ByteBuffer;
  17:         19086        1750464  [Ljava.util.HashMap$Node;
  18:         63466        1523184  javax.management.ObjectName$Property
  19:         61553        1477272  org.apache.cassandra.db.BufferClustering
  20:         29274        1405152  org.apache.cassandra.utils.MerkleTree
  21:         34602        1384080  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$100/78247817
  22:         40972        1311104  java.util.concurrent.ConcurrentHashMap$Node
  23:         39172        1253504  java.util.RandomAccessSubList
  24:         51657        1239768  org.apache.cassandra.db.LivenessInfo
  25:         19013        1216832  java.nio.DirectByteBuffer
  26:         28178        1127120  org.apache.cassandra.db.PreHashedDecoratedKey
  27:         32407        1033120  [Ljavax.management.ObjectName$Property;
  28:         42090        1010160  java.util.EnumMap$EntryIterator$Entry
  29:         40878         981072  java.util.Arrays$ArrayList
  30:         19721         946608  java.util.HashMap
  31:          8359         932600  java.lang.Class
  32:         37277         894648  org.apache.cassandra.dht.Range
  33:         26897         860704  org.apache.cassandra.db.rows.EncodingStats
  34:         19958         798320  org.apache.cassandra.utils.MergeIterator$Candidate
  35:         31281         750744  java.util.ArrayList
  36:         23291         745312  org.apache.cassandra.utils.MerkleTree$TreeRange
  37:         21650         692800  java.util.AbstractList$ListItr
  38:         27675         664200  java.lang.Long
  39:         16204         648160  javax.management.ObjectName
  40:         36873         589968  org.apache.cassandra.utils.WrappedInt
  41:          4100         557600  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference
  42:         21651         519624  java.util.SubList$1
  43:         12275         491000  java.math.BigInteger
  44:          8657         484792  org.apache.cassandra.utils.memory.BufferPool$Chunk
  45:         14732         471424  java.util.ArrayList$Itr
  46:          5371         429680  java.lang.reflect.Constructor
  47:         12640         404480  com.codahale.metrics.LongAdder
  48:         16156         387744  com.sun.jmx.mbeanserver.NamedObject
  49:         16133         387192  com.sun.jmx.mbeanserver.StandardMBeanSupport
  50:          9536         381440  org.apache.cassandra.db.EmptyIterators$EmptyUnfilteredRowIterator
  51:          6035         337960  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator
  52:          6031         337736  org.apache.cassandra.db.transform.UnfilteredRows
  53:          8298         331920  org.apache.cassandra.db.rows.BTreeRow$Builder
  54:          5182         331648  sun.security.provider.SHA2$SHA256
  55:         10356         331392  org.apache.cassandra.utils.btree.BTree$$Lambda$192/259279152
  56:          8145         325800  org.apache.cassandra.db.rows.SerializationHelper
  57:          8144         325760  org.apache.cassandra.io.sstable.SSTableIdentityIterator
  58:          8144         325760  org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator
  59:           176         319536  [Ljava.util.concurrent.ConcurrentHashMap$Node;
  60:          9716         310912  java.net.InetAddress$InetAddressHolder
  61:          7770         310800  com.github.benmanes.caffeine.cache.NodeFactory$SStMW
  62:         18470         295520  org.apache.cassandra.db.rows.CellPath$SingleItemCellPath
  63:          2505         276784  [S
  64:          5646         271008  com.codahale.metrics.EWMA
  65:         11258         270192  java.util.concurrent.ConcurrentLinkedDeque$Node
  66:          8248         263936  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1
  67:         10618         254832  java.lang.Double
  68:          7921         253472  org.apache.cassandra.cache.ChunkCache$Buffer
  69:          7773         248736  org.apache.cassandra.cache.ChunkCache$Key
  70:         10296         247104  org.apache.cassandra.dht.Token$KeyBound
  71:          6096         243816  [Lorg.apache.cassandra.db.transform.Transformation;
  72:          6035         241400  org.apache.cassandra.db.rows.Row$Merger
  73:          6034         241360  org.apache.cassandra.db.rows.RangeTombstoneMarker$Merger
  74:          6034         241360  org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer
  75:          9969         239256  org.apache.cassandra.db.RowIndexEntry
  76:          9699         232776  java.net.Inet4Address
  77:          5750         230000  org.apache.cassandra.utils.concurrent.Ref$State
  78:         13690         219040  java.util.concurrent.atomic.AtomicInteger
  79:          9091         218184  org.apache.cassandra.gms.GossipDigest
  80:         12392         216040  [Ljava.lang.Class;
  81:          5289         211560  org.apache.cassandra.utils.MergeIterator$ManyToOne
  82:         13079         209264  java.lang.Object
  83:          5183         207320  org.apache.cassandra.repair.Validator$CountingDigest
  84:          8157         195768  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge
  85:          6035         193120  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer
  86:          6023         192736  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  87:          5745         183840  com.google.common.collect.RegularImmutableList
  88:          6035         180640  [Lorg.apache.cassandra.db.rows.Row;
  89:          6034         180600  [Lorg.apache.cassandra.db.rows.RangeTombstoneMarker;
  90:          6033         180576  [Lorg.apache.cassandra.db.DeletionTime;
  91:          7464         179136  org.apache.cassandra.db.rows.BTreeRow$$Lambda$109/2102075500
  92:          5288         171488  [Lorg.apache.cassandra.utils.MergeIterator$Candidate;
  93:          5331         170592  com.google.common.collect.Iterators$11
  94:          5183         165856  java.security.MessageDigest$Delegate
  95:          5178         165696  com.google.common.collect.Iterators$7
  96:          5157         165024  org.apache.cassandra.utils.MerkleTree$RowHash
  97:           169         163280  [Lio.netty.util.Recycler$DefaultHandle;
  98:          2304         147456  io.netty.buffer.PoolSubpage
  99:          4608         147456  java.util.EnumMap$EntryIterator
 100:          6034         144816  org.apache.cassandra.db.rows.Row$Merger$CellReducer
 101:          1595         140360  java.lang.reflect.Method
 102:          2893         138864  java.util.TreeMap
 103:          5750         138000  org.apache.cassandra.utils.concurrent.Ref
 104:          8453         135248  org.apache.cassandra.db.rows.BTreeRow$Builder$CellResolver
 105:          5613         134712  java.util.concurrent.atomic.AtomicLong
 106:          5509         132216  org.apache.cassandra.utils.btree.BTree$FiltrationTracker
 107:          5179         124296  com.google.common.collect.Iterables$6
 108:          5179         124296  com.google.common.collect.Iterables$8
 109:          5179         124296  com.google.common.collect.Iterators$5
 110:          5179         124296  com.google.common.collect.Iterators$8
 111:          5177         124248  com.google.common.collect.Iterables$2
 112:          5159         123816  sun.security.jca.GetInstance$Instance
 113:          2577         123696  java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
 114:          2399         115152  org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir
 115:          4643         111432  org.apache.cassandra.db.DeletionTime
 116:          4490         107760  org.apache.cassandra.db.Columns
 117:          2673         106920  java.util.EnumMap
 118:          4202         100848  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxCounter
 119:          6095          97520  org.apache.cassandra.db.transform.BaseIterator$Stop
 120:          4041          96984  java.util.concurrent.ConcurrentLinkedDeque
 121:          4033          96792  org.apache.cassandra.utils.concurrent.Ref$GlobalState
 122:          1882          90336  com.codahale.metrics.Meter
 123:          5596          89536  java.util.concurrent.atomic.AtomicLongArray
 124:          1845          88560  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxTimer
 125:          5179          82864  com.google.common.collect.Iterables$3
 126:          2050          82000  org.apache.cassandra.utils.btree.BTree$Builder
 127:          1111          71104  java.nio.DirectByteBufferR
 128:          1713          68520  java.util.LinkedHashMap$Entry
 129:          2115          67680  io.netty.util.Recycler$DefaultHandle
 130:          1687          67480  java.lang.ref.SoftReference
 131:          1519          66968  [Ljava.lang.String;
 132:          2724          65376  org.apache.cassandra.db.PartitionColumns
 133:          1598          63920  org.apache.cassandra.io.util.MmappedRegions$State
 134:          2572          61728  java.util.concurrent.locks.ReentrantReadWriteLock
 135:          3736          59776  java.util.concurrent.atomic.AtomicBoolean
 136:           154          59136  io.netty.util.concurrent.FastThreadLocalThread
 137:          1835          58720  org.apache.cassandra.utils.MergeIterator$TrivialOneToOne
 138:          1794          57408  org.apache.cassandra.gms.EndpointState
 139:           896          57344  org.apache.cassandra.config.ColumnDefinition
 140:          1385          55400  sun.misc.Cleaner
 141:          2302          55248  org.apache.cassandra.db.commitlog.CommitLogPosition
 142:          1713          54816  java.io.FileDescriptor
 143:           802          51328  sun.nio.ch.FileChannelImpl
 144:          2137          51288  org.apache.cassandra.db.rows.Row$Deletion
 145:           400          51200  org.apache.cassandra.io.sstable.format.big.BigTableReader
 146:          1584          50688  java.lang.StackTraceElement
 147:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
 148:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
 149:          1579          50528  java.lang.ref.WeakReference
 150:          1563          50016  org.apache.cassandra.io.util.Memory
 151:          1559          49888  java.util.concurrent.locks.ReentrantLock$NonfairSync
 152:            60          48760  [D
 153:           867          48552  java.lang.invoke.MemberName
 154:          1176          47040  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper
 155:          1176          47040  org.apache.cassandra.net.MessageIn
 156:          1938          46512  org.apache.cassandra.db.rows.ComplexColumnData
 157:          1157          46280  com.google.common.util.concurrent.AbstractFuture$Sync
 158:          1893          45432  java.util.concurrent.Executors$RunnableAdapter
 159:           400          44800  org.apache.cassandra.io.sstable.metadata.StatsMetadata
 160:           605          43560  java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask
 161:          2713          43408  com.codahale.metrics.Counter
 162:          1794          43056  org.apache.cassandra.gms.HeartBeatState
 163:          1033          41320  org.apache.cassandra.db.rows.BTreeRow$Builder$ComplexColumnDeletion
 164:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
 165:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
 166:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
 167:           616          39424  com.google.common.collect.MapMakerInternalMap$Segment
 168:          1611          38664  com.codahale.metrics.Histogram
 169:          1611          38664  com.codahale.metrics.Timer
 170:          2410          38560  java.util.concurrent.atomic.AtomicReference
 171:           601          38464  java.util.concurrent.ConcurrentHashMap
 172:          1601          38424  org.apache.cassandra.io.util.ChannelProxy
 173:          1587          38088  org.apache.cassandra.cache.KeyCacheKey
 174:          1583          37992  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
 175:           945          37800  org.apache.cassandra.metrics.LatencyMetrics
 176:          1557          37368  org.apache.cassandra.gms.VersionedValue
 177:          1157          37024  java.lang.ThreadLocal$ThreadLocalMap$Entry
 178:          1540          36960  java.util.concurrent.LinkedBlockingQueue$Node
 179:          1525          36600  org.apache.cassandra.repair.NodePair
 180:           151          36240  org.apache.cassandra.metrics.TableMetrics
 181:          1490          35760  java.util.concurrent.ConcurrentLinkedQueue$Node
 182:          2213          35408  java.util.TreeMap$KeySet
 183:           868          34720  java.util.HashMap$ValueIterator
 184:           863          34520  java.lang.invoke.MethodType
 185:           710          34080  org.apache.cassandra.metrics.RestorableMeter$RestorableEWMA
 186:           418          33696  [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry;
 187:           809          32360  sun.nio.ch.FileChannelImpl$Unmapper
 188:          1344          32256  com.google.common.util.concurrent.ExecutionList
 189:          1342          32208  org.apache.cassandra.utils.Pair
 190:          2012          32192  java.lang.Integer
 191:           800          32000  org.apache.cassandra.io.util.FileHandle
 192:          1333          31992  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxHistogram
 193:          1324          31776  [Lorg.apache.cassandra.dht.Range;
 194:           948          30336  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
 195:          1223          29352  java.lang.StringBuilder
 196:           898          28736  sun.security.util.DerInputBuffer
 197:           898          28736  sun.security.util.DerValue
 198:          1196          28704  javax.management.openmbean.CompositeDataSupport
 199:          1176          28224  org.apache.cassandra.concurrent.ExecutorLocals
 200:          1176          28224  org.apache.cassandra.net.MessageDeliveryTask
 201:           866          27712  java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry
 202:          1143          27432  org.apache.cassandra.repair.SyncStat
 203:           685          27400  org.apache.cassandra.io.sstable.IndexInfo
 204:          1109          26616  org.apache.cassandra.utils.Interval
 205:           828          26496  org.apache.cassandra.utils.MergeIterator$OneToOne
 206:           816          26112  java.lang.ref.ReferenceQueue
 207:           800          25600  org.apache.cassandra.io.util.FileHandle$Cleanup
 208:           982          23568  java.util.Collections$UnmodifiableRandomAccessList
 209:           716          22912  org.apache.cassandra.db.context.CounterContext$ContextState
 210:           941          22584  org.apache.cassandra.utils.MerkleTrees
 211:           400          22400  org.apache.cassandra.io.compress.CompressionMetadata
 212:           400          22400  org.apache.cassandra.io.sstable.IndexSummary
 213:           400          22400  org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier
 214:           553          22120  org.apache.cassandra.db.SerializationHeader
 215:           389          21784  sun.nio.cs.UTF_8$Encoder
 216:           160          21760  io.netty.util.internal.InternalThreadLocalMap
 217:           898          21552  sun.security.util.DerInputStream
 218:           445          21360  org.apache.cassandra.repair.RepairJob
 219:           885          21240  [Lsun.security.x509.AVA;
 220:           885          21240  sun.security.x509.AVA
 221:           885          21240  sun.security.x509.RDN
 222:           878          21072  org.apache.cassandra.repair.TreeResponse
 223:           855          20520  java.util.concurrent.ConcurrentSkipListMap$Node
 224:           628          20096  java.util.Hashtable$Entry
 225:           349          20024  [Z
 226:           621          19872  java.io.File
 227:          1233          19728  java.util.TreeMap$Values
 228:          1212          19392  java.util.Optional
 229:           404          19392  org.apache.cassandra.io.sstable.Descriptor
 230:           604          19328  [Lcom.codahale.metrics.Histogram;
 231:           802          19248  sun.nio.ch.NativeThreadSet
 232:           801          19224  org.apache.cassandra.io.util.MmappedRegions
 233:           399          19152  org.apache.cassandra.io.sstable.format.big.BigFormat$BigVersion
 234:           798          19152  org.apache.cassandra.io.util.ChannelProxy$Cleanup
 235:           798          19152  org.apache.cassandra.utils.EstimatedHistogram
 236:           788          18912  org.apache.cassandra.metrics.ClearableHistogram
 237:           766          18384  com.google.common.collect.SingletonImmutableList
 238:           762          18288  org.apache.cassandra.gms.GossipDigestSyn
 239:           569          18208  java.nio.DirectByteBuffer$Deallocator
 240:           569          18208  org.apache.cassandra.db.filter.ColumnFilter
 241:           300          18000  [Ljava.lang.ref.SoftReference;
 242:           160          17920  org.apache.cassandra.config.CFMetaData
 243:           744          17856  java.util.concurrent.CopyOnWriteArrayList
 244:           442          17680  java.util.HashMap$EntryIterator
 245:           221          17680  org.apache.cassandra.io.sstable.format.big.BigTableScanner
 246:           225          17464  [Ljava.lang.StackTraceElement;
 247:          1084          17344  java.util.EnumMap$EntrySet
 248:           424          16960  org.apache.cassandra.utils.btree.NodeCursor
 249:            32          16896  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
 250:           300          16800  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 251:             1          16400  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node;
 252:           512          16384  org.apache.cassandra.repair.RepairJobDesc
 253:           154          16016  com.google.common.collect.MapMakerInternalMap
 254:           500          16000  java.lang.invoke.DirectMethodHandle
 255:           400          16000  org.apache.cassandra.io.sstable.BloomFilterTracker
 256:           998          15968  org.antlr.runtime.BitSet
 257:           664          15936  com.google.common.collect.ImmutableMapEntry$TerminalEntry
 258:           398          15920  java.util.WeakHashMap$Entry
 259:           392          15680  java.lang.ref.Finalizer
 260:           325          15600  java.util.concurrent.ConcurrentSkipListMap
 261:           487          15584  org.apache.cassandra.schema.CompressionParams
 262:           485          15520  sun.security.util.ObjectIdentifier
 263:           483          15456  org.apache.cassandra.db.partitions.AtomicBTreePartition
 264:           161          15456  org.apache.cassandra.schema.TableParams
 265:           170          15440  [Ljava.util.WeakHashMap$Entry;
 266:           384          15360  io.netty.buffer.PoolChunkList
 267:           382          15280  org.apache.cassandra.repair.RemoteSyncTask
 268:           941          15056  org.apache.cassandra.utils.MerkleTrees$TokenRangeComparator
 269:           622          14928  java.util.Collections$1
 270:           622          14928  org.apache.cassandra.db.RowIndexEntry$Serializer
 271:           930          14880  java.util.concurrent.locks.ReentrantLock
 272:           464          14848  org.apache.cassandra.cql3.ColumnIdentifier
 273:           925          14800  java.util.HashSet
 274:           264          14784  java.util.LinkedHashMap
 275:           151          14496  org.apache.cassandra.db.ColumnFamilyStore
 276:           604          14496  org.apache.cassandra.metrics.TableMetrics$TableHistogram
 277:           301          14448  ch.qos.logback.classic.Logger
 278:           355          14200  org.apache.cassandra.metrics.RestorableMeter
 279:           442          14144  org.apache.cassandra.io.util.RandomAccessReader
 280:           430          14056  [Lcom.google.common.collect.ImmutableMapEntry;
 281:           433          13856  com.google.common.collect.MapMakerInternalMap$StrongEntry
 282:           433          13856  com.google.common.collect.MapMakerInternalMap$WeakValueReference
 283:           855          13680  java.nio.channels.spi.AbstractInterruptibleChannel$1
 284:            34          13600  org.apache.cassandra.net.IncomingTcpConnection
 285:           333          13320  com.google.common.collect.RegularImmutableSortedMap
 286:           818          13088  java.lang.ref.ReferenceQueue$Lock
 287:           201          12864  java.net.URL
 288:           803          12848  sun.nio.ch.FileDispatcherImpl
 289:           401          12832  org.apache.cassandra.utils.BloomFilter
 290:           200          12800  java.util.regex.Matcher
 291:           400          12800  org.apache.cassandra.cache.ChunkCache$CachingRebufferer
 292:           400          12800  org.apache.cassandra.io.util.CompressedChunkReader$Mmap
 293:           400          12800  org.apache.cassandra.io.util.MmapRebufferer
 294:           799          12784  org.apache.cassandra.io.util.MmappedRegions$Tidier
 295:           799          12784  org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy
 296:           399          12768  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy
 297:           797          12752  java.util.Collections$SingletonSet
 298:           396          12672  java.util.UUID
 299:           784          12544  java.util.HashMap$KeySet
 300:           521          12504  java.util.concurrent.ConcurrentLinkedQueue
 301:           154          12320  org.apache.cassandra.db.rows.RowAndDeletionMergeIterator
 302:           170          12240  java.lang.reflect.Field
 303:           507          12168  org.apache.cassandra.db.BufferDecoratedKey
 304:           151          12080  org.apache.cassandra.db.Memtable
 305:           302          12080  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategyOptions
 306:           376          12032  java.lang.invoke.LambdaForm$Name
 307:           213          11928  sun.security.ssl.CipherSuite
 308:            27          11880  org.apache.cassandra.net.OutboundTcpConnection
 309:           738          11808  java.util.HashMap$Values
 310:           208          11648  java.lang.Package
 311:           242          11616  org.apache.cassandra.utils.IntervalTree$IntervalNode
 312:           128          11264  [Lio.netty.buffer.PoolSubpage;
 313:           699          11184  java.util.HashMap$EntrySet
 314:           155          11160  org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater
 315:           344          11008  java.util.concurrent.ConcurrentSkipListMap$HeadIndex
 316:           341          10912  sun.misc.FDBigInteger
 317:           227          10896  sun.security.x509.X500Name
 318:           453          10872  org.apache.cassandra.utils.DefaultValue
 319:           333          10656  com.google.common.collect.RegularImmutableSortedSet
 320:           265          10600  java.util.Formatter$FormatSpecifier
 321:           263          10520  [Ljava.util.Formatter$Flags;
 322:           433          10392  org.apache.cassandra.cql3.ColumnIdentifier$InternedKey
 323:            72          10368  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong
 324:           324          10368  sun.security.x509.AlgorithmId
 325:           320          10240  io.netty.util.internal.chmv8.LongAdderV8
 326:           633          10128  java.util.concurrent.atomic.AtomicReferenceArray
 327:           180          10080  java.lang.invoke.MethodTypeForm
 328:           156           9984  io.netty.util.Recycler$Stack
 329:           416           9984  java.lang.ThreadLocal$ThreadLocalMap
 330:           622           9952  org.apache.cassandra.dht.Range$1
 331:           154           9856  org.apache.cassandra.cql3.UpdateParameters
 332:           244           9760  java.util.HashMap$KeyIterator
 333:           304           9728  java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
 334:           302           9664  org.apache.cassandra.metrics.TableMetrics$TableMetricNameFactory
 335:           302           9664  org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator
 336:           400           9600  [Lorg.apache.cassandra.io.util.Memory;
 337:           400           9600  org.apache.cassandra.utils.StreamingHistogram
 338:           399           9576  [Ljava.lang.AutoCloseable;
 339:            25           9400  java.lang.Thread
 340:           195           9360  org.apache.cassandra.net.MessageOut
 341:           292           9344  [Lcom.codahale.metrics.Timer;
 342:            16           9216  io.netty.util.internal.shaded.org.jctools.queues.MpscChunkedArrayQueue
 343:           381           9144  org.apache.cassandra.repair.RepairResult
 344:           362           8688  com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair
 345:            68           8680  [Ljava.util.Hashtable$Entry;
 346:           271           8672  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxMeter
 347:           108           8640  sun.security.x509.X509CertImpl
 348:           269           8608  javax.management.MBeanAttributeInfo
 349:           215           8600  com.google.common.collect.RegularImmutableMap
 350:           215           8600  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator
 351:           151           8456  org.apache.cassandra.db.compaction.CompactionStrategyManager
 352:           260           8320  javax.management.MBeanParameterInfo
 353:           142           7952  java.beans.MethodDescriptor
 354:           331           7944  java.util.Collections$SingletonList
 355:           494           7904  com.google.common.base.Present
 356:           164           7872  java.util.WeakHashMap
 357:           227           7768  [Lsun.security.x509.RDN;
 358:           483           7728  org.apache.cassandra.utils.CounterId
 359:           318           7632  java.util.Collections$SetFromMap
 360:           318           7632  java.util.Formatter$FixedString
 361:           156           7488  org.apache.cassandra.utils.concurrent.OpOrder$Group
 362:           187           7480  com.google.common.util.concurrent.ListenableFutureTask
 363:           308           7392  org.apache.cassandra.utils.btree.BTreeSet
 364:           306           7344  java.beans.MethodRef
 365:           304           7296  org.apache.cassandra.io.util.MmappedRegions$Region
 366:           302           7248  org.apache.cassandra.utils.TopKSampler
 367:           151           7248  org.apache.cassandra.utils.memory.SlabAllocator
 368:           148           7104  java.lang.invoke.LambdaForm
 369:           292           7008  org.apache.cassandra.metrics.TableMetrics$TableTimer
 370:           155           6904  [Ljava.lang.invoke.LambdaForm$Name;
 371:           121           6776  jdk.internal.org.objectweb.asm.Item
 372:           169           6760  java.security.AccessControlContext
 373:           280           6720  java.util.Date
 374:           168           6720  java.util.IdentityHashMap
 375:           209           6688  org.apache.cassandra.db.ClusteringComparator
 376:           278           6672  com.google.common.collect.ImmutableSortedAsList
 377:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet
 378:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet$1
 379:           404           6464  java.util.concurrent.CopyOnWriteArraySet
 380:           200           6400  java.util.Formatter
 381:           400           6400  org.apache.cassandra.io.sstable.format.SSTableReader$UniqueIdentifier
 382:           399           6384  org.apache.cassandra.utils.obs.OffHeapBitSet
 383:            23           6368  [[S
 384:           394           6304  org.apache.cassandra.db.commitlog.IntervalSet
 385:           262           6288  java.util.concurrent.CopyOnWriteArrayList$COWIterator
 386:           156           6240  org.apache.cassandra.cql3.QueryOptions$DefaultQueryOptions
 387:           111           6216  sun.security.util.MemoryCache$SoftCacheEntry
 388:           155           6200  javax.management.MBeanOperationInfo
 389:           155           6200  org.apache.cassandra.db.Mutation
 390:           155           6200  org.apache.cassandra.db.partitions.PartitionUpdate
 391:           155           6200  org.apache.cassandra.utils.memory.AbstractAllocator$CloningBTreeRowBuilder
 392:           193           6176  org.apache.cassandra.net.OutboundTcpConnection$QueuedMessage
 393:           200           6160  [Ljava.util.Formatter$FormatString;
 394:           154           6160  java.util.Collections$SingletonMap
 395:           154           6160  org.apache.cassandra.db.rows.BTreeRow$$Lambda$122/418553968
 396:           154           6160  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$125/1196438970
 397:           152           6080  org.apache.cassandra.db.lifecycle.View
 398:           253           6072  java.util.concurrent.ConcurrentSkipListMap$Index
 399:           189           6048  org.apache.cassandra.repair.ValidationTask
 400:           108           6048  sun.security.x509.X509CertInfo
 401:           251           6024  javax.management.ImmutableDescriptor
 402:            62           5952  java.util.jar.JarFile$JarFileEntry
 403:            82           5904  java.beans.PropertyDescriptor
 404:           244           5856  org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$111/177399658
 405:           243           5832  org.apache.cassandra.cql3.functions.FunctionName
 406:            52           5824  sun.nio.ch.SocketChannelImpl
 407:            90           5760  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$99/328488350
 408:           240           5736  [Lorg.apache.cassandra.db.marshal.AbstractType;
 409:           179           5728  org.apache.cassandra.auth.DataResource
 410:            89           5696  org.apache.cassandra.utils.btree.NodeBuilder
 411:           355           5680  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1
 412:           229           5496  org.apache.cassandra.db.MutableDeletionInfo
 413:           227           5448  java.security.Provider$ServiceKey
 414:           224           5376  com.google.common.collect.SingletonImmutableSet
 415:            74           5328  ch.qos.logback.classic.spi.LoggingEvent
 416:            95           5320  java.security.Provider$Service
 417:           165           5280  java.lang.invoke.BoundMethodHandle$Species_L
 418:           106           5272  [Ljavax.management.MBeanAttributeInfo;
 419:           109           5232  java.util.concurrent.ThreadPoolExecutor$Worker
 420:           325           5200  org.apache.cassandra.utils.concurrent.WaitQueue
 421:           108           5184  javax.management.MBeanInfo
 422:           210           5040  com.google.common.collect.RegularImmutableAsList
 423:           210           5040  com.google.common.collect.RegularImmutableMap$EntrySet
 424:           208           4992  java.util.concurrent.ConcurrentHashMap$KeySetView
 425:           155           4960  org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation
 426:           154           4928  [Lcom.google.common.collect.MapMakerInternalMap$Segment;
 427:           308           4928  org.apache.cassandra.db.Columns$$Lambda$121/617875913
 428:           154           4928  org.apache.cassandra.db.rows.EncodingStats$Collector
 429:           154           4928  org.apache.cassandra.io.util.DataOutputBufferFixed
 430:           102           4896  java.util.TimSort
 431:           152           4864  org.apache.cassandra.db.lifecycle.Tracker
 432:           202           4848  org.apache.cassandra.db.lifecycle.SSTableIntervalTree
 433:           121           4840  java.io.ObjectStreamField
 434:           151           4832  org.apache.cassandra.db.compaction.CompactionLogger
 435:            99           4752  javax.management.Notification
 436:           198           4752  org.apache.cassandra.db.ClusteringBound
 437:           198           4752  org.apache.cassandra.db.rows.ComplexColumnData$Builder
 438:           180           4744  [Ljava.security.ProtectionDomain;
 439:            63           4536  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterator
 440:            40           4480  java.net.SocksSocketImpl
 441:           275           4400  java.util.Formatter$Flags
 442:           273           4368  java.lang.Byte
 443:            32           4352  io.netty.buffer.PoolArena$DirectArena
 444:            32           4352  io.netty.buffer.PoolArena$HeapArena
 445:           181           4344  java.lang.invoke.LambdaForm$NamedFunction
 446:             6           4320  [Ljdk.internal.org.objectweb.asm.Item;
 447:            90           4320  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$313/480779282
 448:           108           4320  org.apache.cassandra.db.CachedHashDecoratedKey
 449:           178           4272  org.apache.cassandra.gms.GossipDigestAck
 450:           177           4248  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
 451:           131           4192  com.sun.jmx.mbeanserver.ConvertingMethod
 452:           128           4096  java.lang.NoSuchMethodException
 453:           256           4096  java.lang.Short
 454:            70           3920  sun.misc.URLClassPath$JarLoader
 455:            60           3840  java.util.jar.JarFile
 456:            80           3840  java.util.logging.LogManager$LoggerWeakRef
 457:           160           3840  org.apache.cassandra.db.Serializers
 458:           160           3840  org.apache.cassandra.db.Serializers$NewFormatSerializer
 459:           160           3840  org.apache.cassandra.io.sstable.IndexInfo$Serializer
 460:           160           3840  org.apache.cassandra.schema.Indexes
 461:            53           3816  java.util.regex.Pattern
 462:            95           3800  sun.security.rsa.RSAPublicKeyImpl
 463:           158           3792  com.sun.jmx.mbeanserver.PerInterface$MethodAndSig
 464:            59           3776  java.text.DateFormatSymbols
 465:           155           3720  org.apache.cassandra.utils.memory.ContextAllocator
 466:           154           3696  [Lorg.apache.cassandra.db.Directories$DataDirectory;
 467:           154           3696  com.google.common.collect.Collections2$TransformedCollection
 468:           154           3696  org.apache.cassandra.cql3.statements.UpdatesCollector
 469:           154           3696  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter
 470:           154           3696  org.apache.cassandra.db.rows.Rows$$Lambda$120/877468788
 471:           151           3624  [Ljava.io.File;
 472:           151           3624  org.apache.cassandra.db.Directories
 473:           151           3624  org.apache.cassandra.db.Memtable$ColumnsCollector
 474:           151           3624  org.apache.cassandra.index.SecondaryIndexManager
 475:           151           3624  org.apache.cassandra.metrics.TableMetrics$10
 476:           151           3624  org.apache.cassandra.metrics.TableMetrics$11
 477:           151           3624  org.apache.cassandra.metrics.TableMetrics$12
 478:           151           3624  org.apache.cassandra.metrics.TableMetrics$14
 479:           151           3624  org.apache.cassandra.metrics.TableMetrics$15
 480:           151           3624  org.apache.cassandra.metrics.TableMetrics$16
 481:           151           3624  org.apache.cassandra.metrics.TableMetrics$17
 482:           151           3624  org.apache.cassandra.metrics.TableMetrics$19
 483:           151           3624  org.apache.cassandra.metrics.TableMetrics$2
 484:           151           3624  org.apache.cassandra.metrics.TableMetrics$21
 485:           151           3624  org.apache.cassandra.metrics.TableMetrics$23
 486:           151           3624  org.apache.cassandra.metrics.TableMetrics$24
 487:           151           3624  org.apache.cassandra.metrics.TableMetrics$25
 488:           151           3624  org.apache.cassandra.metrics.TableMetrics$27
 489:           151           3624  org.apache.cassandra.metrics.TableMetrics$29
 490:           151           3624  org.apache.cassandra.metrics.TableMetrics$3
 491:           151           3624  org.apache.cassandra.metrics.TableMetrics$30
 492:           151           3624  org.apache.cassandra.metrics.TableMetrics$31
 493:           151           3624  org.apache.cassandra.metrics.TableMetrics$32
 494:           151           3624  org.apache.cassandra.metrics.TableMetrics$33
 495:           151           3624  org.apache.cassandra.metrics.TableMetrics$34
 496:           151           3624  org.apache.cassandra.metrics.TableMetrics$4
 497:           151           3624  org.apache.cassandra.metrics.TableMetrics$5
 498:           151           3624  org.apache.cassandra.metrics.TableMetrics$6
 499:           151           3624  org.apache.cassandra.metrics.TableMetrics$7
 500:           151           3624  org.apache.cassandra.metrics.TableMetrics$8
 501:           151           3624  org.apache.cassandra.metrics.TableMetrics$9
 502:           113           3616  [Lorg.apache.cassandra.utils.memory.BufferPool$Chunk;
 503:           113           3616  org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef
 504:           225           3600  org.apache.cassandra.cql3.FieldIdentifier
 505:           149           3576  org.apache.cassandra.cql3.restrictions.RestrictionSet
 506:           221           3536  java.util.zip.CRC32
 507:            63           3528  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController
 508:            63           3528  org.apache.cassandra.repair.Validator
 509:            12           3480  [Ljava.util.concurrent.RunnableScheduledFuture;
 510:           108           3456  java.util.Collections$SynchronizedMap
 511:           143           3432  com.google.common.util.concurrent.Futures$CombinedFuture$2
 512:           143           3432  java.util.LinkedList$Node
 513:           107           3424  java.io.IOException
 514:            37           3384  [Lorg.apache.cassandra.io.sstable.IndexInfo;
 515:            60           3360  org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
 516:           122           3344  [Ljavax.management.MBeanParameterInfo;
 517:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$31/1914108708
 518:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$32/1889757798
 519:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$33/1166106620
 520:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$34/221861886
 521:            41           3328  [Ljava.lang.invoke.MethodHandle;
 522:            32           3328  java.io.ObjectStreamClass
 523:           208           3328  org.apache.cassandra.utils.concurrent.Refs
 524:            69           3312  com.google.common.util.concurrent.Futures$CombinedFuture
 525:           103           3296  org.apache.cassandra.schema.CompactionParams
 526:           137           3288  java.util.ArrayDeque
 527:            24           3264  com.codahale.metrics.Striped64$Cell
 528:           203           3248  org.apache.cassandra.io.util.DataOutputBuffer$GrowingChannel
 529:           135           3240  com.sun.jmx.remote.internal.ArrayNotificationBuffer$NamedNotification
 530:           101           3232  java.util.Vector
 531:           101           3232  org.apache.cassandra.schema.SpeculativeRetryParam
 532:           132           3168  org.apache.cassandra.db.view.TableViews
 533:            79           3160  com.google.common.collect.SingletonImmutableBiMap
 534:            98           3136  org.xml.sax.helpers.LocatorImpl
 535:            98           3136  sun.security.x509.BasicConstraintsExtension
 536:            78           3120  java.security.ProtectionDomain
 537:           129           3096  com.google.common.collect.RegularImmutableMap$NonTerminalMapEntry
 538:            77           3080  sun.nio.cs.UTF_8$Decoder
 539:            64           3072  org.apache.cassandra.db.compaction.CompactionIterator$Purger
 540:            64           3072  org.apache.cassandra.db.transform.UnfilteredPartitions
 541:            96           3072  sun.security.x509.SubjectKeyIdentifierExtension
 542:            24           3032  [Ljava.beans.MethodDescriptor;
 543:            92           3024  [Ljavax.management.MBeanOperationInfo;
 544:            94           3008  java.util.AbstractList$Itr
 545:            91           2912  com.codahale.metrics.Timer$Context
 546:           121           2904  org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate
 547:            60           2880  java.util.zip.Inflater
 548:            45           2880  javax.management.openmbean.OpenMBeanAttributeInfoSupport
 549:           118           2832  java.util.regex.Pattern$1
 550:           118           2832  sun.reflect.generics.tree.SimpleClassTypeSignature
 551:            88           2816  sun.security.x509.KeyUsageExtension
 552:           175           2800  org.apache.cassandra.gms.GossipDigestAck2
 553:           113           2712  org.apache.cassandra.utils.memory.BufferPool$LocalPool
 554:            37           2664  java.util.logging.Logger
 555:           111           2664  sun.security.util.Cache$EqualByteArray
 556:            55           2640  java.util.Hashtable
 557:           163           2608  java.util.IdentityHashMap$KeySet
 558:           162           2592  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable
 559:           108           2592  org.apache.cassandra.dht.LocalPartitioner$LocalToken
 560:            18           2592  sun.reflect.MethodAccessorGenerator
 561:           108           2592  sun.security.util.BitArray
 562:           108           2592  sun.security.x509.CertificateValidity
 563:           138           2584  [Lcom.sun.jmx.mbeanserver.MXBeanMapping;
 564:           107           2568  java.net.InetSocketAddress$InetSocketAddressHolder
 565:            64           2560  com.google.common.collect.Multimaps$UnmodifiableMultimap
 566:            64           2560  java.util.ArrayList$SubList
 567:            64           2560  java.util.ArrayList$SubList$1
 568:            64           2560  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1
 569:           160           2560  org.apache.cassandra.schema.Triggers
 570:            64           2560  org.apache.cassandra.utils.OverlapIterator
 571:            53           2544  java.util.concurrent.LinkedBlockingQueue
 572:           155           2480  org.apache.cassandra.utils.btree.UpdateFunction$Simple
 573:           155           2480  org.apache.cassandra.utils.concurrent.OpOrder
 574:            44           2464  java.lang.Class$ReflectionData
 575:           154           2464  java.util.concurrent.ConcurrentSkipListSet
 576:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$117/1004624941
 577:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$119/1364111969
 578:           154           2464  org.apache.cassandra.utils.WrappedBoolean
 579:           102           2448  org.apache.cassandra.schema.CachingParams
 580:            76           2432  java.security.CodeSource
 581:           151           2416  org.apache.cassandra.db.Memtable$StatsCollector
 582:           151           2416  org.apache.cassandra.utils.memory.EnsureOnHeap$NoOp
 583:            75           2400  java.util.LinkedList
 584:            50           2400  org.apache.cassandra.cql3.restrictions.StatementRestrictions
 585:            99           2376  sun.security.x509.CertificateExtensions
 586:            74           2368  java.io.ObjectStreamClass$WeakClassKey
 587:            98           2352  java.lang.Class$AnnotationData
 588:           147           2352  java.util.concurrent.ConcurrentHashMap$ValuesView
 589:            98           2352  java.util.jar.Attributes$Name
 590:            73           2336  java.util.regex.Pattern$Curly
 591:            97           2328  com.google.common.collect.ImmutableMapKeySet
 592:            48           2304  com.google.common.collect.HashMultimap
 593:            96           2304  com.google.common.collect.ImmutableMapKeySet$1
 594:            16           2304  io.netty.channel.epoll.EpollEventLoop
 595:           144           2304  org.apache.cassandra.db.ColumnFamilyStore$3
 596:            96           2304  org.apache.cassandra.metrics.KeyspaceMetrics$17
 597:            72           2304  sun.reflect.ClassFileAssembler
 598:            70           2240  java.util.concurrent.ConcurrentHashMap$ReservationNode
 599:            70           2240  java.util.logging.LogManager$LogNode
 600:            70           2240  org.apache.cassandra.utils.MerkleTree$TreeRangeIterator
 601:            91           2200  [Lcom.github.benmanes.caffeine.cache.RemovalCause;
 602:            91           2184  com.github.benmanes.caffeine.SingleConsumerQueue$Node
 603:            39           2184  org.apache.cassandra.db.marshal.UserType
 604:            90           2160  [Lcom.github.benmanes.caffeine.cache.Node;
 605:           118           2160  [Lsun.reflect.generics.tree.TypeArgument;
 606:            90           2160  com.github.benmanes.caffeine.cache.BoundedLocalCache$AddTask
 607:            90           2160  java.lang.StringBuffer
 608:            67           2144  java.util.TreeMap$ValueIterator
 609:            89           2136  java.lang.RuntimePermission
 610:            89           2136  org.apache.cassandra.io.compress.CompressionMetadata$Chunk
 611:            53           2120  sun.security.ec.NamedCurve
 612:            66           2112  java.io.FilePermission
 613:            66           2112  java.util.zip.ZipCoder
 614:            52           2080  sun.nio.ch.SocketAdaptor
 615:            37           2072  javax.management.MBeanServerNotification
 616:            37           2072  org.apache.cassandra.db.RowIndexEntry$IndexedEntry
 617:            86           2064  javax.management.openmbean.TabularDataSupport
 618:           129           2064  sun.security.x509.KeyIdentifier
 619:            64           2048  com.google.common.util.concurrent.Futures$ChainingListenableFuture
 620:           128           2048  java.lang.Character
 621:            64           2048  org.apache.cassandra.db.partitions.PurgeFunction$$Lambda$104/2021147872
 622:            64           2048  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2
 623:            64           2048  sun.misc.FloatingDecimal$ASCIIToBinaryBuffer
 624:            84           2016  java.security.Provider$UString
 625:            18           2016  java.util.GregorianCalendar
 626:            62           1984  org.apache.cassandra.utils.MerkleTrees$TreeRangeIterator
 627:            27           1944  sun.reflect.DelegatingClassLoader
 628:           120           1920  com.codahale.metrics.Striped64$HashCode
 629:            80           1920  java.util.regex.Pattern$GroupTail
 630:            34           1904  org.apache.cassandra.cql3.statements.SelectStatement
 631:            79           1896  com.google.common.collect.ImmutableList$1
 632:            79           1896  java.util.regex.Pattern$GroupHead
 633:            59           1888  java.util.RegularEnumSet
 634:           118           1888  sun.reflect.generics.tree.ClassTypeSignature
 635:           118           1888  sun.security.x509.SerialNumber
 636:            13           1872  java.text.DecimalFormat
 637:            39           1872  sun.util.locale.LocaleObjectCache$CacheEntry
 638:            10           1832  [[B
 639:            57           1824  org.apache.cassandra.cql3.functions.CastFcts$JavaFunctionWrapper
 640:            75           1800  java.util.regex.Pattern$Single
 641:            56           1792  java.lang.Throwable
 642:             8           1792  jdk.internal.org.objectweb.asm.MethodWriter
 643:            74           1776  com.google.common.util.concurrent.Futures$6
 644:           111           1776  java.util.LinkedHashMap$LinkedValues
 645:            44           1760  java.io.ObjectStreamClass$FieldReflectorKey
 646:            36           1728  org.apache.cassandra.concurrent.SEPWorker
 647:            72           1728  sun.reflect.ByteVectorImpl
 648:           108           1728  sun.security.x509.CertificateAlgorithmId
 649:           108           1728  sun.security.x509.CertificateSerialNumber
 650:           108           1728  sun.security.x509.CertificateVersion
 651:           108           1728  sun.security.x509.CertificateX509Key
 652:            18           1728  sun.util.calendar.Gregorian$Date
 653:           107           1712  java.net.InetSocketAddress
 654:             4           1696  [Ljava.lang.Thread;
 655:            53           1696  java.security.spec.EllipticCurve
 656:            30           1688  [Ljava.lang.reflect.Method;
 657:             6           1680  java.util.concurrent.ConcurrentHashMap$CounterCell
 658:            52           1664  java.lang.invoke.DirectMethodHandle$Special
 659:            52           1664  sun.nio.ch.SocketAdaptor$SocketInputStream
 660:            68           1632  org.apache.cassandra.cql3.Constants$Marker
 661:            68           1632  sun.reflect.NativeConstructorAccessorImpl
 662:           101           1616  org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/673586830
 663:            40           1600  ch.qos.logback.core.joran.event.StartEvent
 664:            40           1600  com.sun.jmx.mbeanserver.PerInterface
 665:            40           1600  sun.management.DiagnosticCommandArgumentInfo
 666:            99           1584  org.apache.cassandra.db.marshal.AbstractType$$Lambda$4/495702238
 667:            49           1568  java.io.DataOutputStream
 668:            49           1568  java.nio.channels.Channels$1
 669:            65           1560  java.security.spec.ECPoint
 670:            39           1560  org.apache.cassandra.io.util.SafeMemory
 671:            65           1560  org.apache.cassandra.utils.btree.TreeBuilder
 672:            64           1536  org.apache.cassandra.db.compaction.CompactionIterator$GarbageSkipper
 673:            63           1512  com.google.common.util.concurrent.Futures$1
 674:            63           1512  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$EQRestriction
 675:            63           1512  org.apache.cassandra.db.compaction.CompactionManager$13
 676:            47           1504  org.apache.cassandra.cql3.statements.ParsedStatement$Prepared
 677:            47           1504  org.apache.cassandra.io.util.DataOutputBuffer$1$1
 678:            93           1488  java.util.Collections$UnmodifiableSet
 679:            61           1464  java.util.regex.Pattern$Slice
 680:            60           1440  java.util.zip.ZStreamRef
 681:            51           1408  [Ljava.io.ObjectStreamField;
 682:            16           1392  [Ljava.lang.Byte;
 683:             1           1376  [Lsun.misc.FDBigInteger;
 684:            43           1376  java.util.regex.Pattern$Branch
 685:            43           1376  org.apache.cassandra.concurrent.NamedThreadFactory
 686:            34           1360  ch.qos.logback.core.status.InfoStatus
 687:            17           1360  java.net.URI
 688:            34           1360  org.apache.cassandra.cql3.selection.Selection$SimpleSelection
 689:            61           1352  [Ljava.lang.reflect.Type;
 690:            24           1344  java.util.ResourceBundle$CacheKey
 691:            24           1344  javax.management.openmbean.CompositeType
 692:            72           1336  [Ljavax.management.openmbean.CompositeData;
 693:            33           1320  sun.security.x509.AuthorityKeyIdentifierExtension
 694:            79           1312  [Ljava.security.Principal;
 695:            54           1296  ch.qos.logback.classic.spi.StackTraceElementProxy
 696:            23           1288  java.net.SocketPermission
 697:            39           1280  [Ljava.math.BigInteger;
 698:            40           1280  ch.qos.logback.core.joran.event.EndEvent
 699:            16           1280  com.google.common.cache.LocalCache$Segment
 700:            20           1280  org.apache.cassandra.db.RowIndexEntry$ShallowIndexedEntry
 701:            43           1272  [Ljava.util.regex.Pattern$Node;
 702:            53           1272  sun.nio.ch.Util$BufferCache
 703:            79           1264  java.security.ProtectionDomain$Key
 704:            39           1248  java.lang.Thread$WeakClassKey
 705:            38           1240  [Ljava.lang.reflect.Field;
 706:            14           1232  org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
 707:            38           1216  java.security.Permissions
 708:            50           1200  org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions
 709:            50           1200  org.apache.cassandra.cql3.restrictions.IndexRestrictions
 710:            25           1200  org.apache.cassandra.metrics.ClientRequestMetrics
 711:             2           1184  [Lcom.github.benmanes.caffeine.cache.NodeFactory;
 712:            37           1184  java.net.Socket
 713:            49           1176  org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet
 714:            21           1176  sun.util.calendar.ZoneInfo
 715:            52           1168  [Lorg.apache.cassandra.cql3.ColumnSpecification;
 716:            24           1152  java.beans.BeanDescriptor
 717:            24           1152  java.lang.management.MemoryUsage
 718:            72           1152  org.apache.cassandra.db.ColumnFamilyStore$1
 719:            36           1152  org.apache.cassandra.io.util.SafeMemory$MemoryTidy
 720:            24           1152  org.hyperic.sigar.FileSystem
 721:            36           1152  sun.reflect.generics.repository.ClassRepository
 722:            20           1120  javax.management.openmbean.ArrayType
 723:            35           1120  org.apache.cassandra.cql3.ResultSet$ResultMetadata
 724:            69           1104  com.google.common.util.concurrent.Futures$8
 725:            69           1104  com.google.common.util.concurrent.Futures$CombinedFuture$1
 726:            46           1104  org.apache.cassandra.metrics.DefaultNameFactory
 727:            69           1104  sun.reflect.DelegatingConstructorAccessorImpl
 728:             3           1080  [Ljava.lang.Integer;
 729:            27           1080  com.google.common.collect.HashBiMap$BiEntry
 730:            27           1080  org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy
 731:            45           1080  sun.reflect.generics.factory.CoreReflectionFactory
 732:            24           1064  [Ljava.beans.PropertyDescriptor;
 733:             2           1056  [Ljava.lang.Long;
 734:             2           1056  [Ljava.lang.Short;
 735:            26           1040  java.math.BigDecimal
 736:            43           1032  io.netty.channel.ChannelOption
 737:            43           1032  java.io.ExpiringCache$Entry
 738:            64           1024  org.apache.cassandra.db.compaction.AbstractCompactionStrategy$ScannerList
 739:            64           1024  org.apache.cassandra.db.compaction.CompactionIterator$1
 740:            64           1024  org.apache.cassandra.repair.RepairJob$3
 741:            63           1008  org.apache.cassandra.repair.RepairJob$2
 742:            12            960  [Lcom.google.common.collect.HashBiMap$BiEntry;
 743:            24            960  java.beans.GenericBeanInfo
 744:            30            960  java.security.Provider$EngineDescription
 745:            40            960  java.util.regex.Pattern$BitClass
 746:            20            960  org.antlr.runtime.CommonToken
 747:            30            960  org.apache.cassandra.cql3.ColumnSpecification
 748:            40            960  org.apache.cassandra.cql3.statements.SelectStatement$Parameters
 749:            60            960  org.cliffc.high_scale_lib.Counter
 750:            20            960  org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
 751:            40            960  org.codehaus.jackson.map.type.ClassKey
 752:            40            960  org.xml.sax.helpers.AttributesImpl
 753:            46            944  [Lsun.reflect.generics.tree.FormalTypeParameter;
 754:            39            936  java.util.regex.Pattern$5
 755:             8            928  [Lorg.apache.cassandra.db.ClusteringBound;
 756:            29            928  java.security.BasicPermissionCollection
 757:            29            928  org.apache.cassandra.io.util.DataInputPlus$DataInputStreamPlus
 758:            23            920  org.codehaus.jackson.map.type.SimpleType
 759:            19            912  sun.management.DiagnosticCommandInfo
 760:            28            896  java.io.DataInputStream
 761:            18            864  net.jpountz.lz4.LZ4BlockOutputStream
 762:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$26/843299092
 763:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$27/605982374
 764:            54            864  org.apache.cassandra.config.ColumnDefinition$1
 765:            18            864  org.apache.cassandra.utils.SlidingTimeRate
 766:            36            864  sun.reflect.Label$PatchInfo
 767:            27            864  sun.reflect.generics.reflectiveObjects.TypeVariableImpl
 768:            36            864  sun.reflect.generics.tree.ClassSignature
 769:            44            856  [Ljavax.management.MBeanConstructorInfo;
 770:            21            840  com.sun.jmx.mbeanserver.MXBeanSupport
 771:            35            840  net.jpountz.xxhash.StreamingXXHash32JNI
 772:            35            840  sun.reflect.generics.scope.ClassScope
 773:            21            840  sun.util.locale.BaseLocale$Key
 774:             2            832  [Lorg.antlr.runtime.BitSet;
 775:            13            832  com.google.common.util.concurrent.SmoothRateLimiter$SmoothBursty
 776:            13            832  java.text.DecimalFormatSymbols
 777:            38            824  [Lsun.reflect.generics.tree.FieldTypeSignature;
 778:            34            816  org.apache.cassandra.cql3.selection.SelectionColumnMapping
 779:             6            816  org.apache.cassandra.metrics.KeyspaceMetrics
 780:            25            800  java.util.PropertyPermission
 781:            20            800  org.cliffc.high_scale_lib.NonBlockingHashMap
 782:            14            784  java.util.HashMap$TreeNode
 783:            14            784  org.apache.cassandra.cql3.statements.UpdateStatement
 784:            32            768  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$IdentityMapping
 785:            32            768  io.netty.channel.unix.FileDescriptor
 786:            16            768  java.util.ResourceBundle$BundleReference
 787:            24            768  java.util.ResourceBundle$LoaderReference
 788:            16            768  net.jpountz.lz4.LZ4BlockInputStream
 789:            32            768  org.apache.cassandra.cql3.functions.CastFcts$CastAsTextFunction
 790:            32            768  sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl
 791:            24            768  sun.security.x509.OIDMap$OIDInfo
 792:            23            736  javax.management.MBeanConstructorInfo
 793:            23            736  sun.management.MappedMXBeanType$BasicMXBeanType
 794:            30            720  com.google.common.collect.ImmutableEntry
 795:            30            720  java.io.ObjectStreamClass$EntryFuture
 796:            15            720  java.lang.management.PlatformComponent
 797:             9            720  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor
 798:             9            720  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
 799:             1            720  org.apache.cassandra.config.Config
 800:            18            720  org.apache.cassandra.metrics.ThreadPoolMetrics
 801:            22            704  com.sun.jmx.mbeanserver.WeakIdentityHashMap$IdentityWeakReference
 802:            11            704  java.text.SimpleDateFormat
 803:            29            696  org.apache.cassandra.net.MessagingService$Verb
 804:            36            688  [Lsun.reflect.generics.tree.ClassTypeSignature;
 805:            43            688  java.util.regex.Pattern$BranchConn
 806:            17            680  sun.reflect.UnsafeQualifiedStaticLongFieldAccessorImpl
 807:            29            672  [Ljava.lang.reflect.TypeVariable;
 808:            28            672  ch.qos.logback.core.spi.ContextAwareBase
 809:            28            672  java.util.regex.Pattern$Ctype
 810:            28            672  java.util.regex.Pattern$Start
 811:             4            672  jdk.internal.org.objectweb.asm.ClassWriter
 812:            42            672  org.apache.cassandra.config.ColumnDefinition$Raw$Literal
 813:            42            672  org.apache.cassandra.io.sstable.format.big.BigTableScanner$EmptySSTableScanner
 814:            28            672  sun.nio.ch.SocketOptionRegistry$RegistryKey
 815:            12            672  sun.security.ssl.CipherSuite$BulkCipher
 816:            41            656  ch.qos.logback.core.joran.spi.ElementPath
 817:            27            648  java.io.FilePermissionCollection
 818:            27            648  org.apache.cassandra.cql3.selection.RawSelector
 819:            27            648  sun.reflect.generics.tree.FormalTypeParameter
 820:            16            640  io.netty.util.collection.IntObjectHashMap
 821:             8            640  java.util.concurrent.ThreadPoolExecutor
 822:            40            640  java.util.jar.Attributes
 823:             8            640  java.util.zip.ZipEntry
 824:            10            640  jdk.internal.org.objectweb.asm.Label
 825:            20            640  org.apache.cassandra.cql3.functions.BytesConversionFcts$2
 826:            20            640  org.apache.cassandra.db.compaction.OperationType
 827:             3            624  [Ljava.lang.invoke.LambdaForm;
 828:            13            624  java.nio.HeapCharBuffer
 829:            26            624  java.security.spec.ECFieldF2m
 830:            26            624  java.util.regex.Pattern$Ques
 831:            39            624  org.apache.cassandra.serializers.TupleSerializer
 832:            39            624  org.apache.cassandra.serializers.UserTypeSerializer
 833:            27            616  [Ljava.lang.reflect.Constructor;
 834:            19            608  java.io.FileInputStream
 835:            19            608  java.rmi.server.UID
 836:            19            608  java.util.Locale
 837:            19            608  org.apache.cassandra.schema.IndexMetadata
 838:            19            608  sun.management.DiagnosticCommandImpl$Wrapper
 839:            19            608  sun.util.locale.BaseLocale
 840:            15            600  java.lang.ClassNotFoundException
 841:            25            600  java.lang.invoke.Invokers
 842:            25            600  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter
 843:            25            600  org.apache.cassandra.gms.ApplicationState
 844:            25            600  sun.reflect.NativeMethodAccessorImpl
 845:            25            600  sun.reflect.annotation.AnnotationInvocationHandler
 846:            18            576  ch.qos.logback.core.joran.event.BodyEvent
 847:            12            576  java.io.ObjectInputStream$FilterValues
 848:            24            576  jdk.internal.org.objectweb.asm.ByteVector
 849:            12            576  org.apache.cassandra.db.marshal.MapType
 850:             9            576  org.apache.cassandra.metrics.ConnectionMetrics
 851:            24            576  org.apache.cassandra.metrics.ThreadPoolMetricNameFactory
 852:            35            560  ch.qos.logback.core.joran.spi.ElementSelector
 853:            14            560  io.netty.util.Recycler$WeakOrderQueue
 854:            10            560  java.util.zip.ZipFile$ZipFileInflaterInputStream
 855:            10            560  java.util.zip.ZipFile$ZipFileInputStream
 856:            14            560  javax.management.openmbean.SimpleType
 857:            10            560  sun.invoke.util.Wrapper
 858:            23            552  [Ljava.net.InetAddress;
 859:             3            552  [Lorg.apache.cassandra.net.MessagingService$Verb;
 860:            23            552  ch.qos.logback.core.pattern.LiteralConverter
 861:            23            552  io.netty.util.internal.logging.Slf4JLogger
 862:            23            552  org.codehaus.jackson.map.SerializationConfig$Feature
 863:             2            544  [Ljava.lang.Character;
 864:            17            544  io.netty.util.concurrent.DefaultPromise
 865:            34            544  java.io.FilePermission$1
 866:            17            544  java.nio.channels.ClosedChannelException
 867:            17            544  java.util.concurrent.atomic.AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl
 868:            34            544  net.jpountz.xxhash.StreamingXXHash32$1
 869:            17            544  org.apache.cassandra.transport.Message$Type
 870:            17            544  sun.reflect.MethodAccessorGenerator$1
 871:            17            544  sun.security.x509.DistributionPoint
 872:            17            544  sun.security.x509.URIName
 873:            22            528  java.net.URLClassLoader$1
 874:            22            528  org.apache.cassandra.cql3.CQL3Type$Native
 875:            33            528  sun.reflect.DelegatingMethodAccessorImpl
 876:            13            520  com.google.common.base.Stopwatch
 877:            13            520  io.netty.channel.unix.Errors$NativeIoException
 878:            13            520  java.lang.invoke.MethodHandleImpl$IntrinsicMethodHandle
 879:            13            520  java.text.DigitList
 880:             4            512  com.google.common.cache.LocalCache
 881:            16            512  io.netty.channel.epoll.IovArray
 882:            16            512  java.lang.NoSuchFieldException
 883:            32            512  java.util.TreeSet
 884:            16            512  java.util.concurrent.Semaphore$NonfairSync
 885:            16            512  sun.security.ssl.CipherSuite$KeyExchange
 886:            21            504  java.util.Locale$LocaleKey
 887:             9            504  java.util.concurrent.ConcurrentHashMap$ValueIterator
 888:            21            504  org.apache.cassandra.cql3.functions.AggregateFcts$24
 889:             9            504  org.apache.cassandra.net.RateBasedBackPressureState
 890:            21            504  sun.security.x509.AVAKeyword
 891:            31            496  sun.security.x509.GeneralName
 892:            19            488  [Lsun.management.DiagnosticCommandArgumentInfo;
 893:            20            480  java.io.ObjectStreamClass$2
 894:            12            480  java.lang.UNIXProcess$ProcessPipeInputStream
 895:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$22
 896:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$23
 897:            20            480  org.apache.cassandra.cql3.functions.BytesConversionFcts$1
 898:            20            480  org.apache.cassandra.dht.LocalPartitioner
 899:            15            480  org.apache.cassandra.index.internal.composites.RegularColumnIndex
 900:             6            480  org.apache.cassandra.repair.RepairSession
 901:            20            480  org.yaml.snakeyaml.tokens.Token$ID
 902:             6            480  sun.net.www.protocol.jar.URLJarFile
 903:            30            480  sun.security.x509.GeneralNames
 904:             6            456  [Lsun.invoke.util.Wrapper;
 905:            19            456  ch.qos.logback.classic.spi.ClassPackagingData
 906:            19            456  java.lang.Class$1
 907:            19            456  java.util.regex.Pattern$Dollar
 908:             5            448  [[Ljava.lang.Object;
 909:             7            448  java.security.SecureRandom
 910:            28            448  java.util.LinkedHashSet
 911:             8            448  javax.management.openmbean.OpenMBeanParameterInfoSupport
 912:             8            448  jdk.internal.org.objectweb.asm.AnnotationWriter
 913:            14            448  jdk.internal.org.objectweb.asm.Type
 914:            14            448  sun.security.x509.CRLDistributionPointsExtension
 915:            11            440  java.lang.ClassLoader$NativeLibrary
 916:            11            440  sun.security.ec.ECPublicKeyImpl
 917:             9            432  com.sun.jna.Function
 918:            27            432  java.security.spec.ECFieldFp
 919:            18            432  java.text.DateFormat$Field
 920:            18            432  java.util.Collections$UnmodifiableCollection$1
 921:            18            432  org.apache.cassandra.exceptions.ExceptionCode
 922:            18            432  org.apache.cassandra.io.util.WrappedDataOutputStreamPlus
 923:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$1
 924:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$2
 925:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$3
 926:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$4
 927:             9            432  org.apache.cassandra.net.OutboundTcpConnectionPool
 928:            18            432  org.cliffc.high_scale_lib.NonBlockingHashMap$NBHMEntry
 929:            13            416  io.netty.util.Recycler$WeakOrderQueue$Link
 930:            13            416  java.lang.invoke.SimpleMethodHandle
 931:            13            416  java.security.AlgorithmParameters
 932:            13            416  java.util.Stack
 933:             4            416  sun.net.www.protocol.file.FileURLConnection
 934:            17            408  org.apache.cassandra.utils.IntegerInterval
 935:            17            408  org.codehaus.jackson.map.DeserializationConfig$Feature
 936:            10            400  java.io.ObjectStreamClass$FieldReflector
 937:            10            400  java.lang.invoke.DirectMethodHandle$Accessor
 938:            10            400  javax.crypto.CryptoPermission
 939:            10            400  sun.reflect.generics.repository.MethodRepository
 940:             7            392  java.util.Calendar$Builder
 941:             1            392  org.apache.cassandra.utils.memory.MemtableCleanerThread
 942:             8            384  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong;
 943:             1            384  ch.qos.logback.core.AsyncAppenderBase$Worker
 944:             4            384  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap
 945:            16            384  io.netty.channel.epoll.EpollEventArray
 946:            12            384  java.io.EOFException
 947:             1            384  java.lang.ref.Finalizer$FinalizerThread
 948:             8            384  java.net.SocketInputStream
 949:             8            384  java.net.SocketOutputStream
 950:            12            384  java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue
 951:            12            384  java.util.concurrent.atomic.AtomicLongFieldUpdater$CASUpdater
 952:             1            384  java.util.logging.LogManager$Cleaner
 953:            16            384  javax.management.StandardMBean
 954:            16            384  org.apache.cassandra.cql3.Attributes
 955:            16            384  org.apache.cassandra.cql3.Constants$Setter
 956:            16            384  org.apache.cassandra.cql3.Operations
 957:            12            384  org.apache.cassandra.cql3.SingleColumnRelation
 958:             8            384  org.apache.cassandra.hints.HintsStore
 959:            16            384  org.apache.cassandra.metrics.TableMetrics$35
 960:             1            384  org.apache.cassandra.net.MessagingService$SocketThread
 961:            16            384  org.apache.cassandra.schema.TableParams$Option
 962:             1            384  org.apache.cassandra.thrift.ThriftServer$ThriftServerThread
 963:            16            384  sun.misc.MetaIndex
 964:            16            384  sun.nio.ch.OptionKey
 965:             3            384  sun.nio.fs.UnixFileAttributes
 966:            12            384  sun.nio.fs.UnixPath
 967:             1            376  java.lang.ref.Reference$ReferenceHandler
 968:            16            368  [Ljava.security.cert.Certificate;
 969:            17            368  [Ljavax.management.MBeanNotificationInfo;
 970:            23            368  java.lang.ThreadLocal
 971:             3            360  [Lorg.apache.cassandra.gms.ApplicationState;
 972:            15            360  com.sun.jmx.remote.util.ClassLogger
 973:             9            360  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit
 974:             9            360  java.io.BufferedInputStream
 975:            15            360  java.io.ObjectStreamClass$ClassDataSlot
 976:            15            360  java.net.InetAddress
 977:             9            360  org.apache.cassandra.db.marshal.SetType
 978:            15            360  org.apache.cassandra.utils.memory.SlabAllocator$Region
 979:            11            352  java.lang.ClassLoader$1
 980:            11            352  java.util.concurrent.SynchronousQueue
 981:            11            352  org.apache.cassandra.db.ConsistencyLevel
 982:             4            352  sun.rmi.transport.ConnectionInputStream
 983:             7            336  [Ljavax.management.openmbean.OpenType;
 984:             7            336  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CompositeMapping
 985:            14            336  java.lang.invoke.LambdaFormEditor$Transform$Kind
 986:             6            336  java.nio.DirectLongBufferU
 987:            21            336  java.util.Collections$UnmodifiableCollection
 988:             7            336  java.util.Properties
 989:             6            336  org.apache.cassandra.concurrent.SEPExecutor
 990:             6            336  sun.management.MemoryPoolImpl
 991:             5            328  [Ljava.io.ObjectInputStream$HandleTable$HandleList;
 992:            16            328  [Ljava.lang.management.PlatformComponent;
 993:             4            320  [Lio.netty.buffer.PoolArena;
 994:            10            320  [Ljava.lang.invoke.LambdaForm$BasicType;
 995:            10            320  java.io.FileOutputStream
 996:             8            320  java.io.ObjectOutputStream$HandleTable
 997:            10            320  java.lang.OutOfMemoryError
 998:            10            320  java.lang.StringCoding$StringEncoder
 999:            10            320  java.lang.reflect.WeakCache$CacheValue
1000:            10            320  java.security.cert.PolicyQualifierInfo
1001:             8            320  org.apache.cassandra.db.marshal.ListType
1002:            20            320  org.apache.cassandra.dht.LocalPartitioner$1
1003:             8            320  org.apache.cassandra.gms.ArrayBackedBoundedStats
1004:             8            320  org.apache.cassandra.gms.ArrivalWindow
1005:            10            320  sun.reflect.generics.tree.MethodTypeSignature
1006:             8            320  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler
1007:            10            320  sun.security.util.DisabledAlgorithmConstraints$KeySizeConstraint
1008:            13            312  [Ljava.net.InetSocketAddress;
1009:            13            312  com.sun.jna.Pointer
1010:            13            312  java.lang.management.ManagementPermission
1011:            19            304  sun.reflect.BootstrapConstructorAccessorImpl
1012:             1            296  com.github.benmanes.caffeine.SingleConsumerQueue
1013:             1            296  com.github.benmanes.caffeine.cache.BoundedBuffer$RingBuffer
1014:             4            288  [Lch.qos.logback.classic.spi.StackTraceElementProxy;
1015:            12            288  [Lcom.codahale.metrics.Striped64$Cell;
1016:            12            288  ch.qos.logback.core.joran.spi.HostClassAndPropertyDouble
1017:             1            288  com.github.benmanes.caffeine.cache.LocalCacheFactory$SSLiMW
1018:             6            288  com.google.common.collect.HashBiMap
1019:             9            288  com.google.common.collect.RegularImmutableSet
1020:             4            288  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8
1021:            12            288  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ListenerWrapper
1022:             6            288  java.io.BufferedReader
1023:            12            288  java.lang.ProcessEnvironment$Variable
1024:             9            288  java.lang.reflect.Proxy$Key1
1025:             9            288  java.util.concurrent.CountDownLatch$Sync
1026:             9            288  java.util.concurrent.SynchronousQueue$TransferStack$SNode
1027:             9            288  java.util.logging.Level
1028:            18            288  java.util.regex.Pattern$Begin
1029:            12            288  org.apache.cassandra.concurrent.Stage
1030:            18            288  org.apache.cassandra.io.util.DataOutputStreamPlus$2
1031:             4            288  org.apache.cassandra.locator.TokenMetadata
1032:             9            288  org.apache.commons.lang3.JavaVersion
1033:             6            288  sun.nio.cs.StreamDecoder
1034:            18            288  sun.reflect.Label
1035:             4            288  sun.rmi.transport.ConnectionOutputStream
1036:             9            288  sun.security.jca.ProviderConfig
1037:             7            280  java.net.SocketTimeoutException
1038:             7            280  org.apache.cassandra.streaming.messages.StreamMessage$Type
1039:             7            280  org.apache.thrift.transport.TTransportException
1040:             7            280  sun.misc.FloatingDecimal$BinaryToASCIIBuffer
1041:             7            280  sun.rmi.transport.tcp.TCPEndpoint
1042:             1            272  [Lorg.codehaus.jackson.sym.Name;
1043:            17            272  com.sun.proxy.$Proxy3
1044:            17            272  net.jpountz.lz4.LZ4HCJNICompressor
1045:            17            272  org.apache.cassandra.cql3.Constants$Value
1046:            17            272  sun.reflect.ClassDefiner$1
1047:            17            272  sun.security.x509.DNSName
1048:             3            264  [[D
1049:            11            264  com.google.common.collect.ImmutableMapValues
1050:            11            264  java.net.StandardSocketOptions$StdSocketOption
1051:            11            264  java.rmi.server.ObjID
1052:            11            264  java.util.regex.Pattern$SliceI
1053:            11            264  org.apache.cassandra.io.sstable.Component
1054:            11            264  org.apache.cassandra.io.sstable.Component$Type
1055:            11            264  org.apache.cassandra.metrics.DroppedMessageMetrics
1056:            11            264  org.apache.cassandra.metrics.TableMetrics$36
1057:            11            264  org.apache.cassandra.net.MessagingService$DroppedMessages
1058:            11            264  sun.rmi.transport.ObjectEndpoint
1059:            11            264  sun.security.util.DisabledAlgorithmConstraints$DisabledConstraint
1060:            10            256  [Ljava.io.ObjectStreamClass$ClassDataSlot;
1061:             8            256  com.google.common.cache.LocalCache$StrongEntry
1062:            16            256  io.netty.channel.epoll.EpollEventLoop$1
1063:            16            256  io.netty.channel.epoll.EpollEventLoop$2
1064:            16            256  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator
1065:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$2
1066:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$DefaultThreadProperties
1067:             8            256  java.util.Collections$UnmodifiableMap
1068:            16            256  java.util.concurrent.Semaphore
1069:             8            256  javax.management.MBeanNotificationInfo
1070:             8            256  org.apache.cassandra.cql3.functions.CastFcts$JavaCounterFunctionWrapper
1071:             8            256  org.apache.cassandra.db.ClusteringPrefix$Kind
1072:             8            256  org.apache.cassandra.repair.messages.RepairMessage$Type
1073:             8            256  sun.management.NotificationEmitterSupport$ListenerInfo
1074:             8            256  sun.misc.ProxyGenerator$PrimitiveTypeInfo
1075:             8            256  sun.misc.URLClassPath$JarLoader$2
1076:             8            256  sun.security.x509.CertificatePoliciesExtension
1077:             6            240  [Ljava.lang.invoke.BoundMethodHandle$SpeciesData;
1078:            10            240  com.sun.org.apache.xerces.internal.impl.XMLScanner$NameType
1079:            10            240  java.io.BufferedOutputStream
1080:             6            240  java.lang.UNIXProcess
1081:            10            240  java.nio.file.StandardOpenOption
1082:            10            240  java.security.CryptoPrimitive
1083:             3            240  java.util.concurrent.ScheduledThreadPoolExecutor
1084:            15            240  java.util.regex.Pattern$Dot
1085:            10            240  org.apache.cassandra.auth.Permission
1086:             5            240  org.apache.cassandra.config.ViewDefinition
1087:             5            240  org.apache.cassandra.db.lifecycle.LogRecord
1088:             5            240  org.apache.cassandra.db.view.View
1089:             6            240  org.apache.cassandra.metrics.SEPMetrics
1090:             6            240  org.apache.cassandra.schema.KeyspaceMetadata
1091:            10            240  org.codehaus.jackson.JsonParser$Feature
1092:            10            240  org.yaml.snakeyaml.events.Event$ID
1093:            15            240  org.yaml.snakeyaml.nodes.Tag
1094:             6            240  sun.management.MemoryPoolImpl$CollectionSensor
1095:             6            240  sun.management.MemoryPoolImpl$PoolSensor
1096:             5            240  sun.misc.URLClassPath
1097:            10            240  sun.reflect.generics.scope.MethodScope
1098:            15            240  sun.reflect.generics.tree.TypeVariableSignature
1099:            10            240  sun.rmi.runtime.Log$LoggerLog
1100:            10            240  sun.security.x509.Extension
1101:             5            240  sun.util.locale.provider.LocaleResources$ResourceReference
1102:             8            232  [Ljava.lang.Boolean;
1103:             2            224  [Lorg.codehaus.jackson.map.SerializationConfig$Feature;
1104:             7            224  [Lsun.nio.fs.NativeBuffer;
1105:             7            224  com.google.common.util.concurrent.MoreExecutors$DirectExecutorService
1106:             4            224  java.io.ObjectInputStream$BlockDataInputStream
1107:            14            224  java.rmi.server.Operation
1108:             7            224  java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
1109:             7            224  java.util.regex.Pattern$BnM
1110:             7            224  org.codehaus.jackson.JsonGenerator$Feature
1111:             4            224  org.codehaus.jackson.map.introspect.AnnotatedClass
1112:             4            224  org.codehaus.jackson.map.introspect.BasicBeanDescription
1113:             7            224  sun.nio.fs.NativeBuffer
1114:             7            224  sun.reflect.annotation.AnnotationType
1115:             4            224  sun.rmi.transport.Target
1116:             7            224  sun.security.x509.NetscapeCertTypeExtension
1117:             9            216  java.lang.ProcessEnvironment$Value
1118:             9            216  java.util.Collections$SynchronizedSet
1119:             9            216  java.util.logging.Level$KnownLevel
1120:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$1
1121:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$2
1122:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$3
1123:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$4
1124:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$5
1125:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$6
1126:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$7
1127:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$8
1128:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$9
1129:             3            216  sun.security.provider.NativePRNG$RandomIO
1130:             9            216  sun.util.logging.PlatformLogger$Level
1131:             7            208  [Ljava.lang.invoke.LambdaForm$NamedFunction;
1132:             2            208  [Lorg.apache.cassandra.cql3.CQL3Type$Native;
1133:            13            208  com.google.common.util.concurrent.RateLimiter$SleepingStopwatch$1
1134:             2            208  java.lang.invoke.InnerClassLambdaMetafactory
1135:            13            208  sun.nio.ch.SocketAdaptor$2
1136:             2            200  [Ljava.text.DateFormat$Field;
1137:             5            200  io.netty.channel.group.DefaultChannelGroup
1138:             5            200  java.lang.invoke.BoundMethodHandle$SpeciesData
1139:             5            200  java.lang.invoke.DirectMethodHandle$Constructor
1140:             5            200  java.util.stream.StreamOpFlag
1141:             5            200  org.apache.cassandra.cql3.statements.SelectStatement$RawStatement
1142:             5            200  org.apache.cassandra.db.view.ViewBuilder
1143:             5            200  sun.rmi.transport.WeakRef
1144:             6            192  [Ljava.rmi.server.Operation;
1145:             3            192  [Lorg.apache.cassandra.db.ConsistencyLevel;
1146:             4            192  [[Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
1147:             3            192  ch.qos.logback.classic.PatternLayout
1148:             6            192  ch.qos.logback.core.util.CachingDateFormatter
1149:             8            192  com.google.common.cache.LocalCache$AccessQueue$1
1150:             4            192  com.google.common.collect.TreeMultimap
1151:             6            192  java.lang.ProcessBuilder
1152:             6            192  java.lang.invoke.LambdaForm$BasicType
1153:             8            192  java.lang.invoke.MethodHandleImpl$Intrinsic
1154:             8            192  java.math.RoundingMode
1155:            12            192  java.util.concurrent.ConcurrentSkipListMap$EntrySet
1156:             4            192  java.util.concurrent.locks.ReentrantReadWriteLock$FairSync
1157:             8            192  java.util.regex.Pattern$7
1158:             8            192  javax.crypto.CryptoPermissionCollection
1159:             4            192  javax.management.openmbean.TabularType
1160:             3            192  jdk.internal.org.objectweb.asm.FieldWriter
1161:             4            192  jdk.internal.org.objectweb.asm.Frame
1162:             8            192  jdk.net.SocketFlow$Status
1163:             6            192  org.apache.cassandra.db.Keyspace
1164:             4            192  org.apache.cassandra.db.RangeTombstoneList
1165:             8            192  org.apache.cassandra.db.WriteType
1166:             8            192  org.apache.cassandra.serializers.MapSerializer
1167:             8            192  org.apache.cassandra.serializers.SetSerializer
1168:             8            192  org.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State
1169:             8            192  org.apache.cassandra.service.StorageService$Mode
1170:             3            192  org.apache.cassandra.utils.MerkleTree$TreeDifference
1171:             6            192  org.apache.commons.lang3.text.StrBuilder
1172:             8            192  org.yaml.snakeyaml.scanner.Constant
1173:            12            192  sun.nio.ch.SocketAdaptor$1
1174:             6            192  sun.rmi.runtime.NewThreadAction
1175:             2            192  sun.security.provider.Sun
1176:             6            192  sun.security.util.MemoryCache
1177:             8            192  sun.security.x509.PolicyInformation
1178:             2            176  [Lorg.apache.cassandra.transport.Message$Type;
1179:             2            176  [Lorg.codehaus.jackson.map.DeserializationConfig$Feature;
1180:            10            176  [Lsun.reflect.generics.tree.TypeSignature;
1181:            11            176  java.text.NumberFormat$Field
1182:            11            176  java.util.LinkedHashMap$LinkedEntrySet
1183:            11            176  java.util.concurrent.SynchronousQueue$TransferStack
1184:             2            176  javax.management.remote.rmi.NoCallStackClassLoader
1185:             2            176  org.apache.cassandra.db.commitlog.MemoryMappedSegment
1186:            11            176  sun.security.ec.ECParameters
1187:             1            168  [[Ljava.math.BigInteger;
1188:             7            168  ch.qos.logback.classic.Level
1189:             3            168  ch.qos.logback.classic.encoder.PatternLayoutEncoder
1190:             7            168  com.google.common.collect.ImmutableEnumSet
1191:             7            168  com.sun.management.VMOption$Origin
1192:             7            168  com.sun.org.apache.xerces.internal.util.FeatureState
1193:             7            168  java.lang.invoke.MethodHandles$Lookup
1194:             7            168  java.net.NetPermission
1195:             7            168  java.util.BitSet
1196:             3            168  javax.management.openmbean.OpenMBeanOperationInfoSupport
1197:             7            168  javax.security.auth.AuthPermission
1198:             7            168  org.apache.cassandra.cql3.Constants$Type
1199:             7            168  org.apache.cassandra.db.Directories$FileAction
1200:             7            168  org.apache.cassandra.utils.concurrent.SimpleCondition
1201:             7            168  org.apache.cassandra.utils.progress.ProgressEventType
1202:             7            168  org.codehaus.jackson.annotate.JsonMethod
1203:             7            168  sun.nio.fs.NativeBuffer$Deallocator
1204:             7            168  sun.rmi.server.LoaderHandler$LoaderKey
1205:             3            168  sun.rmi.transport.tcp.TCPChannel
1206:             3            168  sun.rmi.transport.tcp.TCPConnection
1207:             3            168  sun.security.provider.SHA
1208:             7            168  sun.security.x509.NetscapeCertTypeExtension$MapEntry
1209:             4            160  [F
1210:             2            160  ch.qos.logback.core.rolling.RollingFileAppender
1211:            10            160  io.netty.util.internal.ConcurrentSet
1212:             4            160  java.io.ObjectOutputStream$BlockDataOutputStream
1213:             5            160  java.io.SerializablePermission
1214:             5            160  java.lang.StringCoding$StringDecoder
1215:             5            160  javax.management.StandardEmitterMBean
1216:             5            160  org.apache.cassandra.db.marshal.CompositeType
1217:             5            160  org.apache.cassandra.repair.RepairRunnable$1
1218:             5            160  org.apache.cassandra.transport.ProtocolVersion
1219:             5            160  org.apache.cassandra.transport.messages.ResultMessage$Kind
1220:             5            160  org.apache.cassandra.utils.CassandraVersion
1221:             4            160  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotV
1222:             5            160  sun.rmi.transport.StreamRemoteCall
1223:             5            160  sun.security.ssl.CipherSuite$MacAlg
1224:            10            160  sun.security.x509.CertificatePolicyId
1225:             5            160  sun.util.locale.provider.LocaleProviderAdapter$Type
1226:             6            144  [Ljava.io.Closeable;
1227:             2            144  [Ljava.math.BigDecimal;
1228:             1            144  [Ljava.util.concurrent.ForkJoinTask$ExceptionNode;
1229:             1            144  [Lorg.codehaus.jackson.sym.CharsToNameCanonicalizer$Bucket;
1230:             3            144  ch.qos.logback.classic.pattern.DateConverter
1231:             3            144  ch.qos.logback.classic.pattern.ExtendedThrowableProxyConverter
1232:             3            144  ch.qos.logback.classic.spi.ThrowableProxy
1233:             6            144  com.google.common.collect.AbstractMultimap$EntrySet
1234:             6            144  com.sun.org.apache.xerces.internal.util.Status
1235:             6            144  java.io.InputStreamReader
1236:             3            144  java.lang.ThreadGroup
1237:             6            144  java.lang.UNIXProcess$$Lambda$15/1221027335
1238:             6            144  java.lang.UNIXProcess$ProcessPipeOutputStream
1239:             9            144  java.util.concurrent.CountDownLatch
1240:             6            144  java.util.regex.Pattern$CharProperty$1
1241:             2            144  org.antlr.runtime.RecognizerSharedState
1242:             6            144  org.apache.cassandra.cql3.CFName
1243:             6            144  org.apache.cassandra.cql3.WhereClause
1244:             6            144  org.apache.cassandra.db.filter.DataLimits$Kind
1245:             6            144  org.apache.cassandra.db.view.ViewManager
1246:             3            144  org.apache.cassandra.locator.SimpleStrategy
1247:             3            144  org.apache.cassandra.metrics.CacheMetrics
1248:             6            144  org.apache.cassandra.metrics.SEPMetrics$1
1249:             6            144  org.apache.cassandra.metrics.SEPMetrics$2
1250:             6            144  org.apache.cassandra.metrics.SEPMetrics$3
1251:             6            144  org.apache.cassandra.metrics.SEPMetrics$4
1252:             6            144  org.apache.cassandra.schema.KeyspaceParams
1253:             6            144  org.apache.cassandra.schema.ReplicationParams
1254:             6            144  org.apache.cassandra.service.ActiveRepairService$1
1255:             6            144  org.apache.cassandra.service.ActiveRepairService$2
1256:             6            144  org.apache.cassandra.streaming.StreamSession$State
1257:             6            144  org.codehaus.jackson.annotate.JsonAutoDetect$Visibility
1258:             6            144  org.github.jamm.MemoryMeter$Guess
1259:             6            144  sun.misc.PerfCounter
1260:             6            144  sun.security.ssl.ProtocolVersion
1261:             6            144  sun.security.util.DisabledAlgorithmConstraints$Constraint$Operator
1262:             4            128  [Lcom.google.common.cache.LocalCache$Segment;
1263:             4            128  [Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
1264:             2            128  [Lorg.apache.cassandra.concurrent.Stage;
1265:             2            128  [Lorg.apache.cassandra.io.sstable.Component$Type;
1266:             2            128  ch.qos.logback.core.rolling.FixedWindowRollingPolicy
1267:             4            128  ch.qos.logback.core.rolling.helper.FileNamePattern
1268:             8            128  com.google.common.cache.LocalCache$AccessQueue
1269:             8            128  com.google.common.cache.LocalCache$StrongValueReference
1270:             4            128  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$ArrayMapping
1271:             2            128  java.io.ExpiringCache$1
1272:             4            128  java.io.ObjectInputStream$HandleTable
1273:             4            128  java.io.ObjectInputStream$PeekInputStream
1274:             4            128  java.lang.UNIXProcess$Platform
1275:             2            128  java.lang.invoke.InvokerBytecodeGenerator
1276:             4            128  java.util.Random
1277:             4            128  java.util.concurrent.ExecutionException
1278:             4            128  net.jpountz.util.Native$OS
1279:             4            128  org.apache.cassandra.cql3.functions.CastFcts$CassandraFunctionWrapper
1280:             4            128  org.apache.cassandra.db.marshal.ReversedType
1281:             1            128  org.apache.cassandra.io.compress.CompressedSequentialWriter
1282:             1            128  org.apache.cassandra.io.sstable.format.big.BigTableWriter
1283:             4            128  org.apache.cassandra.io.util.SequentialWriterOption
1284:             4            128  org.apache.cassandra.locator.PendingRangeMaps
1285:             2            128  org.apache.cassandra.metrics.CASClientRequestMetrics
1286:             8            128  org.apache.cassandra.serializers.MapSerializer$$Lambda$24/2072313080
1287:             8            128  sun.net.www.ParseUtil
1288:             4            128  sun.rmi.transport.LiveRef
1289:             8            128  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$292/1509453068
1290:             4            128  sun.security.ssl.CipherSuite$PRF
1291:             4            128  sun.security.x509.ExtendedKeyUsageExtension
1292:             3            120  [Lorg.codehaus.jackson.annotate.JsonMethod;
1293:             1            120  [[Ljava.lang.String;
1294:             5            120  ch.qos.logback.core.pattern.parser.TokenStream$TokenizerState
1295:             5            120  ch.qos.logback.core.subst.Token$Type
1296:             5            120  ch.qos.logback.core.util.AggregationType
1297:             3            120  com.google.common.collect.AbstractMapBasedMultimap$AsMap
1298:             5            120  com.sun.org.apache.xerces.internal.util.PropertyState
1299:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State
1300:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State
1301:             3            120  java.lang.invoke.BoundMethodHandle$Species_LL
1302:             3            120  java.lang.invoke.MethodHandleImpl$AsVarargsCollector
1303:             5            120  java.util.stream.StreamOpFlag$Type
1304:             3            120  org.apache.cassandra.cache.AutoSavingCache
1305:             5            120  org.apache.cassandra.config.Config$DiskFailurePolicy
1306:             5            120  org.apache.cassandra.cql3.VariableSpecifications
1307:             5            120  org.apache.cassandra.cql3.statements.IndexTarget$Type
1308:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Status
1309:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Type
1310:             3            120  org.apache.cassandra.db.lifecycle.LogTransaction$SSTableTidier
1311:             3            120  org.apache.cassandra.index.internal.composites.ClusteringColumnIndex
1312:             5            120  org.apache.cassandra.schema.CompactionParams$Option
1313:             1            120  org.apache.cassandra.service.StorageService
1314:             5            120  org.apache.cassandra.utils.NativeLibrary$OSType
1315:             5            120  org.yaml.snakeyaml.DumperOptions$ScalarStyle
1316:             5            120  sun.misc.FloatingDecimal$PreparedASCIIToBinaryBuffer
1317:             5            120  sun.security.jca.ServiceId
1318:             5            120  sun.security.util.DisabledAlgorithmConstraints
1319:             2            112  [Ljava.lang.invoke.MethodType;
1320:             2            112  [Ljava.security.CryptoPrimitive;
1321:             2            112  [Ljava.util.List;
1322:             2            112  [Lorg.apache.cassandra.auth.Permission;
1323:             2            112  [Lorg.apache.cassandra.db.PartitionPosition;
1324:             3            112  [Lorg.apache.cassandra.transport.ProtocolVersion;
1325:             7            112  com.google.common.util.concurrent.MoreExecutors$ListeningDecorator
1326:             2            112  com.sun.management.GcInfo
1327:             2            112  io.netty.buffer.PooledByteBufAllocator
1328:             7            112  java.util.concurrent.ConcurrentHashMap$EntrySetView
1329:             2            112  org.apache.cassandra.cql3.statements.DeleteStatement
1330:             2            112  org.apache.cassandra.db.compaction.LeveledCompactionStrategy
1331:             2            112  org.apache.cassandra.repair.LocalSyncTask
1332:             7            112  org.apache.cassandra.serializers.ListSerializer
1333:             2            112  org.apache.cassandra.utils.memory.MemtablePool$SubPool
1334:             7            112  sun.security.provider.NativePRNG
1335:             1            104  com.codahale.metrics.ThreadLocalRandom
1336:             1            104  io.netty.channel.epoll.EpollServerSocketChannel
1337:             1            104  org.apache.cassandra.db.ColumnIndex
1338:             1            104  sun.rmi.server.LoaderHandler$Loader
1339:             2             96  [Lcom.google.common.cache.LocalCache$EntryFactory;
1340:             6             96  [Ljava.io.ObjectStreamClass$MemberSignature;
1341:             2             96  [Ljava.util.concurrent.TimeUnit;
1342:             1             96  [Lorg.apache.cassandra.db.compaction.OperationType;
1343:             2             96  [Lorg.apache.cassandra.repair.messages.RepairMessage$Type;
1344:             1             96  [Lorg.yaml.snakeyaml.tokens.Token$ID;
1345:             1             96  [[J
1346:             1             96  ch.qos.logback.classic.LoggerContext
1347:             3             96  ch.qos.logback.classic.pattern.FileOfCallerConverter
1348:             3             96  ch.qos.logback.classic.pattern.LevelConverter
1349:             3             96  ch.qos.logback.classic.pattern.LineOfCallerConverter
1350:             3             96  ch.qos.logback.classic.pattern.LineSeparatorConverter
1351:             3             96  ch.qos.logback.classic.pattern.MessageConverter
1352:             3             96  ch.qos.logback.classic.pattern.ThreadConverter
1353:             3             96  ch.qos.logback.core.joran.action.AppenderRefAction
1354:             4             96  ch.qos.logback.core.pattern.parser.Token
1355:             2             96  ch.qos.logback.core.recovery.ResilientFileOutputStream
1356:             2             96  ch.qos.logback.core.rolling.helper.DateTokenConverter
1357:             4             96  ch.qos.logback.core.subst.Token
1358:             2             96  ch.qos.logback.core.util.InvocationGate
1359:             4             96  com.google.common.cache.LocalCache$WriteQueue$1
1360:             4             96  com.google.common.collect.AbstractIterator$State
1361:             4             96  com.google.common.collect.Iterators$12
1362:             4             96  com.googlecode.concurrentlinkedhashmap.LinkedDeque
1363:             3             96  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$EnumMapping
1364:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$MBeanInfoMap
1365:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$PerInterfaceMap
1366:             1             96  com.sun.net.ssl.internal.ssl.Provider
1367:             3             96  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap
1368:             3             96  io.netty.buffer.EmptyByteBuf
1369:             3             96  java.io.ByteArrayInputStream
1370:             6             96  java.io.FileInputStream$1
1371:             4             96  java.io.ObjectOutputStream$ReplaceTable
1372:             6             96  java.lang.UNIXProcess$$Lambda$16/1801942731
1373:             6             96  java.net.Socket$2
1374:             6             96  java.net.Socket$3
1375:             4             96  java.net.URLClassLoader$2
1376:             4             96  java.nio.file.FileVisitResult
1377:             4             96  java.text.Normalizer$Form
1378:             6             96  java.util.LinkedHashMap$LinkedKeySet
1379:             2             96  java.util.concurrent.ArrayBlockingQueue
1380:             3             96  java.util.concurrent.ConcurrentHashMap$ForwardingNode
1381:             3             96  java.util.concurrent.locks.ReentrantLock$FairSync
1382:             4             96  java.util.stream.StreamShape
1383:             4             96  javax.management.NotificationBroadcasterSupport$ListenerInfo
1384:             4             96  org.apache.cassandra.auth.IRoleManager$Option
1385:             4             96  org.apache.cassandra.config.CFMetaData$Flag
1386:             4             96  org.apache.cassandra.config.ColumnDefinition$Kind
1387:             4             96  org.apache.cassandra.config.Config$CommitFailurePolicy
1388:             4             96  org.apache.cassandra.config.Config$DiskAccessMode
1389:             4             96  org.apache.cassandra.config.Config$MemtableAllocationType
1390:             4             96  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption
1391:             1             96  org.apache.cassandra.cql3.Cql_Parser
1392:             4             96  org.apache.cassandra.db.SystemKeyspace$BootstrapState
1393:             2             96  org.apache.cassandra.db.compaction.LeveledManifest
1394:             4             96  org.apache.cassandra.db.context.CounterContext$Relationship
1395:             4             96  org.apache.cassandra.db.lifecycle.LogTransaction$Obsoletion
1396:             4             96  org.apache.cassandra.dht.Bounds
1397:             4             96  org.apache.cassandra.hints.HintsDispatcher$Callback$Outcome
1398:             4             96  org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys
1399:             4             96  org.apache.cassandra.io.sstable.format.SSTableReader$OpenReason
1400:             4             96  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason
1401:             4             96  org.apache.cassandra.io.sstable.metadata.MetadataType
1402:             2             96  org.apache.cassandra.io.util.FileHandle$Builder
1403:             2             96  org.apache.cassandra.locator.LocalStrategy
1404:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$1
1405:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$10
1406:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$11
1407:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$12
1408:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$13
1409:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$14
1410:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$15
1411:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$16
1412:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$2
1413:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$3
1414:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$4
1415:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$5
1416:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$6
1417:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$7
1418:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$8
1419:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$9
1420:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$KeyspaceMetricNameFactory
1421:             6             96  org.apache.cassandra.schema.Functions
1422:             4             96  org.apache.cassandra.schema.SpeculativeRetryParam$Kind
1423:             6             96  org.apache.cassandra.schema.Tables
1424:             6             96  org.apache.cassandra.schema.Views
1425:             4             96  org.apache.cassandra.transport.Event$Type
1426:             1             96  org.apache.cassandra.triggers.CustomClassLoader
1427:             4             96  org.apache.cassandra.utils.AbstractIterator$State
1428:             4             96  org.apache.cassandra.utils.AsymmetricOrdering$Op
1429:             3             96  org.apache.cassandra.utils.NoSpamLogger
1430:             4             96  org.apache.cassandra.utils.SortedBiMultiValMap
1431:             4             96  org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State
1432:             2             96  org.codehaus.jackson.map.MapperConfig$Base
1433:             4             96  org.yaml.snakeyaml.nodes.NodeId
1434:             2             96  sun.management.GarbageCollectorImpl
1435:             2             96  sun.management.GcInfoBuilder
1436:             4             96  sun.misc.FormattedFloatingDecimal$Form
1437:             1             96  sun.misc.Launcher$AppClassLoader
1438:             4             96  sun.net.www.MessageHeader
1439:             1             96  sun.nio.ch.ServerSocketChannelImpl
1440:             2             96  sun.nio.cs.StreamEncoder
1441:             6             96  sun.rmi.transport.Transport$$Lambda$295/399097450
1442:             3             96  sun.rmi.transport.Transport$1
1443:             1             96  sun.security.ec.SunEC
1444:             1             96  sun.security.jca.ProviderList$1
1445:             1             96  sun.security.rsa.SunRsaSign
1446:             3             96  sun.security.ssl.ProtocolList
1447:             4             88  [Ljava.util.Map$Entry;
1448:             1             88  [Lnet.jpountz.lz4.LZ4Compressor;
1449:             1             88  [Lorg.apache.cassandra.exceptions.ExceptionCode;
1450:             1             88  [Lsun.security.util.ObjectIdentifier;
1451:             1             88  [[Ljava.lang.Byte;
1452:             1             88  java.util.jar.JarVerifier
1453:             1             88  org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor
1454:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CacheCleanupExecutor
1455:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CompactionExecutor
1456:             1             88  org.apache.cassandra.db.compaction.CompactionManager$ValidationExecutor
1457:             1             88  org.apache.cassandra.gms.Gossiper
1458:             1             88  org.apache.cassandra.io.sstable.IndexSummaryBuilder
1459:             1             88  org.apache.cassandra.io.sstable.metadata.MetadataCollector
1460:             1             88  sun.misc.Launcher$ExtClassLoader
1461:             1             80  [Lio.netty.util.concurrent.SingleThreadEventExecutor;
1462:             2             80  [Ljava.lang.management.MemoryUsage;
1463:             2             80  [Ljava.util.stream.StreamOpFlag$Type;
1464:             5             80  [Lorg.apache.cassandra.config.ColumnDefinition;
1465:             2             80  [Lorg.apache.cassandra.config.Config$DiskFailurePolicy;
1466:             1             80  [Lorg.apache.cassandra.cql3.Operator;
1467:             1             80  [Lorg.apache.cassandra.schema.TableParams$Option;
1468:             2             80  [Lorg.apache.cassandra.transport.messages.ResultMessage$Kind;
1469:             2             80  [Lorg.codehaus.jackson.annotate.JsonAutoDetect$Visibility;
1470:             1             80  [Lsun.security.ssl.CipherSuite$KeyExchange;
1471:             1             80  ch.qos.logback.classic.AsyncAppender
1472:             2             80  ch.qos.logback.classic.filter.ThresholdFilter
1473:             1             80  ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter
1474:             2             80  ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy
1475:             2             80  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$TabularMapping
1476:             1             80  com.sun.jmx.remote.util.ClassLoaderWithRepository
1477:             5             80  com.sun.proxy.$Proxy1
1478:             5             80  io.netty.channel.group.DefaultChannelGroup$1
1479:             2             80  io.netty.channel.unix.Errors$NativeConnectException
1480:             2             80  io.netty.util.Signal
1481:             2             80  java.io.ExpiringCache
1482:             2             80  java.util.Locale$Category
1483:             5             80  java.util.logging.SimpleFormatter
1484:             2             80  java.util.regex.Pattern$Loop
1485:             5             80  javax.security.auth.x500.X500Principal
1486:             1             80  org.apache.cassandra.concurrent.StageManager$TracingExecutor
1487:             1             80  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$SMAwareReconfigureOnChangeFilter
1488:             1             80  org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter
1489:             1             80  org.apache.cassandra.io.sstable.SSTableRewriter
1490:             5             80  org.apache.cassandra.repair.RepairSession$1
1491:             2             80  org.codehaus.jackson.sym.CharsToNameCanonicalizer
1492:             2             80  sun.management.MemoryManagerImpl
1493:             2             80  sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl
1494:             1             80  sun.reflect.misc.MethodUtil
1495:             2             80  sun.rmi.server.LoaderHandler$LoaderEntry
1496:             2             80  sun.rmi.server.UnicastServerRef
1497:             2             80  sun.rmi.server.UnicastServerRef2
1498:             2             80  sun.security.provider.DSAPublicKeyImpl
1499:             5             80  sun.security.util.DisabledAlgorithmConstraints$Constraints
1500:             2             80  sun.util.logging.resources.logging
1501:             1             72  [Ljava.lang.invoke.LambdaFormEditor$Transform$Kind;
1502:             4             72  [Ljava.nio.file.LinkOption;
1503:             3             72  [Ljava.util.concurrent.ConcurrentHashMap$CounterCell;
1504:             1             72  [Ljavax.management.openmbean.SimpleType;
1505:             2             72  [Lsun.security.jca.ProviderConfig;
1506:             1             72  ch.qos.logback.core.ConsoleAppender
1507:             3             72  ch.qos.logback.core.joran.action.NOPAction
1508:             3             72  ch.qos.logback.core.joran.action.PropertyAction
1509:             3             72  ch.qos.logback.core.pattern.FormatInfo
1510:             3             72  ch.qos.logback.core.rolling.helper.CompressionMode
1511:             3             72  ch.qos.logback.core.spi.FilterReply
1512:             3             72  ch.qos.logback.core.subst.Tokenizer$TokenizerState
1513:             3             72  com.github.benmanes.caffeine.cache.AccessOrderDeque
1514:             3             72  com.github.benmanes.caffeine.cache.Caffeine$Strength
1515:             3             72  com.google.common.base.CharMatcher$13
1516:             3             72  com.google.common.base.CharMatcher$RangesMatcher
1517:             3             72  com.google.common.collect.AbstractMapBasedMultimap$KeySet
1518:             1             72  io.netty.channel.DefaultChannelHandlerContext
1519:             1             72  io.netty.channel.DefaultChannelPipeline$HeadContext
1520:             1             72  io.netty.channel.DefaultChannelPipeline$TailContext
1521:             1             72  io.netty.channel.epoll.EpollServerSocketChannelConfig
1522:             3             72  java.io.ObjectStreamClass$ExceptionInfo
1523:             3             72  java.lang.UNIXProcess$LaunchMechanism
1524:             3             72  java.lang.annotation.RetentionPolicy
1525:             3             72  java.nio.file.FileTreeWalker$EventType
1526:             3             72  java.rmi.dgc.VMID
1527:             3             72  java.security.SecurityPermission
1528:             3             72  java.util.Base64$Encoder
1529:             1             72  java.util.ResourceBundle$RBClassLoader
1530:             3             72  java.util.concurrent.atomic.AtomicMarkableReference$Pair
1531:             3             72  java.util.jar.Manifest
1532:             1             72  java.util.logging.LogManager$RootLogger
1533:             1             72  java.util.logging.LogRecord
1534:             3             72  java.util.stream.Collector$Characteristics
1535:             3             72  java.util.stream.MatchOps$MatchKind
1536:             3             72  javax.crypto.CryptoPermissions
1537:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader
1538:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper
1539:             3             72  javax.security.auth.Subject$SecureSet
1540:             3             72  org.apache.cassandra.auth.DataResource$Level
1541:             3             72  org.apache.cassandra.config.ColumnDefinition$ClusteringOrder
1542:             3             72  org.apache.cassandra.config.Config$InternodeCompression
1543:             3             72  org.apache.cassandra.config.Config$UserFunctionTimeoutPolicy
1544:             3             72  org.apache.cassandra.config.ReadRepairDecision
1545:             3             72  org.apache.cassandra.cql3.AssignmentTestable$TestResult
1546:             1             72  org.apache.cassandra.cql3.Cql_Lexer
1547:             3             72  org.apache.cassandra.cql3.ResultSet$Flag
1548:             3             72  org.apache.cassandra.db.Conflicts$Resolution
1549:             3             72  org.apache.cassandra.db.Directories$FileType
1550:             3             72  org.apache.cassandra.db.commitlog.CommitLogSegment$CDCState
1551:             1             72  org.apache.cassandra.db.compaction.CompactionIterator
1552:             3             72  org.apache.cassandra.db.lifecycle.SSTableSet
1553:             3             72  org.apache.cassandra.db.marshal.AbstractType$ComparisonType
1554:             3             72  org.apache.cassandra.db.monitoring.MonitoringState
1555:             3             72  org.apache.cassandra.db.rows.SerializationHelper$Flag
1556:             1             72  org.apache.cassandra.io.util.SequentialWriter
1557:             3             72  org.apache.cassandra.locator.TokenMetadata$Topology
1558:             3             72  org.apache.cassandra.metrics.CacheMetrics$1
1559:             3             72  org.apache.cassandra.metrics.CacheMetrics$6
1560:             3             72  org.apache.cassandra.metrics.CacheMetrics$7
1561:             3             72  org.apache.cassandra.metrics.StreamingMetrics
1562:             3             72  org.apache.cassandra.repair.RepairParallelism
1563:             3             72  org.apache.cassandra.repair.SystemDistributedKeyspace$RepairState
1564:             3             72  org.apache.cassandra.repair.messages.ValidationComplete
1565:             3             72  org.apache.cassandra.schema.CompactionParams$TombstoneOption
1566:             3             72  org.apache.cassandra.schema.IndexMetadata$Kind
1567:             3             72  org.apache.cassandra.service.CacheService$CacheType
1568:             3             72  org.apache.cassandra.streaming.StreamEvent$Type
1569:             3             72  org.apache.cassandra.transport.Server$LatestEvent
1570:             3             72  org.apache.cassandra.utils.BiMultiValMap
1571:             3             72  org.apache.cassandra.utils.NoSpamLogger$Level
1572:             3             72  org.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle
1573:             1             72  org.apache.commons.lang3.builder.ToStringStyle$DefaultToStringStyle
1574:             1             72  org.apache.commons.lang3.builder.ToStringStyle$MultiLineToStringStyle
1575:             1             72  org.apache.commons.lang3.builder.ToStringStyle$NoFieldNameToStringStyle
1576:             1             72  org.apache.commons.lang3.builder.ToStringStyle$ShortPrefixToStringStyle
1577:             1             72  org.apache.commons.lang3.builder.ToStringStyle$SimpleToStringStyle
1578:             1             72  org.apache.thrift.server.TThreadPoolServer$Args
1579:             3             72  org.yaml.snakeyaml.DumperOptions$FlowStyle
1580:             3             72  org.yaml.snakeyaml.DumperOptions$LineBreak
1581:             3             72  org.yaml.snakeyaml.introspector.BeanAccess
1582:             3             72  sun.misc.FloatingDecimal$ExceptionalBinaryToASCIIBuffer
1583:             3             72  sun.misc.ObjectInputFilter$Status
1584:             3             72  sun.misc.Signal
1585:             3             72  sun.nio.fs.UnixFileAttributeViews$Basic
1586:             3             72  sun.rmi.transport.SequenceEntry
1587:             3             72  sun.security.provider.NativePRNG$Variant
1588:             3             72  sun.security.ssl.CipherSuite$CipherType
1589:             3             72  sun.security.ssl.CipherSuiteList
1590:             1             72  sun.util.locale.provider.JRELocaleProviderAdapter
1591:             3             72  sun.util.resources.ParallelListResourceBundle$KeySet
1592:             2             64  [Ljava.lang.UNIXProcess$LaunchMechanism;
1593:             2             64  [Ljava.lang.annotation.RetentionPolicy;
1594:             3             64  [Ljava.security.CodeSigner;
1595:             3             64  [Ljava.security.cert.X509Certificate;
1596:             2             64  [Ljava.util.stream.Collector$Characteristics;
1597:             2             64  [Lorg.apache.cassandra.config.CFMetaData$Flag;
1598:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$ClusteringOrder;
1599:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$Kind;
1600:             2             64  [Lorg.apache.cassandra.config.Config$CommitFailurePolicy;
1601:             2             64  [Lorg.apache.cassandra.config.Config$InternodeCompression;
1602:             2             64  [Lorg.apache.cassandra.config.Config$MemtableAllocationType;
1603:             2             64  [Lorg.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption;
1604:             2             64  [Lorg.apache.cassandra.cql3.ResultSet$Flag;
1605:             2             64  [Lorg.apache.cassandra.db.SystemKeyspace$BootstrapState;
1606:             2             64  [Lorg.apache.cassandra.io.sstable.metadata.MetadataType;
1607:             2             64  [Lorg.apache.cassandra.schema.CompactionParams$TombstoneOption;
1608:             2             64  [Lorg.apache.cassandra.schema.IndexMetadata$Kind;
1609:             2             64  [Lorg.apache.cassandra.transport.Event$Type;
1610:             2             64  [Lorg.yaml.snakeyaml.nodes.NodeId;
1611:             2             64  ch.qos.logback.classic.joran.action.LevelAction
1612:             2             64  ch.qos.logback.core.joran.spi.ConsoleTarget
1613:             2             64  ch.qos.logback.core.rolling.helper.Compressor
1614:             2             64  ch.qos.logback.core.rolling.helper.IntegerTokenConverter
1615:             4             64  ch.qos.logback.core.spi.FilterAttachableImpl
1616:             1             64  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
1617:             2             64  com.github.benmanes.caffeine.cache.References$WeakKeyReference
1618:             1             64  com.github.benmanes.caffeine.cache.stats.CacheStats
1619:             1             64  com.google.common.cache.CacheStats
1620:             4             64  com.google.common.cache.LocalCache$WriteQueue
1621:             2             64  com.google.common.util.concurrent.Striped$LargeLazyStriped
1622:             4             64  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher
1623:             2             64  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CollectionMapping
1624:             1             64  com.sun.jmx.remote.internal.ArrayNotificationBuffer
1625:             2             64  com.sun.management.GarbageCollectionNotificationInfo
1626:             2             64  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property
1627:             1             64  io.netty.channel.ChannelOutboundBuffer
1628:             4             64  io.netty.util.concurrent.FastThreadLocal
1629:             4             64  java.io.ObjectInputStream$ValidationList
1630:             2             64  java.io.PrintStream
1631:             2             64  java.lang.ClassValue$Entry
1632:             2             64  java.lang.NoSuchMethodError
1633:             2             64  java.lang.VirtualMachineError
1634:             2             64  java.lang.ref.ReferenceQueue$Null
1635:             2             64  java.net.Inet6Address
1636:             2             64  java.net.Inet6Address$Inet6AddressHolder
1637:             2             64  java.util.ResourceBundle$Control$1
1638:             2             64  java.util.concurrent.ConcurrentLinkedQueue$Itr
1639:             2             64  java.util.jar.Manifest$FastInputStream
1640:             1             64  javax.management.remote.rmi.RMIConnectionImpl
1641:             1             64  javax.management.remote.rmi.RMIConnectorServer
1642:             4             64  javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag
1643:             4             64  org.apache.cassandra.concurrent.SEPWorker$Work
1644:             2             64  org.apache.cassandra.cql3.functions.TokenFct
1645:             2             64  org.apache.cassandra.db.commitlog.CommitLogDescriptor
1646:             2             64  org.apache.cassandra.db.lifecycle.LogFile
1647:             2             64  org.apache.cassandra.db.lifecycle.LogTransaction
1648:             2             64  org.apache.cassandra.io.sstable.format.SSTableFormat$Type
1649:             2             64  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxIntTracker
1650:             2             64  org.apache.cassandra.io.util.SafeMemoryWriter
1651:             1             64  org.apache.cassandra.locator.DynamicEndpointSnitch
1652:             1             64  org.apache.cassandra.metrics.ViewWriteMetrics
1653:             1             64  org.apache.cassandra.net.MessagingService
1654:             2             64  org.apache.cassandra.service.ClientState
1655:             2             64  org.apache.cassandra.service.GCInspector$GCState
1656:             1             64  org.apache.cassandra.service.GCInspector$State
1657:             1             64  org.apache.cassandra.thrift.CustomTThreadPoolServer
1658:             1             64  org.apache.cassandra.utils.SigarLibrary
1659:             4             64  org.apache.cassandra.utils.SortedBiMultiValMap$1
1660:             4             64  org.codehaus.jackson.map.introspect.AnnotationMap
1661:             4             64  sun.net.www.protocol.jar.Handler
1662:             4             64  sun.rmi.server.MarshalOutputStream$1
1663:             2             64  sun.rmi.transport.DGCImpl$LeaseInfo
1664:             2             64  sun.rmi.transport.tcp.TCPTransport
1665:             2             64  sun.security.ssl.EphemeralKeyManager$EphemeralKeyPair
1666:             2             64  sun.security.ssl.SSLSessionContextImpl
1667:             2             64  sun.security.x509.PrivateKeyUsageExtension
1668:             2             64  sun.security.x509.SubjectAlternativeNameExtension
1669:             2             64  sun.util.locale.provider.LocaleServiceProviderPool
1670:             1             56  [Lcom.sun.org.apache.xerces.internal.impl.XMLScanner$NameType;
1671:             1             56  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit;
1672:             1             56  [Ljava.lang.Runnable;
1673:             1             56  [Ljava.nio.file.StandardOpenOption;
1674:             2             56  [Ljdk.internal.org.objectweb.asm.Type;
1675:             1             56  [Lorg.apache.commons.lang3.JavaVersion;
1676:             1             56  [Lorg.codehaus.jackson.JsonParser$Feature;
1677:             1             56  [Lorg.yaml.snakeyaml.events.Event$ID;
1678:             1             56  [Lsun.util.logging.PlatformLogger$Level;
1679:             1             56  [[I
1680:             1             56  com.sun.jmx.remote.internal.ServerNotifForwarder
1681:             1             56  io.netty.util.concurrent.ScheduledFutureTask
1682:             1             56  java.lang.invoke.LambdaFormEditor$Transform
1683:             1             56  java.util.concurrent.ConcurrentHashMap$KeyIterator
1684:             1             56  java.util.logging.ConsoleHandler
1685:             1             56  java.util.logging.LogManager
1686:             1             56  javax.management.remote.JMXConnectionNotification
1687:             1             56  javax.management.remote.rmi.RMIJRMPServerImpl
1688:             1             56  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache
1689:             1             56  org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
1690:             1             56  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions
1691:             1             56  org.apache.cassandra.cql3.CqlLexer$DFA1
1692:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA14
1693:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA22
1694:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA24
1695:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA28
1696:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA30
1697:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA37
1698:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA44
1699:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA9
1700:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA1
1701:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA15
1702:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA153
1703:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA154
1704:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA172
1705:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA174
1706:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA176
1707:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA178
1708:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA181
1709:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA189
1710:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA194
1711:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA195
1712:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA204
1713:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA44
1714:             1             56  org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard
1715:             1             56  org.apache.cassandra.db.commitlog.PeriodicCommitLogService
1716:             1             56  org.apache.cassandra.db.compaction.CompactionController
1717:             1             56  org.apache.cassandra.db.lifecycle.LifecycleTransaction
1718:             1             56  org.apache.cassandra.io.compress.CompressionMetadata$Writer
1719:             1             56  org.apache.cassandra.metrics.CacheMissMetrics
1720:             1             56  org.codehaus.jackson.map.ObjectMapper
1721:             1             56  org.codehaus.jackson.map.ser.StdSerializerProvider
1722:             1             56  org.codehaus.jackson.sym.BytesToNameCanonicalizer
1723:             1             56  org.hyperic.sigar.SigarLoader
1724:             1             56  sun.rmi.runtime.Log$InternalStreamHandler
1725:             1             48  [Lcom.sun.beans.util.Cache$CacheEntry;
1726:             1             48  [Lcom.sun.management.VMOption$Origin;
1727:             1             48  [Ljava.beans.WeakIdentityMap$Entry;
1728:             3             48  [Ljava.lang.annotation.Annotation;
1729:             1             48  [Ljava.lang.invoke.MethodHandleImpl$Intrinsic;
1730:             1             48  [Ljava.math.RoundingMode;
1731:             2             48  [Ljava.nio.file.FileVisitOption;
1732:             1             48  [Ljdk.net.SocketFlow$Status;
1733:             2             48  [Lorg.apache.cassandra.config.Config$CommitLogSync;
1734:             1             48  [Lorg.apache.cassandra.cql3.Constants$Type;
1735:             1             48  [Lorg.apache.cassandra.db.ClusteringPrefix$Kind;
1736:             1             48  [Lorg.apache.cassandra.db.Directories$FileAction;
1737:             1             48  [Lorg.apache.cassandra.db.WriteType;
1738:             2             48  [Lorg.apache.cassandra.exceptions.RequestFailureReason;
1739:             2             48  [Lorg.apache.cassandra.net.RateBasedBackPressure$Flow;
1740:             1             48  [Lorg.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State;
1741:             1             48  [Lorg.apache.cassandra.service.StorageService$Mode;
1742:             1             48  [Lorg.apache.cassandra.streaming.messages.StreamMessage$Type;
1743:             1             48  [Lorg.apache.cassandra.utils.progress.ProgressEventType;
1744:             1             48  [Lorg.codehaus.jackson.JsonGenerator$Feature;
1745:             1             48  [Lsun.security.x509.NetscapeCertTypeExtension$MapEntry;
1746:             1             48  ch.qos.logback.classic.jmx.JMXConfigurator
1747:             3             48  ch.qos.logback.classic.pattern.EnsureExceptionHandling
1748:             3             48  ch.qos.logback.classic.spi.PackagingDataCalculator
1749:             1             48  ch.qos.logback.core.joran.action.DefinePropertyAction
1750:             1             48  ch.qos.logback.core.joran.spi.InterpretationContext
1751:             1             48  ch.qos.logback.core.joran.spi.Interpreter
1752:             2             48  ch.qos.logback.core.rolling.helper.RenameUtil
1753:             3             48  ch.qos.logback.core.spi.LogbackLock
1754:             2             48  ch.qos.logback.core.subst.Node$Type
1755:             2             48  ch.qos.logback.core.util.FileSize
1756:             2             48  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format
1757:             3             48  com.google.common.cache.LocalCache$LocalLoadingCache
1758:             1             48  com.google.common.collect.EmptyImmutableListMultimap
1759:             2             48  com.google.common.collect.HashBiMap$Inverse
1760:             1             48  com.google.common.collect.ImmutableListMultimap
1761:             2             48  com.google.common.collect.ImmutableMultimap$Values
1762:             2             48  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry
1763:             1             48  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$Mappings
1764:             2             48  com.sun.jmx.mbeanserver.WeakIdentityHashMap
1765:             2             48  com.sun.jmx.remote.internal.ServerNotifForwarder$IdAndFilter
1766:             1             48  com.sun.jna.NativeLibrary
1767:             3             48  com.sun.org.apache.xerces.internal.impl.dv.dtd.ListDatatypeValidator
1768:             2             48  io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache
1769:             2             48  io.netty.channel.VoidChannelPromise
1770:             2             48  io.netty.util.Recycler$2
1771:             2             48  io.netty.util.UniqueName
1772:             1             48  io.netty.util.concurrent.GlobalEventExecutor
1773:             3             48  io.netty.util.internal.TypeParameterMatcher$ReflectiveMatcher
1774:             2             48  java.io.ByteArrayOutputStream
1775:             2             48  java.io.File$PathStatus
1776:             3             48  java.io.FileOutputStream$1
1777:             2             48  java.io.OutputStreamWriter
1778:             2             48  java.io.SerialCallbackContext
1779:             3             48  java.lang.Boolean
1780:             3             48  java.lang.Float
1781:             3             48  java.lang.InheritableThreadLocal
1782:             1             48  java.lang.invoke.BoundMethodHandle$Species_L4
1783:             2             48  java.lang.invoke.ConstantCallSite
1784:             2             48  java.lang.invoke.InfoFromMemberName
1785:             2             48  java.lang.invoke.InnerClassLambdaMetafactory$ForwardingMethodGenerator
1786:             2             48  java.lang.management.MemoryType
1787:             2             48  java.lang.reflect.ReflectPermission
1788:             2             48  java.net.InetAddress$Cache
1789:             2             48  java.net.InetAddress$Cache$Type
1790:             2             48  java.net.InetAddress$CacheEntry
1791:             1             48  java.net.NetworkInterface
1792:             2             48  java.net.ServerSocket
1793:             2             48  java.net.SocketPermissionCollection
1794:             2             48  java.net.StandardProtocolFamily
1795:             3             48  java.nio.channels.FileChannel$MapMode
1796:             2             48  java.nio.charset.CoderResult
1797:             3             48  java.nio.charset.CodingErrorAction
1798:             2             48  java.rmi.dgc.Lease
1799:             2             48  java.security.AllPermissionCollection
1800:             3             48  java.text.AttributedCharacterIterator$Attribute
1801:             3             48  java.util.Base64$Decoder
1802:             2             48  java.util.PropertyPermissionCollection
1803:             3             48  java.util.TreeMap$EntrySet
1804:             2             48  java.util.concurrent.Executors$DefaultThreadFactory
1805:             3             48  java.util.concurrent.atomic.AtomicMarkableReference
1806:             2             48  java.util.logging.Logger$LoggerBundle
1807:             1             48  java.util.regex.Pattern$GroupCurly
1808:             2             48  java.util.regex.Pattern$Prolog
1809:             2             48  javax.management.MBeanServerInvocationHandler
1810:             1             48  javax.management.remote.rmi.RMIConnectionImpl$RMIServerCommunicatorAdmin
1811:             1             48  javax.security.auth.SubjectDomainCombiner$WeakKeyValueMap
1812:             1             48  org.antlr.runtime.ANTLRStringStream
1813:             2             48  org.apache.cassandra.cache.AutoSavingCache$2
1814:             2             48  org.apache.cassandra.config.Config$CommitLogSync
1815:             2             48  org.apache.cassandra.config.Config$DiskOptimizationStrategy
1816:             2             48  org.apache.cassandra.config.ParameterizedClass
1817:             2             48  org.apache.cassandra.cql3.Sets$Marker
1818:             2             48  org.apache.cassandra.cql3.Sets$Setter
1819:             2             48  org.apache.cassandra.cql3.functions.FunctionCall
1820:             2             48  org.apache.cassandra.cql3.statements.Bound
1821:             2             48  org.apache.cassandra.db.Directories$OnTxnErr
1822:             2             48  org.apache.cassandra.db.Memtable$LastCommitLogPosition
1823:             2             48  org.apache.cassandra.db.ReadCommand$Kind
1824:             2             48  org.apache.cassandra.db.aggregation.AggregationSpecification$Kind
1825:             1             48  org.apache.cassandra.db.commitlog.CommitLogArchiver
1826:             1             48  org.apache.cassandra.db.compaction.CompactionInfo
1827:             2             48  org.apache.cassandra.db.filter.ClusteringIndexFilter$Kind
1828:             2             48  org.apache.cassandra.db.lifecycle.LifecycleTransaction$State
1829:             2             48  org.apache.cassandra.db.lifecycle.LogReplica
1830:             2             48  org.apache.cassandra.db.rows.Unfiltered$Kind
1831:             2             48  org.apache.cassandra.exceptions.RequestFailureReason
1832:             1             48  org.apache.cassandra.gms.FailureDetector
1833:             2             48  org.apache.cassandra.hints.HintsDispatcher$Action
1834:             1             48  org.apache.cassandra.hints.HintsService
1835:             2             48  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason
1836:             1             48  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter
1837:             1             48  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxLongTracker
1838:             2             48  org.apache.cassandra.io.util.NIODataInputStream
1839:             1             48  org.apache.cassandra.locator.NetworkTopologyStrategy
1840:             3             48  org.apache.cassandra.metrics.CacheMetrics$2
1841:             3             48  org.apache.cassandra.metrics.CacheMetrics$3
1842:             3             48  org.apache.cassandra.metrics.CacheMetrics$4
1843:             3             48  org.apache.cassandra.metrics.CacheMetrics$5
1844:             2             48  org.apache.cassandra.metrics.TableMetrics$Sampler
1845:             1             48  org.apache.cassandra.net.MessagingService$2
1846:             1             48  org.apache.cassandra.net.RateBasedBackPressure
1847:             2             48  org.apache.cassandra.net.RateBasedBackPressure$Flow
1848:             1             48  org.apache.cassandra.repair.messages.RepairOption
1849:             2             48  org.apache.cassandra.schema.CachingParams$Option
1850:             2             48  org.apache.cassandra.schema.KeyspaceParams$Option
1851:             1             48  org.apache.cassandra.service.ActiveRepairService$ParentRepairSession
1852:             2             48  org.apache.cassandra.streaming.ProgressInfo$Direction
1853:             2             48  org.apache.cassandra.transport.Event$StatusChange$Status
1854:             2             48  org.apache.cassandra.transport.Message$Direction
1855:             2             48  org.apache.cassandra.utils.ChecksumType$3
1856:             2             48  org.apache.cassandra.utils.Throwables$FileOpType
1857:             2             48  org.apache.cassandra.utils.btree.BTree$Dir
1858:             2             48  org.apache.cassandra.utils.concurrent.WaitQueue$RegisteredSignal
1859:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotE
1860:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotK
1861:             1             48  org.codehaus.jackson.JsonFactory
1862:             1             48  org.codehaus.jackson.map.DeserializationConfig
1863:             1             48  org.codehaus.jackson.map.SerializationConfig
1864:             2             48  org.codehaus.jackson.map.deser.std.CalendarDeserializer
1865:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$BooleanDeserializer
1866:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ByteDeserializer
1867:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$CharacterDeserializer
1868:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$DoubleDeserializer
1869:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$FloatDeserializer
1870:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$IntegerDeserializer
1871:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$LongDeserializer
1872:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ShortDeserializer
1873:             2             48  org.codehaus.jackson.map.ser.StdSerializers$BooleanSerializer
1874:             1             48  org.hyperic.sigar.FileSystemMap
1875:             1             48  org.hyperic.sigar.Sigar
1876:             2             48  sun.management.ManagementFactoryHelper$1
1877:             2             48  sun.misc.NativeSignalHandler
1878:             2             48  sun.misc.URLClassPath$FileLoader
1879:             3             48  sun.nio.fs.UnixFileAttributes$UnixAsBasicFileAttributes
1880:             2             48  sun.rmi.server.UnicastServerRef$1
1881:             3             48  sun.rmi.server.WeakClassHashMap$ValueCell
1882:             2             48  sun.security.jca.ProviderList
1883:             2             48  sun.security.jca.ProviderList$3
1884:             2             48  sun.security.provider.DSAParameters
1885:             2             48  sun.security.ssl.SSLAlgorithmConstraints
1886:             3             48  sun.security.util.AlgorithmDecomposer
1887:             2             48  sun.security.util.DisabledAlgorithmConstraints$UsageConstraint
1888:             2             48  sun.security.util.DisabledAlgorithmConstraints$jdkCAConstraint
1889:             3             48  sun.security.x509.RFC822Name
1890:             3             48  sun.text.normalizer.NormalizerBase$QuickCheckResult
1891:             1             48  sun.text.resources.FormatData
1892:             1             48  sun.text.resources.en.FormatData_en
1893:             1             48  sun.text.resources.en.FormatData_en_US
1894:             1             40  [Lch.qos.logback.core.pattern.parser.TokenStream$TokenizerState;
1895:             1             40  [Lch.qos.logback.core.subst.Token$Type;
1896:             1             40  [Lch.qos.logback.core.util.AggregationType;
1897:             1             40  [Lcom.google.common.collect.SortedLists$KeyPresentBehavior;
1898:             2             40  [Lcom.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry;
1899:             1             40  [Lcom.sun.org.apache.xerces.internal.util.Status;
1900:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State;
1901:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State;
1902:             1             40  [Ljava.lang.management.MemoryPoolMXBean;
1903:             2             40  [Ljava.util.logging.Handler;
1904:             1             40  [Ljava.util.stream.StreamOpFlag;
1905:             1             40  [Lorg.apache.cassandra.cql3.statements.IndexTarget$Type;
1906:             1             40  [Lorg.apache.cassandra.db.filter.DataLimits$Kind;
1907:             1             40  [Lorg.apache.cassandra.db.lifecycle.LogRecord$Type;
1908:             1             40  [Lorg.apache.cassandra.schema.CompactionParams$Option;
1909:             1             40  [Lorg.apache.cassandra.streaming.StreamSession$State;
1910:             1             40  [Lorg.apache.cassandra.utils.NativeLibrary$OSType;
1911:             1             40  [Lorg.github.jamm.MemoryMeter$Guess;
1912:             1             40  [Lorg.yaml.snakeyaml.DumperOptions$ScalarStyle;
1913:             1             40  [Lsun.security.jca.ServiceId;
1914:             1             40  [Lsun.security.util.DisabledAlgorithmConstraints$Constraint$Operator;
1915:             1             40  [Lsun.util.locale.provider.LocaleProviderAdapter$Type;
1916:             1             40  [[Ljava.lang.invoke.LambdaForm$Name;
1917:             1             40  ch.qos.logback.core.BasicStatusManager
1918:             1             40  ch.qos.logback.core.joran.spi.ConfigurationWatchList
1919:             1             40  com.google.common.collect.AbstractMapBasedMultimap$2
1920:             1             40  com.google.common.collect.AbstractMapBasedMultimap$WrappedSet
1921:             1             40  com.google.common.collect.EmptyImmutableSortedMap
1922:             1             40  com.sun.beans.finder.MethodFinder$1
1923:             1             40  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor
1924:             1             40  com.sun.jmx.mbeanserver.JmxMBeanServer
1925:             1             40  com.sun.jmx.mbeanserver.MBeanServerDelegateImpl
1926:             1             40  io.netty.channel.AbstractChannel$CloseFuture
1927:             1             40  io.netty.channel.DefaultChannelPipeline
1928:             1             40  io.netty.channel.epoll.AbstractEpollServerChannel$EpollServerSocketUnsafe
1929:             1             40  java.beans.WeakIdentityMap$Entry
1930:             1             40  java.lang.reflect.Proxy$Key2
1931:             1             40  java.rmi.NoSuchObjectException
1932:             1             40  java.util.ResourceBundle$1
1933:             1             40  javax.crypto.CryptoAllPermission
1934:             1             40  net.jpountz.lz4.LZ4Factory
1935:             1             40  org.antlr.runtime.CommonTokenStream
1936:             1             40  org.apache.cassandra.concurrent.SharedExecutorPool
1937:             1             40  org.apache.cassandra.config.TransparentDataEncryptionOptions
1938:             1             40  org.apache.cassandra.cql3.CqlLexer
1939:             1             40  org.apache.cassandra.db.commitlog.CommitLog
1940:             1             40  org.apache.cassandra.db.compaction.CompactionTask
1941:             1             40  org.apache.cassandra.exceptions.RepairException
1942:             1             40  org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy
1943:             1             40  org.apache.cassandra.locator.GossipingPropertyFileSnitch
1944:             1             40  org.apache.cassandra.net.MessagingService$1
1945:             1             40  org.apache.cassandra.net.MessagingService$3
1946:             1             40  org.apache.cassandra.streaming.management.StreamEventJMXNotifier
1947:             1             40  org.apache.cassandra.transport.Server
1948:             1             40  org.apache.cassandra.utils.NoSpamLogger$NoSpamLogStatement
1949:             1             40  org.apache.cassandra.utils.memory.SlabPool
1950:             1             40  org.codehaus.jackson.map.util.StdDateFormat
1951:             1             40  sun.management.DiagnosticCommandImpl
1952:             1             40  sun.management.MappedMXBeanType$CompositeDataMXBeanType
1953:             1             40  sun.management.MappedMXBeanType$MapMXBeanType
1954:             1             40  sun.nio.cs.StandardCharsets$Aliases
1955:             1             40  sun.nio.cs.StandardCharsets$Cache
1956:             1             40  sun.nio.cs.StandardCharsets$Classes
1957:             1             40  sun.security.ssl.SSLContextImpl$DefaultSSLContext
1958:             1             32  [Lch.qos.logback.core.rolling.helper.CompressionMode;
1959:             1             32  [Lch.qos.logback.core.spi.FilterReply;
1960:             1             32  [Lch.qos.logback.core.subst.Tokenizer$TokenizerState;
1961:             1             32  [Lcom.github.benmanes.caffeine.cache.Caffeine$Strength;
1962:             1             32  [Lcom.google.common.base.Predicates$ObjectPredicate;
1963:             1             32  [Lcom.google.common.cache.LocalCache$Strength;
1964:             1             32  [Lcom.google.common.collect.AbstractIterator$State;
1965:             1             32  [Lcom.google.common.collect.MapMakerInternalMap$Strength;
1966:             1             32  [Lcom.google.common.collect.SortedLists$KeyAbsentBehavior;
1967:             1             32  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus;
1968:             1             32  [Lcom.sun.beans.util.Cache$Kind;
1969:             1             32  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap;
1970:             2             32  [Ljava.lang.Enum;
1971:             1             32  [Ljava.lang.OutOfMemoryError;
1972:             1             32  [Ljava.lang.ThreadGroup;
1973:             1             32  [Ljava.lang.UNIXProcess$Platform;
1974:             1             32  [Ljava.lang.management.MemoryManagerMXBean;
1975:             1             32  [Ljava.nio.file.FileTreeWalker$EventType;
1976:             1             32  [Ljava.nio.file.FileVisitResult;
1977:             1             32  [Ljava.text.Normalizer$Form;
1978:             1             32  [Ljava.util.concurrent.atomic.AtomicReference;
1979:             1             32  [Ljava.util.stream.MatchOps$MatchKind;
1980:             1             32  [Ljava.util.stream.StreamShape;
1981:             1             32  [Lnet.jpountz.util.Native$OS;
1982:             1             32  [Lorg.apache.cassandra.auth.DataResource$Level;
1983:             1             32  [Lorg.apache.cassandra.auth.IRoleManager$Option;
1984:             1             32  [Lorg.apache.cassandra.config.Config$DiskAccessMode;
1985:             1             32  [Lorg.apache.cassandra.config.Config$UserFunctionTimeoutPolicy;
1986:             1             32  [Lorg.apache.cassandra.config.ReadRepairDecision;
1987:             1             32  [Lorg.apache.cassandra.cql3.AssignmentTestable$TestResult;
1988:             1             32  [Lorg.apache.cassandra.cql3.statements.StatementType;
1989:             1             32  [Lorg.apache.cassandra.db.Conflicts$Resolution;
1990:             1             32  [Lorg.apache.cassandra.db.Directories$FileType;
1991:             1             32  [Lorg.apache.cassandra.db.commitlog.CommitLogSegment$CDCState;
1992:             1             32  [Lorg.apache.cassandra.db.context.CounterContext$Relationship;
1993:             1             32  [Lorg.apache.cassandra.db.lifecycle.SSTableSet;
1994:             1             32  [Lorg.apache.cassandra.db.marshal.AbstractType$ComparisonType;
1995:             1             32  [Lorg.apache.cassandra.db.marshal.CollectionType$Kind;
1996:             1             32  [Lorg.apache.cassandra.db.monitoring.MonitoringState;
1997:             1             32  [Lorg.apache.cassandra.db.rows.SerializationHelper$Flag;
1998:             1             32  [Lorg.apache.cassandra.hints.HintsDispatcher$Callback$Outcome;
1999:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReader$OpenReason;
2000:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason;
2001:             1             32  [Lorg.apache.cassandra.repair.RepairParallelism;
2002:             1             32  [Lorg.apache.cassandra.repair.SystemDistributedKeyspace$RepairState;
2003:             1             32  [Lorg.apache.cassandra.schema.SpeculativeRetryParam$Kind;
2004:             1             32  [Lorg.apache.cassandra.service.CacheService$CacheType;
2005:             1             32  [Lorg.apache.cassandra.streaming.StreamEvent$Type;
2006:             1             32  [Lorg.apache.cassandra.utils.AbstractIterator$State;
2007:             1             32  [Lorg.apache.cassandra.utils.AsymmetricOrdering$Op;
2008:             1             32  [Lorg.apache.cassandra.utils.NoSpamLogger$Level;
2009:             1             32  [Lorg.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State;
2010:             1             32  [Lorg.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle;
2011:             2             32  [Lorg.codehaus.jackson.type.JavaType;
2012:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$FlowStyle;
2013:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$LineBreak;
2014:             1             32  [Lorg.yaml.snakeyaml.introspector.BeanAccess;
2015:             1             32  [Lsun.misc.FormattedFloatingDecimal$Form;
2016:             1             32  [Lsun.misc.ObjectInputFilter$Status;
2017:             1             32  [Lsun.security.provider.NativePRNG$Variant;
2018:             1             32  [Lsun.security.ssl.CipherSuite$CipherType;
2019:             1             32  [Lsun.security.ssl.CipherSuite$PRF;
2020:             1             32  [[Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
2021:             1             32  ch.qos.logback.classic.joran.JoranConfigurator
2022:             1             32  ch.qos.logback.classic.joran.action.ConfigurationAction
2023:             1             32  ch.qos.logback.classic.joran.action.EvaluatorAction
2024:             1             32  ch.qos.logback.classic.joran.action.LoggerAction
2025:             1             32  ch.qos.logback.classic.joran.action.LoggerContextListenerAction
2026:             1             32  ch.qos.logback.classic.joran.action.ReceiverAction
2027:             1             32  ch.qos.logback.classic.joran.action.RootLoggerAction
2028:             1             32  ch.qos.logback.classic.sift.SiftAction
2029:             1             32  ch.qos.logback.classic.spi.LoggerContextVO
2030:             1             32  ch.qos.logback.core.helpers.CyclicBuffer
2031:             1             32  ch.qos.logback.core.joran.action.AppenderAction
2032:             1             32  ch.qos.logback.core.joran.action.ConversionRuleAction
2033:             1             32  ch.qos.logback.core.joran.action.IncludeAction
2034:             1             32  ch.qos.logback.core.joran.action.NestedBasicPropertyIA
2035:             1             32  ch.qos.logback.core.joran.action.NestedComplexPropertyIA
2036:             1             32  ch.qos.logback.core.joran.action.NewRuleAction
2037:             1             32  ch.qos.logback.core.joran.action.ParamAction
2038:             1             32  ch.qos.logback.core.joran.action.ShutdownHookAction
2039:             1             32  ch.qos.logback.core.joran.action.StatusListenerAction
2040:             1             32  ch.qos.logback.core.joran.action.TimestampAction
2041:             1             32  ch.qos.logback.core.joran.conditional.ElseAction
2042:             1             32  ch.qos.logback.core.joran.conditional.IfAction
2043:             1             32  ch.qos.logback.core.joran.conditional.ThenAction
2044:             1             32  ch.qos.logback.core.joran.spi.SimpleRuleStore
2045:             2             32  ch.qos.logback.core.spi.AppenderAttachableImpl
2046:             1             32  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache
2047:             1             32  com.github.benmanes.caffeine.cache.FrequencySketch
2048:             2             32  com.google.common.base.Joiner
2049:             2             32  com.google.common.base.Predicates$InPredicate
2050:             1             32  com.google.common.collect.AbstractMapBasedMultimap$NavigableKeySet
2051:             1             32  com.google.common.collect.EmptyImmutableBiMap
2052:             2             32  com.google.common.util.concurrent.Striped$2
2053:             2             32  com.sun.beans.WeakCache
2054:             1             32  com.sun.beans.finder.BeanInfoFinder
2055:             1             32  com.sun.jmx.mbeanserver.Repository
2056:             1             32  com.sun.jmx.remote.internal.ArrayQueue
2057:             1             32  com.sun.jmx.remote.security.JMXSubjectDomainCombiner
2058:             1             32  com.sun.org.apache.xerces.internal.impl.XMLEntityScanner$1
2059:             2             32  com.sun.org.apache.xerces.internal.impl.dv.dtd.ENTITYDatatypeValidator
2060:             2             32  com.sun.proxy.$Proxy5
2061:             1             32  io.netty.bootstrap.ServerBootstrap$ServerBootstrapAcceptor
2062:             1             32  io.netty.channel.epoll.EpollEventLoopGroup
2063:             2             32  io.netty.channel.group.ChannelMatchers$ClassMatcher
2064:             1             32  io.netty.util.concurrent.DefaultThreadFactory
2065:             2             32  io.netty.util.internal.logging.Slf4JLoggerFactory
2066:             1             32  java.beans.ThreadGroupContext
2067:             1             32  java.beans.ThreadGroupContext$1
2068:             2             32  java.io.ObjectStreamClass$1
2069:             2             32  java.io.ObjectStreamClass$3
2070:             2             32  java.io.ObjectStreamClass$4
2071:             2             32  java.io.ObjectStreamClass$5
2072:             1             32  java.io.UnixFileSystem
2073:             1             32  java.lang.ArithmeticException
2074:             1             32  java.lang.ArrayIndexOutOfBoundsException
2075:             1             32  java.lang.ClassCastException
2076:             1             32  java.lang.Exception
2077:             1             32  java.lang.NullPointerException
2078:             2             32  java.lang.Shutdown$Lock
2079:             1             32  java.lang.UnsupportedOperationException
2080:             1             32  java.lang.reflect.WeakCache
2081:             1             32  java.lang.reflect.WeakCache$CacheKey
2082:             2             32  java.nio.ByteOrder
2083:             1             32  java.nio.channels.NotYetConnectedException
2084:             1             32  java.text.DontCareFieldPosition
2085:             2             32  java.util.Hashtable$EntrySet
2086:             1             32  java.util.PriorityQueue
2087:             1             32  java.util.TreeMap$EntryIterator
2088:             1             32  java.util.TreeMap$KeyIterator
2089:             1             32  java.util.concurrent.CancellationException
2090:             1             32  java.util.concurrent.ConcurrentSkipListMap$KeyIterator
2091:             2             32  java.util.concurrent.ConcurrentSkipListMap$KeySet
2092:             1             32  java.util.concurrent.FutureTask
2093:             1             32  java.util.concurrent.ThreadLocalRandom
2094:             2             32  java.util.logging.ErrorManager
2095:             1             32  java.util.logging.LogManager$SystemLoggerContext
2096:             1             32  java.util.regex.Pattern$3
2097:             1             32  javax.crypto.spec.RC5ParameterSpec
2098:             2             32  javax.management.NotificationFilterSupport
2099:             1             32  javax.management.remote.JMXServiceURL
2100:             1             32  javax.security.auth.Subject
2101:             1             32  net.jpountz.xxhash.XXHashFactory
2102:             1             32  org.apache.cassandra.auth.CassandraRoleManager
2103:             1             32  org.apache.cassandra.batchlog.BatchlogManager
2104:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache
2105:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache$1
2106:             1             32  org.apache.cassandra.config.Schema
2107:             1             32  org.apache.cassandra.cql3.QueryOptions$SpecificOptions
2108:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$4
2109:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$5
2110:             2             32  org.apache.cassandra.db.RangeSliceVerbHandler
2111:             1             32  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool
2112:             1             32  org.apache.cassandra.db.compaction.CompactionManager
2113:             2             32  org.apache.cassandra.db.lifecycle.LogReplicaSet
2114:             2             32  org.apache.cassandra.db.lifecycle.LogTransaction$TransactionTidier
2115:             1             32  org.apache.cassandra.db.marshal.AsciiType
2116:             1             32  org.apache.cassandra.db.marshal.PartitionerDefinedOrder
2117:             2             32  org.apache.cassandra.db.rows.CellPath$EmptyCellPath
2118:             2             32  org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer
2119:             1             32  org.apache.cassandra.hints.HintsBuffer
2120:             1             32  org.apache.cassandra.hints.HintsBufferPool
2121:             1             32  org.apache.cassandra.hints.HintsDispatchExecutor
2122:             1             32  org.apache.cassandra.hints.HintsDispatchTrigger
2123:             1             32  org.apache.cassandra.index.internal.composites.CollectionKeyIndex
2124:             1             32  org.apache.cassandra.io.compress.CompressedSequentialWriter$TransactionalProxy
2125:             1             32  org.apache.cassandra.io.compress.LZ4Compressor
2126:             1             32  org.apache.cassandra.io.sstable.IndexSummaryManager
2127:             1             32  org.apache.cassandra.metrics.CQLMetrics
2128:             2             32  org.apache.cassandra.metrics.ClientMetrics$$Lambda$278/1979648826
2129:             1             32  org.apache.cassandra.metrics.CommitLogMetrics
2130:             1             32  org.apache.cassandra.metrics.CompactionMetrics
2131:             2             32  org.apache.cassandra.metrics.TableMetrics$AllTableMetricNameFactory
2132:             2             32  org.apache.cassandra.net.ResponseVerbHandler
2133:             1             32  org.apache.cassandra.repair.RepairRunnable
2134:             2             32  org.apache.cassandra.schema.Types
2135:             1             32  org.apache.cassandra.security.EncryptionContext
2136:             1             32  org.apache.cassandra.service.ActiveRepairService
2137:             1             32  org.apache.cassandra.service.CassandraDaemon
2138:             1             32  org.apache.cassandra.service.NativeTransportService
2139:             1             32  org.apache.cassandra.thrift.TCustomServerSocket
2140:             1             32  org.apache.cassandra.thrift.ThriftServer
2141:             1             32  org.apache.cassandra.utils.ExpiringMap
2142:             2             32  org.apache.cassandra.utils.IntegerInterval$Set
2143:             1             32  org.apache.cassandra.utils.ResourceWatcher$WatchedResource
2144:             1             32  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder
2145:             1             32  org.apache.cassandra.utils.btree.BTree$1
2146:             1             32  org.apache.cassandra.utils.btree.TreeBuilder$1
2147:             1             32  org.apache.cassandra.utils.concurrent.WaitQueue$TimedSignal
2148:             1             32  org.apache.cassandra.utils.memory.BufferPool$GlobalPool
2149:             1             32  org.apache.thrift.protocol.TBinaryProtocol$Factory
2150:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$2
2151:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$3
2152:             1             32  org.codehaus.jackson.map.deser.BeanDeserializerFactory$ConfigImpl
2153:             1             32  org.codehaus.jackson.map.deser.StdDeserializerProvider
2154:             1             32  org.codehaus.jackson.map.introspect.VisibilityChecker$Std
2155:             2             32  org.codehaus.jackson.map.ser.StdSerializers$NumberSerializer
2156:             2             32  org.codehaus.jackson.map.ser.std.StdKeySerializer
2157:             1             32  org.codehaus.jackson.map.type.TypeFactory
2158:             2             32  org.codehaus.jackson.map.util.RootNameLookup
2159:             1             32  org.github.jamm.MemoryMeter
2160:             1             32  sun.instrument.InstrumentationImpl
2161:             1             32  sun.management.GcInfoCompositeData
2162:             1             32  sun.management.MappedMXBeanType$InProgress
2163:             1             32  sun.nio.ch.ServerSocketAdaptor
2164:             2             32  sun.nio.ch.SocketDispatcher
2165:             1             32  sun.nio.cs.StandardCharsets
2166:             1             32  sun.nio.fs.LinuxFileSystem
2167:             1             32  sun.reflect.UnsafeIntegerFieldAccessorImpl
2168:             1             32  sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl
2169:             2             32  sun.rmi.server.UnicastRef
2170:             2             32  sun.rmi.server.UnicastRef2
2171:             2             32  sun.rmi.transport.DGCImpl$1
2172:             1             32  sun.rmi.transport.proxy.RMIMasterSocketFactory
2173:             1             32  sun.rmi.transport.tcp.TCPTransport$AcceptLoop
2174:             1             32  sun.security.provider.SecureRandom
2175:             2             32  sun.security.ssl.SSLAlgorithmDecomposer
2176:             1             32  sun.security.ssl.X509TrustManagerImpl
2177:             1             32  sun.security.validator.SimpleValidator
2178:             1             32  sun.security.x509.AuthorityInfoAccessExtension
2179:             1             32  sun.security.x509.IssuerAlternativeNameExtension
2180:             1             32  sun.security.x509.PolicyMappingsExtension
2181:             1             32  sun.util.locale.provider.LocaleResources
2182:             1             24  [Lch.qos.logback.core.joran.spi.ConsoleTarget;
2183:             1             24  [Lch.qos.logback.core.subst.Node$Type;
2184:             1             24  [Lcom.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format;
2185:             1             24  [Lcom.github.benmanes.caffeine.cache.Buffer;
2186:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledTicker;
2187:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledWriter;
2188:             1             24  [Lcom.github.benmanes.caffeine.cache.SingletonWeigher;
2189:             1             24  [Lcom.github.benmanes.caffeine.cache.stats.DisabledStatsCounter;
2190:             1             24  [Lcom.google.common.base.Functions$IdentityFunction;
2191:             1             24  [Lcom.google.common.cache.CacheBuilder$NullListener;
2192:             1             24  [Lcom.google.common.cache.CacheBuilder$OneWeigher;
2193:             1             24  [Lcom.google.common.collect.GenericMapMaker$NullListener;
2194:             1             24  [Lcom.google.common.collect.Maps$EntryFunction;
2195:             1             24  [Lcom.google.common.util.concurrent.MoreExecutors$DirectExecutor;
2196:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener;
2197:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher;
2198:             1             24  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property;
2199:             1             24  [Ljava.io.File$PathStatus;
2200:             1             24  [Ljava.lang.ClassValue$Entry;
2201:             1             24  [Ljava.lang.management.MemoryType;
2202:             1             24  [Ljava.net.InetAddress$Cache$Type;
2203:             1             24  [Ljava.net.InterfaceAddress;
2204:             1             24  [Ljava.net.StandardProtocolFamily;
2205:             1             24  [Ljava.rmi.server.ObjID;
2206:             1             24  [Ljava.util.Comparators$NaturalOrderComparator;
2207:             1             24  [Ljava.util.Locale$Category;
2208:             1             24  [Ljava.util.concurrent.ExecutorService;
2209:             1             24  [Ljava.util.concurrent.ThreadPoolExecutor;
2210:             1             24  [Ljavax.net.ssl.KeyManager;
2211:             1             24  [Ljavax.net.ssl.TrustManager;
2212:             1             24  [Lorg.apache.cassandra.concurrent.ExecutorLocal;
2213:             1             24  [Lorg.apache.cassandra.config.Config$DiskOptimizationStrategy;
2214:             1             24  [Lorg.apache.cassandra.config.Config$RequestSchedulerId;
2215:             1             24  [Lorg.apache.cassandra.cql3.QueryProcessor$InternalStateInstance;
2216:             1             24  [Lorg.apache.cassandra.cql3.Term;
2217:             1             24  [Lorg.apache.cassandra.cql3.statements.Bound;
2218:             1             24  [Lorg.apache.cassandra.db.Directories$OnTxnErr;
2219:             1             24  [Lorg.apache.cassandra.db.ReadCommand$Kind;
2220:             1             24  [Lorg.apache.cassandra.db.aggregation.AggregationSpecification$Kind;
2221:             1             24  [Lorg.apache.cassandra.db.filter.ClusteringIndexFilter$Kind;
2222:             1             24  [Lorg.apache.cassandra.db.rows.Unfiltered$Kind;
2223:             1             24  [Lorg.apache.cassandra.hints.HintsDispatcher$Action;
2224:             1             24  [Lorg.apache.cassandra.io.compress.BufferType;
2225:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableFormat$Type;
2226:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason;
2227:             1             24  [Lorg.apache.cassandra.metrics.TableMetrics$Sampler;
2228:             1             24  [Lorg.apache.cassandra.schema.CachingParams$Option;
2229:             1             24  [Lorg.apache.cassandra.schema.KeyspaceParams$Option;
2230:             1             24  [Lorg.apache.cassandra.streaming.ProgressInfo$Direction;
2231:             1             24  [Lorg.apache.cassandra.transport.Event$StatusChange$Status;
2232:             1             24  [Lorg.apache.cassandra.transport.Message$Direction;
2233:             1             24  [Lorg.apache.cassandra.utils.ChecksumType;
2234:             1             24  [Lorg.apache.cassandra.utils.Throwables$FileOpType;
2235:             1             24  [Lorg.apache.cassandra.utils.btree.BTree$Dir;
2236:             1             24  [Lsun.launcher.LauncherHelper;
2237:             1             24  [Lsun.security.ssl.EphemeralKeyManager$EphemeralKeyPair;
2238:             1             24  ch.qos.logback.classic.joran.action.ConsolePluginAction
2239:             1             24  ch.qos.logback.classic.joran.action.ContextNameAction
2240:             1             24  ch.qos.logback.classic.joran.action.InsertFromJNDIAction
2241:             1             24  ch.qos.logback.classic.joran.action.JMXConfiguratorAction
2242:             1             24  ch.qos.logback.classic.spi.TurboFilterList
2243:             1             24  ch.qos.logback.classic.util.ContextSelectorStaticBinder
2244:             1             24  ch.qos.logback.classic.util.LogbackMDCAdapter
2245:             1             24  ch.qos.logback.core.joran.action.ContextPropertyAction
2246:             1             24  ch.qos.logback.core.joran.spi.CAI_WithLocatorSupport
2247:             1             24  ch.qos.logback.core.joran.spi.EventPlayer
2248:             1             24  com.clearspring.analytics.stream.cardinality.RegisterSet
2249:             1             24  com.codahale.metrics.MetricRegistry
2250:             1             24  com.github.benmanes.caffeine.cache.BoundedBuffer
2251:             1             24  com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask
2252:             1             24  com.github.benmanes.caffeine.cache.DisabledTicker
2253:             1             24  com.github.benmanes.caffeine.cache.DisabledWriter
2254:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$1
2255:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$10
2256:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$100
2257:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$101
2258:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$102
2259:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$103
2260:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$104
2261:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$105
2262:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$106
2263:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$107
2264:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$108
2265:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$109
2266:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$11
2267:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$110
2268:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$111
2269:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$112
2270:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$113
2271:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$114
2272:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$115
2273:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$116
2274:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$117
2275:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$118
2276:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$119
2277:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$12
2278:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$120
2279:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$121
2280:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$122
2281:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$123
2282:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$124
2283:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$125
2284:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$126
2285:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$127
2286:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$128
2287:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$129
2288:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$13
2289:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$130
2290:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$131
2291:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$132
2292:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$133
2293:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$134
2294:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$135
2295:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$136
2296:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$137
2297:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$138
2298:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$139
2299:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$14
2300:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$140
2301:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$141
2302:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$142
2303:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$143
2304:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$144
2305:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$15
2306:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$16
2307:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$17
2308:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$18
2309:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$19
2310:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$2
2311:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$20
2312:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$21
2313:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$22
2314:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$23
2315:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$24
2316:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$25
2317:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$26
2318:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$27
2319:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$28
2320:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$29
2321:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$3
2322:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$30
2323:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$31
2324:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$32
2325:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$33
2326:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$34
2327:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$35
2328:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$36
2329:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$37
2330:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$38
2331:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$39
2332:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$4
2333:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$40
2334:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$41
2335:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$42
2336:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$43
2337:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$44
2338:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$45
2339:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$46
2340:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$47
2341:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$48
2342:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$49
2343:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$5
2344:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$50
2345:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$51
2346:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$52
2347:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$53
2348:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$54
2349:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$55
2350:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$56
2351:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$57
2352:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$58
2353:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$59
2354:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$6
2355:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$60
2356:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$61
2357:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$62
2358:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$63
2359:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$64
2360:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$65
2361:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$66
2362:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$67
2363:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$68
2364:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$69
2365:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$7
2366:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$70
2367:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$71
2368:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$72
2369:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$73
2370:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$74
2371:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$75
2372:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$76
2373:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$77
2374:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$78
2375:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$79
2376:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$8
2377:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$80
2378:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$81
2379:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$82
2380:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$83
2381:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$84
2382:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$85
2383:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$86
2384:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$87
2385:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$88
2386:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$89
2387:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$9
2388:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$90
2389:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$91
2390:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$92
2391:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$93
2392:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$94
2393:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$95
2394:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$96
2395:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$97
2396:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$98
2397:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$99
2398:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$1
2399:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$2
2400:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$3
2401:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$4
2402:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$5
2403:             1             24  com.github.benmanes.caffeine.cache.SingletonWeigher
2404:             1             24  com.github.benmanes.caffeine.cache.stats.DisabledStatsCounter
2405:             1             24  com.google.common.base.CharMatcher$Or
2406:             1             24  com.google.common.base.Functions$IdentityFunction
2407:             1             24  com.google.common.base.Joiner$1
2408:             1             24  com.google.common.base.Joiner$MapJoiner
2409:             1             24  com.google.common.base.Predicates$ObjectPredicate$1
2410:             1             24  com.google.common.base.Predicates$ObjectPredicate$2
2411:             1             24  com.google.common.base.Predicates$ObjectPredicate$3
2412:             1             24  com.google.common.base.Predicates$ObjectPredicate$4
2413:             1             24  com.google.common.cache.CacheBuilder$NullListener
2414:             1             24  com.google.common.cache.CacheBuilder$OneWeigher
2415:             1             24  com.google.common.cache.LocalCache$EntryFactory$1
2416:             1             24  com.google.common.cache.LocalCache$EntryFactory$2
2417:             1             24  com.google.common.cache.LocalCache$EntryFactory$3
2418:             1             24  com.google.common.cache.LocalCache$EntryFactory$4
2419:             1             24  com.google.common.cache.LocalCache$EntryFactory$5
2420:             1             24  com.google.common.cache.LocalCache$EntryFactory$6
2421:             1             24  com.google.common.cache.LocalCache$EntryFactory$7
2422:             1             24  com.google.common.cache.LocalCache$EntryFactory$8
2423:             1             24  com.google.common.cache.LocalCache$Strength$1
2424:             1             24  com.google.common.cache.LocalCache$Strength$2
2425:             1             24  com.google.common.cache.LocalCache$Strength$3
2426:             1             24  com.google.common.collect.ByFunctionOrdering
2427:             1             24  com.google.common.collect.ConcurrentHashMultiset
2428:             1             24  com.google.common.collect.EmptyImmutableSortedSet
2429:             1             24  com.google.common.collect.GenericMapMaker$NullListener
2430:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$1
2431:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$2
2432:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$3
2433:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$4
2434:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$5
2435:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$6
2436:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$7
2437:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$8
2438:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$1
2439:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$2
2440:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$3
2441:             1             24  com.google.common.collect.Maps$EntryFunction$1
2442:             1             24  com.google.common.collect.Maps$EntryFunction$2
2443:             1             24  com.google.common.collect.Sets$3
2444:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$1
2445:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$2
2446:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$3
2447:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$1
2448:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$2
2449:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$3
2450:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$4
2451:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$5
2452:             1             24  com.google.common.util.concurrent.Futures$1$1
2453:             1             24  com.google.common.util.concurrent.Futures$ChainingListenableFuture$1
2454:             1             24  com.google.common.util.concurrent.MoreExecutors$DirectExecutor
2455:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$KeySetView
2456:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener
2457:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$1
2458:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$2
2459:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$3
2460:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$KeySet
2461:             1             24  com.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher
2462:             1             24  com.sun.beans.util.Cache$Kind$1
2463:             1             24  com.sun.beans.util.Cache$Kind$2
2464:             1             24  com.sun.beans.util.Cache$Kind$3
2465:             1             24  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport
2466:             1             24  com.sun.jmx.mbeanserver.MXBeanLookup
2467:             1             24  com.sun.jmx.remote.internal.ArrayNotificationBuffer$ShareBuffer
2468:             1             24  com.sun.jna.Structure$3
2469:             1             24  com.sun.org.apache.xerces.internal.impl.Constants$ArrayEnumeration
2470:             1             24  io.netty.buffer.UnpooledByteBufAllocator
2471:             1             24  io.netty.channel.AdaptiveRecvByteBufAllocator
2472:             1             24  io.netty.channel.SucceededChannelFuture
2473:             1             24  io.netty.channel.unix.Socket
2474:             1             24  io.netty.util.concurrent.FailedFuture
2475:             1             24  java.lang.ClassValue$Version
2476:             1             24  java.lang.Package$1
2477:             1             24  java.lang.ProcessEnvironment$StringEnvironment
2478:             1             24  java.lang.invoke.MethodHandleImpl$4
2479:             1             24  java.lang.invoke.MethodType$ConcurrentWeakInternSet
2480:             1             24  java.math.MutableBigInteger
2481:             1             24  java.net.Inet4AddressImpl
2482:             1             24  java.net.InterfaceAddress
2483:             1             24  java.nio.file.FileVisitOption
2484:             1             24  java.nio.file.LinkOption
2485:             1             24  java.security.CodeSigner
2486:             1             24  java.security.Policy$PolicyInfo
2487:             1             24  java.security.Policy$UnsupportedEmptyCollection
2488:             1             24  java.util.Collections$EmptyMap
2489:             1             24  java.util.Collections$UnmodifiableList
2490:             1             24  java.util.Comparators$NaturalOrderComparator
2491:             1             24  java.util.Currency
2492:             1             24  java.util.Locale$Cache
2493:             1             24  java.util.OptionalLong
2494:             1             24  java.util.ResourceBundle$Control$CandidateListCache
2495:             1             24  java.util.Vector$1
2496:             1             24  java.util.concurrent.Executors$DelegatedScheduledExecutorService
2497:             1             24  java.util.concurrent.TimeUnit$1
2498:             1             24  java.util.concurrent.TimeUnit$2
2499:             1             24  java.util.concurrent.TimeUnit$3
2500:             1             24  java.util.concurrent.TimeUnit$4
2501:             1             24  java.util.concurrent.TimeUnit$5
2502:             1             24  java.util.concurrent.TimeUnit$6
2503:             1             24  java.util.concurrent.TimeUnit$7
2504:             1             24  java.util.logging.LogManager$5
2505:             1             24  java.util.logging.LogManager$LoggerContext
2506:             1             24  java.util.logging.LoggingPermission
2507:             1             24  java.util.regex.Pattern$SingleI
2508:             1             24  javax.crypto.spec.RC2ParameterSpec
2509:             1             24  javax.management.NotificationBroadcasterSupport
2510:             1             24  javax.net.ssl.SSLContext
2511:             1             24  org.antlr.runtime.CharStreamState
2512:             1             24  org.apache.cassandra.auth.CassandraAuthorizer
2513:             1             24  org.apache.cassandra.auth.CassandraRoleManager$Role
2514:             1             24  org.apache.cassandra.auth.PasswordAuthenticator
2515:             1             24  org.apache.cassandra.cache.ChunkCache
2516:             1             24  org.apache.cassandra.config.Config$1
2517:             1             24  org.apache.cassandra.config.Config$RequestSchedulerId
2518:             1             24  org.apache.cassandra.config.RequestSchedulerOptions
2519:             1             24  org.apache.cassandra.cql3.Attributes$Raw
2520:             1             24  org.apache.cassandra.cql3.ColumnConditions
2521:             1             24  org.apache.cassandra.cql3.CqlParser
2522:             1             24  org.apache.cassandra.cql3.ErrorCollector
2523:             1             24  org.apache.cassandra.cql3.Lists$Marker
2524:             1             24  org.apache.cassandra.cql3.Maps$DiscarderByKey
2525:             1             24  org.apache.cassandra.cql3.Maps$Marker
2526:             1             24  org.apache.cassandra.cql3.Maps$Setter
2527:             1             24  org.apache.cassandra.cql3.Operator$1
2528:             1             24  org.apache.cassandra.cql3.Operator$10
2529:             1             24  org.apache.cassandra.cql3.Operator$11
2530:             1             24  org.apache.cassandra.cql3.Operator$12
2531:             1             24  org.apache.cassandra.cql3.Operator$13
2532:             1             24  org.apache.cassandra.cql3.Operator$14
2533:             1             24  org.apache.cassandra.cql3.Operator$15
2534:             1             24  org.apache.cassandra.cql3.Operator$2
2535:             1             24  org.apache.cassandra.cql3.Operator$3
2536:             1             24  org.apache.cassandra.cql3.Operator$4
2537:             1             24  org.apache.cassandra.cql3.Operator$5
2538:             1             24  org.apache.cassandra.cql3.Operator$6
2539:             1             24  org.apache.cassandra.cql3.Operator$7
2540:             1             24  org.apache.cassandra.cql3.Operator$8
2541:             1             24  org.apache.cassandra.cql3.Operator$9
2542:             1             24  org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance
2543:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$1
2544:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$10
2545:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$11
2546:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$12
2547:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$13
2548:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$14
2549:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$15
2550:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$16
2551:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$17
2552:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$18
2553:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$19
2554:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$2
2555:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$20
2556:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$21
2557:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$3
2558:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$4
2559:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$5
2560:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$6
2561:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$7
2562:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$8
2563:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$9
2564:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$3
2565:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$4
2566:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$1
2567:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$10
2568:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$11
2569:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$12
2570:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$2
2571:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$3
2572:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$6
2573:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$7
2574:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$8
2575:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$9
2576:             1             24  org.apache.cassandra.cql3.functions.UuidFcts$1
2577:             1             24  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$InRestrictionWithMarker
2578:             1             24  org.apache.cassandra.cql3.restrictions.TermSlice
2579:             1             24  org.apache.cassandra.cql3.restrictions.TokenRestriction$SliceRestriction
2580:             1             24  org.apache.cassandra.cql3.statements.StatementType$1
2581:             1             24  org.apache.cassandra.cql3.statements.StatementType$2
2582:             1             24  org.apache.cassandra.cql3.statements.StatementType$3
2583:             1             24  org.apache.cassandra.cql3.statements.StatementType$4
2584:             1             24  org.apache.cassandra.db.BlacklistedDirectories
2585:             1             24  org.apache.cassandra.db.Clustering$1
2586:             1             24  org.apache.cassandra.db.Clustering$2
2587:             1             24  org.apache.cassandra.db.Slice$1
2588:             1             24  org.apache.cassandra.db.commitlog.CommitLog$Configuration
2589:             1             24  org.apache.cassandra.db.compaction.CompactionLogger$CompactionLogSerializer
2590:             1             24  org.apache.cassandra.db.filter.DataLimits$1
2591:             1             24  org.apache.cassandra.db.filter.DataLimits$CQLLimits
2592:             1             24  org.apache.cassandra.db.marshal.AsciiType$1
2593:             1             24  org.apache.cassandra.db.marshal.BooleanType
2594:             1             24  org.apache.cassandra.db.marshal.ByteType
2595:             1             24  org.apache.cassandra.db.marshal.BytesType
2596:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$1
2597:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$2
2598:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$3
2599:             1             24  org.apache.cassandra.db.marshal.CounterColumnType
2600:             1             24  org.apache.cassandra.db.marshal.DecimalType
2601:             1             24  org.apache.cassandra.db.marshal.DoubleType
2602:             1             24  org.apache.cassandra.db.marshal.DurationType
2603:             1             24  org.apache.cassandra.db.marshal.EmptyType
2604:             1             24  org.apache.cassandra.db.marshal.FloatType
2605:             1             24  org.apache.cassandra.db.marshal.InetAddressType
2606:             1             24  org.apache.cassandra.db.marshal.Int32Type
2607:             1             24  org.apache.cassandra.db.marshal.IntegerType
2608:             1             24  org.apache.cassandra.db.marshal.LongType
2609:             1             24  org.apache.cassandra.db.marshal.ShortType
2610:             1             24  org.apache.cassandra.db.marshal.SimpleDateType
2611:             1             24  org.apache.cassandra.db.marshal.TimeType
2612:             1             24  org.apache.cassandra.db.marshal.TimeUUIDType
2613:             1             24  org.apache.cassandra.db.marshal.TimestampType
2614:             1             24  org.apache.cassandra.db.marshal.TypeParser
2615:             1             24  org.apache.cassandra.db.marshal.UTF8Type
2616:             1             24  org.apache.cassandra.db.marshal.UUIDType
2617:             1             24  org.apache.cassandra.db.transform.Stack
2618:             1             24  org.apache.cassandra.dht.Murmur3Partitioner
2619:             1             24  org.apache.cassandra.dht.Murmur3Partitioner$1
2620:             1             24  org.apache.cassandra.hints.HintsCatalog
2621:             1             24  org.apache.cassandra.hints.HintsWriteExecutor
2622:             1             24  org.apache.cassandra.io.compress.BufferType$1
2623:             1             24  org.apache.cassandra.io.compress.BufferType$2
2624:             1             24  org.apache.cassandra.io.util.ChecksumWriter
2625:             1             24  org.apache.cassandra.io.util.SequentialWriter$TransactionalProxy
2626:             1             24  org.apache.cassandra.io.util.SsdDiskOptimizationStrategy
2627:             1             24  org.apache.cassandra.locator.ReconnectableSnitchHelper
2628:             1             24  org.apache.cassandra.metrics.AuthMetrics
2629:             1             24  org.apache.cassandra.metrics.BufferPoolMetrics
2630:             1             24  org.apache.cassandra.metrics.CassandraMetricsRegistry
2631:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$1
2632:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$2
2633:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$3
2634:             1             24  org.apache.cassandra.metrics.CompactionMetrics$3
2635:             1             24  org.apache.cassandra.metrics.HintedHandoffMetrics
2636:             1             24  org.apache.cassandra.metrics.MessagingMetrics
2637:             1             24  org.apache.cassandra.net.MessagingService$Verb$1
2638:             1             24  org.apache.cassandra.net.MessagingService$Verb$10
2639:             1             24  org.apache.cassandra.net.MessagingService$Verb$11
2640:             1             24  org.apache.cassandra.net.MessagingService$Verb$12
2641:             1             24  org.apache.cassandra.net.MessagingService$Verb$13
2642:             1             24  org.apache.cassandra.net.MessagingService$Verb$2
2643:             1             24  org.apache.cassandra.net.MessagingService$Verb$3
2644:             1             24  org.apache.cassandra.net.MessagingService$Verb$4
2645:             1             24  org.apache.cassandra.net.MessagingService$Verb$5
2646:             1             24  org.apache.cassandra.net.MessagingService$Verb$6
2647:             1             24  org.apache.cassandra.net.MessagingService$Verb$7
2648:             1             24  org.apache.cassandra.net.MessagingService$Verb$8
2649:             1             24  org.apache.cassandra.net.MessagingService$Verb$9
2650:             1             24  org.apache.cassandra.service.CacheService
2651:             1             24  org.apache.cassandra.service.GCInspector
2652:             1             24  org.apache.cassandra.service.PendingRangeCalculatorService
2653:             1             24  org.apache.cassandra.service.QueryState
2654:             1             24  org.apache.cassandra.service.StartupChecks
2655:             1             24  org.apache.cassandra.service.StartupChecks$8
2656:             1             24  org.apache.cassandra.streaming.StreamManager
2657:             1             24  org.apache.cassandra.thrift.Cassandra$Processor
2658:             1             24  org.apache.cassandra.tracing.TracingImpl
2659:             1             24  org.apache.cassandra.transport.ConnectionLimitHandler
2660:             1             24  org.apache.cassandra.transport.Frame$Compressor
2661:             1             24  org.apache.cassandra.transport.Frame$Decompressor
2662:             1             24  org.apache.cassandra.transport.Frame$Encoder
2663:             1             24  org.apache.cassandra.transport.Message$Dispatcher
2664:             1             24  org.apache.cassandra.transport.Message$ProtocolDecoder
2665:             1             24  org.apache.cassandra.transport.Message$ProtocolEncoder
2666:             1             24  org.apache.cassandra.transport.RequestThreadPoolExecutor
2667:             1             24  org.apache.cassandra.transport.Server$ConnectionTracker
2668:             1             24  org.apache.cassandra.transport.Server$EventNotifier
2669:             1             24  org.apache.cassandra.transport.Server$Initializer
2670:             1             24  org.apache.cassandra.triggers.TriggerExecutor
2671:             1             24  org.apache.cassandra.utils.ChecksumType$1
2672:             1             24  org.apache.cassandra.utils.ChecksumType$2
2673:             1             24  org.apache.cassandra.utils.ConcurrentBiMap
2674:             1             24  org.apache.cassandra.utils.ExpiringMap$1
2675:             1             24  org.apache.cassandra.utils.HistogramBuilder
2676:             1             24  org.apache.cassandra.utils.IntervalTree
2677:             1             24  org.apache.cassandra.utils.JMXServerUtils$Registry
2678:             1             24  org.apache.cassandra.utils.concurrent.OpOrder$Barrier
2679:             1             24  org.apache.cassandra.utils.memory.BufferPool$Debug
2680:             1             24  org.apache.cassandra.utils.progress.jmx.JMXProgressSupport
2681:             1             24  org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport
2682:             1             24  org.codehaus.jackson.map.deser.BeanDeserializerFactory
2683:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory
2684:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory$ConfigImpl
2685:             1             24  org.codehaus.jackson.map.ser.impl.FailingSerializer
2686:             1             24  org.codehaus.jackson.map.ser.impl.SerializerCache
2687:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$BooleanArraySerializer
2688:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$DoubleArraySerializer
2689:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$FloatArraySerializer
2690:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$IntArraySerializer
2691:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$LongArraySerializer
2692:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$ShortArraySerializer
2693:             1             24  org.slf4j.helpers.FormattingTuple
2694:             1             24  org.slf4j.impl.StaticLoggerBinder
2695:             1             24  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.PercentEscaper
2696:             1             24  sun.instrument.TransformerManager
2697:             1             24  sun.launcher.LauncherHelper
2698:             1             24  sun.management.CompilationImpl
2699:             1             24  sun.management.GarbageCollectionNotifInfoCompositeData
2700:             1             24  sun.management.MemoryImpl
2701:             1             24  sun.management.OperatingSystemImpl
2702:             1             24  sun.management.RuntimeImpl
2703:             1             24  sun.management.ThreadImpl
2704:             1             24  sun.management.VMManagementImpl
2705:             1             24  sun.misc.JarIndex
2706:             1             24  sun.net.ProgressMonitor
2707:             1             24  sun.net.sdp.SdpProvider
2708:             1             24  sun.net.www.protocol.http.Handler
2709:             1             24  sun.nio.cs.ISO_8859_1
2710:             1             24  sun.nio.cs.US_ASCII
2711:             1             24  sun.nio.cs.UTF_16
2712:             1             24  sun.nio.cs.UTF_16BE
2713:             1             24  sun.nio.cs.UTF_16LE
2714:             1             24  sun.nio.cs.UTF_8
2715:             1             24  sun.rmi.runtime.RuntimeUtil$1
2716:             1             24  sun.rmi.server.LoaderHandler$1
2717:             1             24  sun.rmi.transport.DGCImpl
2718:             1             24  sun.rmi.transport.Target$$Lambda$338/684260999
2719:             1             24  sun.security.provider.certpath.X509CertPath
2720:             1             24  sun.security.ssl.SunX509KeyManagerImpl
2721:             1             24  sun.security.validator.EndEntityChecker
2722:             1             24  sun.security.x509.AccessDescription
2723:             1             24  sun.security.x509.CertificatePolicyMap
2724:             1             24  sun.util.locale.BaseLocale$Cache
2725:             1             24  sun.util.locale.provider.CalendarDataProviderImpl
2726:             1             24  sun.util.locale.provider.CalendarProviderImpl
2727:             1             24  sun.util.locale.provider.CurrencyNameProviderImpl
2728:             1             24  sun.util.locale.provider.DateFormatSymbolsProviderImpl
2729:             1             24  sun.util.locale.provider.DecimalFormatSymbolsProviderImpl
2730:             1             24  sun.util.locale.provider.NumberFormatProviderImpl
2731:             1             24  sun.util.logging.PlatformLogger
2732:             1             24  sun.util.logging.PlatformLogger$JavaLoggerProxy
2733:             1             24  sun.util.resources.LocaleData$1
2734:             1             16  [Lch.qos.logback.classic.spi.ThrowableProxy;
2735:             1             16  [Ljava.beans.EventSetDescriptor;
2736:             1             16  [Ljava.lang.Double;
2737:             1             16  [Ljava.lang.Float;
2738:             1             16  [Ljava.lang.Throwable;
2739:             1             16  [Ljava.net.NetworkInterface;
2740:             1             16  [Ljava.net.URL;
2741:             1             16  [Ljava.nio.file.attribute.FileAttribute;
2742:             1             16  [Ljava.security.Provider;
2743:             1             16  [Ljava.text.FieldPosition;
2744:             1             16  [Ljavax.security.cert.X509Certificate;
2745:             1             16  [Lnet.jpountz.lz4.LZ4JNI;
2746:             1             16  [Lnet.jpountz.lz4.LZ4Utils;
2747:             1             16  [Lnet.jpountz.util.ByteBufferUtils;
2748:             1             16  [Lnet.jpountz.util.Native;
2749:             1             16  [Lnet.jpountz.util.SafeUtils;
2750:             1             16  [Lnet.jpountz.xxhash.XXHashJNI;
2751:             1             16  [Lorg.apache.cassandra.db.rows.Cell;
2752:             1             16  [Lorg.apache.cassandra.db.transform.Stack$MoreContentsHolder;
2753:             1             16  [Lorg.codehaus.jackson.map.AbstractTypeResolver;
2754:             1             16  [Lorg.codehaus.jackson.map.Deserializers;
2755:             1             16  [Lorg.codehaus.jackson.map.KeyDeserializers;
2756:             1             16  [Lorg.codehaus.jackson.map.Serializers;
2757:             1             16  [Lorg.codehaus.jackson.map.deser.BeanDeserializerModifier;
2758:             1             16  [Lorg.codehaus.jackson.map.deser.ValueInstantiators;
2759:             1             16  [Lorg.codehaus.jackson.map.introspect.AnnotationMap;
2760:             1             16  [Lorg.codehaus.jackson.map.ser.BeanSerializerModifier;
2761:             1             16  [Lsun.instrument.TransformerManager$TransformerInfo;
2762:             1             16  ch.qos.logback.classic.selector.DefaultContextSelector
2763:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$1
2764:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$2
2765:             1             16  ch.qos.logback.core.joran.spi.DefaultNestedComponentRegistry
2766:             1             16  ch.qos.logback.core.joran.util.ConfigurationWatchListUtil
2767:             1             16  com.codahale.metrics.Clock$UserTimeClock
2768:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$1
2769:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$2
2770:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$3
2771:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$4
2772:             1             16  com.codahale.metrics.Striped64$ThreadHashCode
2773:             1             16  com.codahale.metrics.ThreadLocalRandom$1
2774:             1             16  com.github.benmanes.caffeine.SingleConsumerQueue$$Lambda$80/692511295
2775:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$79/608770405
2776:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache$$Lambda$81/1858886571
2777:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$EntrySetView
2778:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$KeySetView
2779:             1             16  com.github.benmanes.caffeine.cache.BoundedWeigher
2780:             1             16  com.github.benmanes.caffeine.cache.Caffeine$$Lambda$77/2064869182
2781:             1             16  com.google.common.base.Absent
2782:             1             16  com.google.common.base.CharMatcher$1
2783:             1             16  com.google.common.base.CharMatcher$15
2784:             1             16  com.google.common.base.CharMatcher$2
2785:             1             16  com.google.common.base.CharMatcher$3
2786:             1             16  com.google.common.base.CharMatcher$4
2787:             1             16  com.google.common.base.CharMatcher$5
2788:             1             16  com.google.common.base.CharMatcher$6
2789:             1             16  com.google.common.base.CharMatcher$7
2790:             1             16  com.google.common.base.CharMatcher$8
2791:             1             16  com.google.common.base.Equivalence$Equals
2792:             1             16  com.google.common.base.Equivalence$Identity
2793:             1             16  com.google.common.base.Predicates$NotPredicate
2794:             1             16  com.google.common.base.Predicates$OrPredicate
2795:             1             16  com.google.common.base.Suppliers$SupplierOfInstance
2796:             1             16  com.google.common.base.Ticker$1
2797:             1             16  com.google.common.cache.CacheBuilder$1
2798:             1             16  com.google.common.cache.CacheBuilder$2
2799:             1             16  com.google.common.cache.CacheBuilder$3
2800:             1             16  com.google.common.cache.LocalCache$1
2801:             1             16  com.google.common.cache.LocalCache$2
2802:             1             16  com.google.common.cache.LocalCache$LocalManualCache
2803:             1             16  com.google.common.collect.ComparatorOrdering
2804:             1             16  com.google.common.collect.EmptyImmutableSet
2805:             1             16  com.google.common.collect.Iterators$1
2806:             1             16  com.google.common.collect.Iterators$2
2807:             1             16  com.google.common.collect.MapMakerInternalMap$1
2808:             1             16  com.google.common.collect.MapMakerInternalMap$2
2809:             1             16  com.google.common.collect.Multisets$5
2810:             1             16  com.google.common.collect.NaturalOrdering
2811:             1             16  com.google.common.collect.ReverseOrdering
2812:             1             16  com.google.common.io.ByteStreams$1
2813:             1             16  com.google.common.util.concurrent.Futures$4
2814:             1             16  com.google.common.util.concurrent.Futures$7
2815:             1             16  com.google.common.util.concurrent.Runnables$1
2816:             1             16  com.google.common.util.concurrent.Striped$5
2817:             1             16  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingQueue
2818:             1             16  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ResourceContext$1
2819:             1             16  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory
2820:             1             16  com.sun.jmx.mbeanserver.DescriptorCache
2821:             1             16  com.sun.jmx.mbeanserver.MBeanAnalyzer$MethodOrder
2822:             1             16  com.sun.jmx.mbeanserver.MBeanInstantiator
2823:             1             16  com.sun.jmx.mbeanserver.MXBeanIntrospector
2824:             1             16  com.sun.jmx.mbeanserver.SecureClassLoaderRepository
2825:             1             16  com.sun.jmx.mbeanserver.StandardMBeanIntrospector
2826:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$5
2827:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BroadcasterQuery
2828:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BufferListener
2829:             1             16  com.sun.jmx.remote.internal.ServerCommunicatorAdmin$Timeout
2830:             1             16  com.sun.jmx.remote.internal.ServerNotifForwarder$NotifForwarderBufferFilter
2831:             1             16  com.sun.jmx.remote.protocol.iiop.IIOPProxyImpl
2832:             1             16  com.sun.jmx.remote.security.SubjectDelegator
2833:             1             16  com.sun.jna.Native$1
2834:             1             16  com.sun.jna.Native$2
2835:             1             16  com.sun.jna.Native$7
2836:             1             16  com.sun.jna.Structure$1
2837:             1             16  com.sun.jna.Structure$2
2838:             1             16  com.sun.jna.VarArgsChecker$RealVarArgsChecker
2839:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDDatatypeValidator
2840:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDREFDatatypeValidator
2841:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NMTOKENDatatypeValidator
2842:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NOTATIONDatatypeValidator
2843:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.StringDatatypeValidator
2844:             1             16  com.sun.org.apache.xerces.internal.utils.SecuritySupport
2845:             1             16  com.sun.proxy.$Proxy2
2846:             1             16  com.sun.proxy.$Proxy4
2847:             1             16  com.sun.proxy.$Proxy7
2848:             1             16  io.netty.buffer.ByteBufUtil$1
2849:             1             16  io.netty.buffer.ByteBufUtil$2
2850:             1             16  io.netty.channel.ChannelFutureListener$1
2851:             1             16  io.netty.channel.ChannelFutureListener$2
2852:             1             16  io.netty.channel.ChannelFutureListener$3
2853:             1             16  io.netty.channel.ChannelMetadata
2854:             1             16  io.netty.channel.ChannelOutboundBuffer$1
2855:             1             16  io.netty.channel.DefaultChannelPipeline$1
2856:             1             16  io.netty.channel.DefaultMessageSizeEstimator
2857:             1             16  io.netty.channel.DefaultMessageSizeEstimator$HandleImpl
2858:             1             16  io.netty.channel.DefaultSelectStrategy
2859:             1             16  io.netty.channel.DefaultSelectStrategyFactory
2860:             1             16  io.netty.channel.group.ChannelMatchers$1
2861:             1             16  io.netty.channel.group.ChannelMatchers$InvertMatcher
2862:             1             16  io.netty.util.Recycler$1
2863:             1             16  io.netty.util.Recycler$3
2864:             1             16  io.netty.util.concurrent.DefaultPromise$CauseHolder
2865:             1             16  io.netty.util.concurrent.GlobalEventExecutor$1
2866:             1             16  io.netty.util.concurrent.GlobalEventExecutor$TaskRunner
2867:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$1
2868:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$PowerOfTwoEventExecutorChooser
2869:             1             16  io.netty.util.concurrent.RejectedExecutionHandlers$1
2870:             1             16  io.netty.util.concurrent.SingleThreadEventExecutor$1
2871:             1             16  io.netty.util.internal.NoOpTypeParameterMatcher
2872:             1             16  java.io.DeleteOnExitHook$1
2873:             1             16  java.io.FileDescriptor$1
2874:             1             16  java.io.ObjectInputStream$$Lambda$293/697818519
2875:             1             16  java.io.ObjectInputStream$1
2876:             1             16  java.lang.ApplicationShutdownHooks$1
2877:             1             16  java.lang.CharacterDataLatin1
2878:             1             16  java.lang.ClassValue$Identity
2879:             1             16  java.lang.ProcessBuilder$NullInputStream
2880:             1             16  java.lang.ProcessBuilder$NullOutputStream
2881:             1             16  java.lang.Runtime
2882:             1             16  java.lang.String$CaseInsensitiveComparator
2883:             1             16  java.lang.System$2
2884:             1             16  java.lang.Terminator$1
2885:             1             16  java.lang.UNIXProcess$$Lambda$13/1784131088
2886:             1             16  java.lang.UNIXProcess$$Lambda$14/2143582219
2887:             1             16  java.lang.UNIXProcess$Platform$$Lambda$10/616881582
2888:             1             16  java.lang.invoke.MemberName$Factory
2889:             1             16  java.lang.invoke.MethodHandleImpl$2
2890:             1             16  java.lang.invoke.MethodHandleImpl$3
2891:             1             16  java.lang.management.PlatformComponent$1
2892:             1             16  java.lang.management.PlatformComponent$10
2893:             1             16  java.lang.management.PlatformComponent$11
2894:             1             16  java.lang.management.PlatformComponent$12
2895:             1             16  java.lang.management.PlatformComponent$13
2896:             1             16  java.lang.management.PlatformComponent$14
2897:             1             16  java.lang.management.PlatformComponent$15
2898:             1             16  java.lang.management.PlatformComponent$2
2899:             1             16  java.lang.management.PlatformComponent$3
2900:             1             16  java.lang.management.PlatformComponent$4
2901:             1             16  java.lang.management.PlatformComponent$5
2902:             1             16  java.lang.management.PlatformComponent$6
2903:             1             16  java.lang.management.PlatformComponent$7
2904:             1             16  java.lang.management.PlatformComponent$8
2905:             1             16  java.lang.management.PlatformComponent$9
2906:             1             16  java.lang.ref.Reference$1
2907:             1             16  java.lang.ref.Reference$Lock
2908:             1             16  java.lang.reflect.Proxy$KeyFactory
2909:             1             16  java.lang.reflect.Proxy$ProxyClassFactory
2910:             1             16  java.lang.reflect.ReflectAccess
2911:             1             16  java.math.BigDecimal$1
2912:             1             16  java.net.InetAddress$2
2913:             1             16  java.net.URLClassLoader$7
2914:             1             16  java.nio.Bits$1
2915:             1             16  java.nio.Bits$1$1
2916:             1             16  java.nio.charset.CoderResult$1
2917:             1             16  java.nio.charset.CoderResult$2
2918:             1             16  java.nio.file.Files$AcceptAllFilter
2919:             1             16  java.rmi.server.RMIClassLoader$2
2920:             1             16  java.security.AllPermission
2921:             1             16  java.security.ProtectionDomain$2
2922:             1             16  java.security.ProtectionDomain$JavaSecurityAccessImpl
2923:             1             16  java.text.DontCareFieldPosition$1
2924:             1             16  java.util.Collections$EmptyEnumeration
2925:             1             16  java.util.Collections$EmptyIterator
2926:             1             16  java.util.Collections$EmptyList
2927:             1             16  java.util.Collections$EmptySet
2928:             1             16  java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet
2929:             1             16  java.util.Currency$CurrencyNameGetter
2930:             1             16  java.util.EnumMap$1
2931:             1             16  java.util.ResourceBundle$Control
2932:             1             16  java.util.Spliterators$EmptySpliterator$OfDouble
2933:             1             16  java.util.Spliterators$EmptySpliterator$OfInt
2934:             1             16  java.util.Spliterators$EmptySpliterator$OfLong
2935:             1             16  java.util.Spliterators$EmptySpliterator$OfRef
2936:             1             16  java.util.TreeMap$EntrySpliterator$$Lambda$68/1819038759
2937:             1             16  java.util.WeakHashMap$KeySet
2938:             1             16  java.util.concurrent.Executors$FinalizableDelegatedExecutorService
2939:             1             16  java.util.concurrent.ThreadPoolExecutor$AbortPolicy
2940:             1             16  java.util.jar.JarVerifier$3
2941:             1             16  java.util.jar.JavaUtilJarAccessImpl
2942:             1             16  java.util.logging.LoggingProxyImpl
2943:             1             16  java.util.regex.Pattern$4
2944:             1             16  java.util.regex.Pattern$LastNode
2945:             1             16  java.util.regex.Pattern$Node
2946:             1             16  java.util.stream.Collectors$$Lambda$178/1708585783
2947:             1             16  java.util.stream.Collectors$$Lambda$179/2048467502
2948:             1             16  java.util.stream.Collectors$$Lambda$180/1269763229
2949:             1             16  java.util.stream.Collectors$$Lambda$221/1489469437
2950:             1             16  java.util.stream.Collectors$$Lambda$222/431613642
2951:             1             16  java.util.stream.Collectors$$Lambda$223/1098744211
2952:             1             16  java.util.stream.Collectors$$Lambda$247/1746129463
2953:             1             16  java.util.stream.Collectors$$Lambda$60/1724814719
2954:             1             16  java.util.stream.Collectors$$Lambda$61/1718322084
2955:             1             16  java.util.stream.Collectors$$Lambda$62/24039137
2956:             1             16  java.util.stream.Collectors$$Lambda$63/992086987
2957:             1             16  java.util.stream.LongPipeline$$Lambda$189/1888591113
2958:             1             16  java.util.stream.LongPipeline$$Lambda$325/1014276638
2959:             1             16  java.util.zip.ZipFile$1
2960:             1             16  javax.crypto.JceSecurityManager
2961:             1             16  javax.management.JMX
2962:             1             16  javax.management.MBeanServerBuilder
2963:             1             16  javax.management.NotificationBroadcasterSupport$1
2964:             1             16  javax.management.remote.JMXPrincipal
2965:             1             16  javax.management.remote.rmi.RMIConnectionImpl_Stub
2966:             1             16  javax.management.remote.rmi.RMIServerImpl_Stub
2967:             1             16  javax.xml.parsers.SecuritySupport
2968:             1             16  net.jpountz.lz4.LZ4JNICompressor
2969:             1             16  net.jpountz.lz4.LZ4JNIFastDecompressor
2970:             1             16  net.jpountz.lz4.LZ4JNISafeDecompressor
2971:             1             16  net.jpountz.xxhash.StreamingXXHash32JNI$Factory
2972:             1             16  net.jpountz.xxhash.StreamingXXHash64JNI$Factory
2973:             1             16  net.jpountz.xxhash.XXHash32JNI
2974:             1             16  net.jpountz.xxhash.XXHash64JNI
2975:             1             16  org.apache.cassandra.auth.AllowAllAuthenticator$Negotiator
2976:             1             16  org.apache.cassandra.auth.AllowAllInternodeAuthenticator
2977:             1             16  org.apache.cassandra.auth.AuthCache$1
2978:             1             16  org.apache.cassandra.auth.AuthMigrationListener
2979:             1             16  org.apache.cassandra.auth.CassandraRoleManager$$Lambda$264/195066780
2980:             1             16  org.apache.cassandra.auth.CassandraRoleManager$1
2981:             1             16  org.apache.cassandra.auth.CassandraRoleManager$2
2982:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$265/385180766
2983:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$266/694021194
2984:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$267/767298601
2985:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$268/274090580
2986:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$269/1588510401
2987:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$270/331234425
2988:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$271/996989596
2989:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$272/1507030140
2990:             1             16  org.apache.cassandra.batchlog.Batch$Serializer
2991:             1             16  org.apache.cassandra.batchlog.BatchRemoveVerbHandler
2992:             1             16  org.apache.cassandra.batchlog.BatchStoreVerbHandler
2993:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$258/2042553130
2994:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$290/1638031626
2995:             1             16  org.apache.cassandra.cache.AutoSavingCache$1
2996:             1             16  org.apache.cassandra.cache.ChunkCache$$Lambda$78/420307438
2997:             1             16  org.apache.cassandra.cache.NopCacheProvider$NopCache
2998:             1             16  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$1
2999:             1             16  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1
3000:             1             16  org.apache.cassandra.concurrent.StageManager$1
3001:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$213/1328645530
3002:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$214/2107098463
3003:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$232/1529326426
3004:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$233/570714518
3005:             1             16  org.apache.cassandra.config.CFMetaData$Builder$$Lambda$30/671596011
3006:             1             16  org.apache.cassandra.config.CFMetaData$Serializer
3007:             1             16  org.apache.cassandra.config.ColumnDefinition$$Lambda$25/207471778
3008:             1             16  org.apache.cassandra.config.DatabaseDescriptor$1
3009:             1             16  org.apache.cassandra.config.Schema$$Lambda$262/956354740
3010:             1             16  org.apache.cassandra.config.Schema$$Lambda$263/2080528880
3011:             1             16  org.apache.cassandra.cql3.ColumnConditions$$Lambda$116/841977955
3012:             1             16  org.apache.cassandra.cql3.Constants$1
3013:             1             16  org.apache.cassandra.cql3.Constants$NullLiteral
3014:             1             16  org.apache.cassandra.cql3.Constants$UnsetLiteral
3015:             1             16  org.apache.cassandra.cql3.Cql_Parser$1
3016:             1             16  org.apache.cassandra.cql3.IfExistsCondition
3017:             1             16  org.apache.cassandra.cql3.IfNotExistsCondition
3018:             1             16  org.apache.cassandra.cql3.QueryOptions$Codec
3019:             1             16  org.apache.cassandra.cql3.QueryProcessor
3020:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$17/951221468
3021:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$18/1046545660
3022:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$19/1545827753
3023:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$20/1611832218
3024:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$21/2027317551
3025:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$22/273077527
3026:             1             16  org.apache.cassandra.cql3.QueryProcessor$MigrationSubscriber
3027:             1             16  org.apache.cassandra.cql3.ResultSet$Codec
3028:             1             16  org.apache.cassandra.cql3.ResultSet$ResultMetadata$Codec
3029:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$41/1614133563
3030:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$42/839771540
3031:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$43/1751403001
3032:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$44/1756819670
3033:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$45/178604517
3034:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$46/1543518287
3035:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$47/464872674
3036:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$48/1659286984
3037:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$49/1793899405
3038:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager
3039:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$1
3040:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$2
3041:             1             16  org.apache.cassandra.cql3.restrictions.RestrictionSet$1
3042:             1             16  org.apache.cassandra.cql3.selection.Selection$1
3043:             1             16  org.apache.cassandra.cql3.statements.CreateTableStatement$$Lambda$23/1470868839
3044:             1             16  org.apache.cassandra.db.CBuilder$1
3045:             1             16  org.apache.cassandra.db.Clustering$Serializer
3046:             1             16  org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer
3047:             1             16  org.apache.cassandra.db.ClusteringPrefix$Serializer
3048:             1             16  org.apache.cassandra.db.ColumnFamilyStore$$Lambda$190/1269783694
3049:             1             16  org.apache.cassandra.db.ColumnFamilyStore$2
3050:             1             16  org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily
3051:             1             16  org.apache.cassandra.db.Columns$$Lambda$205/2092785251
3052:             1             16  org.apache.cassandra.db.Columns$Serializer
3053:             1             16  org.apache.cassandra.db.CounterMutation$CounterMutationSerializer
3054:             1             16  org.apache.cassandra.db.CounterMutationVerbHandler
3055:             1             16  org.apache.cassandra.db.DataRange$Serializer
3056:             1             16  org.apache.cassandra.db.DecoratedKey$1
3057:             1             16  org.apache.cassandra.db.DefinitionsUpdateVerbHandler
3058:             1             16  org.apache.cassandra.db.DeletionPurger$$Lambda$105/2116697030
3059:             1             16  org.apache.cassandra.db.DeletionTime$Serializer
3060:             1             16  org.apache.cassandra.db.Directories$3
3061:             1             16  org.apache.cassandra.db.Directories$DataDirectory
3062:             1             16  org.apache.cassandra.db.EmptyIterators$EmptyPartitionIterator
3063:             1             16  org.apache.cassandra.db.HintedHandOffManager
3064:             1             16  org.apache.cassandra.db.Keyspace$1
3065:             1             16  org.apache.cassandra.db.MigrationRequestVerbHandler
3066:             1             16  org.apache.cassandra.db.Mutation$MutationSerializer
3067:             1             16  org.apache.cassandra.db.MutationVerbHandler
3068:             1             16  org.apache.cassandra.db.PartitionPosition$RowPositionSerializer
3069:             1             16  org.apache.cassandra.db.PartitionRangeReadCommand$Deserializer
3070:             1             16  org.apache.cassandra.db.ReadCommand$1
3071:             1             16  org.apache.cassandra.db.ReadCommand$1WithoutPurgeableTombstones$$Lambda$110/208106294
3072:             1             16  org.apache.cassandra.db.ReadCommand$2
3073:             1             16  org.apache.cassandra.db.ReadCommand$3
3074:             1             16  org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer
3075:             1             16  org.apache.cassandra.db.ReadCommand$LegacyRangeSliceCommandSerializer
3076:             1             16  org.apache.cassandra.db.ReadCommand$LegacyReadCommandSerializer
3077:             1             16  org.apache.cassandra.db.ReadCommand$Serializer
3078:             1             16  org.apache.cassandra.db.ReadCommandVerbHandler
3079:             1             16  org.apache.cassandra.db.ReadQuery$1
3080:             1             16  org.apache.cassandra.db.ReadRepairVerbHandler
3081:             1             16  org.apache.cassandra.db.ReadResponse$1
3082:             1             16  org.apache.cassandra.db.ReadResponse$LegacyRangeSliceReplySerializer
3083:             1             16  org.apache.cassandra.db.ReadResponse$Serializer
3084:             1             16  org.apache.cassandra.db.SchemaCheckVerbHandler
3085:             1             16  org.apache.cassandra.db.SerializationHeader$Serializer
3086:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer
3087:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Group$$Lambda$106/1952605049
3088:             1             16  org.apache.cassandra.db.SizeEstimatesRecorder
3089:             1             16  org.apache.cassandra.db.Slice$Serializer
3090:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices
3091:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices$1
3092:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices
3093:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices$1
3094:             1             16  org.apache.cassandra.db.Slices$Serializer
3095:             1             16  org.apache.cassandra.db.SnapshotCommandSerializer
3096:             1             16  org.apache.cassandra.db.StorageHook$1
3097:             1             16  org.apache.cassandra.db.SystemKeyspace$$Lambda$186/1473888912
3098:             1             16  org.apache.cassandra.db.TruncateResponse$TruncateResponseSerializer
3099:             1             16  org.apache.cassandra.db.TruncateVerbHandler
3100:             1             16  org.apache.cassandra.db.TruncationSerializer
3101:             1             16  org.apache.cassandra.db.WriteResponse
3102:             1             16  org.apache.cassandra.db.WriteResponse$Serializer
3103:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$1
3104:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$Serializer
3105:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$$Lambda$72/500233312
3106:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1
3107:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogService$1
3108:             1             16  org.apache.cassandra.db.commitlog.CommitLog$$Lambda$227/2024217158
3109:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$1
3110:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$CommitLogPositionSerializer
3111:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$$Lambda$228/1186545861
3112:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator
3113:             1             16  org.apache.cassandra.db.commitlog.CommitLogSegment$$Lambda$175/1833918497
3114:             1             16  org.apache.cassandra.db.commitlog.IntervalSet$1
3115:             1             16  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool$1
3116:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$184/889018651
3117:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$185/638825183
3118:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$242/1509719872
3119:             1             16  org.apache.cassandra.db.compaction.CompactionManager$1
3120:             1             16  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController$$Lambda$307/363853319
3121:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$133/1728760599
3122:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$134/703363283
3123:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$172/1546684896
3124:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$85/654029265
3125:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$86/2030162789
3126:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$87/1306548322
3127:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$88/973942848
3128:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$89/558033602
3129:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$90/1361733480
3130:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$91/999951331
3131:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$92/1918201666
3132:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$93/1181004273
3133:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$95/1423931162
3134:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$96/1090942546
3135:             1             16  org.apache.cassandra.db.compaction.LeveledManifest$1
3136:             1             16  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy$1
3137:             1             16  org.apache.cassandra.db.context.CounterContext
3138:             1             16  org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer
3139:             1             16  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$NamesDeserializer
3140:             1             16  org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer
3141:             1             16  org.apache.cassandra.db.filter.ColumnFilter$Serializer
3142:             1             16  org.apache.cassandra.db.filter.DataLimits$Serializer
3143:             1             16  org.apache.cassandra.db.filter.RowFilter$CQLFilter
3144:             1             16  org.apache.cassandra.db.filter.RowFilter$Serializer
3145:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$58/435914790
3146:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$59/1273958371
3147:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$64/731243659
3148:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$66/1037955032
3149:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$70/331596257
3150:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$165/1814072734
3151:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$203/2022031193
3152:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$204/1336053009
3153:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$140/1142908098
3154:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$141/423008343
3155:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$142/88843440
3156:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$177/1035048662
3157:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$162/1676168006
3158:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$166/1882192501
3159:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$168/700891016
3160:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$52/894421232
3161:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$54/276869158
3162:             1             16  org.apache.cassandra.db.lifecycle.Tracker$$Lambda$170/1786214274
3163:             1             16  org.apache.cassandra.db.marshal.CollectionType$CollectionPathSerializer
3164:             1             16  org.apache.cassandra.db.monitoring.ApproximateTime$$Lambda$108/2001863314
3165:             1             16  org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer
3166:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$$Lambda$107/2345640
3167:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer
3168:             1             16  org.apache.cassandra.db.rows.AbstractTypeVersionComparator
3169:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$118/474868079
3170:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$123/164389557
3171:             1             16  org.apache.cassandra.db.rows.Cell$$Lambda$101/1913147328
3172:             1             16  org.apache.cassandra.db.rows.Cell$Serializer
3173:             1             16  org.apache.cassandra.db.rows.ColumnData$$Lambda$28/494077446
3174:             1             16  org.apache.cassandra.db.rows.EncodingStats$Serializer
3175:             1             16  org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer
3176:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer
3177:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$194/5263871
3178:             1             16  org.apache.cassandra.db.view.View$$Lambda$219/1557380482
3179:             1             16  org.apache.cassandra.dht.BootStrapper$StringSerializer
3180:             1             16  org.apache.cassandra.dht.Murmur3Partitioner$2
3181:             1             16  org.apache.cassandra.dht.StreamStateStore
3182:             1             16  org.apache.cassandra.dht.Token$TokenSerializer
3183:             1             16  org.apache.cassandra.gms.EchoMessage
3184:             1             16  org.apache.cassandra.gms.EchoMessage$EchoMessageSerializer
3185:             1             16  org.apache.cassandra.gms.EndpointStateSerializer
3186:             1             16  org.apache.cassandra.gms.GossipDigestAck2Serializer
3187:             1             16  org.apache.cassandra.gms.GossipDigestAck2VerbHandler
3188:             1             16  org.apache.cassandra.gms.GossipDigestAckSerializer
3189:             1             16  org.apache.cassandra.gms.GossipDigestAckVerbHandler
3190:             1             16  org.apache.cassandra.gms.GossipDigestSerializer
3191:             1             16  org.apache.cassandra.gms.GossipDigestSynSerializer
3192:             1             16  org.apache.cassandra.gms.GossipDigestSynVerbHandler
3193:             1             16  org.apache.cassandra.gms.GossipShutdownVerbHandler
3194:             1             16  org.apache.cassandra.gms.Gossiper$1
3195:             1             16  org.apache.cassandra.gms.Gossiper$GossipTask
3196:             1             16  org.apache.cassandra.gms.HeartBeatStateSerializer
3197:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueFactory
3198:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueSerializer
3199:             1             16  org.apache.cassandra.hints.EncodedHintMessage$Serializer
3200:             1             16  org.apache.cassandra.hints.Hint$Serializer
3201:             1             16  org.apache.cassandra.hints.HintMessage$Serializer
3202:             1             16  org.apache.cassandra.hints.HintResponse
3203:             1             16  org.apache.cassandra.hints.HintResponse$Serializer
3204:             1             16  org.apache.cassandra.hints.HintVerbHandler
3205:             1             16  org.apache.cassandra.hints.HintsBuffer$$Lambda$327/1070755303
3206:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$244/955891688
3207:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$245/1579667951
3208:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$246/2099786968
3209:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$282/2033605821
3210:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$283/1986677941
3211:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$284/355640298
3212:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$250/1791992279
3213:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$251/1557383930
3214:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$252/763495689
3215:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$318/991892116
3216:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$322/1059094831
3217:             1             16  org.apache.cassandra.hints.HintsWriteExecutor$FsyncWritersTask$$Lambda$289/2053564305
3218:             1             16  org.apache.cassandra.index.Index$CollatedViewIndexBuildingSupport
3219:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$152/111521464
3220:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$153/118079547
3221:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$182/992085984
3222:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$188/887656608
3223:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$312/1070341018
3224:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$1
3225:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$2
3226:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$3
3227:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$4
3228:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$5
3229:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$6
3230:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$7
3231:             1             16  org.apache.cassandra.index.transactions.UpdateTransaction$1
3232:             1             16  org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer
3233:             1             16  org.apache.cassandra.io.compress.SnappyCompressor
3234:             1             16  org.apache.cassandra.io.sstable.Descriptor$$Lambda$71/999647352
3235:             1             16  org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer
3236:             1             16  org.apache.cassandra.io.sstable.IndexSummaryManager$1
3237:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$73/1687768728
3238:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$74/15478307
3239:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$75/1394837936
3240:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$1
3241:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$Equals
3242:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThan
3243:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThanOrEqualTo
3244:             1             16  org.apache.cassandra.io.sstable.format.SSTableReadsListener$1
3245:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$160/1520196427
3246:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$311/1357900831
3247:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat
3248:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory
3249:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory
3250:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$150/504911193
3251:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$151/451889382
3252:             1             16  org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer
3253:             1             16  org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer
3254:             1             16  org.apache.cassandra.io.sstable.metadata.ValidationMetadata$ValidationMetadataSerializer
3255:             1             16  org.apache.cassandra.io.util.DataOutputBuffer$1
3256:             1             16  org.apache.cassandra.io.util.DataOutputStreamPlus$1
3257:             1             16  org.apache.cassandra.io.util.FileHandle$$Lambda$158/795408782
3258:             1             16  org.apache.cassandra.io.util.MmappedRegions$State$$Lambda$197/1396226930
3259:             1             16  org.apache.cassandra.io.util.Rebufferer$1
3260:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$1
3261:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$2
3262:             1             16  org.apache.cassandra.locator.EndpointSnitchInfo
3263:             1             16  org.apache.cassandra.locator.PendingRangeMaps$1
3264:             1             16  org.apache.cassandra.locator.PendingRangeMaps$2
3265:             1             16  org.apache.cassandra.locator.PendingRangeMaps$3
3266:             1             16  org.apache.cassandra.locator.PendingRangeMaps$4
3267:             1             16  org.apache.cassandra.locator.PropertyFileSnitch
3268:             1             16  org.apache.cassandra.locator.PropertyFileSnitch$1
3269:             1             16  org.apache.cassandra.locator.SimpleSeedProvider
3270:             1             16  org.apache.cassandra.locator.TokenMetadata$1
3271:             1             16  org.apache.cassandra.metrics.BufferPoolMetrics$1
3272:             1             16  org.apache.cassandra.metrics.CQLMetrics$1
3273:             1             16  org.apache.cassandra.metrics.CQLMetrics$2
3274:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$82/1609657810
3275:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$83/2101898459
3276:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$84/342161168
3277:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$1
3278:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$2
3279:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$3
3280:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$4
3281:             1             16  org.apache.cassandra.metrics.ClientMetrics
3282:             1             16  org.apache.cassandra.metrics.CompactionMetrics$1
3283:             1             16  org.apache.cassandra.metrics.CompactionMetrics$2
3284:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$1
3285:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$2
3286:             1             16  org.apache.cassandra.metrics.TableMetrics$1
3287:             1             16  org.apache.cassandra.metrics.TableMetrics$13
3288:             1             16  org.apache.cassandra.metrics.TableMetrics$18
3289:             1             16  org.apache.cassandra.metrics.TableMetrics$20
3290:             1             16  org.apache.cassandra.metrics.TableMetrics$22
3291:             1             16  org.apache.cassandra.metrics.TableMetrics$26
3292:             1             16  org.apache.cassandra.metrics.TableMetrics$28
3293:             1             16  org.apache.cassandra.metrics.ViewWriteMetrics$1
3294:             1             16  org.apache.cassandra.net.IAsyncCallback$1
3295:             1             16  org.apache.cassandra.net.MessagingService$4
3296:             1             16  org.apache.cassandra.net.MessagingService$5
3297:             1             16  org.apache.cassandra.net.MessagingService$CallbackDeterminedSerializer
3298:             1             16  org.apache.cassandra.notifications.SSTableDeletingNotification
3299:             1             16  org.apache.cassandra.repair.NodePair$NodePairSerializer
3300:             1             16  org.apache.cassandra.repair.RepairJobDesc$RepairJobDescSerializer
3301:             1             16  org.apache.cassandra.repair.RepairMessageVerbHandler
3302:             1             16  org.apache.cassandra.repair.messages.AnticompactionRequest$AnticompactionRequestSerializer
3303:             1             16  org.apache.cassandra.repair.messages.CleanupMessage$CleanupMessageSerializer
3304:             1             16  org.apache.cassandra.repair.messages.PrepareMessage$PrepareMessageSerializer
3305:             1             16  org.apache.cassandra.repair.messages.RepairMessage$RepairMessageSerializer
3306:             1             16  org.apache.cassandra.repair.messages.SnapshotMessage$SnapshotMessageSerializer
3307:             1             16  org.apache.cassandra.repair.messages.SyncComplete$SyncCompleteSerializer
3308:             1             16  org.apache.cassandra.repair.messages.SyncRequest$SyncRequestSerializer
3309:             1             16  org.apache.cassandra.repair.messages.ValidationComplete$ValidationCompleteSerializer
3310:             1             16  org.apache.cassandra.repair.messages.ValidationRequest$ValidationRequestSerializer
3311:             1             16  org.apache.cassandra.scheduler.NoScheduler
3312:             1             16  org.apache.cassandra.schema.CQLTypeParser$$Lambda$207/2843617
3313:             1             16  org.apache.cassandra.schema.CompressionParams$Serializer
3314:             1             16  org.apache.cassandra.schema.Functions$$Lambda$236/1017996482
3315:             1             16  org.apache.cassandra.schema.Functions$$Lambda$237/2135117754
3316:             1             16  org.apache.cassandra.schema.Functions$$Lambda$239/854637578
3317:             1             16  org.apache.cassandra.schema.Functions$$Lambda$240/305461269
3318:             1             16  org.apache.cassandra.schema.Functions$Builder$$Lambda$36/146874094
3319:             1             16  org.apache.cassandra.schema.IndexMetadata$Serializer
3320:             1             16  org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$132/399524457
3321:             1             16  org.apache.cassandra.schema.SchemaKeyspace$$Lambda$216/2137640552
3322:             1             16  org.apache.cassandra.schema.Types$RawBuilder$$Lambda$206/1399449613
3323:             1             16  org.apache.cassandra.schema.Types$RawBuilder$RawUDT$$Lambda$210/2069170964
3324:             1             16  org.apache.cassandra.schema.Views$$Lambda$50/1348115836
3325:             1             16  org.apache.cassandra.serializers.BooleanSerializer
3326:             1             16  org.apache.cassandra.serializers.ByteSerializer
3327:             1             16  org.apache.cassandra.serializers.BytesSerializer
3328:             1             16  org.apache.cassandra.serializers.DecimalSerializer
3329:             1             16  org.apache.cassandra.serializers.DoubleSerializer
3330:             1             16  org.apache.cassandra.serializers.InetAddressSerializer
3331:             1             16  org.apache.cassandra.serializers.Int32Serializer
3332:             1             16  org.apache.cassandra.serializers.LongSerializer
3333:             1             16  org.apache.cassandra.serializers.TimeUUIDSerializer
3334:             1             16  org.apache.cassandra.serializers.TimestampSerializer
3335:             1             16  org.apache.cassandra.serializers.TimestampSerializer$1
3336:             1             16  org.apache.cassandra.serializers.TimestampSerializer$2
3337:             1             16  org.apache.cassandra.serializers.TimestampSerializer$3
3338:             1             16  org.apache.cassandra.serializers.UTF8Serializer
3339:             1             16  org.apache.cassandra.serializers.UUIDSerializer
3340:             1             16  org.apache.cassandra.service.CacheService$CounterCacheSerializer
3341:             1             16  org.apache.cassandra.service.CacheService$KeyCacheSerializer
3342:             1             16  org.apache.cassandra.service.CacheService$RowCacheSerializer
3343:             1             16  org.apache.cassandra.service.CassandraDaemon$$Lambda$273/1244026033
3344:             1             16  org.apache.cassandra.service.CassandraDaemon$1
3345:             1             16  org.apache.cassandra.service.CassandraDaemon$2
3346:             1             16  org.apache.cassandra.service.CassandraDaemon$NativeAccess
3347:             1             16  org.apache.cassandra.service.ClientState$$Lambda$97/466481125
3348:             1             16  org.apache.cassandra.service.ClientWarn
3349:             1             16  org.apache.cassandra.service.DefaultFSErrorHandler
3350:             1             16  org.apache.cassandra.service.EchoVerbHandler
3351:             1             16  org.apache.cassandra.service.LoadBroadcaster
3352:             1             16  org.apache.cassandra.service.LoadBroadcaster$1
3353:             1             16  org.apache.cassandra.service.MigrationManager
3354:             1             16  org.apache.cassandra.service.MigrationManager$MigrationsSerializer
3355:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$277/794251840
3356:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$279/1246696592
3357:             1             16  org.apache.cassandra.service.PendingRangeCalculatorService$1
3358:             1             16  org.apache.cassandra.service.SnapshotVerbHandler
3359:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$1/1204167249
3360:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$114/1819989346
3361:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$2/1615780336
3362:             1             16  org.apache.cassandra.service.StartupChecks$1
3363:             1             16  org.apache.cassandra.service.StartupChecks$10
3364:             1             16  org.apache.cassandra.service.StartupChecks$11
3365:             1             16  org.apache.cassandra.service.StartupChecks$12
3366:             1             16  org.apache.cassandra.service.StartupChecks$2
3367:             1             16  org.apache.cassandra.service.StartupChecks$3
3368:             1             16  org.apache.cassandra.service.StartupChecks$4
3369:             1             16  org.apache.cassandra.service.StartupChecks$5
3370:             1             16  org.apache.cassandra.service.StartupChecks$6
3371:             1             16  org.apache.cassandra.service.StartupChecks$7
3372:             1             16  org.apache.cassandra.service.StartupChecks$9
3373:             1             16  org.apache.cassandra.service.StorageProxy
3374:             1             16  org.apache.cassandra.service.StorageProxy$1
3375:             1             16  org.apache.cassandra.service.StorageProxy$2
3376:             1             16  org.apache.cassandra.service.StorageProxy$3
3377:             1             16  org.apache.cassandra.service.StorageProxy$4
3378:             1             16  org.apache.cassandra.service.StorageService$$Lambda$259/1361973748
3379:             1             16  org.apache.cassandra.service.StorageService$1
3380:             1             16  org.apache.cassandra.service.paxos.Commit$CommitSerializer
3381:             1             16  org.apache.cassandra.service.paxos.CommitVerbHandler
3382:             1             16  org.apache.cassandra.service.paxos.PrepareResponse$PrepareResponseSerializer
3383:             1             16  org.apache.cassandra.service.paxos.PrepareVerbHandler
3384:             1             16  org.apache.cassandra.service.paxos.ProposeVerbHandler
3385:             1             16  org.apache.cassandra.streaming.ReplicationFinishedVerbHandler
3386:             1             16  org.apache.cassandra.streaming.StreamHook$1
3387:             1             16  org.apache.cassandra.streaming.StreamRequest$StreamRequestSerializer
3388:             1             16  org.apache.cassandra.streaming.StreamSummary$StreamSummarySerializer
3389:             1             16  org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer
3390:             1             16  org.apache.cassandra.streaming.messages.CompleteMessage$1
3391:             1             16  org.apache.cassandra.streaming.messages.FileMessageHeader$FileMessageHeaderSerializer
3392:             1             16  org.apache.cassandra.streaming.messages.IncomingFileMessage$1
3393:             1             16  org.apache.cassandra.streaming.messages.KeepAliveMessage$1
3394:             1             16  org.apache.cassandra.streaming.messages.OutgoingFileMessage$1
3395:             1             16  org.apache.cassandra.streaming.messages.PrepareMessage$1
3396:             1             16  org.apache.cassandra.streaming.messages.ReceivedMessage$1
3397:             1             16  org.apache.cassandra.streaming.messages.RetryMessage$1
3398:             1             16  org.apache.cassandra.streaming.messages.SessionFailedMessage$1
3399:             1             16  org.apache.cassandra.streaming.messages.StreamInitMessage$StreamInitMessageSerializer
3400:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$add
3401:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$atomic_batch_mutate
3402:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate
3403:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$cas
3404:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_cluster_name
3405:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace
3406:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspaces
3407:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_local_ring
3408:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_partitioner
3409:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_ring
3410:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_schema_versions
3411:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_snitch
3412:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits
3413:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits_ex
3414:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_token_map
3415:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_version
3416:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query
3417:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query
3418:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query
3419:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql_query
3420:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get
3421:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_count
3422:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices
3423:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_multi_slice
3424:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice
3425:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices
3426:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_slice
3427:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$insert
3428:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$login
3429:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_count
3430:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice
3431:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql3_query
3432:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query
3433:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove
3434:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove_counter
3435:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_cql_version
3436:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_keyspace
3437:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family
3438:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace
3439:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family
3440:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace
3441:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family
3442:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_keyspace
3443:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$trace_next_query
3444:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$truncate
3445:             1             16  org.apache.cassandra.thrift.CassandraServer
3446:             1             16  org.apache.cassandra.thrift.CassandraServer$1
3447:             1             16  org.apache.cassandra.transport.CBUtil$1
3448:             1             16  org.apache.cassandra.transport.Message$ExceptionHandler
3449:             1             16  org.apache.cassandra.transport.Server$1
3450:             1             16  org.apache.cassandra.transport.messages.AuthChallenge$1
3451:             1             16  org.apache.cassandra.transport.messages.AuthResponse$1
3452:             1             16  org.apache.cassandra.transport.messages.AuthSuccess$1
3453:             1             16  org.apache.cassandra.transport.messages.AuthenticateMessage$1
3454:             1             16  org.apache.cassandra.transport.messages.BatchMessage$1
3455:             1             16  org.apache.cassandra.transport.messages.CredentialsMessage$1
3456:             1             16  org.apache.cassandra.transport.messages.ErrorMessage$1
3457:             1             16  org.apache.cassandra.transport.messages.EventMessage$1
3458:             1             16  org.apache.cassandra.transport.messages.ExecuteMessage$1
3459:             1             16  org.apache.cassandra.transport.messages.OptionsMessage$1
3460:             1             16  org.apache.cassandra.transport.messages.PrepareMessage$1
3461:             1             16  org.apache.cassandra.transport.messages.QueryMessage$1
3462:             1             16  org.apache.cassandra.transport.messages.ReadyMessage$1
3463:             1             16  org.apache.cassandra.transport.messages.RegisterMessage$1
3464:             1             16  org.apache.cassandra.transport.messages.ResultMessage$1
3465:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Prepared$1
3466:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Rows$1
3467:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SchemaChange$1
3468:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SetKeyspace$1
3469:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Void$1
3470:             1             16  org.apache.cassandra.transport.messages.StartupMessage$1
3471:             1             16  org.apache.cassandra.transport.messages.SupportedMessage$1
3472:             1             16  org.apache.cassandra.utils.AlwaysPresentFilter
3473:             1             16  org.apache.cassandra.utils.AsymmetricOrdering$Reversed
3474:             1             16  org.apache.cassandra.utils.BloomFilter$1
3475:             1             16  org.apache.cassandra.utils.BooleanSerializer
3476:             1             16  org.apache.cassandra.utils.Clock
3477:             1             16  org.apache.cassandra.utils.CoalescingStrategies$1
3478:             1             16  org.apache.cassandra.utils.CoalescingStrategies$2
3479:             1             16  org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer
3480:             1             16  org.apache.cassandra.utils.FBUtilities$1
3481:             1             16  org.apache.cassandra.utils.FastByteOperations$UnsafeOperations
3482:             1             16  org.apache.cassandra.utils.Interval$1
3483:             1             16  org.apache.cassandra.utils.Interval$2
3484:             1             16  org.apache.cassandra.utils.JMXServerUtils$Exporter
3485:             1             16  org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper
3486:             1             16  org.apache.cassandra.utils.JVMStabilityInspector$Killer
3487:             1             16  org.apache.cassandra.utils.MerkleTree$Hashable$HashableSerializer
3488:             1             16  org.apache.cassandra.utils.MerkleTree$Inner$InnerSerializer
3489:             1             16  org.apache.cassandra.utils.MerkleTree$Leaf$LeafSerializer
3490:             1             16  org.apache.cassandra.utils.MerkleTree$MerkleTreeSerializer
3491:             1             16  org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer
3492:             1             16  org.apache.cassandra.utils.NanoTimeToCurrentTimeMillis$$Lambda$255/703776031
3493:             1             16  org.apache.cassandra.utils.NativeLibraryLinux
3494:             1             16  org.apache.cassandra.utils.NoSpamLogger$1
3495:             1             16  org.apache.cassandra.utils.StreamingHistogram$$Lambda$76/244613162
3496:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$136/1321552491
3497:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$137/732447846
3498:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramSerializer
3499:             1             16  org.apache.cassandra.utils.SystemTimeSource
3500:             1             16  org.apache.cassandra.utils.UUIDGen
3501:             1             16  org.apache.cassandra.utils.UUIDSerializer
3502:             1             16  org.apache.cassandra.utils.btree.BTree$$Lambda$193/1448037571
3503:             1             16  org.apache.cassandra.utils.btree.UpdateFunction$$Lambda$29/24650043
3504:             1             16  org.apache.cassandra.utils.concurrent.Ref$ReferenceReaper
3505:             1             16  org.apache.cassandra.utils.memory.BufferPool$1
3506:             1             16  org.apache.cassandra.utils.memory.BufferPool$2
3507:             1             16  org.apache.cassandra.utils.memory.HeapAllocator
3508:             1             16  org.apache.cassandra.utils.vint.VIntCoding$1
3509:             1             16  org.apache.thrift.TProcessorFactory
3510:             1             16  org.apache.thrift.transport.TFramedTransport$Factory
3511:             1             16  org.cliffc.high_scale_lib.NonBlockingHashMap$Prime
3512:             1             16  org.cliffc.high_scale_lib.NonBlockingHashSet
3513:             1             16  org.codehaus.jackson.map.deser.std.AtomicBooleanDeserializer
3514:             1             16  org.codehaus.jackson.map.deser.std.ClassDeserializer
3515:             1             16  org.codehaus.jackson.map.deser.std.DateDeserializer
3516:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$CurrencyDeserializer
3517:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$InetAddressDeserializer
3518:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$LocaleDeserializer
3519:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$PatternDeserializer
3520:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$TimeZoneDeserializer
3521:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URIDeserializer
3522:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URLDeserializer
3523:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$UUIDDeserializer
3524:             1             16  org.codehaus.jackson.map.deser.std.JavaTypeDeserializer
3525:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers
3526:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$BooleanDeser
3527:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ByteDeser
3528:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$CharDeser
3529:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$DoubleDeser
3530:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$FloatDeser
3531:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$IntDeser
3532:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$LongDeser
3533:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ShortDeser
3534:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$StringDeser
3535:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigDecimalDeserializer
3536:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigIntegerDeserializer
3537:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$NumberDeserializer
3538:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$SqlDateDeserializer
3539:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$StackTraceElementDeserializer
3540:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$BoolKD
3541:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$ByteKD
3542:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$CharKD
3543:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$DoubleKD
3544:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$FloatKD
3545:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$IntKD
3546:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$LongKD
3547:             1             16  org.codehaus.jackson.map.deser.std.StringDeserializer
3548:             1             16  org.codehaus.jackson.map.deser.std.TimestampDeserializer
3549:             1             16  org.codehaus.jackson.map.deser.std.TokenBufferDeserializer
3550:             1             16  org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer
3551:             1             16  org.codehaus.jackson.map.ext.OptionalHandlerFactory
3552:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector
3553:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$GetterMethodFilter
3554:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$MinimalMethodFilter
3555:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterAndGetterMethodFilter
3556:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterMethodFilter
3557:             1             16  org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector
3558:             1             16  org.codehaus.jackson.map.ser.StdSerializers$DoubleSerializer
3559:             1             16  org.codehaus.jackson.map.ser.StdSerializers$FloatSerializer
3560:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntLikeSerializer
3561:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntegerSerializer
3562:             1             16  org.codehaus.jackson.map.ser.StdSerializers$LongSerializer
3563:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlDateSerializer
3564:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlTimeSerializer
3565:             1             16  org.codehaus.jackson.map.ser.impl.UnknownSerializer
3566:             1             16  org.codehaus.jackson.map.ser.std.CalendarSerializer
3567:             1             16  org.codehaus.jackson.map.ser.std.DateSerializer
3568:             1             16  org.codehaus.jackson.map.ser.std.NullSerializer
3569:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$ByteArraySerializer
3570:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$CharArraySerializer
3571:             1             16  org.codehaus.jackson.map.ser.std.StringSerializer
3572:             1             16  org.codehaus.jackson.map.ser.std.ToStringSerializer
3573:             1             16  org.codehaus.jackson.map.type.TypeParser
3574:             1             16  org.codehaus.jackson.node.JsonNodeFactory
3575:             1             16  org.github.jamm.MemoryLayoutSpecification$2
3576:             1             16  org.github.jamm.MemoryMeter$1
3577:             1             16  org.github.jamm.NoopMemoryMeterListener
3578:             1             16  org.github.jamm.NoopMemoryMeterListener$1
3579:             1             16  org.slf4j.helpers.NOPLoggerFactory
3580:             1             16  org.slf4j.helpers.SubstituteLoggerFactory
3581:             1             16  org.slf4j.impl.StaticMDCBinder
3582:             1             16  org.xerial.snappy.SnappyNative
3583:             1             16  org.yaml.snakeyaml.constructor.SafeConstructor$ConstructUndefined
3584:             1             16  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.UnicodeEscaper$2
3585:             1             16  sun.management.ClassLoadingImpl
3586:             1             16  sun.management.HotSpotDiagnostic
3587:             1             16  sun.management.ManagementFactoryHelper$PlatformLoggingImpl
3588:             1             16  sun.misc.ASCIICaseInsensitiveComparator
3589:             1             16  sun.misc.FloatingDecimal$1
3590:             1             16  sun.misc.FormattedFloatingDecimal$1
3591:             1             16  sun.misc.Launcher
3592:             1             16  sun.misc.Launcher$Factory
3593:             1             16  sun.misc.ObjectInputFilter$Config$$Lambda$294/1344368391
3594:             1             16  sun.misc.Perf
3595:             1             16  sun.misc.Unsafe
3596:             1             16  sun.net.DefaultProgressMeteringPolicy
3597:             1             16  sun.net.ExtendedOptionsImpl$$Lambda$253/1943122657
3598:             1             16  sun.net.www.protocol.file.Handler
3599:             1             16  sun.net.www.protocol.jar.JarFileFactory
3600:             1             16  sun.nio.ch.EPollSelectorProvider
3601:             1             16  sun.nio.ch.ExtendedSocketOption$1
3602:             1             16  sun.nio.ch.FileChannelImpl$1
3603:             1             16  sun.nio.ch.Net$1
3604:             1             16  sun.nio.ch.Util$1
3605:             1             16  sun.nio.fs.LinuxFileSystemProvider
3606:             1             16  sun.reflect.GeneratedConstructorAccessor12
3607:             1             16  sun.reflect.GeneratedConstructorAccessor18
3608:             1             16  sun.reflect.GeneratedMethodAccessor10
3609:             1             16  sun.reflect.GeneratedMethodAccessor11
3610:             1             16  sun.reflect.GeneratedMethodAccessor12
3611:             1             16  sun.reflect.GeneratedMethodAccessor13
3612:             1             16  sun.reflect.GeneratedMethodAccessor14
3613:             1             16  sun.reflect.GeneratedMethodAccessor15
3614:             1             16  sun.reflect.GeneratedMethodAccessor6
3615:             1             16  sun.reflect.GeneratedMethodAccessor7
3616:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor36
3617:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor37
3618:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor38
3619:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor39
3620:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor40
3621:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor41
3622:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor42
3623:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor43
3624:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor44
3625:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor45
3626:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor46
3627:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor47
3628:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor49
3629:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor50
3630:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor51
3631:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor52
3632:             1             16  sun.reflect.ReflectionFactory
3633:             1             16  sun.reflect.generics.tree.BooleanSignature
3634:             1             16  sun.reflect.generics.tree.BottomSignature
3635:             1             16  sun.reflect.generics.tree.VoidDescriptor
3636:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$8/817299424
3637:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$9/2031951755
3638:             1             16  sun.rmi.registry.RegistryImpl_Skel
3639:             1             16  sun.rmi.registry.RegistryImpl_Stub
3640:             1             16  sun.rmi.runtime.Log$LoggerLogFactory
3641:             1             16  sun.rmi.runtime.RuntimeUtil
3642:             1             16  sun.rmi.server.LoaderHandler$2
3643:             1             16  sun.rmi.server.UnicastServerRef$HashToMethod_Maps
3644:             1             16  sun.rmi.transport.DGCImpl$$Lambda$6/516537656
3645:             1             16  sun.rmi.transport.DGCImpl$2$$Lambda$7/1023268896
3646:             1             16  sun.rmi.transport.DGCImpl_Skel
3647:             1             16  sun.rmi.transport.DGCImpl_Stub
3648:             1             16  sun.rmi.transport.Target$$Lambda$339/2000963151
3649:             1             16  sun.rmi.transport.proxy.RMIDirectSocketFactory
3650:             1             16  sun.rmi.transport.tcp.TCPTransport$1
3651:             1             16  sun.security.rsa.RSAKeyFactory
3652:             1             16  sun.security.ssl.EphemeralKeyManager
3653:             1             16  sun.security.util.ByteArrayLexOrder
3654:             1             16  sun.security.util.ByteArrayTagOrder
3655:             1             16  sun.text.normalizer.NormalizerBase$Mode
3656:             1             16  sun.text.normalizer.NormalizerBase$NFCMode
3657:             1             16  sun.text.normalizer.NormalizerBase$NFDMode
3658:             1             16  sun.text.normalizer.NormalizerBase$NFKCMode
3659:             1             16  sun.text.normalizer.NormalizerBase$NFKDMode
3660:             1             16  sun.util.calendar.Gregorian
3661:             1             16  sun.util.locale.provider.AuxLocaleProviderAdapter$NullProvider
3662:             1             16  sun.util.locale.provider.CalendarDataUtility$CalendarWeekParameterGetter
3663:             1             16  sun.util.locale.provider.SPILocaleProviderAdapter
3664:             1             16  sun.util.resources.LocaleData
3665:             1             16  sun.util.resources.LocaleData$LocaleDataResourceBundleControl
Total     119374210     4034601936
{code}

","Operation System: Debian Jessie
Java: Oracle JDK 1.8.0_151
Cassandra: 3.11.1",aleksey,bdeggleston,cscotta,dikanggu,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,jolynch,laxmikant99,lboutros,lizg,madega,marcuse,mbyrd,molsson,n.v.harikrishna,padakwaak,pauloricardomg,rschildmeijer,serhatd,syegournov,vinaykumarcse,VincentWhite,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/18 14:07;molsson;CASSANDRA-14096-3.0.patch;https://issues.apache.org/jira/secure/attachment/12930399/CASSANDRA-14096-3.0.patch","15/Dec/18 20:08;jolynch;Merkle_On_Heap_Sizes.png;https://issues.apache.org/jira/secure/attachment/12951908/Merkle_On_Heap_Sizes.png","15/Dec/18 02:43;jolynch;Repair_Retaining_Merkel_Trees.png;https://issues.apache.org/jira/secure/attachment/12951877/Repair_Retaining_Merkel_Trees.png","15/Dec/18 02:43;jolynch;Trees_Retained_SyncingTasks.png;https://issues.apache.org/jira/secure/attachment/12951875/Trees_Retained_SyncingTasks.png",,,,,,,,,,,,,,,,4.0,molsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 17 23:29:54 UTC 2019,,,,,,,,,,"0|i3njdb:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Critical,,,,,,,,,,,,,,,,,,,"05/Dec/17 09:06;spod;Can you try to repair tables one by one and see if any particular table and schema is causing the issue?
Did you upgrade from an older Cassandra version before?;;;","05/Dec/17 10:30;serhatd;[~spodxx@gmail.com] 
I tried one by one, it seems big ones (from point of my view) causing the issue.
I upgraded from DDC 3.5.;;;","05/Dec/17 17:56;molsson;Seeing as it was merkle trees using the memory I remembered CASSANDRA-11390 and I believe that [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1421] line could be the cause of this:

{code}
int maxDepth = rangeOwningRatio > 0 ? (int) Math.floor(20 - Math.log(1 / rangeOwningRatio) / Math.log(2)) : 0;
{code}

I believe the '-' sign in the middle should actually be a '+' since log(1 / rangeOwningRatio) will produce a negative result. As it is, this could create merkle trees with a depth larger than 20 instead of lower which was the intended behavior and that could explain the higher memory usage. Do you have the possibility to patch this and verify if this indeed was the cause?;;;","06/Dec/17 07:26;serhatd;OK, I will patch and test.;;;","11/Dec/17 14:29;marcuse;yeah looks like I missed changing that to a + during the process of CASSANDRA-11390, want to create a patch [~molsson]?

edit: never mind, since we do {{Math.log(1 / rangeOwningRatio)}} it should still be positive right?;;;","11/Dec/17 14:39;serhatd;Changed the ""-"" sign to ""+"", still repairing a node causes high memory usage ;;;","12/Dec/17 17:32;molsson;[~krummas] Yes, that should be the case, I was a bit hasty in my judgement of the log-function unfortunately.

As far as I can tell there has been one other ticket changing merkle tree size calculations, CASSANDRA-12580. But it does not seem like it should affect the total maximum size of the trees?

Assuming that the tree size limitations apply as before CASSANDRA-5220 (max 2^20 for a single repair session) and with a replication factor of 3 the total amount of merkle tree nodes in memory should not be more than (2^20) * 3, if I'm not mistaken. The assumption builds on that the three merkle trees accumulate in the repair coordinator. In the heap dump both the number of inner and leaf nodes are roughly 10 times that number. It's also roughly the same number of LongTokens in memory which seems odd.

[~serhatd] How many partitions are there in the table if you run nodetool tablestats?;;;","08/Jan/18 21:38;syegournov;I have similar issue on 3.11.1. IMHO [https://issues.apache.org/jira/browse/CASSANDRA-11390] works well - I verified the depth of trees created with respect to rangeOwningRatio  (I added some extra logging to log each instance of MerkleTree being created):

{code:java}

DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 587271, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 16384 and maxDepth determined to be 14, numPartitions 219402, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 8192 and maxDepth determined to be 13, numPartitions 122506, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 476682, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 648328, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 435338, allPartitions 8774183
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,619 CompactionManager.java:1427 - Created single merkle tree with size 16384 and maxDepth determined to be 14, numPartitions 70665, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,619 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 280970, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 301579, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 153994, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 131072 and maxDepth determined to be 17, numPartitions 690696, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 131072 and maxDepth determined to be 17, numPartitions 836873, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,621 CompactionManager.java:1427 - Created single merkle tree with size 262144 and maxDepth determined to be 18, numPartitions 1437704, allPartitions 3773014
{code}

Here is tablestats (RF=3 in each of the two datacenters, 256 vnodes each node) and first live histogram from repair coordinator (running full repair). The first histogram was taken on coordinator when not a single Merkle Tree was yet received (i.e. no ""Received merkle tree for %s from %s"" logged yet).

{code:java}

Read Count: 509888
	Read Latency: 0.18279529229948538 ms.
	Write Count: 748705
	Write Latency: 0.018887297400177642 ms.
	Pending Flushes: 0
		Table: document_signature
		SSTable count: 12
		Space used (live): 71321036931
		Space used (total): 71321036931
		Space used by snapshots (total): 0
		Off heap memory used (total): 860944484
		SSTable Compression Ratio: 0.7950665041038063
		Number of partitions (estimate): 418442461
		Memtable cell count: 8847
		Memtable data size: 537349
		Memtable off heap memory used: 634552
		Memtable switch count: 1
		Local read count: 3542
		Local read latency: 0.426 ms
		Local write count: 16916
		Local write latency: 0.020 ms
		Pending flushes: 0
		Percent repaired: 97.79
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 586304912
		Bloom filter off heap memory used: 586304816
		Index summary off heap memory used: 268176412
		Compression metadata off heap memory used: 5828704
		Compacted partition minimum bytes: 87
		Compacted partition maximum bytes: 124
		Compacted partition mean bytes: 123
		Average live cells per slice (last five minutes): 1.0
		Maximum live cells per slice (last five minutes): 1
		Average tombstones per slice (last five minutes): 1.0
		Maximum tombstones per slice (last five minutes): 1
		Dropped Mutations: 0

{code}
 
{code:java}
  #      Instances          Bytes  Type
   1:      45135165     2166487920  org.apache.cassandra.utils.MerkleTree$Inner
   2:      45215104     1808604160  java.math.BigInteger
   3:      45218792     1447964176  [I
   4:      45136673     1444373536  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      45213909      723422544  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   6:       3293590      171240328  [B
   7:        239596       15581200  [Ljava.lang.Object;
   8:        201671       12906944  java.nio.DirectByteBuffer
   9:        105623        9402016  [C
  10:        136213        4358816  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  11:        136213        4358816  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  12:         12312        3890512  [J
  13:         88490        3539600  org.apache.cassandra.db.rows.BufferCell
  14:        137639        3303336  org.apache.cassandra.db.RowIndexEntry
  15:        136278        3270672  org.apache.cassandra.cache.KeyCacheKey
  16:        136213        3269112  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  17:        104492        2507808  java.lang.String
  18:         77919        2493408  org.apache.cassandra.db.rows.BTreeRow
...
 83:          1508          72384  org.apache.cassandra.utils.MerkleTree

{code}

To me it looks like validation tasks on the coordinating node itself already take quite some by memory for MerkleTrees - and then when MerkleTrees start to accumulate from other nodes things get worse and worse till JVM crash:

{code:java}
   #      Instances          Bytes  Type
   1:      50976925     2446892400  org.apache.cassandra.utils.MerkleTree$Inner
   2:      51092256     2043690240  java.math.BigInteger
   3:      51096217     1636064888  [I
   4:      50978516     1631312512  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      51091061      817456976  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   6:      12850540      598727368  [B
   7:        346308       18478816  [Ljava.lang.Object;
   8:        274678       17579392  java.nio.DirectByteBuffer
   9:        106347        9386112  [C
  10:        123749        4949960  org.apache.cassandra.db.rows.BufferCell
  11:        153443        4910176  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  12:        153443        4910176  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  13:         12693        4000864  [J
  14:        154841        3716184  org.apache.cassandra.db.RowIndexEntry
  15:        153508        3684192  org.apache.cassandra.cache.KeyCacheKey
  16:        153443        3682632  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  17:        113042        3617344  org.apache.cassandra.db.rows.BTreeRow
  18:        106816        3418112  org.apache.cassandra.db.rows.EncodingStats

{code}

And another one, few seconds before JVM became unresponsive:

{code:java}
   #      Instances          Bytes  Type
   1:      86668240     4160075520  org.apache.cassandra.utils.MerkleTree$Inner
   2:      86866263     3474650520  java.math.BigInteger
   3:      86870454     2780862192  [I
   4:      86670565     2773458080  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      56604877     2664969056  [B
   6:      86865068     1389841088  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   7:        443148       28361472  java.nio.DirectByteBuffer
   8:        592525       24834288  [Ljava.lang.Object;
   9:        107122        9437936  [C
  10:        205317        8212680  org.apache.cassandra.db.rows.BufferCell
  11:        197149        6308768  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  12:        197149        6308768  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  13:        194391        6220512  org.apache.cassandra.db.rows.BTreeRow
  14:        188155        6020960  org.apache.cassandra.db.rows.EncodingStats
  15:        187964        6014848  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
  16:        187960        6014720  org.apache.cassandra.db.partitions.AtomicBTreePartition
  17:        187518        6000576  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  18:        198500        4764000  org.apache.cassandra.db.RowIndexEntry
  19:        197215        4733160  org.apache.cassandra.cache.KeyCacheKey
  20:        197149        4731576  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  21:        194451        4666824  org.apache.cassandra.db.BufferClustering
  22:        189176        4540224  java.util.concurrent.ConcurrentSkipListMap$Node
  23:        188127        4515048  org.apache.cassandra.db.PartitionColumns
  24:        188017        4512408  org.apache.cassandra.db.Columns
...
  83:          2325         111600  org.apache.cassandra.utils.MerkleTree
{code}

The only way I was able to run repairs was incremental repair with in sequential mode (obviously I had to mark all sstables as repaired and maintain high repaired %). But this does not work all the time because there is another bug with a race condition causing assertion error (that's a separate bug so I won't cover the details here)

IMHO the main problem is that all repair sessions run in parallel - and with high RF and multiple DCs it's very easy to fill up JVM even with the way depth of the MerkleTree is calculated using ratio of owned partitions for the range. Coordinating node in turn suffers even more - because it has MerkleTrees allocated as a part of doValidationCompaction() and also accumulates MerkleTrees from the cluster in ValidationTask as part of its coordinating activity.;;;","08/Jan/18 21:55;jjirsa;Number of elements in a binary tree of depth h should be {{2^(h+1) - 1}}, so if we build trees with h=20, then we're looking at about half a 2M {{Inner}} nodes at 48 bytes per for each table (for each replica). Your histogram shows ~86M, which would be ~43 tables, which seems like it'd be on the order of ""expected"" in terms of schemas - 4G of Inner nodes for that does indeed seem painful.

I suspect we may be able to do better here (not sure yet, havent really thought about it), but from a practical standpoint, subrange repair may be a bit easier on your heap, too.;;;","25/Jan/18 08:04;syegournov;[~jjirsa] Yes, I know that depth of trees is on the expected level - just wanted to make a point that the change to dynamically calculate the depth of tree with rangeOwningRatio in mind seems to work quite well.

Indeed from practical standpoint I cannot see how full/incremental repairs can be used at all in production because even if the column family repair does not put such a big pressure on GC - it still hurts higher percentile read latencies badly. Not to mention that during long repairs there is high risk of a repair failure due to gossip detecting node down - and restarting from scratch is a pain. IMHO subrange repair with persisted repair state (e.g. with Cassandra Reaper) is the only real life option.

Because basically both full/incremental repairs can hurt cluster badly in a matter of minutes - not to mention incremental repair having its own bugs. Considering the impact of full/incremental repairs maybe it's worth to warn a user that things might go really bad.;;;","05/Jul/18 14:06;molsson;*TL;DR*
I believe the original JIRA has two problems, CASSANDRA-14332 as well as my findings below.

We store MerkleTrees in the repair session until a full keyspace has been repaired. So it seems this problem gets worse the more tables you have in a single keyspace.
*TL;DR*


When I noticed CASSANDRA-14332 I thought it might be related to this ticket as simultaneous validation compactions would result in multiple MerkleTrees. Even for repairing a single table we could perform multiple validation compactions if we have both vnodes and *nodes > replication factor*.

So I decided to set up a three-node cluster with RF=3 and check the difference with and without CASSANDRA-14332. In my setup I had ten tables in a single keyspace with 20 million partitions in each of the tables to make sure the repairs were long running as well as using the max MerkleTree size. All tests were performed on version 3.0.15, but I believe they would apply for more recent versions as well (3.0.x, 3.11.x and maybe even 4.x).

Without CASSANDRA-14332 you could see multiple validation compactions starting at the same time (as expected) and the top memory consumers were:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      22412689     1075809072  org.apache.cassandra.utils.MerkleTree$Inner
   2:      21801471     1060797400  [B
   3:      22438801      718041632  org.apache.cassandra.utils.MerkleTree$Leaf
   4:      22466069      539185656  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        105245        8449496  [C
   6:         37102        4554912  [Ljava.lang.Object;
   7:        110463        4418520  java.util.TreeMap$Entry
{code}
~3GB MerkleTrees/byte arrays/long tokens.

With CASSANDRA-14332 the validation compactions started one by one (as expected) and the heap started out with:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:        660043       47091600  [B
   2:        747730       35891040  org.apache.cassandra.utils.MerkleTree$Inner
   3:        748498       23951936  org.apache.cassandra.utils.MerkleTree$Leaf
   4:        750376       18009024  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        103072        8418992  [C
   6:         21823        3787144  [Ljava.lang.Object;
   7:          4496        3528432  [J
{code}
But after a while you could see the following build-up:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      24055994     1171217840  [B
   2:      12702738      609731424  org.apache.cassandra.utils.MerkleTree$Inner
   3:      12717330      406954560  org.apache.cassandra.utils.MerkleTree$Leaf
   4:      12733033      305592792  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102947        8382984  [C
   6:         23384        3826696  [Ljava.lang.Object;
   7:          4793        3552624  [J
{code}
~2GB MerkleTrees/byte arrays/long tokens.

After creating a heap dump I could see that the MerkleTrees were being kept in the map [*RepairSession#syncingTasks*|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairSession.java#L99]. This map isn't cleared until all tables in the keyspace has been repaired so it seems like this problem gets worse the more tables you have in a single keyspace.

+Background+
After validations complete the trees are sent to the repair coordinator which collects and compares them to see what needs to be streamed. The actual work of checking the MerkleTree difference and synchronizing the data is handed off to a separate task (LocalSyncTask or RemoteSyncTask). In the case of RemoteSyncTasks they are stored in *RepairSession#syncingTasks* and waits for the remote replica to notify it that it has synchronized the data with the other replica. The problem is that the SyncTasks keeps the MerkleTrees and since they are stored in the map they can't be GC:d until they are removed from the map. This doesn't happen until the full session is completed.

+Potential solutions+
*1. Calculate MerkleTrees difference before handing it off to RemoteSyncTask*
Avoid storing the MerkleTrees in the SyncTasks by calculating the difference before creating the task.
This could still cause some build-up of memory (as we still store them all in the map) but can probably be neglected.

This approach could potentially affect the performance of the repairs as the calculations will be performed in a single thread. A similar approach seems to be taken in 4.0 though for the [optimized stream path|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/RepairJob.java#L205].

*2. Remove MerkleTrees as sync requests completes* (patch attached)
When we get a response from the remote replica that the sync was completed (*RepairSession#syncComplete*) we can remove the RemoteSyncTask from the map instead of [retrieving it|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairSession.java#L190].
But then we still have the scenario when there are no differences in the MerkleTrees as that would not send a sync request and in turn not trigger the removal.
In that case we could make sure to remove the left-overs when all nodes have been synchronized for the table.

I believe this is a rather non-invasive approach although it feels kind of hackish.

With the patch applied the maximum memory usage I observed was:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:       2238987      125061864  [B
   2:       2241647      107599056  org.apache.cassandra.utils.MerkleTree$Inner
   3:       2243951       71806432  org.apache.cassandra.utils.MerkleTree$Leaf
   4:       2247405       53937720  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102724        8344368  [C
   6:         22298        3855408  [Ljava.lang.Object;
   7:          4700        3558992  [J
{code}
This was from when the repair coordinator had not finished the validation compaction for a table while the two replicas had.

After the coordinator was finished and started on the next table it went down to the expected single MerkleTree:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:        745967       35806416  org.apache.cassandra.utils.MerkleTree$Inner
   2:        746735       23895520  org.apache.cassandra.utils.MerkleTree$Leaf
   3:         28892       19281632  [B
   4:        748653       17967672  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102882        8371456  [C
   6:         22386        3858952  [Ljava.lang.Object;
   7:          4712        3559952  [J
{code}

Feel free to try out the patch in combination with CASSANDRA-14332. It should apply cleanly to 3.11.x but trunk has changed somewhat so I wouldn't feel confident in supplying a patch without more verification.


Sorry for the long comment but I won't be able to stay updated on this JIRA ticket in the coming weeks so I tried to get as much information as possible in.;;;","24/Aug/18 03:09;madega;Checking if there is a patch fix/release for this?  It appears the issue is also on 3.11.2;;;","24/Aug/18 08:08;molsson;[~madega], do you also get OOM while running repair on 3.11.2?

If that's the case, how is your schema? Do you have a large number of tables? That could be related to either CASSANDRA-14332 which was fixed in 3.11.3 or to the issue I mentioned above. A patch for the above issue is available on this ticket if you want to try it out, but it's probably not that useful without CASSANDRA-14332 as well.;;;","05/Nov/18 18:49;jjirsa;Removing ready-to-commit. Anonymous visitors, please don't change the status. ;;;","13/Dec/18 23:27;bdeggleston;I think it would be better to just go with option #1 and compute the differences as soon as possible. I wouldn’t consider changing the SyncTask ctor args from {{TreeResponse r1, TreeResponse r2}} to {{InetAddress ep1, InetAddress ep2, List<Range<Token>> differences}} as an invasive change. It also means we don’t keep the trees in memory any longer than they’re useful, and not holding onto them during streaming is a plus.
;;;","13/Dec/18 23:32;bdeggleston;removing the [4.0-feature-freeze-review-requested|https://issues.apache.org/jira/issues/?jql=labels+%3D+4.0-feature-freeze-review-requested] tag, since CASSANDRA-3200 fixes this issue in trunk;;;","15/Dec/18 02:43;jolynch;[~bdeggleston] we just got hit by this in a 3.0.17 single token per node production cluster while running parallel subrange repair on a single table at a time. I think it's because when CASSANDRA-5263 expanded Merkle max tree depth to 20 ([source|https://github.com/apache/cassandra/blob/1816520d6c59cece5ef8346c95e4f12e7c285751/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1220]) it looks like we can allocate close to ~190 MiB _per tree_ since Cassandra 2.1 (the estimation logic changed a bunch but the max of 20 is still there). With a RF of e.g. 3*3 = 9 we allocate gigabytes of memory per {{RepairSession}} and then with this bug we hold it on heap for the duration of the repair. Then when we do parallel subranges to speed up full repair, e.g. 5 of them, we can easily run out of memory. I agree with the rest of this report that we shouldn't be holding them on heap for the duration of the repair, but I also think we shouldn't allocating that much heap in the first place.

According to my heap dumps of the node each Merkle tree has {{2^20}} leaves (and ~{{2^20}} inner nodes) which are retaining 191 MiB of heap memory resident per tree, and we have RF of them (so in our case 3 * 3 = 9). This leads to 191 MiB * 9 = 1.7 GiB per repair session of memory that is held on the heap and due to the bug talked about in this ticket we hold that for the entire repair session. If we then do 4 or 5 parallel subranges we quickly allocate 8+ GiB of memory and can OOM the node. What do you think about the following changes I could make:
 # Per your recommendation above, don't hold onto the trees for the entire repair
 # Provide a yaml configuration option called {{repair_session_heap_space_in_mb}} which defaults to something like 1% of the heap (so 80MB in the 8GB use case). Then we use this information and the RF information of the keyspace to decide the maximum depth of a single merkle tree instead of arbitrarily picking {{2^15}} or {{2^20}}.

I think we can work backwards to acceptable max tree size with something like the following formula (someone please check my math):
{noformat}
sizeof(leaf) = sizeof(hash) + rowsInRange + sizeOfRange + object_overhead
             = 48           + 8           + 8           + 16 
             = 80 bytes
sizeof(inner) = sizeof(leaf) + 2 * sizeof(pointer) + sizeof(token)
              = 80           + 8                   + 24           
              = 112 bytes

In a merkle tree of depth (n)

#leafs = 2^n
#inner = 2^n - 1 ~= 2^n

So for a tree of depth n:
sizeof(tree_n) ~= #leafs  * sizeof(leaf) + #inner  * sizeof(leaf)
sizeof(tree_n) ~= 2^n     * 80           + 2^n     * 112
sizeof(tree_n) ~= 2^n     * (192 bytes)

So to solve for n given sizeof(tree_n):

n = floor(log_2(sizeof(tree_n) / 192)){noformat}
I've attached screenshots showing evidence of the [^Merkle_On_Heap_Sizes.png] that I based my calculations on. The heap dump was taken on a 3.0.17 node that was coordinating 5 subrange repairs on a large table. As we can see when we drill into the heap dump we are not retaining 48 bytes per leaf node, we are retaining about ~80 bytes per leaf node and then ~120 bytes per inner node (object overhead is ... tricky to calculate) and we have like {{2^n}} of both.;;;","16/Dec/18 01:38;bdeggleston;I’m a little worried about limiting tree depth like that. As I understand it (and I’m not super familiar with this area of repair), merkle tree depth is correlated to data density, so limiting the depth is going to inflate the streaming done when there are differences, causing other problems.

It seems like the simplest solution might be to just repair smaller subranges if you’re ooming on huge merkle trees during repair? That should limit tree size, right? There is an anti-compaction cost associated with that, but that seems more predictable/controllable than the other options.

I realize that’s not a super great solution, but I’d like to avoid making any significant behavioral changes here for 3.x and 4.0. That said, I think repairing dense nodes and large partitions is an area that should get some attention for 4.next. WDYT?;;;","16/Dec/18 02:06;jolynch;{quote}I’m a little worried about limiting tree depth like that. As I understand it (and I’m not super familiar with this area of repair), merkle tree depth is correlated to data density, so limiting the depth is going to inflate the streaming done when there are differences, causing other problems.
{quote}
I think that the choice to increase the max resolution to {{2^20}} from {{2^15}} was very dangerous given the flawed estimation math in that ticket and from the production data we can see a small subrange repair can allocate gigabytes of memory. For example, in the case above we were doing splits of 1048576 partition keys in the first place and still ended up with trees that had 1048576 leaf nodes, I don't recall ever seeing that in our 2.1 clusters where we run the same adaptive subrange algorithm. It's possible that the range estimation logic in 2.1 was less likely to create large trees as the 3.0 partition estimation does, but I also think that the most reasonable thing we can do is have the highest resolution tree we can have for a given amount of memory, which is what my proposal would do. If you're particularly worried I could default it to something like 25% of the heap to preserve the current behavior of allocating close to 2 GiB for an RF=3*3 cluster?  
{quote}It seems like the simplest solution might be to just repair smaller subranges if you’re ooming on huge merkle trees during repair? That should limit tree size, right? There is an anti-compaction cost associated with that, but that seems more predictable/controllable than the other options.
{quote}
I completely agree but the above OOM happened when we were repairing _very_ small ranges (1048576 partitions per split). I would be hesitant to go much smaller as small ranges create too many small sstables that create other issues. I do agree more smaller ranges are better, but we are already repairing tiny subranges as it is and this bug is such a big stability regression in 3.0 that it would block us from moving many clusters until it is fixed. Not being able to do parallel subrange means that we can't repair the large clusters fast enough.
{quote}I realize that’s not a super great solution, but I’d like to avoid making any significant behavioral changes here for 3.x and 4.0. That said, I think repairing dense nodes and large partitions is an area that should get some attention for 4.next. WDYT?
{quote}
Our repair algorithm already does the proposed solution (small subranges, on single token nodes, repairing a single table at a time), and even with that we still cause OOMs on any reasonably sized dataset. I think introducing a (potentially optional) cap on heap allocation by a single {{RepairSession}} is the least invasive way to ensure that users of the database can run full/subrange repair on 3.0 without fear of OOMing. Of course this limit would be in addition to your suggestion to not hold all the trees on memory during streaming as well.;;;","16/Dec/18 02:30;jolynch;Ah, so it looks like a bug was fixed in [CASSANDRA-11390|https://github.com/apache/cassandra/commit/1d1bfae580d44d3b8a4678c5af5767ff17102128] where in 2.1 we calculated the size of the tree from the partition count based on the [natural log|https://github.com/apache/cassandra/blob/34a1d5da58fb8edcad39633084541bb4162f5ede/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1059] instead of in the [log base 2|https://github.com/apache/cassandra/blob/1816520d6c59cece5ef8346c95e4f12e7c285751/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1220] as we do in 3.0. So in 2.1 if we did a spit range of 1048576 partitions we'd end up with a tree that was limited to size {{floor(log(1048576)) = 13}} and in 3.0 we end up with a tree that is limited to {{floor(log_2(1048576)) = 20}}. So even though 2.1 added the possibility for the trees to expand to {{2^20}}, the use of natural log protected us from OOMing when we did split ranges since a tree with depth 13 allocates 1.5 MiB vs a tree with depth 20 which allocates 192 MiB, 128x more memory to repair the same tiny range of data in 3.0+ as 2.1.

I still think that limiting the amount of memory that can be used by a single {{RepairSession}} is the most reasonable thing we can do in the 3.0/3.11 releases to make them stable again. I think that's more understandable to the user than arbitrarily picking {{2^15}} or {{2^20}} as our maximum tree size, just say ""we will get the maximum resolution we can given the memory you gave us"".;;;","16/Dec/18 03:02;jjirsa;“Repair smaller sub ranges to work around the fact that the default behavior is untenable” is a bad place for us to put users

;;;","16/Dec/18 05:01;bdeggleston;bq. “Repair smaller sub ranges to work around the fact that the default behavior is untenable” is a bad place for us to put users

You’re not wrong. Though the users this puts in a bad place seem to 1) be few and far between, and 2) seasoned C* operators. Inconveniencing them seems less bad than changing the default behavior of repair for everyone else (no offense). That said, I didn’t realize this was happening on small subranges. [~jolynch], [~serhatd], is this something you're seeing a lot of?

I think my main concern is that adding logic to fit merkle trees into a max memory size seems overly fancy, especially for 3.x. Would adding a configurable max merkle tree depth be good enough?;;;","16/Dec/18 05:44;jolynch;{quote}I think my main concern is that adding logic to fit merkle trees into a max memory size seems overly fancy, especially for 3.x. Would adding a configurable max merkle tree depth be good enough?
{quote}
Sure that could fix it too, but I think users may not understand the implication of having a tree of depth 15 vs one of depth 20 (from what I can tell even experienced developers/operators don't understand the implications of having such a large tree resident in memory). If my math above is correct, wouldn't it be as easy as a function that we could thoroughly test, e.g.:
{noformat}
static int estimatedMaxSizeForBytes(long numBytes)
{
    long adjustedBytes = Math.max(1, numBytes / 192);
    return Math.max(1, (int) Math.floor(Math.log(adjustedBytes) / Math.log(2)
}
{noformat}
Then when we allocate the trees we just take the memory budget given by the user, divide by the RF of the keyspace, and plug it into this formula. Then the advice we give to users who want very precise trees is ""give it more memory"", which is a tradeoff I think users can actually make easily.
{quote}That said, I didn’t realize this was happening on small subranges. Joseph Lynch, Serhat Rıfat Demircan, is this something you're seeing a lot of?
{quote}
At least for us we only recently started moving clusters to 3.0 from 2.1 in production, and we are seeing OOMs on _all_ of the large dataset clusters that have to do parallel subranges to meet gc_grace. As a result we have had to disable parallel subrange and extend gc_grace as a workaround due to these very large Merkle trees (note these workloads worked fine on 2.1). Full range or single worker subrange isn't OOMing but still allocates 2GiB of heap and it is much slower naturally than our parallel subrange. To be frank, many of our large clusters simply can't repair on 3.0 fast enough whereas on 2.1 we could repair about 4-8x faster due to being able to do ranges in parallel. Since we have single tokens we can re-factor our algorithm to drift across the ring instead of doing one range faster, but that won't really work for vnode clusters and tbh is way more complex then just fixing this bug.

We were just going to roll back the 3.0 sizing change internally, but I was hoping that I could do slightly more work and fix it for the community as well with an option users can easily configure. I'm happy to do that if you have time to review.;;;","17/Dec/18 03:41;bdeggleston;Specifying max depth is less elegant than specifying a max tree size, but I have a lot fewer concerns about it being implemented correctly, and the risk involved in putting it into 3.x. [~krummas], what do you think?
;;;","17/Dec/18 10:20;marcuse;[~bdeggleston] agreed, but taking RF in to account also makes sense;

How about something like this for 3.0+3.11? And then do what [~jolynch] suggests in 4.0
1. make max depth configurable, default at 20
2. add a flag to factor in RF when calculating the depth, default off;;;","17/Dec/18 19:26;bdeggleston;Seems reasonable. [~molsson], we just expanded the scope and complexity of this ticket, do you still want to do it? If not, I (or [~jolynch] if he's interested) can take it.;;;","19/Dec/18 07:24;jolynch;[~krummas] [~bdeggleston]

I took a whack at Marcus' suggestion to make max depth configurable (also a hot property) for 3.0+3.11 and then have a patch for trunk which limits the total memory used taking into account RF and the partitioner etc... Let me know what you guys think. I'll give it the 3.0 patch a spin in our clusters with max depth set to 16 to see if it alleviates the OOM issues.

 
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

I'll run the single failed 3.11 unit test locally, and the trunk unit test that's failing is CASSANDRA-14922.

I think that the tree size estimation logic is not [too complicated|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-e657fd15ed537a2bf54a672b6b84afecR1182]. Most of the patch is tests tbh including testing the range splitting adjustment introduced in CASSANDRA-11390 ([1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-3f1f2846fe6fbee924c09edaf34753deR238], [2|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-ec7f7df91e07f7bac10771e53e47d5f9R572]).

 ;;;","20/Dec/18 03:17;jolynch;I added a commit to the 3.0 and 3.11 branches which I think is the minimally invasive change I can make that will make sure that the {{SyncTasks}} do not hold the merkle trees on heap longer than they need to. It's different than the way that trunk does it, but if I understand the trunk patch properly the tree differences will now be single threaded, which isn't that big a deal in a single token case (computing differences even with RF=15 would be a few seconds even on the largest trees) but in the vnode case it may hold the trees on heap for longer than we'd like (as it's no longer RF merkle trees, it's RF * number of shared ranges merkle trees). I'm tinkering with the patch to see if I can more or less backport the trunk strategy but still keep the difference calculations in parallel.

With regards to the trunk patch, right now the max repair session size defaults to 1/8 of the heap, but this is actually quite a lot of memory given that each repair job could allocate up to this amount which would mean 1/2 the heap could be used if someone used 4 job threads I believe. [~krummas] made a nice suggestion offline to somehow communicate the budget from the coordinator to the nodes running the validation, so I'm seeing how hard that would be. If that's too tricky we could probably just lower the memory limit to 1/16 the heap and be happy knowing that even if you repair with 4 jobs you'll only allocate ~1/8 of the heap max.;;;","20/Dec/18 14:30;molsson;[~bdeggleston] I was going to say that I think I'll have limited time for it in the coming weeks for it so it's good that [~jolynch] has already uploaded patches. I can try and take some time to do some review/testing on it.

A few initial comments on the patches:
 * I like the 3.0/3.11 approach of keeping the list and clearing it. It seems like the solution that would change behavior the least (except for the upside of not keeping the trees of course).
 * The flag for factoring in RF does not seem to be in the 3.x branches (should it?).
 * [In 4.0|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-b6397657542c94b41574354981a11843R69] the definition of _repair_session_max_tree_depth_ has effectively changed. Before the total size was divided over different trees based on *rangeOwningRatio*. Now each tree can be up to _repair_session_max_tree_depth_. Since we have the memory cap as well this is probably not a huge problem but was this an intended change?

For _repair_session_space_in_mb_: With *1/8* and a heap of 8GB and rf = 3 the cap should effectively be defined by the max tree depth. The limit would kick in for heaps below ~4.8GB. With *1/16* it would kick in on heaps below ~9GB, so 8GB heaps will have a minor change in behavior for rf = 3. I'm not sure which heap size/rf is the most common but it would probably change the default tree depth on clusters with *rf > 3*.
{code}
# rf = 3 and max tree depth of 20, 1/8 of heap for merkle trees
3 * 200MB = ~600MB # Coordinator tree sizes, holds true for both cases
600MB * 8 = ~4.8GB # Total heap size when the 1/8 memory limit would cap at 600MB

# rf = 3 and max tree depth of 20, 1/16 of heap for merkle trees
600MB * 16 = ~9.6GB # Total heap size when the 1/16 memory limit would cap at 600MB
{code}

While I'm on the side of wanting to lower the default tree size caps (and effectively change the default behavior of repair) I get the feeling that the risks with over streaming is causing people to be cautious here. (Almost) regardless of what we choose as default here it might change behavior for someone, so unless I misunderstood the concern (i.e. if the concern is only for 3.x) we might have to make a decision:
# Set the default to total heap space (should be capped at the default max tree size, as it is today)
# Set the default to x% of available heap (would change default behavior)
# Other suggestions

I would argue that the default tree depth should be a bit more on the safe side for new users. But then there is of course the other side with over streaming, so what is safe and not is a hard question to get an answer to. In general I get the feeling that the changes done to the merkle trees (cap at 20, correctly calculate the necessary depth) have made repair more unstable performance wise, which would suggest that lowering the default cap would be safer. But I might of course be missing the over streaming issues. So I'm not going to push too much here to get lower defaults but I think that it might be good to take that into consideration at least for 4.0+, especially if we leave it configurable for the experienced users (with NEWS/upgrade entries).;;;","20/Dec/18 21:26;jolynch;Thanks [~molsson] for the feedback! Let's see what Blake and the other Marcus think about the list+clear vs the trunk approach. I do think that tree calculations shouldn't be too slow, but if we're wrong 4.0 seems like the safer place to introduce that change.
{quote}The flag for factoring in RF does not seem to be in the 3.x branches (should it?).
{quote}
Hm, I'm not sure how to incorporate the RF information without some form of the memory estimation as well. The current 3.0/3.11 algorithm of subtracting {{log_2(1 / ratio)}} reduces the tree size but not by a particularly predictable quantity. 
{quote}Before the total size was divided over different trees based on rangeOwningRatio. Now each tree can be up to repair_session_max_tree_depth
{quote}
For the trunk patch I split the memory cap up [proportional to each range|https://github.com/apache/cassandra/commit/3f721b7c931c0e9e9b118c491930aa390df56afe#diff-b6397657542c94b41574354981a11843R66] using the {{rangeOwningRatio}}. So if you have 330MB to work with (after taking into account RF of 3 on a 8GB heap), and you have 4 ranges with (0.25, 0.5, 0.25, 0.25) of the partitions, then the ranges get (82MB, 165MB, 82MB, 82MB) of space each yielding trees of depth (18, 19, 18, 18) compared to the status quo where we would get {{20-log_2(1 / ratio)}} which yields (18, 19, 18, 18).

I agree with your analysis concerning 1/8 and rf=3, I chose 1/8 so that the trees would be roughly the same depth as they are today with 8GB heaps for RF=3. The main goal is to keep that memory usage fixed as RF grows to 6, 9 or 15.

I don't understand your analysis for 1/16 heap though. The memory budget is reduced by [RF|https://github.com/apache/cassandra/commit/3f721b7c931c0e9e9b118c491930aa390df56afe#diff-b6397657542c94b41574354981a11843R57] in the trunk path, so with 1/16 of 8GiB and RF=3 a given validation compaction would not exceed {{8GiB / (48) = 170 MiB}} which yields a tree of depth 19 so the coordinator would hold {{3 * 170 = ~512MiB}} on heap which is slightly smaller than status quo. The main difference would be when you have RF=6 or 9 where the status quo allocates 1.1GiB and 1.7GiB respectively and a 1/16 default size would allocate a fixed ~512MiB.
{quote}I would argue that the default tree depth should be a bit more on the safe side for new users. But then there is of course the other side with over streaming, so what is safe and not is a hard question to get an answer to. In general I get the feeling that the changes done to the merkle trees (cap at 20, correctly calculate the necessary depth) have made repair more unstable performance wise, which would suggest that lowering the default cap would be safer. But I might of course be missing the over streaming issues.
{quote}
I personally feel strongly that the change in behavior to allow trees to go to depth 2^20 represents a regression in database stability for common configurations (RF=6 and 9 in particular) and we should be revert the default max depth to something closer to 18 in the 3.0+3.11 branch and with the configuration option (which is hot swappable) if someone really wants it to be 20 they can do that. I believe the correct solution to overstreaming has been in the past, and continues to be, to do subrange repair which allows us to spend the memory in a controlled fashion over time rather than having the larger range repairs use a lot of memory all at once. This is also, practically speaking, how most teams I know of are successfully repairing large datasets.
{quote}So I'm not going to push too much here to get lower defaults but I think that it might be good to take that into consideration at least for 4.0+, especially if we leave it configurable for the experienced users (with NEWS/upgrade entries).
{quote}
If we do the memory budget in 4.0 I'm less concerned about the max depth being 20, since the memory budget will protect high RF users. I do think though that at the point someone is hitting depths of 20 and encounter overstreaming they _really_ need to start doing subrange.;;;","21/Dec/18 03:05;jolynch;Just put up an alternative 3.0 [patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-trunk-backport] which just backports the strategy used in trunk and still keeps the computation parallel by [transforming|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-trunk-backport#diff-1ec1e6dd403ead5e9e44a57a0702f79eR106] all of the {{TreeResponse}} s into an intermediate {{TreeDifference}} which then get's transformed into {{SyncStats}}. This option is closer to what trunk does but also keeps the computation of trees parallel. I can of course just rip out the parallel part and just compute the differences on the single thread as well and it'll probably be fine.;;;","21/Dec/18 12:49;molsson;No problem, thanks for driving this issue further.

bq. Hm, I'm not sure how to incorporate the RF information without some form of the memory estimation as well. The current 3.0/3.11 algorithm of subtracting log_2(1 / ratio) reduces the tree size but not by a particularly predictable quantity.

I think it could be done by creating an adjusted range owning ratio.
{code:java}
double adjustedRangeOwningRatio = rangeOwningRatio / rf;
int maxDepth = adjustedRangeOwningRatio > 0 ? (int) Math.floor(DatabaseDescriptor.getRepairSessionMaxTreeDepth() - Math.log(1 / adjustedRangeOwningRatio) / Math.log(2)) : 0;
{code}
IIRC the max depth prior to this makes sure that locally allocated trees don't contain more nodes than a single tree of size 20. If we add the division by the replication factor here it should make each replica create merkle trees that when combined would not exceed one tree of size 20. So if we have a range which consists of 30% of the data and a replication factor of 3 it would say that locally we should not create a tree which exceeds 10% of the max tree. Even if we have a variance in data density per range between nodes it should even out between the different ranges as one node should not create merkle trees that combined exceeds 33% of the max tree. Basically I think this would extend the current limitation that is local node only to the whole cluster so that we don't allocate more than ~190MB for the coordinator.

bq. For the trunk patch I split the memory cap up proportional to each range using the rangeOwningRatio. So if you have 330MB to work with (after taking into account RF of 3 on a 8GB heap), and you have 4 ranges with (0.25, 0.5, 0.25, 0.25) of the partitions, then the ranges get (82MB, 165MB, 82MB, 82MB) of space each yielding trees of depth (18, 19, 18, 18) compared to the status quo where we would get 20-log_2(1 / ratio) which yields (18, 19, 18, 18).

That is true for the default settings but if memory limits are increased you could get a result of (20, 20, 20, 20) as the calculation for  _20-log_2(1 / ratio)_ [was removed in the patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-b6397657542c94b41574354981a11843R69]. Now the max tree depth is used as a static max depth for a single tree rather than a limit for all trees together.

bq. I don't understand your analysis for 1/16 heap though. The memory budget is reduced by RF in the trunk path, so with 1/16 of 8GiB and RF=3 a given validation compaction would not exceed 8GiB / (48) = 170 MiB which yields a tree of depth 19 so the coordinator would hold 3 * 170 = ~512MiB on heap which is slightly smaller than status quo. The main difference would be when you have RF=6 or 9 where the status quo allocates 1.1GiB and 1.7GiB respectively and a 1/16 default size would allocate a fixed ~512MiB.

I agree that it would change mostly for rf=6/9 and my main point was that the patch would change default repair behavior. I got the feeling that there was some caution regarding this from others in the community so I wanted to point out where we could expect the default behavior to change.;;;","04/Jan/19 23:08;jolynch;{quote}I think it could be done by creating an adjusted range owning ratio.
{noformat}
double adjustedRangeOwningRatio = rangeOwningRatio / rf;
int maxDepth = adjustedRangeOwningRatio > 0 ? (int) Math.floor(DatabaseDescriptor.getRepairSessionMaxTreeDepth() - Math.log(1 / adjustedRangeOwningRatio) / Math.log(2)) : 0;
{noformat}
{quote}
I think I see what you're saying, and I think that's a pretty safe optional change (we'd default to off) for the 3.x series. Ok I added another commit on the [3.0|https://github.com/apache/cassandra/commit/20a9b32f620168657dcfa2e98b92db20e90deb52] and [3.11|https://github.com/apache/cassandra/commit/09a164b60cfb41cd3ae8c3d68fbe8169069a4cb8] branch which does this.
{quote}That is true for the default settings but if memory limits are increased you could get a result of (20, 20, 20, 20) as the calculation for 20-log_2(1 / ratio) was removed in the patch. Now the max tree depth is used as a static max depth for a single tree rather than a limit for all trees together.
{quote}
I'm curious if we should even have the maximum depth option anymore in trunk once we have size limit as the memory usage is really the only thing that matters. If a user gives us 2GB of memory, we should (imo) calculate the most precise trees we can with 2GB. In the trunk patch, if my understanding is correct, the {{repair_session_space_in_mb}} limit is the limit for all trees together across all replicas and all ranges participating in the repair session.;;;","08/Jan/19 00:24;jolynch;I believe the remaining decision points are as follows:

*3.0/3.11*:
 # Do we want to use the list+clear [approach|https://github.com/apache/cassandra/commit/342921c54e463180171a4d64944acd54b5d97727] or the trunk approach for clearing merkel tree differences. If we go with the trunk approach should we refactor to keep the calculation [parallel|https://github.com/apache/cassandra/commit/7c61e843a32c5e9b94ab9269432bd3b1f84cddd9]?
 # Do we introduce {{repair_session_account_for_rf}} option that defaults to false?
 # Should we reduce the default max from 20 to something like 18?

I think we should opt for the choices in 3.x that provide maximal stability (so list+clear, introduce the option and default to false, reduce to 18 as a compromise between the old 2^15 and new 2^20 depths). I think the reduction in max depth is a move towards stability because it seems to me from this ticket and discussion on irc [[1|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-12-17], [2|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-12-18]] that as most users are upgrading to 3.0 they are having issues running repair and so I feel it is prudent to roll back slightly to the older, safer, defaults.

*trunk*:
 # What should the default memory budget be. Currently it is 1/8 of the heap in the patch but 1/16 might be more reasonable.
 # Should trunk only have the {{repair_session_space_in_mb}} or also support overriding the maximum tree depth to 20.

I think we should default to a memory budget of 1/16 of the heap by default so that we have a _safe_ default that doesn't cause OOM when a user runs with the maximum {{[job_threads|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/tools/nodetool/Repair.java#L86]}} of 4 (which would allocate 1/8 of the heap). We would very minorly reduce resolution for the most common setups (8GB heaps with RF=3 would get 2^19 instead of 2^20). For users that want higher resolution they can use subrange repair (correct but harder) or allocate more memory (incorrect but easier). Personally I think just having the memory option should be sufficient but having the max depth option present (commented out) and explaining clearly that if you want to give the trees more memory you may need to raise that max depth as well isn't that bad (and perhaps prevents really bad mistakes like allocating GiB of memory by accident).

[~molsson], [~krummas], [~bdeggleston]. What do you think? If there is rough consensus I'd like to commit to those choices and squash down the patches to move this forward.;;;","08/Jan/19 22:07;bdeggleston;My 2 cents:

*3.0/3.11*
{quote}Do we want to use the list+clear approach or the trunk approach for clearing merkel tree differences. If we go with the trunk approach should we refactor to keep the calculation parallel?
{quote}
I’d prefer the trunk approach. Releasing the memory in the task adds mutable state to the task, and doing the calculations in the RepairJob didn’t seem terrible based on our offline back of the envelope calculations.
{quote}Do we introduce repair_session_account_for_rf option that defaults to false?
{quote}
I’m -0 on accounting for rf. I don’t think it adds a lot of value without a memory based calculation, but I don’t have a strong opinion here.
{quote}Should we reduce the default max from 20 to something like 18?
{quote}
I think so, yes.

*trunk*
{quote}What should the default memory budget be. Currently it is 1/8 of the heap in the patch but 1/16 might be more reasonable.
{quote}
Maybe 1/16, but not to exceed something like 80-100MB? This would prevent huge trees on nodes with large heaps, without consuming too much memory on nodes with small heaps
{quote}Should trunk only have the repair_session_space_in_mb or also support overriding the maximum tree depth to 20.
{quote}
If the max_depth option is in 3.x, we need to put it in trunk as well.;;;","17/Jan/19 16:32;molsson;*3.0/3.11*
1. I would prefer the trunk approach as well (non-parallel) as long as there is no dramatic performance regression.
2. I think this feature could be useful to avoid having to change the max tree depth when adding data centers (and extend current replication to it). But if we set the default values reasonably low it shouldn't be necessary. +0
3. +1 from me.

*trunk*
1. 1/16 sounds reasonable to me.
2. I think we should keep the option and align it with the default we choose for 3.x.;;;","22/Jan/19 10:38;jolynch;Alright, it sounds like we have rough consensus that we should backport the trunk approach to handling the trees staying in memory, reduce the maximum depth to 18 by default, and we will not include the account for rf logic in the 3.x patchset (trunk does). For trunk it sounds like we keep the max depth option (default of 18) and default the max size to 1/16 the heap, whichever is smaller.

I've put up patches that I believe implement this consensus as well as cleaning up the syncing tasks at the end per the suggestion far up in the comments by Marcus Olsson so that we don't accumulate RF of them per table (even now that they're just holding the differences we probably shouldn't be holding onto them). I've tried to add some basic unit tests of the RepairJob but it is somewhat hard to do without more refactoring of the RepairJob code since it's all so tightly coupled; I tried to walk the right line of refactoring for testability (sort of like Alex did in trunk) but not introduce too many changes ... but I can either remove the changes to make the diffs easier to check or I can try refactoring more to make unit testing more possible.

Unit tests are running at:
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-final]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-final]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

[~bdeggleston] let me know what you think, if you would like changes please let me know. If you are happy I'll squash them down to one commit for merging/testing. Once we're ready to squash I can give the 3.0 change a test in my repro environment to see if it alleviates our sub-range stability issues.

 

 ;;;","29/Jan/19 19:22;bdeggleston;First round of feedback. Most of the 3.x stuff applies to the trunk patch as well.

3.x patch:

cassandra.yaml
* The comment here is pretty verbose. Something more concise like ""Limits the max merkle tree depth to avoid consuming too much memory during repair. The default is 18, set as low as 15 if you're running out of memory during repairs"" would be more easily digestable.

DatabaseDescriptor
* validate that the max depth is something sensible (like at least > 10), and log a warning if it's too high (>20?).
* regarding ConfigurationException message. “must be > 0” is more easily understood than “must be strictly positive”

CompactionManager
* should add some additional min/maxing to prevent negative/zero max depths in extreme cases if the max depth is set really low

RepairJob
* since the trees don’t escape {{createSyncTasks}}, I don’t think we need to call {{removeSyncingTask}} here (or have a comment in {{onFailure}})

RepairSession
* class internals are overexposed for testing. Instead of making members package private, you could support your tests with some accessor methods {{getNumSyncTasks}} / overridable protected methods {{createExecutor()}}, etc.

trunk:

cassandra.yaml
* Let’s remove {{repair_session_max_tree_depth}} as a commented out param in cassandra.yaml. Sorry my previous comment on this was vague. We do need to support yaml files with this param in them for back compat, but we should treat it as a deprecated config value. We should log a warning mentioning it’s deprecated on startup if the yaml file has it. Having 2 config params that achieve the same thing in different ways is confusing. This will also simplify the comment blurb.

DatabaseDescriptor
* should log warning if max tree depth is configured.

StorageService.
* I think the new jmx endpoints would make more sense be better on ActiveRepairServiceMBean?
;;;","30/Jan/19 20:40;jolynch;Thanks for the feedback! I'm working on implementing this but just a quick question:

{quote}
RepairJob
 * since the trees don’t escape {{createSyncTasks}}, I don’t think we need to call {{removeSyncingTask}} here (or have a comment in {{onFailure}})
{quote}
I agree that we don't need to remove them since the remote sync tasks are much smaller now (as they just hold the tree differences), and the ones that get removed here are only the empty ones that didn't have any differences, but isn't it good practice to remove them once they're done either way? I'm thinking it might be a problem for keyspaces with many tables that are all fully consistent as we will have RF sync tasks held in that map per table. I suppose it's reasonable to just leave the empty ones since they're so small but I figured it would be better to be thorough. Trunk doesn't have this problem because we don't generate any sync task at all if there are no differences in trunk (I am hesitant to backport that change since I'm not sure of the full context of that change).;;;","30/Jan/19 23:51;bdeggleston;It’s just best to minimize changes when dealing with bug fix branches. This is a small change in behavior with a negligible benefit. Would this change actually cause any real problems? Probably not, but it takes longer to verify that than it does to just not change it.;;;","31/Jan/19 07:35;jolynch;Ok, that makes sense. I've pushed a commit to each branch that I believe takes into account your feedback:

||3.0||3.11||trunk||
|[26e5b2e6|https://github.com/apache/cassandra/commit/26e5b2e68810e724c9f14321fd0f6ca43b66d08e]|[e139f536|https://github.com/apache/cassandra/commit/e139f5366f62ba17ac00794258270a82dc395345]|[a1ab1f6a|https://github.com/apache/cassandra/commit/a1ab1f6a28c2e6ea98cc2c43a8cd5fa42edf2c06]|

If the branches look good let me know and I will squash them down and add the NEWS/CHANGES entries.;;;","31/Jan/19 19:29;bdeggleston;ok nearly there. Could you make the following changes and rebase each branch? I'll squash on commit.

all branches:
* make jmx configurable Config members volatile
* jmx methods should throw exceptions if they get invalid input (not just log warnings)
 
3.x:
* in RepairSession: syncingTasks.remove should be syncingTasks.get
* there are extra parentheses in the ConfigurationException thrown in DatabaseDescriptor

trunk:
* max depth code shouldn’t be removed (the only change should be the yaml changes, the Config changes, and warning that it’s deprecated on startup);;;","01/Feb/19 08:20;jolynch;Awesome, I've made the changes and pushed to all three branches. I can't edit this JIRA title so I didn't add the CHANGES entries (my understanding is that the Jira title, CHANGE entry, and commit title should all match) but I was thinking something like ""Reduce memory held by merkle trees during repair to prevent OOM (CASSANDRA-14096)"" for the CHANGES entry?

If you want to do the squash I've left the original branches as is but rebased on top of latest:
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-final]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-final]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

I've also created rebase/squashed commits for running the dtests you could cherry-pick if you prefer:
||3.0||3.11||trunk||
|[e633d6bc|https://github.com/apache/cassandra/commit/e633d6bc9dca621ea7ac7a8a5d7675c93baee66d]|[9c227ac2|https://github.com/apache/cassandra/commit/9c227ac251536339c080897aabbdcd7c650ae526]|[bd58a4c0|https://github.com/apache/cassandra/commit/bd58a4c062d0adb97bb3afac75687959bf103d4d]|
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-squash]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-squash]|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk-squash]|
|dtests: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-squash]|dtests: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-squash]|dtest: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk-squash]|

*Test Failures:*

trunk unit tests:
 * {{PagingTest}}: Unrelated, I posted a patch that fixes this on CASSANDRA-14956
 * {{PendingAntiCompactionBytemanTest#testExceptionAnticompaction}}: Pretty sure this is a flake, and the dtest+unit test run is fully green.
 * {{DistributedReadWritePathTest#readRepairTest}}: I believe this is CASSANDRA-14922, debugging that separately

3.x dtests:
 * {{thrift_hsha_test.TestThriftHSHA#test_closing_connections}}: Doesn't appear related
 * {{materialized_views_test.TestMaterializedViews#test_interrupt_build_process}}: Doesn't appear related
  ;;;","14/Feb/19 21:29;bdeggleston;commited to 3.0 as [b30c8c98a594a5682f6ea1f0b5511463b700b6e8|https://github.com/apache/cassandra/commit/b30c8c98a594a5682f6ea1f0b5511463b700b6e8] and merged up to trunk. Thanks [~jolynch];;;","17/Mar/19 23:29;jjirsa;[~bdeggleston] / [~jolynch] - can one of you set fix versions?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Max ttl of 20 years will overflow localDeletionTime,CASSANDRA-14092,13122467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,03/Dec/17 22:01,09/Aug/19 10:56,14/Jul/23 05:56,11/Feb/18 13:38,2.1.20,2.2.12,3.0.16,3.11.2,,,Legacy/Core,,,,,2,,,,,"CASSANDRA-4771 added a max value of 20 years for ttl to protect against [year 2038 overflow bug|https://en.wikipedia.org/wiki/Year_2038_problem] for {{localDeletionTime}}.

It turns out that next year the {{localDeletionTime}} will start overflowing with the maximum ttl of 20 years ({{System.currentTimeMillis() + ttl(20 years) > Integer.MAX_VALUE}}), so we should remove this limitation.",,christianmovi,dimitarndimitrov,eanujwa,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,KurtG,maedhroz,mshuler,pauloricardomg,rbfblk,samt,sayap,tommy_s,varuna,VincentWhite,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14227,,,,,,,CASSANDRA-14188,,,,,,,,,,,CASSANDRA-4771,CASSANDRA-14228,,,,,,,,,"11/Feb/18 13:27;pauloricardomg;2.1-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910103/2.1-14092-dtest.png","11/Feb/18 13:27;pauloricardomg;2.1-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910102/2.1-14092-testall.png","11/Feb/18 13:27;pauloricardomg;2.2-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910101/2.2-14092-dtest.png","11/Feb/18 13:27;pauloricardomg;2.2-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910100/2.2-14092-testall.png","11/Feb/18 13:27;pauloricardomg;3.0-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910099/3.0-14092-dtest.png","11/Feb/18 13:27;pauloricardomg;3.0-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910098/3.0-14092-testall.png","11/Feb/18 13:27;pauloricardomg;3.11-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910097/3.11-14092-dtest.png","11/Feb/18 13:27;pauloricardomg;3.11-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910096/3.11-14092-testall.png","11/Feb/18 13:27;pauloricardomg;trunk-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910095/trunk-14092-dtest.png","11/Feb/18 13:27;pauloricardomg;trunk-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910094/trunk-14092-testall.png",,,,,,,,,,10.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 11 13:37:48 UTC 2018,,,,,,,,,,"0|i3nhbz:",9223372036854775807,3.0.15,,,,,,samt,,samt,,,Critical,,,,,,,,,,,,,,,,,,,"03/Dec/17 22:04;pauloricardomg;I think that CASSANDRA-8099 already removed this limitation from the storage engine by encoding {{localDeletionTime}} as a varint, so it should be relatively easy to update the code to represent {{localDeletionTime}} as a long instead.;;;","24/Jan/18 17:15;christianmovi;Reproduced in 3.0.15.;;;","24/Jan/18 19:46;rbfblk;Copying/paraphrasing my recent comment from CASSANDRA-4771 here:

It looks like local deletion time is encoded per cell as an offset in seconds from the minimum local deletion time in the SSTable. If I'm reading it right the offset is serialized as a variable length integer, so reading it into a long instead of an int would give a very large range of offsets that would be more than sufficient. However it looks to me like the minimum local deletion time at the SSTable metadata level is saved as a fixed-size 4-bytes and de-serialized into a (signed) int. Without changing the serialization format we could change the interpretation of that metadata field to assume it is an unsigned integer (and de-serialize it also into a long variable instead of an int) which should give us another 68 years. A better fix would probably be to also change the serialization format of minLocalDeletionTime in the metadata to an 8 byte integer or a variable size integer so it could hold values in the far far future. Changing the SSTable format might be a 4.0+ thing though. Either way the in-memory representation of local delete time would become a long not an int.

I haven't yet looked at what if anything this means for the inter-node wire format between coordinator and replica, that might be a complicating factor if this is a breaking change to the format.;;;","26/Jan/18 02:55;pauloricardomg;The patch below [reduces the maximum allowed TTL|https://github.com/apache/cassandra/commit/6e66a4730a6f606fe1bb609ba4d8ec650877605e] from 20 years to 15 years to prevent integer overflow on the {{localDeletionTime}} field what causes inserts with TTL close to the current maximum of 20 years to fail with AssertionError on 2.1 and expire as soon as they are inserted on 3.0, as discussed on the [mailing list thread|https://www.mail-archive.com/dev@cassandra.apache.org/msg11888.html].This is just an immediate measure to prevent data loss on 3.0+ while we look for a more permanent solution to raise this limitation.

In addition to lowering the max default TTL, this patch also [adds ability to scrub to detect and fix rows/cels|https://github.com/apache/cassandra/commit/d9bbb7992696a02ccc490229280894f0437ae0b7] with overflowed {{localDeletionTime}} which were not yet turned into tombstones by compaction and detect rows which were already turned into tombstones to allow operators to identify which rows were affected. Furthermore, I [modified the compaction purger|https://github.com/apache/cassandra/commit/ac04a4199947d41962c34bce07d9aad4ebf87e47] to not purge rows with overflowed localDeletionTime to allow them to be fixed via scrub.

This is ready for a initial round of review, but I will still add some tests for scrub and a more detailed NEWS.txt entry.
||2.1||3.0||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092]|;;;","26/Jan/18 16:02;pauloricardomg;Canceling the patch while I work on an alternative version after discussion on the ML.;;;","26/Jan/18 22:59;pauloricardomg;This patch below takes a simpler and more transparent approach. The idea is to keep the maximum allowed TTL of 20 years, but cap the maximum expiration time to 2038-01-19T03:14:06+00:00.

When this capping is done, the following log is print in the client and system.log:
{noformat}
WARN  [SharedPool-Worker-2] 2018-01-26 19:15:36,877 NoSpamLogger.java:94 - TTL of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00. Rows with expiration date exceeding the maximum supported date will expire in the limit date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
{noformat}
In addition to this, the patch converts any negative localExpirationTime that may have been written to 2038-01-19T03:14:06+00:00 - fixing any wrong entries that may still be present but were not yet purged.

Since we store the TTL as a separate field, once we fix this limitation we can recompute the correct expiration time with the timestamp/1000+ttl time during upgradesstables.

I added tests to check that data inserted with a TTL exceeding the maximum allowed expiration time is present and a warning is logged. I also added tests to check that SSTables written with the negative localExpirationTime are correctly interpreted.

||2.1||2.2||3.0||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v2]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v2]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|


Submitted CI, will attach results when ready.;;;","29/Jan/18 14:06;dimitarndimitrov;Some review comments on the dtest and trunk changes:
 * On the dtest change:
 ## Shouldn't the dtest docstring [here|https://github.com/apache/cassandra-dtest/commit/83c73ef0a3cbe50232d3a9eea4fd26c877ea58db#diff-a8f4dac4af77196a8c7881abd067a5b9R345] say something related to the TTL problem?
 ## The start time [here|https://github.com/apache/cassandra-dtest/commit/83c73ef0a3cbe50232d3a9eea4fd26c877ea58db#diff-a8f4dac4af77196a8c7881abd067a5b9R348] seems redundant
 ## It may be good to extract the max TTL value in a variable - we may decide to keep a version of this test after we patch by just reducing that value, but before we fix it nicely
 * On the trunk change:
 ## Maybe it's my English, but [this wording|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-5414c0e96996be355c3aff1184ec859aR48] sounds a bit confusing to me, using ""maximum supported date"" and ""limit date"" for the same thing. Thoughts? If you're also hesitant, what do you think about ""Rows that should expire after that date would still expire on that date.""?
 ## You can quickly mention the relevant JIRA ticket [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-b7ca4b9c415e93b6cbfb31daf90cc598R185]
 ## Qualify the static access to {{Cell.sanitizeLocalDeletionTime}} [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-b7ca4b9c415e93b6cbfb31daf90cc598R53]
 ## Could you please add some comments/Javadoc for [{{Cell.sanitizeLocalDeletionTime}}|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-3e9f1fc67f99d27e92a3eb32201d8ca6R311]? I would assume that {{NO_TTL}} and {{NO_DELETION_TIME}} are needed to determine whether the cell is an expiring one, an expired one, or a tombstone, but I'm not too sure
 ## There are missing spaces between the boolean arguments of the delegation call for some of the unit tests (e.g. [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-0d8cf6ca6ed99c947903359c1beaf386R74]);;;","29/Jan/18 16:46;VincentWhite;I haven't had a chance to completely catch up on the ML discussion but I thought I would post a proof of concept branch I had that promotes localDeletionTime to long on trunk. The code definitely isn't meant to be a final product but it might be a useful starting point regardless of whether we actually decide to go this route or not. It's fairly straight forward and unfortunately most of the code went towards undoing optimizations introduced to tombstone histograms in CASSANDRA-13444. I put together the majority of this last year so I may be forgetting some glaring issues but I believe it was basically complete minus a few unit tests to be cleaned up and tools updated.

One outstanding issue I do recall is related to EXPIRED_LIVENESS_TTL which is currently Integer.MAX_VALUE but sounds like that should be resolved/removed at some point by CASSANDRA-13826.
||trunk||
|[branch|https://github.com/vincewhite/cassandra/commit/364f9ac848ae54eae9a1360d72aad4ba0a2b63a8]|;;;","30/Jan/18 02:22;jjirsa;Given the nature of this bug, can we get someone familiar with engine internals (read: an existing committer) to review it? Like maybe [~beobal] ? ;;;","30/Jan/18 05:13;pauloricardomg;{quote}Some review comments on the dtest and trunk changes:
{quote}
Thanks for your feedback! I addressed your suggestions/nits and updated the warning message to:
{noformat}
Request on table ks.ttl_table with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
{noformat}
In addition to the fixes above, [~jasonstack] pointed out offline that the {{default_time_to_live}} is not capped to 20 years on the 2.1 branch, so I added the cap [here|https://github.com/apache/cassandra/commit/4d592eb990c1f7a84ae79738ea3ffb22f67282d3]. This made me realize that the warning was not being print when the insert was using the table default TTL, so I fixed this [here|https://github.com/apache/cassandra/commit/109fa214fbedc3b803e47f8dc79b41132ef2a1eb] and added a dtest [here|https://github.com/apache/cassandra-dtest/commit/09f39d5910d4c0dd1b800698deec7f7a6c5c747a]. I also added some dtests to check that the warning is print when the access is done via thrift ([here|https://github.com/apache/cassandra-dtest/commit/748ab67a1ce3950640747d6b980ef57b7871e59b]).

I updated all branches above with the changes above + other minor fixes and test fixes. Will update with test results when CI is ready.
{quote}I thought I would post a proof of concept branch I had that promotes localDeletionTime to long on trunk.
{quote}
Thanks a lot for your effort! While this looks like a straightforward approach and I don't see any particular problems with it at first glance, I'd like to focus right now on shipping a temporary solution to fix the silent data loss problem on 3.0+ as soon as we can before working on a permanent solution with proper vetting, testing and impact analysis.;;;","30/Jan/18 19:15;samt;This is a nasty problem because all the potential solutions are bad and I should say that I've only gone in depth on the 3.0 patch as of yet, but I assume that the other versions are functionally equivalent.

I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.

However, there is another issue as the current patch can also lead to previously 'gone' data being resurrected without warning or notification. If an SSTable contains data with an overflowed expiration, from the client's perspective that data is not present. Applying the patch before the data is purged fixes up the expiration date, capping it at the limit date and so the previously gone data will once again be returned in query results. Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible. My preference would be for gone data to stay gone, with another -D flag to turn on post-insert capping of the expiration date.

Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.
;;;","30/Jan/18 19:41;jjordan;{quote}I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.
{quote}
I think our default should be to cap with warnings.  What is a user going to do when they get the error?  They are going to just go change there code to pick a new date that is still real big, so I don't see a reason to fail things.
{quote}Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible.
{quote}
We need a way to let users recover the data that was silently dropped.  I could see an argument for a -D to let the data stay dropped, rather than recovering it, but I definitely think our default here should be to recover the lost data.
{quote}Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.
{quote}
Does this actually happen?  The expiration date will be fixed up when the cell is loaded, so on compaction it should be written back out with the new time?  If this is not what happens then it is an over sight in the patch and should be fixed.;;;","30/Jan/18 20:02;pauloricardomg;bq. I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.

My reasoning for this is that once we fix the issue permanently, the idea is to restore/recompute the correct localExpirationTime via (timestamp/1000 + ttl). This is obviously not perfect as the timestamp could be provided by the client, so there could be some slight variance here, but if someone is setting a TTL 20 years in advance, I think it is able to tolerate a few seconds or even minutes of difference in the expiration time. I don't think the case where the client is using a different timestamp format plus overflowing TTL is realistic enough that it will create problems, but we can also protect against this and perhaps provide and option to opt out of fix by default behavior if necessary.;;;","30/Jan/18 20:17;pauloricardomg;bq. Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible. My preference would be for gone data to stay gone, with another -D flag to turn on post-insert capping of the expiration date.

I don't expect us to take longer than a few weeks (at worst a couple of months) to come up with a permanent solution for this, our goal here is to prevent users from inadvertedly losing data in the time frame where the permanent fix is not available. People that detect that data have gone missing without issuing a deletion, will be aware of these issues and will likely take additional measures to remediate it - like reducing the TTL or update their application to correctly handle the data in the affected time period. I personally find it highly unlikely that this scenario will be a problem in practice,  as long we properly document and communicate the problem on NEWS.txt, users that were affected and care about their data will likely take additional measure to recover their state. ;;;","30/Jan/18 20:18;samt;bq. I think our default should be to cap with warnings.  What is a user going to do when they get the error?  They are going to just go change there code to pick a new date that is still real big, so I don't see a reason to fail things.

Yes, I imagine that's exactly what they will do, but the user will fully aware that it's happening, not having their data changed on them. We ought to be being conservative by default here.

bq. We need a way to let users recover the data that was silently dropped.  I could see an argument for a -D to let the data stay dropped, rather than recovering it, but I definitely think our default here should be to recover the lost data.

I don't agree. It's definitely bad that the data was silently dropped, but like I said we (the db) has no way of knowing what (application) decisions have been taken based on the visible state of the database. I think it's pretty clear that if the data appeared to be gone at any point (and we have no good reason to assume that the client has not observed such a state) that it must stay gone. 

bq. Does this actually happen?

Yes, of course I tested it.;;;","30/Jan/18 20:23;pauloricardomg;bq.  Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.

This was definitely not supposed to happen, I'll take a look at it. Would you have an easy repro for this?

Thanks for taking the time to review this [~beobal] and for the comments [~jjordan]!;;;","31/Jan/18 09:48;samt;{quote}Would you have an easy repro for this?
{quote}
Sure:
 * On an unpatched version (I used 3.0), insert a row with a big enough TTL
 * Query & see that the data is not returned.
 * Apply the patch & restart
 * Query & see that the row now appears
 * Run nodetool compact on the table
 * Query & the row has gone again

If you do *only* these steps on a clean node, then the compaction will result in 0 SSTables for the table. A more normal workflow of inserting rows then flushing until an automatic compaction is triggered also has the same effect, but of course doesn't remove any data without the overflowed expiry.;;;","01/Feb/18 02:28;KurtG;I don't think it's a good idea not to fix the data. That's incrementally going to break more and more applications as we get closer to 2038.  We should be fixing the data, making it very clear that we are in fact fixing the data (client and server warnings) and advertising heavily on the website/ML/NEWS.txt for people to get off affected versions.;;;","01/Feb/18 02:58;jjirsa;I don’t care if there’s logic to resurrect data, but it must be off by default. People may have already adapted and reacted to the absence of that data, and we can’t assume it's always safe to resurrect
;;;","01/Feb/18 06:45;pauloricardomg;After a second thought I agree that trying to automatically recover lost data is bad not only because users may have reacted to the absence of data, but also because due to the negative {{localDeletionTime}}, the lost data will likely be purged instantly during compaction, so not much data will be left to recover after a while and the auto-recovery behavior will be pretty much undefined. Furthermore, we will need to keep the recovery logic in the storage engine at least until 4.0 with limited benefit.

The only way a user that set TTL 20 years ahead can get away with reliably recovering its data is if it has {{incremental_backups}} enabled, and in this case, it can easily find out which SSTables have a negative {{minLocalDeletionTime}} and run scrub on them to fix the overflow, so I'm strongly leaning towards my previous approach of providing a way for scrub to fix negative timestamps, rather then embedding this recovery logic in the storage engine itself - which would be arguably useful in a handful of cases.

Regarding what to do with TTLs exceeding the maximum expiry date until we have a permanent solution to the problem, I think it's fair to expose the cap vs reject trade-off as flag to the operator, so I introduced a {{-Dcassandra.expiration_date_overflow_policy={REJECT, CAP_NOWARN, CAP_WARN}} option, which defaults to the conservative REJECT behavior and has the following meaning explained on NEWS.txt:
{noformat}
      - REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.
      - CAP_NOWARN: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00.
      - CAP_WARN: same as previous, but will log a warning message when inserted data is capped.
{noformat}
Any feedback and suggestions of alternative more meaningful naming is welcome.

One downside of defaulting to REJECT I see is that applications will start to break as time progresses and TTLs start reaching the maximum date more often, but given that we plan to fix the underlying issue soon in upcoming versions and users have a way to change the policy I think it shouldn't be a big problem, but I don't feel strongly about it so if anyone does please chime in.

The patch below implements and tests the expiration_date_overflow_policy, but also adds a new flag {{cassandra.recover_overflowed_expiration_best_effort}} to perform the auto-recovery, which I intend to remove due to the reasons explained above and replace it with the [scrub-based recovery|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092] (unless someone objects).

I plan to cleanup these details later today, but if anyone can take a look and give early feedback that would be nice, as I'm hoping to get this in as soon as we can to start a vote.

The 2.1 and 3.0 are functionally equivalent, except that the 2.1 patch adds a missing 20-years cap for the default_time_to_live option, and that the 3.0 patch adds the to-be-removed {{cassandra.recover_overflowed_expiration_best_effort}} option. The versions for the other branches are pretty much derived from those so I will do that after the final review round.
||2.1||3.0||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v3]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v3]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|;;;","01/Feb/18 11:22;samt;Sounds good to me. Reject and optionally cap at input time, plus a offline route to full recovery (if possible & desired) via scrub sounds like the best way forward IMO. I'll do a full review when you have the cleaned up patches ready, but the WIP branches generally LGTM at first glance. The wording of the NEWS.txt entry is good, I do wonder if we should maybe place it right at the top of the file rather than just in the 3.0.16 section for extra emphasis. Any thoughts on that? 

I also have one piece of feedback on the policies; I don't see any benefit in being able to turn off logging of capped expirations (especially since we're using NoSpamLogger) but I do I think the client warning is useful. So I would change the policies to:
{noformat}
- REJECT (default and as you've defined it)
- CAP (cap, log and issue client warning)
- CAP_NOWARN (cap and log)
{noformat}

I also noticed that the logging of a parse error/invalid value for the policy sysprop is at DEBUG in the current patches, but it might be sensible to draw a bit more attention to that if it happens.
;;;","01/Feb/18 23:51;KurtG;To clarify, CAP and CAP_NOWARN will cap the expiry but we'll have NO INTENTION of ever fixing it in an upgrade? Or would they have to do a scrub to convert anything that got capped to its actual TTL?

I think it's worth pointing out that REJECT is a ticking time bomb. The main concern being people who are still running anything <4.0 when their TTL's breach 2038-01-19 (which could be literally at any time). If the default was CAP and warn, fixing after upgrade then at least we wouldn't be bound to break peoples application in the future, and we'd still have almost 20 years to get everyone off these versions without breaking their applications.;;;","02/Feb/18 05:54;pauloricardomg;Thanks for the quick turnaround [~beobal]! See follow-up below:
{quote}The wording of the NEWS.txt entry is good, I do wonder if we should maybe place it right at the top of the file rather than just in the 3.0.16 section for extra emphasis. Any thoughts on that?
{quote}
Good idea, I did this and also updated the text to contemplate the possibility of data loss before this patch and how to fix it with scrub:
{noformat}
MAXIMUM TTL EXPIRATION DATE NOTICE
-----------------------------------

The maximum expiration timestamp that can be represented by the storage engine is 2038-01-19T03:14:06+00:00,
which means that inserts with TTL that expire after this date are not currently supported.

Prior to 3.0.16 in the 3.0.X series and 3.11.2 in the 3.11 series, there was no protection against INSERTS
with TTL expiring after the maximum supported date, causing the expiration time field to overflow and the
records to expire immediately. Expired records due to overflow may have been removed permanently after a
compaction. The 2.1.X and 2.2.X series are not subject to data loss due to this issue if assertions are enabled,
since an AssertionError is thrown during INSERT when the expiration time field overflows on these versions.

In practice this issue will affect only users that use very large TTLs, close to the maximum allowed value of
630720000 seconds (20 years), starting from 2018-01-19T03:14:06+00:00. As time progresses, the maximum supported
TTL will be gradually reduced as the the maximum expiration date approaches. For instance, a user on an affected
version on 2028-01-19T03:14:06 with a TTL of 10 years will be affected by this bug, so we urge users of very
large TTLs to upgrade to a version where this issue is addressed as soon as possible.

Potentially affected users should inspect their SSTables and search for negative min local deletion times to
detect this issue. SSTables in this state must be backed up immediately, as they are subject to data loss
during auto-compactions, and may be recovered by running the sstablescrub tool from versions 3.0.16+ and/or 3.11.2+.

The Cassandra project plans to fix this limitation in newer versions, but while the fix is not available, operators
can decide which policy to apply when dealing with inserts with TTL exceeding the maximum supported expiration date:
  - REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.
  - CAP: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00 and the client will receive a warning.
  - CAP_NOWARN: same as previous, except that the client warning will not be emitted.

These policies may be specified via the -Dcassandra.expiration_date_overflow_policy=POLICY startup option which can be set in the jvm.options file.

See CASSANDRA-14092 for more details about this issue.
{noformat}
Please let me know what do you think of the updated text. We should also probably publish this text (or a subset of it) during the release announcement e-mail.

While writing the text above, I figured that there is also a remote possibility of data loss in 2.1/2.2 if assertions are disabled, but didn't backport the scrub recovery since it was not a straightforward backport and I didn't think it was worth the effort right now. We can always do that later if necessary, the most important thing right now is to ship the policies. To reflect this I updated the 4th paragraph on 2.1 and 2.2 to:
{noformat}
2.1.X / 2.2.X users in the conditions above should not be subject to data loss unless assertions are disabled, in which
case the suspect SSTables must be backed up immediately and manually recovered, as they are subject to data loss
during auto-compaction.
{noformat}
 
{quote}I also have one piece of feedback on the policies; I don't see any benefit in being able to turn off logging of capped expirations (especially since we're using NoSpamLogger) but I do I think the client warning is useful.
{quote}
I agree and updated the patch with this suggestion, but at the same time I think advanced operators may want to control the periodicity of the logging, so I created a property {{cassandra.expiration_overflow_warning_interval_minutes=5}} to control this.
  
{quote}I also noticed that the logging of a parse error/invalid value for the policy sysprop is at DEBUG in the current patches, but it might be sensible to draw a bit more attention to that if it happens.
{quote}
Agreed, changed the logging to WARN.

I finished the cleanup of the patch and already provided a version for all branches. The 2.1 and 2.2 versions are pretty much the same, as well as the 3.0/3.11/trunk, except for some minor conflicts. Please find below a short summary of the changes per branch:
 * 2.1:
 ** Add REJECT and CAP expiration date overflow policies and tests
 ** Cap max default TTL at 20 years and tests
 ** Add NEWS.txt entry
 * 2.2:
 ** Same as 2.1, few minor import conflicts
 * 3.0
 ** Add REJECT and CAP, CAP_NOWARN expiration date overflow policies and tests
 ** Add ability to scrub to fix negative localDeletionTime and tests with broken SSTables
 ** Add ability to sstablemetadata to show minLocalDeletionTime
 ** Add expiration date overflow policies to jvm.options file
 ** Add NEWS.txt entry
 * 3.11
 ** Same as 3.0, few minor conflicts during merge
 * master
 ** Same as 3.11, few minor conflicts during merge
 ** Removed ability of scrub to fix sstables with negative localdeletionTime and tests
 * dtest
 ** Test all policies on CQL for default and user supplied TTL
 ** Test cap policy on thrift for default and user supplied TTL
 ** Check that offline scrub recovers sstable with negative localDeletionTime

I submitted a preliminary round of CI with the non-cleaned up patch and the results looked good. I will submit again for all the branches below and post the results here when they are ready.
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-v5]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v5]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092-v5]|;;;","02/Feb/18 06:07;pauloricardomg;{quote}To clarify, CAP and CAP_NOWARN will cap the expiry but we'll have NO INTENTION of ever fixing it in an upgrade?
{quote}
We're not promising to fix the correct TTL in an upgrade, but we could do it in some cases, but I prefer to leave this decision for later.
{quote}Or would they have to do a scrub to convert anything that got capped to its actual TTL?
{quote}
The scrub is just to fix SSTables of affected systems that overflowed from 2018-01-19T03:14:06+00:00 until the upgrade and were backed up. As I said before, we're not doing any promise yet regarding honoring the actual TTL when it's capped. But if we were to implement this would probably be done during upgradesstables and not scrub.
{quote}I think it's worth pointing out that REJECT is a ticking time bomb.
{quote}
I agree but I don't feel strongly about the default, because the policies will be clearly specified in big letters in the NEWS.txt which is the document which everyone should read before upgrading, so if you don't want applications to break in your organization just change your policy to CAP.

Do you mind proof-reading the NEWS.txt and check if something is not clear/can be improved? Thanks!;;;","02/Feb/18 08:36;mshuler;{quote}We should also probably publish this text (or a subset of it) during the release announcement e-mail.
{quote}
I'll definitely include the text with the release, which sounds pretty complete to me. Thanks for writing it up.;;;","05/Feb/18 02:09;KurtG;bq. Do you mind proof-reading the NEWS.txt and check if something is not clear/can be improved? Thanks!
Looks good, but a few things to reduce confusion:
# Can we stick a PLEASE READ/WARNING/ALERT or something in the title to make it clear that users actually have to read that section?
Can we be a bit more clear on the following points?
# Expired records due to overflow *will* be removed permanently after a compaction. (there's only a very small chance that it would survive a compaction, and that's only if they previously wrote the same record and it overflowed by more, which somehow made it into a separate SSTable and didn't get compacted with )
# Data that is expired due to overflow *will* not be queryable and that prior to patching they'll have no way of telling they inserted with an overflowed TTL.
# How to search for negative min local deletion times (sstablemetadata)
# Probably a good time to note that all users will need to upgrade to >4.0 by 2038 and that during upgrade their TTL's will be set to their expected value.
;;;","05/Feb/18 20:04;samt;I think we're almost there with this, I just have a few smallish comments on the v5 patches:
 * In the 3.0+ branches, {{ExpirationDateOverflowHandling::maybeApplyExpirationDateOverflowPolicy}} can use {{Cell.NO_TTL}} rather than 0 in the first check.
 * In 3.0+ you renamed the static policy field to have a shorter name, but missed that in the 2.1 & 2.2 branches.
 * In 2.1 I saw some (very) intermittent test failures in TTLTest. I instrumented {{checkTTLIsCapped}} to print out the (min | actual | max) TTLs to sysout and eventually managed to repro it. You can see from the output that in the first line, the min and actual are actually > the max, which caused the test to fail (this happened around 10% of the time).

{code:java}
testlist:
 [echo] running test bucket 0 tests
 [junit] WARNING: multiple versions of ant detected in path for junit 
 [junit] jar:file:/usr/local/Cellar/ant@1.9/1.9.8/libexec/lib/ant.jar!/org/apache/tools/ant/Project.class
 [junit] and jar:file:/Users/sam/git/cassandra/build/lib/jars/ant-1.9.4.jar!/org/apache/tools/ant/Project.class
 [junit] Testsuite: org.apache.cassandra.cql3.validation.operations.TTLTest
 [junit] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.024 sec
 [junit] 
 [junit] ------------- Standard Output ---------------
 [junit] WARN 18:01:55 JNA link failure, one or more native method will be unavailable.
 [junit] WARN 18:01:55 JNA link failure, one or more native method will be unavailable.
 [junit] WARN 18:01:55 Request on table cql_test_keyspace.table_1 with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
 [junit] WARN 18:01:55 Request on table cql_test_keyspace.table_1 with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
 [junit] 629629931 | 629629931 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] ------------- ---------------- ---------------
 [junit] Testcase: testCapExpirationDatePolicyDefaultTTL(org.apache.cassandra.cql3.validation.operations.TTLTest): FAILED
 [junit] null
 [junit] junit.framework.AssertionFailedError
 [junit] at org.apache.cassandra.cql3.validation.operations.TTLTest.checkTTLIsCapped(TTLTest.java:259)
 [junit] at org.apache.cassandra.cql3.validation.operations.TTLTest.testCapExpirationDatePolicyDefaultTTL(TTLTest.java:139)
 [junit] 
 [junit]
{code}
The last thing is about providing a route to fix up overflowed dates via scrub, I think we should definitely leave the remedial Scrubber code in trunk until we have a proper fix committed. This will be in for 4.0, and at that point we can hapilly remove it, but until then it feels wrong to not have it. I also think we should have the scrub fix in 2.1 & 2.2 as some users have not yet moved to 3.x and they should probably get the chance to repair (maybe) their data if they want/are able to.

 ;;;","08/Feb/18 17:10;pauloricardomg;Thanks for the review, this took a bit longer than expected as I found an edge case which required a change from the previous approach as I describe later. First, see the follow-up from the previous round of review:
{quote}In the 3.0+ branches, ExpirationDateOverflowHandling::maybeApplyExpirationDateOverflowPolicy can use Cell.NO_TTL rather than 0 in the first check.
{quote}
Agreed, fixed [here|https://github.com/apache/cassandra/commit/4bcea858f62b619b83a5db83f0cd93e192fea80c].
{quote}In 3.0+ you renamed the static policy field to have a shorter name, but missed that in the 2.1 & 2.2 branches.
{quote}
Good catch, fixed [here|https://github.com/apache/cassandra/commit/df59f2de8042e7ea67e520d509d675da1d53bbce].
{quote}In 2.1 I saw some (very) intermittent test failures in TTLTest. I instrumented checkTTLIsCapped to print out the (min | actual | max) TTLs to sysout and eventually managed to repro it. You can see from the output that in the first line, the min and actual are actually > the max, which caused the test to fail (this happened around 10% of the time).
{quote}
Oh, that's funny! Perhaps it could be related to the platform, as I cannot reproduce this locally? In any case, I updated the check [here|https://github.com/apache/cassandra/commit/18be7f140c3c2159f69d50b1bfb068927c734a13] to be more deterministic.

I also noticed that we weren't applying the expiration overflow policy in the CQLv2 interface, so I updated it [here|https://github.com/apache/cassandra/commit/4230a5b4126e972827990dc33c1a9140af07afe1]. I find it hard someone is using this but I wouldn't be totally surprised.
{quote}I think we should have the scrub fix in 2.1 & 2.2 as some users have not yet moved to 3.x and they should probably get the chance to repair (maybe) their data if they want/are able to.
{quote}
Agreed, as noted in the NEWS.txt, 2.1/2.2 is only affected if users have assertions disabled, but given that we have this [comment|https://github.com/apache/cassandra/blob/cassandra-2.1/conf/cassandra-env.sh#L173] on 2.1 I wouldn't be surprised if some users with assertions disabled hit this.

While writing the 2.1 scrubber (which is slightly different from 3.0 due to CASSANDRA-8099), I found a nasty edge case with the recovery process. The problem is that, if the cell with negative/overflowed {{localExpirationTime}} [is converted to a tombstone during compaction|https://github.com/apache/cassandra/blob/30ed83d9266a03debad98ffac5610dcb3ae30934/src/java/org/apache/cassandra/db/rows/AbstractCell.java#L95], we can't convert the tombstone back into an expired cell because the TTL value is lost and we can't differentiate this from an ordinary tombstone. Furthermore, even if the original SSTable is scrubbed and the negative {{localExpirationTime}} is converted to {{MAX_DELETION_TIME}}, if the tombstone was already generated, it will shadow/delete the fixed expired cell, because [deleted cells have precedence over live cells when the timestamp is the same|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/db/Conflicts.java#L43].

I updated {{recover_negative_expiration_date_sstables_with_scrub}} to [add an SSTable with the expired cell converted to a tombstone|https://github.com/apache/cassandra-dtest/commit/63b0e7d51fbacf4fabbb860d81873605c3b66c92], and verified that an existing tombstone shadows recovered data in the current version.

In order to fix this, the idea is during scrub increment the timestamp of cells with negative {{localExpirationTime}} by one in addition to setting the {{localExpirationTime}} to {{MAX_DELETION_TIME}}, so the recovered cell will shadow/supersede the potential tombstone that was already written.

Since this will not recreate the row, but technically reinsert the row with a higher timestamp, we cannot do this automatically on scrub, since it may overwrite an existing row inserted just one millisecond after the original row (while very hard to happen in practice, this may depend on application logic). For this reason, the user needs to specify the {{--reinsert-overflowed-ttl}} option during scrub to perform the recovery.

This approach is implemented [here|https://github.com/apache/cassandra/commit/5f9f704449ed1ef579c61953b02a9ef32f13082b] on 3.0 and ported to all other branches in a separate commit.
{quote}The last thing is about providing a route to fix up overflowed dates via scrub, I think we should definitely leave the remedial Scrubber code in trunk until we have a proper fix committed.
{quote}
Since 4.0 was not released, and it will come out with this fix it's impossible that someone will hit this on 4.0, but I don't see any problem in keeping the scrub option for the lazy ones upgrading from 3.x without fixing their SSTables beforehand. :) For this reason I kept the 3.11 SSTables in the 4.X (d)tests.

To prevent scrub and the tests from throwing an {{AssertionError}} on 2.1 I removed [this assertion|https://github.com/apache/cassandra/commit/4501eee5c962547c059a4f624155c5e18d5369d3], since it's no longer necessary given we apply the overflow policy.

Finally, I [refactored|https://github.com/apache/cassandra/commit/c3e1fcc25b3db9db212dfc805a47430ff537b101] the NEWS.txt section a bit to take [~KurtG] comments into account and also to give more emphasis to the expiration overflow policies since this will be what most users need to care about, and added a recovery subsection with detailed recovery instructions.

I now wonder if this notice has gotten to big and is cluttering the NEWS.txt file too much - what may confuse users looking for upgrade instructions - and we should maybe create a new WARNING/IMPORTANT file and add a pointer to it in the NEWS.txt file instead?

To facilitate review, I squashed the previous commits and created a new branch with new commits only representing changes since last review (except for the 2.2 patch, which is basically the same as 2.1 except the additional commit simplifying TTLTest). The previous CI looked good, I will submit a new CI run and post the results here later.
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-v6]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v6]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|;;;","09/Feb/18 12:15;samt;{quote}Since 4.0 was not released, and it will come out with this fix it's impossible that someone will hit this on 4.0, but I don't see any problem in keeping the scrub option for the lazy ones upgrading from 3.x without fixing their SSTables beforehand.
{quote}
yeah, this was the scenario I was thinking about.
{quote}I now wonder if this notice has gotten to big and is cluttering the NEWS.txt file too much - what may confuse users looking for upgrade instructions - and we should maybe create a new WARNING/IMPORTANT file and add a pointer to it in the NEWS.txt file instead?
{quote}
I tend to agree with you. Even if adding another file is less than ideal, it's probably better than adding a large, unchanging header as that's likely just going to lead to more users not properly reading it and missing critical info about other future upgrades. So I'd be +1 on adding a new file ({{CASSANDRA\-14092.txt}} maybe?) and just a brief explanation of the severity of the issue plus a *very* clear pointer to more detailed file at the top of {{NEWS.txt}}. I think you should keep the
{code:java}
WARNING: MAXIMUM TTL EXPIRATION DATE NOTICE
--------------------------------------------
(General upgrading instructions are available in the next section)

The maximum expiration timestamp that can be represented by the storage engine is 2038-01-19T03:14:06+00:00,
which means that inserts with TTL that expire after this date are not currently supported.
{code}

and add the pointer after that.

1 nit on the text in {{NEWS.txt}}, you have a double ""the"" in the sentence 
 {{""As time progresses, the maximum supported TTL will be gradually reduced as the the maximum expiration date approaches.""}}

Extracting the news text to a new file notwithstanding, I'm +1 on the latest patches (pending CI, of course). Thanks for all the hard work on this [~pauloricardomg];;;","11/Feb/18 13:37;pauloricardomg;{quote}I tend to agree with you. Even if adding another file is less than ideal, it's probably better than adding a large, unchanging header as that's likely just going to lead to more users not properly reading it and missing critical info about other future upgrades. So I'd be +1 on adding a new file (CASSANDRA-14092.txt maybe?) and just a brief explanation of the severity of the issue plus a very clear pointer to more detailed file at the top of NEWS.txt.
{quote}
Sounds good. Settled on this (not-so) summarized note for NEWS.txt (the full text goes on CASSANDRA-14092.txt):
{noformat}
PLEASE READ: MAXIMUM TTL EXPIRATION DATE NOTICE (CASSANDRA-14092)
------------------------------------------------------------------
(General upgrading instructions are available in the next section)

The maximum expiration timestamp that can be represented by the storage engine is
2038-01-19T03:14:06+00:00, which means that inserts with TTL thatl expire after
this date are not currently supported. By default, INSERTS with TTL exceeding the
maximum supported date are rejected, but it's possible to choose a different
 expiration overflow policy. See CASSANDRA]-14092.txt for more details.

Prior to 3.0.16 (3.0.X) and 3.11.2 (3.11.x) there was no protection against INSERTS
with TTL expiring after the maximum supported date, causing the expiration time
field to overflow and the records to expire immediately. Clusters in the 2.X and
lower series are not subject to this when assertions are enabled. Backed up SSTables
can be potentially recovered and recovery instructions can be found on the
CASSANDRA-14092.txt file.

If you use or plan to use very large TTLS (10 to 20 years), read CASSANDRA-14092.txt
for more information.
{noformat}

{quote}Extracting the news text to a new file notwithstanding, I'm +1 on the latest patches (pending CI, of course).
{quote}
Still had to fix a couple of test nits and resubmit tests after rebase to get a clean run (attached internal CI screenshots, verified other failures and they look unrelated).

I've added the new CASSANDRA-14092.txt file to the debian/redhat package manifests [here|https://github.com/apache/cassandra/commit/a25898253ea3b5bc7262ff2d17f8466d489eaf96] (thanks [~mshuler] for the help).

Committed as {{b2949439ec62077128103540e42570238520f4ee}} to cassandra-2.1 and merged up to cassandra-2.2, cassandra-3.0, cassandra-3.11 and master (phew). cassandra-dtest commit merged as {{781c4ecdf67bb63f508e8af70599b45b896c28ae}}.

I've kept the previous branches with separate commits, and created a new branch with the squashed + rebased version:
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-final]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-final]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092-rebased]|

I will open follow-up tickets to fix this limitation permanently and add the expiration overflow notice to the docs.

Thanks for all who helped with feedback and Sam for kindly reviewing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicSnitch creates a lot of garbage,CASSANDRA-14091,13122301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,01/Dec/17 23:01,15/May/20 08:05,14/Jul/23 05:56,06/Dec/17 00:11,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,,,0,,,,,"The ExponentiallyDecayingReservoir snapshots we take during score updates generate a lot of garbage, and we call getSnapshot twice per endpoint when we only need to call it once.",,bdeggleston,cnlwsu,githubbot,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 00:45:21 UTC 2017,,,,,,,,,,"0|i3ngbr:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"02/Dec/17 00:19;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14091-3.0] |[3.11|https://github.com/bdeggleston/cassandra/tree/14091-3.11] |[trunk|https://github.com/bdeggleston/cassandra/tree/14091-trunk]|
|[tests|https://circleci.com/gh/bdeggleston/cassandra/180] |[tests|https://circleci.com/gh/bdeggleston/cassandra/181] |[tests|https://circleci.com/gh/bdeggleston/cassandra/179] |;;;","02/Dec/17 13:36;jasobrown;Nice find. +1;;;","06/Dec/17 00:11;bdeggleston;Thanks, committed as {{10ca7e47ca63c43b4e0ba593fb4c736130764af9}};;;","06/Dec/17 00:45;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/178
  
    Hi @Benmartin92 , That looks like a perfectly reasonable patch. It appears that @bdeggleston also noticed this in https://issues.apache.org/jira/browse/CASSANDRA-14091 (a week or so after your patch). Unfortunately, I don't think anyone noticed your patch, probably because we don't typically use Github PRs (ASF repo isn't writable on github, so we can't merge this PR directly) - our contribution workflow is through JIRA. Appreciate the contribution, sorry we didn't see it in time. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.generate.Distribution.average broken on trunk,CASSANDRA-14090,13122298,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,01/Dec/17 22:48,15/May/20 08:01,14/Jul/23 05:56,06/Dec/17 00:01,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Looks like the lgtm.com fixes slightly changed the behavior of Distribution.average, which prevents stress from starting up",,bdeggleston,jasonstack,jay.zhuang,rustyrazorblade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 13:54:37 UTC 2017,,,,,,,,,,"0|i3ngb3:",9223372036854775807,,,,,,,rustyrazorblade,,rustyrazorblade,,,Low,,,,,,,,,,,,,,,,,,,"01/Dec/17 22:52;bdeggleston;patch against [trunk|https://github.com/bdeggleston/cassandra/tree/14090];;;","04/Dec/17 22:23;rustyrazorblade;Verified bug with:

{code}
tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1,simple=1)' n=1000 cl=ONE -mode native cql3 -rate threads=100 fixed=1000/s
{code}

Fixed by your patch, +1.;;;","06/Dec/17 00:01;bdeggleston;thanks, committed as {{d8f0361228aec146689b8f9402c7aa61d95bdc05}};;;","11/Dec/17 13:54;jasonstack;[~bdeggleston]  

bq. tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml ops(insert=1)
should we also change the following line to {{50}} ?

{code}
return (long) (sum / 51);
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward slash in role name breaks CassandraAuthorizer,CASSANDRA-14088,13122203,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,KurtG,jhaberku,jhaberku,01/Dec/17 16:09,15/May/20 08:01,14/Jul/23 05:56,07/Dec/17 21:22,3.0.16,3.11.2,4.0,4.0-alpha1,,,Feature/Authorization,,,,,0,,,,,"The standard system authorizer ({{org.apache.cassandra.auth.CassandraAuthorizer}}) stores the permissions granted to each user for a given resource in {{system_auth.role_permissions}}.

A resource like the {{my_keyspace.items}} table is stored as {{""data/my_keyspace/items""}} (note the {{/}} delimiter).

Similarly, role resources (like the {{joe}} role) are stored as {{""roles/joe""}}.

The problem is that roles can be created with {{/}} in their names, which confuses the authorizer when the table is queried.

For example,

{code}
$ bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> CREATE ROLE emperor;
cassandra@cqlsh> CREATE ROLE ""ki/ng"";
cassandra@cqlsh> GRANT ALTER ON ROLE ""ki/ng"" TO emperor;
cassandra@cqlsh> LIST ROLES;

 role      | super | login | options
-----------+-------+-------+---------
 cassandra |  True |  True |        {}
   emperor | False | False |        {}
     ki/ng | False | False |        {}

(3 rows)
cassandra@cqlsh> SELECT * FROM system_auth.role_permissions;

 role      | resource      | permissions
-----------+---------------+--------------------------------
   emperor |   roles/ki/ng |                      {'ALTER'}
 cassandra | roles/emperor | {'ALTER', 'AUTHORIZE', 'DROP'}
 cassandra |   roles/ki/ng | {'ALTER', 'AUTHORIZE', 'DROP'}

(3 rows)
cassandra@cqlsh> LIST ALL PERMISSIONS OF emperor;
ServerError: java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
{code}

Here's the backtrace from the server process:

{code}
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,811 QueryMessage.java:129 - Unexpected error during query
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,812 ErrorMessage.java:389 - Unexpected exception during request
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}",Git commit: 4c80eeece37d79f434078224a0504400ae10a20d ({{HEAD}} of {{trunk}}).,jhaberku,jjirsa,jjordan,KurtG,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 23:52:12 UTC 2017,,,,,,,,,,"0|i3nfq7:",9223372036854775807,,,,,,,jjordan,,jjordan,,,Low,,,,,,,,,,,,,,,,,,,"04/Dec/17 03:38;KurtG;[3.0|https://github.com/apache/cassandra/compare/trunk...kgreav:14088-3.0]
No reason for us to split in {{RoleResource::fromName}} more than once, so just specified a max and added some tests to make sure we accept forward slashes and some other names correctly.
Patch is the same for trunk and 3.11. Should merge cleanly.;;;","04/Dec/17 09:13;snazy;Before we continue with this one - what's the reason for the forward slash(es) in the role name?
The delimiter {{/}} was chosen to split components - handling that differently across different resource types would be inconsistent.
I'd be much more in favor of validating the role name in {{CreateRoleStatement}} and restrict role names to a defined set of characters, like we do for keyspaces and tables.;;;","06/Dec/17 23:21;KurtG;Wild guess but I'd say it's probably because they have complicated role/permission domains and break them up by slashes in their environment, and would find it easiest to continue to use the same roles in C*, rather than having to change their delimiter. I've seen similar cases before w.r.t PKI/CN's/DN's.

Seeing as fromName is defined per resource I don't see why we can't have specific implementations for each {{Resource}}. In fact, in {{DataResource}} and {{FunctionResource}} we already handle each name differently as we require 3 {{/}} separators (+different sep's for {{FunctionResource}}.
At the moment any character is allowed in a role name except for slash, because of this issue. We only really care about the first slash, if we ever cared about more than that we'd be creating a new {{Resource}} anyway.;;;","07/Dec/17 02:26;jjordan;Agreed, we should just limit to only splitting the first ""/"".

Patch LGTM +1.;;;","07/Dec/17 20:56;jjirsa;This is marked ready to commit, but there's no mention of utest/dtests at all - if they're clean I'll commit, but could someone confirm that they're clean?
;;;","07/Dec/17 21:11;jjordan;I ran the unit test locally that extensively tests the change.  Feel free to remove the ready to commit if you want a full dtest run for it, but given we don't have any dtests which uses ""/"" in a name I did not feel it was needed.;;;","07/Dec/17 21:22;jjirsa;Works for me. Committed as {{f9de26a79de02e61624994e67e64f2c93fb5a35b}} and merged up to 3.11/trunk. Thanks to all 3 of you [~KurtG], [~jjordan] , [~jhaberku]!
;;;","07/Dec/17 23:52;KurtG;For the record, unit tests passed for me (I just didn't bother checking because I'd recently been flooded by build failures from CircleCI). [unit|https://circleci.com/gh/kgreav/cassandra/45]

Seems that it failed on [3.11|https://circleci.com/gh/kgreav/cassandra/44] but on an unrelated error.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when CAS encounters empty frozen collection,CASSANDRA-14087,13122153,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,jens.b,jens.b,01/Dec/17 11:18,15/May/20 08:00,14/Jul/23 05:56,08/Mar/18 23:57,3.0.17,3.11.3,4.0,4.0-alpha1,,,Feature/Lightweight Transactions,,,,,0,LWT,,,,"When a compare-and-set operation specifying an equality criterion with a non-{{null}} value encounters an empty collection ({{null}} cell), the server throws a {{NullPointerException}} and the query fails.

This does not happen for non-frozen collections.

There's a self-contained test case at [github|https://github.com/incub8/cassandra-npe-in-cas].

The stack trace for 3.11.0 is:

{code}
ERROR [Native-Transport-Requests-1] 2017-11-27 12:59:26,924 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
        at org.apache.cassandra.cql3.ColumnCondition$CollectionBound.appliesTo(ColumnCondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest$ColumnsConditions.appliesTo(CQL3CasRequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest.appliesTo(CQL3CasRequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}
",,bdeggleston,blerer,jasonstack,jay.zhuang,jens.b,jjirsa,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 13 23:57:10 UTC 2018,,,,,,,,,,"0|i3nff3:",9223372036854775807,3.0.0,3.11.0,3.11.1,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"05/Dec/17 04:19;KurtG;[3.0|https://github.com/apache/cassandra/compare/trunk...kgreav:14087-3.0]
We simply weren't checking (or testing) for this case. Trunk is fine as CASSANDRA-12981 fixed the issue.;;;","20/Dec/17 18:07;bdeggleston;|3.0|3.11|
|[tests|https://circleci.com/gh/bdeggleston/cassandra/196]|[tests|https://circleci.com/gh/bdeggleston/cassandra/197]|;;;","01/Feb/18 04:04;KurtG;[~bdeggleston] have you had a chance to review?;;;","07/Feb/18 20:50;bdeggleston;Sorry [~KurtG], I did some test runs and then forgot about them over the holidays. So I'm +1 on the patch for 3.0 and 3.11, however the test case you've added fails against trunk. Could you take a look at that?;;;","08/Feb/18 01:07;KurtG;That's worrying, pretty sure it worked when I tested it :S. Thanks, I'll have a look.;;;","19/Feb/18 03:17;KurtG;Turns out I didn't run the unit test on trunk, just manually tested on trunk. So we're throwing an {{AssertionError}} [here|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/cql3/Terms.java#L169] because a {{Lists.Marker}} is created out of the prepared statement used in the test. The assertion kind of implies you can't use a list in the condition of a prepared statement, but this was always possible previously. It's really unclear why that assert statement is there, and removing it solves all the problems. Anyway, LMK what you think. [~blerer] will likely know more about the assert.

[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14087-trunk];;;","19/Feb/18 08:09;blerer;[~KurtG] There are 2 {{of}} methods. One for a single term and one for a list of terms. For some reasons the wrong one is being picked.
I will have a look later today.;;;","19/Feb/18 09:40;blerer;Sorry, I misunderstood the problem. You can remove the assertion. It is a mistake from my part. I was only considering the {{IN}} logic and forgot about 
 frozen lists.

Small nit. {{CQLTester}} has some {{list}}, {{set}} and {{map}} methods that you can use instead of the {{Collections}} {{singleton}} methods.It makes the code more readable. ;;;","21/Feb/18 04:43;KurtG;Thanks [~blerer], I've updated both branches for the nit. 3.0 should still merge cleanly into 3.11.

[~bdeggleston] 'tis ready for review again.;;;","06/Mar/18 17:45;bdeggleston;started another round of tests:
|3.0|3.11|trunk|
|[tests|https://circleci.com/workflow-run/79cca9f9-fb55-4a6e-b53d-862018222bc8]|[tests|https://circleci.com/workflow-run/eac51382-1994-4218-ab7b-ea6e466580f2]|[tests|https://circleci.com/workflow-run/c932587d-a6e8-463e-881b-c28bb76bc81b]|;;;","08/Mar/18 23:57;bdeggleston;All the test failures that were already failing in their respective branches, so +1.

Committed as {{2c150980cc1bfea81fd039f304e74fc2fb30fb45}}. Thanks, and sorry for the delay getting this in;;;","11/Mar/18 15:34;jasonstack;{quote}Sorry, I misunderstood the problem. You can remove the assertion. 
{quote}
It looks like the [assertion|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/cql3/Terms.java#L169] is not removed from trunk and causing new unit test to fail..;;;","11/Mar/18 16:19;bdeggleston;oops, sorry about that. ninja removed it;;;","12/Mar/18 01:53;jasonstack;Thanks!;;;","13/Mar/18 23:57;KurtG;Thanks [~bdeggleston];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disks can be imbalanced during replace of same address when using JBOD,CASSANDRA-14084,13121966,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,30/Nov/17 17:00,15/May/20 08:06,14/Jul/23 05:56,11/Dec/17 20:24,3.11.2,4.0,4.0-alpha1,,,,,,,,,0,,,,,"While investigating CASSANDRA-14083, I noticed that [we use the pending ranges to calculate the disk boundaries|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/db/DiskBoundaryManager.java#L91] when the node is bootstrapping.

The problem is that when the node is replacing a node with the same address, it [sets itself as normal locally|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/service/StorageService.java#L1449] (for other unrelated reasons), so the local ranges will be null and consequently the disk boundaries will be null. This will cause the sstables to be randomly spread across disks potentially causing imbalance.",,jeromatron,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14083,CASSANDRA-12344,,,,,,,,,"01/Dec/17 13:35;pauloricardomg;dtest14084.png;https://issues.apache.org/jira/secure/attachment/12900219/dtest14084.png",,,,,,,,,,,,,,,,,,,1.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 20:24:57 UTC 2017,,,,,,,,,,"0|i3ne9r:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"30/Nov/17 17:18;pauloricardomg;This situation is reproduced by [this dest|https://github.com/pauloricardomg/cassandra-dtest/commit/1b96dfd855d1b2fc10cbb4cf2e4c95d236ecd951#diff-1ef92939c7765f8c4041bada71208eebR51].

The simple fix is to use normal tokens for replacement nodes with the same address:
* [3.11|https://github.com/pauloricardomg/cassandra/tree/3.11-14084]

CI looked clean when this was in CASSANDRA-13948, but I will submit again just to make sure this will not cause problems when committed separately.;;;","01/Dec/17 13:36;pauloricardomg;testall is clean, dtest failures [seem unrelated|https://issues.apache.org/jira/secure/attachment/12900219/dtest14084.png];;;","11/Dec/17 08:10;marcuse;+1;;;","11/Dec/17 20:24;pauloricardomg;Committed as {{50e6e721b2a81da7f11f60a2fa405fd46e5415d4}} to cassandra-3.11 and merged up to master, and dtest as {{3d2a6cc738d87d30cca8d747305a5899ccf3712d}}. Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not expose compaction strategy index publicly,CASSANDRA-14082,13121912,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,30/Nov/17 13:22,15/May/20 08:01,14/Jul/23 05:56,26/Dec/17 12:55,3.11.2,4.0,4.0-alpha1,,,,,,,,,0,,,,,"Before CASSANDRA-13215 we used the compaction strategy index to decide which disk to place a given sstable, but now we can get this directly from the disk boundary manager and keep the compaction strategy index internal only.

This will ensure external consumers will use a consistent {{DiskBoundaries}} object to perform operations on multiple disks, rather than risking getting inconsistent indexes if the compaction strategy indexes change between successive calls to {{CSM.getCompactionStrategyIndex}}.",,jeromatron,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13948,,,,,,,,,,,,,,,,,,,,,,"14/Dec/17 22:01;pauloricardomg;3.11-14082-dtest.png;https://issues.apache.org/jira/secure/attachment/12902161/3.11-14082-dtest.png","14/Dec/17 22:01;pauloricardomg;3.11-14082-testall.png;https://issues.apache.org/jira/secure/attachment/12902160/3.11-14082-testall.png","14/Dec/17 22:01;pauloricardomg;trunk-14082-dtest.png;https://issues.apache.org/jira/secure/attachment/12902159/trunk-14082-dtest.png","14/Dec/17 22:01;pauloricardomg;trunk-14082-testall.png;https://issues.apache.org/jira/secure/attachment/12902158/trunk-14082-testall.png",,,,,,,,,,,,,,,,4.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 26 12:55:00 UTC 2017,,,,,,,,,,"0|i3ndxr:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"30/Nov/17 13:26;pauloricardomg;Currently the scrubber and relocate sstables were relying on the compaction strategy index, so this patches change these operation to use a {{DiskBoundaries}} object instead and make {{CSM.getCompactionStrategyIndex}} private.

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-14082]

Since this depends on CASSANDRA-13948, I will wait until that is committed before setting this as PA.;;;","14/Dec/17 22:35;pauloricardomg;Rebased on top of latest now that CASSANDRA-13948, setting this to PA, tests look good:

||3.11||trunk||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14082]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14082]|
|[testall|https://issues.apache.org/jira/secure/attachment/12902160/3.11-14082-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12902158/trunk-14082-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12902161/3.11-14082-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12902159/trunk-14082-dtest.png]|;;;","22/Dec/17 11:27;marcuse;+1;;;","26/Dec/17 12:55;pauloricardomg;Committed as {{e208a6a210d172b991b40fb66a4763e30b3e4d7d}} to cassandra-3.11 branch and merged up to master. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle incompletely written hint descriptors during startup,CASSANDRA-14080,13121897,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alourie,alekiv,alekiv,30/Nov/17 12:06,07/Mar/23 11:52,14/Jul/23 05:56,29/Mar/18 13:05,3.0.17,3.11.3,4.0,4.0-alpha1,,,Consistency/Hints,,,,,0,,,,,"Continuation of CASSANDRA-12728 bug.

Problem: Cassandra didn't start due to 0 size hints files

Log form v3.0.14:
{code:java}
INFO  [main] 2017-11-28 19:10:13,554 StorageService.java:575 - Cassandra version: 3.0.14
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:576 - Thrift API version: 20.1.0
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:577 - CQL supported versions: 3.4.0 (default: 3.4.0)
ERROR [main] 2017-11-28 19:10:13,592 CassandraDaemon.java:710 - Exception encountered during startup
org.apache.cassandra.io.FSReadError: java.io.EOFException
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:142) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_141]
        at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[na:1.8.0_141]
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_141]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsCatalog.load(HintsCatalog.java:65) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<init>(HintsService.java:88) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<clinit>(HintsService.java:63) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:121) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.lang.Class.forName0(Native Method) ~[na:1.8.0_141]
        at java.lang.Class.forName(Class.java:264) ~[na:1.8.0_141]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:585) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:570) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:346) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.14.jar:3.0.14]
Caused by: java.io.EOFException: null
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:803) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsDescriptor.deserialize(HintsDescriptor.java:237) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:138) ~[apache-cassandra-3.0.14.jar:3.0.14]
        ... 20 common frames omitted
{code}



After several 0 size hints files deletion Cassandra started successfully.

Jeff Jirsa added a comment - Yesterday
Aleksandr Ivanov can you open a new JIRA and link it back to this one? It's possible that the original patch didn't consider 0 byte files (I don't have time to go back and look at the commit, and it was long enough ago that I've forgotten) - were all of your files 0 bytes?

Not all, 8..10 hints files were with 0 size.",,alekiv,aleksey,alourie,jay.zhuang,jeromatron,jjirsa,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11090,,,CASSANDRA-12728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alourie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 13:03:52 UTC 2018,,,,,,,,,,"0|i3nduf:",9223372036854775807,3.0.14,5.0,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"30/Nov/17 12:10;aleksey;There has been a resolved JIRA some time ago to not write out empty hints files - don't have the number handy, but try searching for it.;;;","30/Nov/17 12:19;alekiv;[~iamaleksey], this particular report is about handling empty hints files during Cassandra start. Seems currently this situation is not handled properly.;;;","30/Nov/17 12:21;aleksey;Sorry. I don't mean that you shouldn't file/have filed this JIRA. Just saying that the similar one we closed recenlty-ish might have some useful context, so you might want to look it up and link to this one.;;;","30/Nov/17 18:22;jjirsa;Probably: CASSANDRA-13740;;;","04/Dec/17 02:48;alourie;A patch for trunk is in https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-14080;;;","04/Dec/17 11:56;aleksey;[~jjirsa] Nope. Talking about a JIRA specifically about writing out empty hint buffers into empty files.

That would do it - although I'd prefer to just slap a second filter call instead of combining the two here.

What I'd prefer even more is if we looked into why an empty hint file gets written in the first place and taking care of it. Which would have the added benefit of not having confusing empty files around.;;;","04/Dec/17 11:58;aleksey;CASSANDRA-11090 (https://github.com/apache/cassandra/commit/1f626087c8819b75f17fcbe757603fc0026d3cc1).;;;","05/Dec/17 00:16;alourie;[~iamaleksey] That should prevent empty hint files to be written as much as C* is concerned. The problem is that we have no way to reproduce creation of the empty hint files and for all we know these hint files could have been created from external circumstances such as corrupt disks. We should still handle 0 length files but this isn't a ticket for a long drawn out investigation with possibly no results.;;;","05/Dec/17 12:50;aleksey;If that's the intent, then maybe we should probably generalize it beyond handling empty files? As that's just one of many possible manifestations of corruption.;;;","07/Dec/17 04:19;alourie;[~iamaleksey] Would you mind elaborating, please, which other manifestations of corruption you have in mind? I thought crc32 checks are supposed to cover those other cases.

Thanks.;;;","13/Dec/17 13:55;aleksey;Having any incomplete header, perhaps? For the purposes of this ticket, does it matter if the file has size 0 or size 1?;;;","18/Dec/17 15:05;alourie;[~iamaleksey] 

I was under the impression that CRC check would handle that.
I had a look at the deserialization code that does the CRC check, and the problem is that _any_ CRC check error (including incomplete header or file size 0) will end up as an FSError, which will cause C* to fail on start. So I've reworked a patch from just checking the size to be > 0 to ignoring the corrupted files instead (including one of size 0). Would that be a better solution for the types of issues you were thinking about?

Thanks.;;;","01/Feb/18 08:19;alourie;[~iamaleksey] it would be great if you could give a feedback on the previous comment. Thanks!;;;","27/Mar/18 04:42;alourie;[~iamaleksey] I would appreciate if you could have a look at this. Thanks!;;;","29/Mar/18 12:09;aleksey;The logic is sound. Let me have a look and commit.;;;","29/Mar/18 12:36;aleksey;So, arguably the original code was written somewhat lazily (by me).

Swallowing any IOException there and re-trhowing as an FSError, while making the code slightly simpler - by avoiding the need to handle IOException properly elsewhere - was too blunt.

We should still throw the FSError on checksum mismatches - but not in other scenarios. I'll expand a bit, on top of your patch, to differentiate between these two different exceptions.;;;","29/Mar/18 13:03;aleksey;Committed to 3.0 as [68079e4b2ed4e58dbede70af45414b3d4214e195|https://github.com/apache/cassandra/commit/68079e4b2ed4e58dbede70af45414b3d4214e195] and merged up with 3.11 and trunk, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest code style check failed,CASSANDRA-14076,13121512,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,29/Nov/17 01:46,16/Apr/19 09:29,14/Jul/23 05:56,04/Dec/17 10:24,,,,,,,Test/dtest/python,,,,,0,,,,,"https://travis-ci.org/cooldoger/cassandra-dtest
{noformat}
$ flake8 --ignore=E501,F811,F812,F822,F823,F831,F841,N8,C9 --exclude=thrift_bindings,cassandra-thrift .
./consistency_test.py:547:17: E722 do not use bare except'
./consistency_test.py:976:49: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:976:51: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:981:63: E703 statement ends with a semicolon
./consistency_test.py:1037:49: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1037:51: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1054:46: E261 at least two spaces before inline comment
./consistency_test.py:1103:22: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1103:24: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1175:22: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1175:24: E251 unexpected spaces around keyword / parameter equals
./counter_tests.py:59:24: E703 statement ends with a semicolon
./counter_tests.py:383:37: E261 at least two spaces before inline comment
./dtest.py:586:13: E722 do not use bare except'
./dtest.py:1130:1: E302 expected 2 blank lines, found 1
./nodetool_test.py:9:1: E302 expected 2 blank lines, found 1
./nodetool_test.py:78:1: W293 blank line contains whitespace
./nodetool_test.py:174:45: E261 at least two spaces before inline comment
./run_dtests.py:220:54: E221 multiple spaces before operator
./secondary_indexes_test.py:14:1: F401 'dtest.DtestTimeoutError' imported but unused
./secondary_indexes_test.py:17:1: F401 'tools.data.index_is_built' imported but unused
./secondary_indexes_test.py:21:1: E302 expected 2 blank lines, found 1
./sslnodetonode_test.py:15:1: E302 expected 2 blank lines, found 1
./sslnodetonode_test.py:191:1: W293 blank line contains whitespace
./sslnodetonode_test.py:191:1: W391 blank line at end of file
./system_keyspaces_test.py:6:1: E302 expected 2 blank lines, found 1
./system_keyspaces_test.py:28:59: E241 multiple spaces after ','
./system_keyspaces_test.py:50:62: E241 multiple spaces after ','
./write_failures_test.py:5:1: F401 'distutils.version.LooseVersion' imported but unused
./plugins/dtestcollect.py:1:1: F401 'collections.namedtuple' imported but unused
./plugins/dtestcollect.py:3:1: F401 'pprint.pprint' imported but unused
./plugins/dtestcollect.py:5:1: F401 'inspect' imported but unused
./plugins/dtestcollect.py:13:1: E302 expected 2 blank lines, found 1
./plugins/dtestcollect.py:44:9: E306 expected 1 blank line before a nested definition, found 0
./plugins/dtestcollect.py:62:22: E703 statement ends with a semicolon
./plugins/dtestcollect.py:64:1: E302 expected 2 blank lines, found 1
./plugins/dtesttag.py:1:1: F401 'collections.namedtuple' imported but unused
./plugins/dtesttag.py:4:1: F401 'pprint.pprint' imported but unused
./plugins/dtesttag.py:8:1: E302 expected 2 blank lines, found 1
./plugins/dtesttag.py:20:1: W293 blank line contains whitespace
./plugins/dtesttag.py:25:1: W293 blank line contains whitespace
./plugins/dtestxunit.py:43:1: F401 'doctest' imported but unused
./plugins/dtestxunit.py:46:1: F401 'traceback' imported but unused
./plugins/dtestxunit.py:62:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:66:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:70:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:76:29: E226 missing whitespace around arithmetic operator
./plugins/dtestxunit.py:84:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:107:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:126:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:219:32: W503 line break before binary operator
./plugins/dtestxunit.py:269:25: E126 continuation line over-indented for hanging indent
./plugins/dtestxunit.py:277:25: E126 continuation line over-indented for hanging indent
./repair_tests/deprecated_repair_test.py:159:9: E741 ambiguous variable name 'l'
./repair_tests/incremental_repair_test.py:772:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:773:76: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:774:69: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:804:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:805:87: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:806:73: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:830:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:831:87: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:832:73: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:855:55: E231 missing whitespace after ','
./repair_tests/incremental_repair_test.py:855:57: E231 missing whitespace after ','
./repair_tests/incremental_repair_test.py:855:59: E231 missing whitespace after ','
./tools/data.py:154:1: E302 expected 2 blank lines, found 1
./tools/data.py:167:34: W292 no newline at end of file
./upgrade_tests/thrift_upgrade_test.py:25:65: E261 at least two spaces before inline comment
./upgrade_tests/thrift_upgrade_test.py:28:1: E302 expected 2 blank lines, found 1
./upgrade_tests/thrift_upgrade_test.py:37:1: E302 expected 2 blank lines, found 1
{noformat}

Running {{pep8}} is also failed because it's renamed to {{pycodestyle}}.",,jay.zhuang,jjirsa,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 04 10:24:10 UTC 2017,,,,,,,,,,"0|i3nbhj:",9223372036854775807,,,,,,,philipthompson,,philipthompson,,,Normal,,,,,,,,,,,,,,,,,,,"30/Nov/17 23:22;jay.zhuang;Here is the patch, please review:
| Branch | TravisCI Build Status |
| [14076|https://github.com/cooldoger/cassandra-dtest/tree/14076] | [!https://travis-ci.org/cooldoger/cassandra-dtest.svg?branch=14076!|https://travis-ci.org/cooldoger/cassandra-dtest/builds/309766256] |;;;","30/Nov/17 23:23;jjirsa;cc [~philipthompson]

(Also actual branch is https://github.com/cooldoger/cassandra-dtest/tree/14076 ) 
;;;","01/Dec/17 10:21;philipthompson;+1, changes all look like valid, trivial [as in not behavior changing] pep8 cleanup.;;;","04/Dec/17 10:24;philipthompson;Committed at 44fd9be91d38345bf666c2118693489f9a46de20. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Many sslnodetonode_test.TestNodeToNodeSSLEncryption tests failing with ""Please remove properties [optional, enabled] from your cassandra.yaml""",CASSANDRA-14075,13121496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,mkjellman,mkjellman,29/Nov/17 00:32,16/Apr/19 09:29,14/Jul/23 05:56,01/Dec/17 13:32,,,,,,,Legacy/Testing,,,,,0,,,,,"Many sslnodetonode_test.TestNodeToNodeSSLEncryption dtests are failing on 3.11 with an exception on startup due to invalid yaml properties.

Unexpected error in node1 log, error: 
ERROR [main] 2017-11-18 21:01:54,781 CassandraDaemon.java:706 - Exception encountered during startup: Invalid yaml. Please remove properties [optional, enabled] from your cassandra.yaml 

Although ccm was updated in https://github.com/pcmanus/ccm/commit/eaaa425b70edb84786924516aee3920d685c0e53 to include a version check for >= 4.0, enabled and optional are emitted unconditionally in the actual dtest itself -- they should also be conditional on >= 4.0

{code:java}
node.set_configuration_options(values={
            'server_encryption_options': {
                'enabled': encryption_enabled,
                'optional': encryption_optional,
                'internode_encryption': internode_encryption,
                'keystore': kspath,
                'keystore_password': 'cassandra',
                'truststore': tspath,
                'truststore_password': 'cassandra',
                'require_endpoint_verification': endpoint_verification,
                'require_client_auth': client_auth,
            }
        })
{code}",,jasobrown,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 01 13:32:40 UTC 2017,,,,,,,,,,"0|i3nbdz:",9223372036854775807,,,,,,,mkjellman,,mkjellman,,,Normal,,,,,,,,,,,,,,,,,,,"30/Nov/17 19:04;jasobrown;[~mkjellman]'s evaluations is correct: in CASSANDRA-10404, I didn't correctly support pre-4.0 in this dtest. Here is a [dtest patch|https://github.com/jasobrown/cassandra-dtest/tree/14075] that checks the cluster version and only adds the new props if the it's greater than or equal to 4.0.

Here are runs of the dtest patch against both 3.11 and trunk:

||3.11||trunk||
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14075-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14075-trunk]|
||


Note: I also ran this locally with jdk1.8.0_151, and started getting this warning:

{noformat}
Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using ""keytool -importkeystore -srckeystore /tmp/tmpICn9py/ca.keystore -destkeystore /tmp/tmpICn9py/ca.keystore -deststoretype pkcs12"".
{noformat}

I've also updated {{sslkeygen.py}} in this patch with a trivial fix to eliminate the warning.
;;;","30/Nov/17 22:56;mkjellman;looks good! +1;;;","01/Dec/17 13:32;jasobrown;committed as sha {{9da3a2594bf75cbfd4852ee9b4b3e44c28ff618f}} to dtests repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Materialized view on table with TTL issue,CASSANDRA-14071,13120890,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,sbutnariu,sbutnariu,27/Nov/17 10:52,16/Apr/19 09:29,14/Jul/23 05:56,06/Dec/17 21:42,3.0.16,3.11.2,,,,,Feature/Materialized Views,Legacy/Coordination,,,,0,correctness,,,,"Materialized views that cluster by a column that is not part of table's PK and are created from tables that have *default_time_to_live* seems to malfunction.

Having this table
{code:java}
CREATE TABLE sbutnariu.test_bug (
    field1 smallint,
    field2 smallint,
    date timestamp,
    PRIMARY KEY ((field1), field2)
) WITH default_time_to_live = 1000;
{code}

and the materialized view
{code:java}
CREATE MATERIALIZED VIEW sbutnariu.test_bug_by_date AS SELECT * FROM sbutnariu.test_bug WHERE field1 IS NOT NULL AND field2 IS NOT NULL AND date IS NOT NULL PRIMARY KEY ((field1), date, field2) WITH CLUSTERING ORDER BY (date desc, field2 asc);
{code}

After inserting 3 rows with same PK (should upsert), the materialized view will have 3 rows.
{code:java}
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*3 rows*/
{code}

If I remove the ttl and try again, it works as expected:
{code:java}
truncate sbutnariu.test_bug;
alter table sbutnariu.test_bug with default_time_to_live = 0;

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*1 row*/
{code}

I've tested on versions 3.0.14 and 3.0.15. The bug was introduced in 3.0.15, as in 3.0.14 it works as expected.",Cassandra 3,aweisberg,easyoups,jasonstack,jeromatron,KurtG,pauloricardomg,sbutnariu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14441,,,,,,,CASSANDRA-14193,,,,CASSANDRA-11500,,,,,,,,,,"06/Dec/17 21:27;pauloricardomg;14071-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12900961/14071-3.0-dtest.png","06/Dec/17 21:27;pauloricardomg;14071-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12900960/14071-3.0-testall.png","06/Dec/17 21:27;pauloricardomg;14071-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12900959/14071-3.11-dtest.png","06/Dec/17 21:27;pauloricardomg;14071-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12900958/14071-3.11-testall.png","06/Dec/17 21:27;pauloricardomg;14071-trunk-dtest.png;https://issues.apache.org/jira/secure/attachment/12900957/14071-trunk-dtest.png","06/Dec/17 21:27;pauloricardomg;14071-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12900956/14071-trunk-testall.png",,,,,,,,,,,,,,6.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 17:15:39 UTC 2018,,,,,,,,,,"0|i3n7n3:",9223372036854775807,3.0.15,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,3.0.15,,,,,,,,,,,,,,,,,"27/Nov/17 13:49;pauloricardomg;Thanks for the bug report and reproduction steps. I think that this may be related to CASSANDRA-11500. Will have a look later this week (cc [~jasonstack]);;;","28/Nov/17 08:59;jasonstack;Thanks for the report. This is indeed an issue from CASSANDRA-11500.

In 11500, an expired livenessInfo concept is introduced to MV, in order to mark entire view row as dead without purging row cells as tombstone.
When table has default ttl, the MV row to be removed has a bigger {{localDeletionTime}} than the generated expired livenessInfo's. (see LivenessInfo.supersedes())

The fix will be: when livenessInfo timestamp ties,  MV expired livenessInfo (by checking TTL==MAX) will supersedes another. I will check again if there is anything left for MV reconciliation.;;;","29/Nov/17 09:21;jasonstack;| source | utest | dtest |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071] | [3.0|https://circleci.com/gh/jasonstack/cassandra/658] | failures not related |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071-3.11] | [3.11|https://circleci.com/gh/jasonstack/cassandra/656] | running |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071-trunk] | [trunk|https://circleci.com/gh/jasonstack/cassandra/657] | failures not related |
| [dtest|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:CASSANDRA-14071?expand=1]|

{Code}
Changes:
1. Added ExpiredLivenessInfo as subclass of ExpiringLivenessInfo, it's always {{expired}}(act as tombstone) regardless {{nowInSecs}} local time.
2. Added additional check in {{LivenessInfo.supersedes()}}. When timestamp tie, ExpiredLivenessInfo (by checking ttl==MAX) will supersedes another non-ExpiredLivenessInfo.
{Code}

[~pauloricardomg] what do you think?;;;","01/Dec/17 21:19;pauloricardomg;bq. In 11500, an expired livenessInfo concept is introduced to MV, in order to mark entire view row as dead without purging row cells as tombstone. When table has default ttl, the MV row to be removed has a bigger localDeletionTime than the generated expired livenessInfo's. (see LivenessInfo.supersedes())

Good investigation! On #11500 we focused on the case were TTLs are mixed with ordinary writes but missed the overwrite with ttl case.

bq. The fix will be: when livenessInfo timestamp ties, MV expired livenessInfo (by checking TTL==MAX) will supersedes another. I will check again if there is anything left for MV reconciliation.

The approach looks good to me but even though this looks safe I think we should restrict this new resolution rule only to pk liveness info of views to ensure no other code path can be affected by this since this is a pretty critical path. What do you think? 

Also, besides the case with default ttl, this also affects overwrites with ttl right? Can you add a test with this case as well?

Thanks!;;;","03/Dec/17 13:09;jasonstack;Thanks for the feedback.

bq. The approach looks good to me but even though this looks safe I think we should restrict this new resolution rule only to pk liveness info of views to ensure no other code path can be affected by this since this is a pretty critical path. What do you think?

Current TTL in insert/update request is at most 20 yrs defined in Attributes.java, but there is no TTL limit for table default_time_to_live. I think using a very large default_time_to_live is not reasonable because {{TTL + nowInSeconds}} can cause int32 overflow. 

I added an validation logic in {{TableParams}} to make sure default_time_to_live will never be {{Integer.MAX_VALUE}} which is reserved for MV..

Or do you think adding a method param, eg {{isView}}, to {{LivenessInfo.supersedes()}} is safer?

bq. Also, besides the case with default ttl, this also affects overwrites with ttl right? Can you add a test with this case as well?

Make sense, I have updated the test with user-provided TTL;;;","05/Dec/17 03:30;jasonstack;As we discussed offline, it's better to keep table default_time_to_live having the same maximum as per-request ttl, ie. 20 years. 
* If ttl is 21 yrs, then ttl + nowInSecond(2017/12/03) will overflow int32, newly inserted data is dead.

Updated the patch and restarted CI.;;;","05/Dec/17 09:20;sbutnariu;Great news that you find and are on your way of fixing the issue! Any idea on what version will this fix be launched?;;;","06/Dec/17 01:38;jasonstack;It will be in latest bugfix version: 3.0.x/3.11.x/4.0;;;","06/Dec/17 01:39;jasonstack;CI looks good. There is some NPE on view unit test while creating keyspace, but it looks related to CASSANDRA-14010;;;","06/Dec/17 02:27;pauloricardomg;Patch and tests look good, I just made this following minor changes:
* Improve docs a bit to give more information on how {{ExpiredLivenessInfo}} is used [here|https://github.com/pauloricardomg/cassandra/commit/0098a04f50db06266ecd54cd115de8b167c38862]
* Avoid unnecessary comparison on [LivenessInfo.supersedes|https://github.com/pauloricardomg/cassandra/commit/d4eafba373973c870158bcb2b005746f9f59aa2d].
* Reinstate [removed assertion|https://github.com/pauloricardomg/cassandra/commit/7a79e0837004415885b8cddaf2f45ba21d493bc3]

I rebased submitted CI with these changes:
* [3.0|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071]
* [3.11|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071-3.11]
* [trunk|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071-trunk]

Please let me know what do you think. ;;;","06/Dec/17 02:33;jasonstack;LGTM, thanks for the review;;;","06/Dec/17 21:32;pauloricardomg;Patch is very well tested, good job! CI looks as good as it gets (only unrelated failures, screenshots of internal CI attached). Committed patch to cassandra-3.0 branch as {{461af5b9a6f58b6ed3db78a879840816b906cac8}} and merged up to cassandra-3.11 and trunk.

Committed dtest as {{b5fde208857a11a13cabf8f2e00aca986d133b0f}} and {{ccc6e188b4b419dd4a0d8d1245a6138ab26d3d7e}}. Thanks!;;;","26/Jan/18 17:15;aweisberg;I bisected and this causes two test failures in TTLTest. I bisected to 461af5b9a6f58b6ed3db78a879840816b906cac8. Created [CASSANDRA-14193|https://issues.apache.org/jira/browse/CASSANDRA-14193] to track it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node stopped serving write requests when a table has a lot of sstables,CASSANDRA-14069,13120527,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slapukhov,slapukhov,slapukhov,23/Nov/17 16:52,08/Sep/20 11:23,14/Jul/23 05:56,08/Sep/20 11:23,,,,,,,Local/Compaction,,,,,0,,,,,"Cluster was flooded with SSTables. A table had ~20000 sstables. Write requests started failing. 

Steps to reproduce:
* Create cluster with 3 nodes
* Specify 
   {noformat}
memtable_heap_space_in_mb: 10
{noformat}
 in cassandra.yaml
* Create table standard1 in keyspace1 (for the cassandra-stress tool) with the script [^create.cql]. Please note
{noformat} compaction = {'class': 'SizeTieredCompactionStrategy', 'enabled': 'false'} {noformat}
  i.e. compaction will be turned off for now.
*  Populate node with data:
  {noformat} cassandra-stress write n=1000000000 -node 127.0.0.1 {noformat}
* After node was populated, put both read and write pressure on it:
  {noformat}  cassandra-stress read n=1000000000 -node 127.0.0.1 
  cassandra-stress write n=1000000000 -node 127.0.0.1 {noformat}
* While still under pressure, enable LeveledCompactionStrategy
{code:java}  echo ""ALTER TABLE keyspace1.standard1 WITH compaction = { 'class' : 'LeveledCompactionStrategy', 'sstable_size_in_mb' : 1 }; DESC keyspace1.standard1; exit"" | bin/cqlsh; {code}

*Results:*
Write requests failing.
'bin/nodetool cfstats' and 'bin/nodetool compactionstats' commands hanging, if issued from the node running cassandra-stress tool.

If issued from another node:

{noformat}
 $ bin/nodetool cfstats
...
Table: standard1
                SSTable count: 22637
                SSTables in each level: [22651/4, 0, 0, 0, 0, 0, 0, 0, 0] 
...
{noformat}


{noformat}
$ bin/nodetool compactionstats

pending tasks: 12656
                                     id   compaction type    keyspace       table   completed       total    unit   progress
   935bbc00-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    59556014    59557860   bytes    100.00%
   a29ee660-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    80432114   742151655   bytes     10.84%
   9766e400-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    58891604    58893215   bytes    100.00%
   9cdc9880-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    20289449    20290800   bytes     99.99%
   90f98910-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    59689824    59695545   bytes     99.99%
   986ede20-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    40598594    40598820   bytes    100.00%
   9cd322a0-d03b-11e7-a47d-2b44293495b8        Compaction   keyspace1   standard1    60756739    60766660   bytes     99.98% 
{noformat}

Special note about 'bin/nodetool compactionstats' - picture above is quite typical for this issue. I.e. compaction tasks manage to make it through, but hinder near the full completion (around 99.9 %).

Maybe the root of the problem is in this thread (see [^stack.txt]):

{noformat}
""CompactionExecutor:1748"" #4649 daemon prio=1 os_prio=4 tid=0x00007f35a0096100 nid=0x65f6 runnable [0x00007f3228bce000]
   java.lang.Thread.State: RUNNABLE
  at org.apache.cassandra.dht.AbstractBounds.<init>(AbstractBounds.java:53)
  at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:42)
  at org.apache.cassandra.db.compaction.LeveledManifest.overlapping(LeveledManifest.java:562)
  at org.apache.cassandra.db.compaction.LeveledManifest.overlapping(LeveledManifest.java:549)
  at org.apache.cassandra.db.compaction.LeveledManifest.getCandidatesFor(LeveledManifest.java:624)
  at org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates(LeveledManifest.java:378)
  - locked <0x000000070d4c3bc8> (a org.apache.cassandra.db.compaction.LeveledManifest)
  at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:105)
  - locked <0x000000070d6cb2c8> (a org.apache.cassandra.db.compaction.LeveledCompactionStrategy)
  at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:102)
  - locked <0x00000006467268b8> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
  at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:258)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
  at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1671507048.run(Unknown Source)
  at java.lang.Thread.run(Thread.java:748)
{noformat}

As I see it, thread is running a cycle, which has O(n^2) dependency on the number of SSTables, while still holding the lock. 

Say this one:

{noformat}
""CompactionExecutor:1743"" #4644 daemon prio=1 os_prio=4 tid=0x00007f35a01591a0 nid=0x65f1 waiting for monitor entry [0x00007f322d016000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:252)
	- waiting to lock <0x00000006467268b8> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.lifecycle.Tracker.notifyDeleting(Tracker.java:448)
	at org.apache.cassandra.db.lifecycle.LogTransaction.obsoleted(LogTransaction.java:161)
	at org.apache.cassandra.db.lifecycle.Helpers.prepareForObsoletion(Helpers.java:134)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.doPrepare(LifecycleTransaction.java:199)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:376)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:84)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:94)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:206)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1671507048.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,jasonstack,jeromatron,jjirsa,marcuse,pauloricardomg,slapukhov,tvdw,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14826,,,,,,,,,CASSANDRA-12763,,,,,,,,,,"10/Jan/18 11:03;slapukhov;CAS-14069.patch;https://issues.apache.org/jira/secure/attachment/12905430/CAS-14069.patch","10/Jan/18 11:06;slapukhov;afterPatch.svg;https://issues.apache.org/jira/secure/attachment/12905431/afterPatch.svg","10/Jan/18 11:06;slapukhov;beforePatch.svg;https://issues.apache.org/jira/secure/attachment/12905432/beforePatch.svg","23/Nov/17 16:45;slapukhov;create.cql;https://issues.apache.org/jira/secure/attachment/12899086/create.cql","23/Nov/17 16:44;slapukhov;stack.txt;https://issues.apache.org/jira/secure/attachment/12899087/stack.txt",,,,,,,,,,,,,,,5.0,slapukhov,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 11:23:45 UTC 2020,,,,,,,,,,"0|i3n5en:",9223372036854775807,3.0.15,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"23/Nov/17 18:20;jjirsa;Here are a few points in LCS that are especially expensive as you increase the number of 
sstables

CASSANDRA-12763 Has a different stack that represents the same basic problem. ;;;","28/Nov/17 18:21;jjirsa;Linking to the somewhat similar CASSANDRA-12763.
;;;","10/Jan/18 11:18;slapukhov;I profiled Cassandra (using [Honest profiler|https://github.com/jvm-profiling-tools/honest-profiler]). Please see [^beforePatch.svg] flame graph.

Definitely, [CASSANDRA-12763|https://issues.apache.org/jira/browse/CASSANDRA-12763] has the main influence on performance, with java.io.UnixFileSystem.list taking about 40% of time on cluster flooded with SSTables. 

However, org.apache.cassandra.db.compaction.LeveledManifest.overlapping(LeveledManifest.java:549) is having second largest impact on performance, taking about 20% of time.

If search overlapping sstables using interval tree (see CAS-14069.patch) instead of just O(n^2) iteration this 20% hotspot goes away, (see [^afterPatch.svg]).;;;","10/Jan/18 15:42;jjirsa;[~krummas] have time for another review? 
;;;","12/Jan/18 09:12;marcuse;[~slapukhov] do you have a node where you can reproduce this?

In 3.11 we have CASSANDRA-11571 - could you test with that patch applied to 3.0? Maybe that is enough to make perf acceptable;;;","08/Sep/20 11:23;marcuse;This is most likely duplicated by CASSANDRA-14826 and I'll handle that in CASSANDRA-14103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unittest failed: CommitLogSegmentManagerCDCTest.testReplayLogic,CASSANDRA-14066,13120073,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,21/Nov/17 22:10,15/May/20 08:01,14/Jul/23 05:56,20/Dec/17 08:34,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,"To reproduce:
{noformat}
$ ant test-cdc -Dtest.name=CommitLogSegmentManagerCDCTest
{noformat}

Error:
{noformat}
    [junit] Testcase: testReplayLogic(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest)-cdc:    FAILED
    [junit] Missing old CDCIndexData in new set after replay: CommitLog-7-1511301220821_cdc.idx,3497190
    [junit] List of CDCIndexData in new set of indexes after replay:
    [junit]    CommitLog-7-1511301220822_cdc.idx,3509214
    [junit]    CommitLog-7-1511301220823_cdc.idx,100
    [junit]
    [junit] junit.framework.AssertionFailedError: Missing old CDCIndexData in new set after replay: CommitLog-7-1511301220821_cdc.idx,3497190
    [junit] List of CDCIndexData in new set of indexes after replay:
    [junit]    CommitLog-7-1511301220822_cdc.idx,3509214
    [junit]    CommitLog-7-1511301220823_cdc.idx,100
    [junit]
    [junit]     at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testReplayLogic(CommitLogSegmentManagerCDCTest.java:345)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest FAILED
{noformat}",,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 08:34:50 UTC 2017,,,,,,,,,,"0|i3n2lz:",9223372036854775807,,,,,,,spod,,spod,,,Normal,,,,,,,,,,,,,,,,,,,"22/Nov/17 00:11;jay.zhuang;It's a real issue with [{{CommitLogReplayer.replayFiles()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L145]. For the CDC data, it's handling the wrong file (the right one + 1, and in the wrong list: [{{CommitLogReplayer.java:151}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L151]).

Sometimes, it's causing the following exception:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 4
at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommittLogReplayer.java:155)
at org.apache.cassandra.db.commitlog.CDCCTestReplayer.examineCommitLog(CDCTestReplayer.java:48)
at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDDCTest.testReplayLogic(CommitLogSegmentManagerCDCTest.java:318)
{noformat}

Here is the fix, please review (it's only impacting trunk, 3.11 branch is fine):
| Branch | uTest |
| [14066-trunk|https://github.com/cooldoger/cassandra/tree/14066-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14066-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14066-trunk] |;;;","20/Dec/17 08:34;spod;Thanks again, Jay!

Merged on trunk as 882c28230ef3b31bd17689d51e83ea624116c769

* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/465/#showFailuresLink]
* [circleCI|https://circleci.com/gh/spodkowinski/cassandra/166#tests/containers/0]



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
trunk eclipse-warnings,CASSANDRA-14061,13119560,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,20/Nov/17 06:23,15/May/20 08:04,14/Jul/23 05:56,08/Mar/18 23:35,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,"{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 59)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, file.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 79)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, dfile.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2 problems (2 errors)
{noformat}",,jasonstack,jay.zhuang,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 23:35:07 UTC 2018,,,,,,,,,,"0|i3mzgn:",9223372036854775807,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"20/Nov/17 08:07;jay.zhuang;Seems they're not the real problem, here is the patch to {{SupressWarning}}:
| Branch | uTest |
| [14061|https://github.com/cooldoger/cassandra/tree/14061] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14061.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14061] |

Also, making {{ant eclipse-warnings}} required in circleci build.;;;","27/Nov/17 13:30;spod;Can't reproduce the issue on trunk. Ant eclipse-warnings will finish without any errors. ;;;","27/Nov/17 17:36;jay.zhuang;[~spodxx@gmail.com]
I'm still able to reproduce it, for example: https://circleci.com/gh/cooldoger/cassandra/148#tests/containers/0
{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 59)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, file.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 79)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, dfile.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2 problems (2 errors)
{noformat}
And the last trunk has the same problem: https://circleci.com/gh/cooldoger/cassandra/149#queue-placeholder/containers/0
It won't fail the build, so I would suggest to make {{ant eclipse-warnings}} required in the build.;;;","30/Nov/17 21:10;jjirsa;[~spodxx@gmail.com] interested in being the official reviewer on this?
;;;","01/Dec/17 12:01;spod;[~jay.zhuang], if we're going to make eclipse-warnings a hard requirement for the build, shouldn't we also fix the reported issues instead of suppressing them? Doing both seems to be a bit ambivalent to me. ;;;","02/Dec/17 20:57;jay.zhuang;I think the warnings are false alert, maybe could add {{iterator.close();}} here: [SSTableIdentityIterator.java:175|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java#L175]. The warning is introduced in CASSANDRA-13299. cc [~jasonstack];;;","11/Dec/17 15:39;spod;[~jay.zhuang], do you plan to change your patch or keep it as is at this point?;;;","11/Dec/17 18:09;jay.zhuang;[~spodxx@gmail.com] For adding {{iterator.close();}} I don't know. Anyway, it's doing nothing: [AbstractIterator.java:86|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/AbstractIterator.java#L86] and won't fix the warning. What do you think?;;;","07/Mar/18 13:07;spod;Looks like {{eclipse-warnings}} is breaking [trunk-test-all|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/] on b.a.o, so we should get this fixed at some point. There's also a new type of error. Would you like to rebase and look at this again, [~jay.zhuang]? The suggested SuppressWarnings annotation should be safe, so let's not unnecessarily complicate matters and go with the suggested changes.
/cc [~aweisberg];;;","07/Mar/18 23:48;jay.zhuang;CASSANDRA-14296 would fix the new ones.

For the warnings in {{SSTableIdentityIterator.java}}, it cannot be reproduced consistently: [Cassandra-trunk-test-all: 499|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/498/console] vs. [Cassandra-trunk-test-all: 499|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/498/console].

Anyway, I rebased to the trunk to suppress warnings:
| Branch | uTest |
| [14061|https://github.com/cooldoger/cassandra/tree/14061] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14061.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14061] |
;;;","08/Mar/18 12:29;spod;+1;;;","08/Mar/18 12:32;jasonstack;Actually, I am not sure how the warning is introduced in 13299.. Thanks for fixing it!;;;","08/Mar/18 23:35;jay.zhuang;Thanks for the review.
Committed as [a7141e6|https://github.com/apache/cassandra/commit/a7141e6c9df03287567c22c76372e166fc83d18e].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Root logging formatter broken in dtests,CASSANDRA-14059,13119377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jkni,jkni,jkni,18/Nov/17 00:26,16/Apr/19 09:29,14/Jul/23 05:56,05/Dec/17 03:13,,,,,,,Test/dtest/python,,,,,0,,,,,"Since the ccm dependency in dtest was bumped to {{3.1.0}} in {{7cc06a086f89ed76499837558ff263d84337acba}}, when dtests are run with --nologcapture, errors of the following form are printed:
{code}
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 861, in emit
    msg = self.format(record)
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 734, in format
    return fmt.format(record)
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 469, in format
    s = self._fmt % record.__dict__
KeyError: 'current_test'
Logged from file dtest.py, line 485
{code}

This is because CCM no longer installs a basic root logger configuration, which is probably a more correct behavior than what it did prior to this change. Now, dtest installs its own basic root logger configuration which writes to 'dtest.log' using the formatter {{'%(asctime)s,%(msecs)d %(name)s %(current_test)s %(levelname)s %(message)s'}}. This means that anything logging a message must provide the current_test key in its extras map. The dtest {{debug}} and {{warning}} functions do this, but logging from dependencies doesn't, producing these {{KeyError}} s. ",,jjirsa,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 03:13:28 UTC 2017,,,,,,,,,,"0|i3mybz:",9223372036854775807,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"18/Nov/17 00:31;jkni;Patch here: [https://github.com/jkni/cassandra-dtest/commit/91e860da6b5959df02d14cc56b0d5c09a2926a83]. This removes the field from the formatter and instead concatenates it to the message inside dtest logging functions. It also changes the way we construct the CURRENT_TEST global. In my testing, the test id already contained the method name, so method names were duplicated in log output.

There may be a convincing argument to removing the dtest.log logger configuration entirely. It appears to have been missing for some time without anyone noticing. For now, I introduced a behavior that's close to the original intention of the dtest.log, as far as I can tell.;;;","27/Nov/17 14:27;spod;Sounds reasonable. I'd also suggest to add another character after the test method name, to make it easier to recognize where the actual message starts.

{noformat}
""15:08:46,800 dtest DEBUG paging_test.TestPagingData.group_by_paging_test Done setting configuration options:""
{noformat};;;","30/Nov/17 21:11;jjirsa;[~spodxx@gmail.com] can I mark you as reviewer here as well? ;;;","01/Dec/17 17:41;jkni;Thanks for taking a look. Any suggestions for the extra character to serve as the delimiter? More whitespace? I have very few opinions here.;;;","01/Dec/17 18:04;spod;Maybe something like a '-' character? I don't mind that much either.;;;","01/Dec/17 18:56;jkni;Works for me - okay with you if I go ahead and commit this with that change?;;;","01/Dec/17 19:07;spod;+1;;;","01/Dec/17 22:22;jkni;Thanks for the review! Committed to cassandra-dtest as {{c1bcc18664cd9e9035f05a98ed23e763173fafd9}}.;;;","04/Dec/17 23:55;jjirsa;FWIW, this has some fallout - seems like a fair number of callers (especially of {{debug}} aren't passing in a simple string, but a {{list}}) :

{code}
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/cassandra/cassandra-dtest/cqlsh_tests/cqlsh_tests.py"", line 1447, in test_client_warnings
    debug(fut.warnings)
  File ""/home/cassandra/cassandra-dtest/dtest.py"", line 187, in debug
    LOG.debug(CURRENT_TEST + ' - ' + msg)
""cannot concatenate 'str' and 'list' objects
{code}

{code}
 File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/jmx_test.py"", line 225, in test_compactionstats
    debug(jmx.read_attribute(compaction_manager, 'CompactionSummary'))
  File ""/home/cassandra/cassandra-dtest/dtest.py"", line 187, in debug
    LOG.debug(CURRENT_TEST + ' - ' + msg)
'cannot concatenate \'str\' and \'list\' objects
{code}

Dunno if you want to re-open or open a new JIRA for it.
;;;","05/Dec/17 00:30;jkni;I'm really sorry about that. I ran this with a subset of dtests that were clearly inadequate to cover all cases here.

Reopening and I'll supply a patch on this issue shortly.;;;","05/Dec/17 00:52;jkni;Follow-up commit here https://github.com/jkni/cassandra-dtest/commit/179bf9f16a03394aee9715b745bfd4773c5b05ba

It uses string formatting instead of concatenation, so we'll automatically stringify lists. I smoke tested this on the old set as well as manually on the tests listed above. I'm running a full set of dtests as well, but this is very likely no worse than the current situation if there's support for committing it sooner. Otherwise, I'll commit once tests are clean.;;;","05/Dec/17 01:16;jjirsa;lgtm!
;;;","05/Dec/17 03:13;jkni;Committed the fix to cassandra-dtest as {{413b18a87d9416446cf4adec5f8483ad408b3e83}}. Thanks for taking a look and noticing the error.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The size of a byte is not 2,CASSANDRA-14057,13119243,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Nov/17 14:45,16/Apr/19 09:29,14/Jul/23 05:56,18/Nov/17 13:19,3.0.16,3.11.2,,,,,Legacy/Streaming and Messaging,,,,,0,,,,,"{{DataLimits}} serializer uses {{TypeSizes.sizeof((byte)limits.kind().ordinal())}} in it's {{serializedSize}} method, but {{TypeSizes}} does not (on 3.0/3.11) have a {{sizeof(byte)}} override, so {{sizeof(short)}} is picked and it returns 2, which is wrong.

This is actually fixed on trunk, [~jasobrown] committed the fix as part of CASSANDRA-8457, but it wasn't committed in 3.0/3.11 and that still feel dodgy to leave as is.

To be clear, I don't think it's a problem as of now: it does break the size computed for read commands, and thus the payload size of such message, but said payload size is not use (for read requests that is).

Still, not reason to leave something wrong like this and risk a future bug when the fix is trivial.",,jasobrown,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 18 13:19:00 UTC 2017,,,,,,,,,,"0|i3mxi7:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"17/Nov/17 14:48;slebresne;I've pushed the super simple fix [here|https://github.com/pcmanus/cassandra/commits/14057] (I suppose I could have almost ninja-fixed but well). [~jasobrown], mind having a very quick double check since you did the exact same on trunk?;;;","17/Nov/17 18:48;jasobrown;Damnit, shame on me for not backporting that fix (or just fixing it outside of CASSANDRA-8457).

+1, [~slebresne], and thanks for taking the time to make sure we do this correctly.;;;","18/Nov/17 13:19;slebresne;No problem, thanks for the quick review, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Many dtests fail with ConfigurationException: offheap_objects are not available in 3.0 when OFFHEAP_MEMTABLES=""true""",CASSANDRA-14056,13119115,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alourie,mkjellman,mkjellman,17/Nov/17 05:03,16/Apr/19 09:29,14/Jul/23 05:56,21/Jun/18 13:39,,,,,,,Test/dtest/python,,,,,0,,,,,"Tons of dtests are running when they shouldn't as it looks like the path is no longer supported.. we need to add a bunch of logic that's missing to fully support running dtests with off-heap memtables enabled (via the OFFHEAP_MEMTABLES=""true"" environment variable)

{code}[node2 ERROR] java.lang.ExceptionInInitializerError
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:394)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:361)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:577)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:554)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:368)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:305)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:887)
	at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:354)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:110)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:179)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: offheap_objects are not available in 3.0. They will be re-introduced in a future release, see https://issues.apache.org/jira/browse/CASSANDRA-9472 for details
	at org.apache.cassandra.config.DatabaseDescriptor.getMemtableAllocatorPool(DatabaseDescriptor.java:1907)
	at org.apache.cassandra.db.Memtable.<clinit>(Memtable.java:65)
	... 14 more
{code}",,alourie,jasobrown,jay.zhuang,KurtG,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alourie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 04:11:38 UTC 2018,,,,,,,,,,"0|i3mwpz:",9223372036854775807,3.11.1,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"05/Dec/17 01:13;alourie;I'll have a poke at this.;;;","07/Dec/17 02:41;alourie;[~mkjellman] I've added a check for a specific C* version to avoid actually running any tests if the C* version is 3.0.x and less than 3.4; the patch is  https://github.com/alourie/cassandra-dtest/commit/0a9661e3ed404856302ab05de4d51b2d65e9e872. Please have a look whether that's what you had in mind.

Thanks.;;;","01/Feb/18 08:11;alourie;[~mkjellman] poke. I hope you find some time to have a look at the patch.;;;","01/Feb/18 10:58;jasobrown;[~alourie] Thanks for the patch. Is this still a problem after CASSANDRA-14134? At a minimum, [~mkjellman] killed off the environment varibles, and the literal string ""OFFHEAP_MEMTABLES"" doesn't exist {{master}} anymore.;;;","06/Feb/18 03:42;alourie;[~jasobrown] [~mkjellman] Yes, this would still be an issue with the new dtests. Someone might want to test with the ""offheap_objects"" configuration option set. The new dtests supports this with a command-line parameter, ""--use-off-heap-memtables""; as a result, I've updated the patch and opened a PR at [https://github.com/apache/cassandra-dtest/pull/18.]

 

Please let me know if you think this is the appropriate solution, and if there are any problems with the patch.

Thanks!;;;","27/Mar/18 04:46;alourie;[~jasobrown] [~mkjellman] Just checking with you if you have any more comments or feedback. Thanks!;;;","20/Jun/18 03:20;alourie;Hey [~jasobrown] [~mkjellman], could you spare few moments to look at the patch? Thanks!;;;","20/Jun/18 13:31;jasobrown;[~alourie] PR doesn't seem to be able to show diffs. Can you take a look?

UPDATE: I see [this branch|https://github.com/alourie/cassandra-dtest/tree/CASSANDRA-14056]. I'll assume it's the correct one?;;;","20/Jun/18 13:52;jasobrown;[~alourie] running dtests with your patch locally still yields the same error in the c* instance logs. I'm running like this:

{code}
pytest --cassandra-dir=/opt/dev/cassandra --use-off-heap-memtables jmx_test.py
{code}

Am I missing something? I ran with both 3.0 and 3.2. 

fwiw, I think your patch needs a less than *or equal* to 3.0 check, like this:

{code}
if args.use_off_heap_memtables and (""3.0"" <= CASSANDRA_VERSION < ""3.4""):
{code}
;;;","20/Jun/18 13:53;jasobrown;ohh, your patch applies to run_dtests, not just invoking pytest directly.;;;","21/Jun/18 05:08;alourie;[~jasobrown] You're right, I don't know how I tested before and it worked...in any case, I've fixed both issues. Now it checks the versions including 3.0 and when invoking pytest directly. The PR is at https://github.com/apache/cassandra-dtest/pull/18 and the branch is https://github.com/apache/cassandra-dtest/compare/master...alourie:CASSANDRA-14056. Please have a look again if you can.

Thanks!;;;","21/Jun/18 13:39;jasobrown;Updated commit lgtm. committed as {{2313e39ee19ea12f304d3e6dd04c41d4f03798ca}}. Thanks!

[~alourie] Can you close the PR, as well?;;;","22/Jun/18 22:43;jasobrown;committed followup sha {{b617ea1b0a2ce9dd0ec16af20d9d9e045ce3fdc7}} to fix a bad import line from {{run_dtests.py}};;;","23/Jun/18 11:32;jasobrown;uggh, one more followup commit, sha {{b76b49e6ac895ead13dde707ab606bdef99d4894}};;;","25/Jun/18 04:11;alourie;PR is closed too. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index redistribution breaks SASI index,CASSANDRA-14055,13118958,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jwest,lboutros,lboutros,16/Nov/17 15:51,16/Mar/22 11:21,14/Jul/23 05:56,14/May/18 19:48,3.11.3,4.0,4.0-alpha1,,,,Feature/SASI,,,,,0,patch,sasi,,,"During index redistribution process, a new view is created.
During this creation, old indexes should be released.

But, new indexes are ""attached"" to the same SSTable as the old indexes.

This leads to the deletion of the last SASI index file and breaks the index.

The issue is in this function : [https://github.com/apache/cassandra/blob/9ee44db49b13d4b4c91c9d6332ce06a6e2abf944/src/java/org/apache/cassandra/index/sasi/conf/view/View.java#L62]



",,ehubert,githubbot,ifesdjeen,jasobrown,jay.zhuang,jeromatron,jjirsa,jwest,lboutros,mck,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #174:
URL: https://github.com/apache/cassandra/pull/174


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:21;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/18 23:40;jwest;14055-jrwest-3.11.patch;https://issues.apache.org/jira/secure/attachment/12911170/14055-jrwest-3.11.patch","19/Feb/18 23:40;jwest;14055-jrwest-trunk.patch;https://issues.apache.org/jira/secure/attachment/12911171/14055-jrwest-trunk.patch","12/Feb/18 23:55;jwest;CASSANDRA-14055-jrwest.patch;https://issues.apache.org/jira/secure/attachment/12910301/CASSANDRA-14055-jrwest.patch","23/Nov/17 11:49;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12899051/CASSANDRA-14055.patch","17/Nov/17 17:36;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12898241/CASSANDRA-14055.patch","16/Nov/17 15:57;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12898006/CASSANDRA-14055.patch",,,,,,,,,,,,,,6.0,jwest,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 11:53:51 UTC 2018,,,,,,,,,,"0|i3mvrb:",9223372036854775807,3.10,3.11.1,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"16/Nov/17 15:56;lboutros;Here is a first little patch with a unit test and a simple fix.

I have somme issues with the unit test ""SASIIndexTest.testIndexMemtableSwitching"".
It works alone, but fails if the whole test is runned. 

Any help would be really appreciated.;;;","17/Nov/17 17:36;lboutros;A new patch with unit tests fixed.;;;","20/Nov/17 14:43;githubbot;GitHub user ludovic-boutros opened a pull request:

    https://github.com/apache/cassandra/pull/174

    CASSANDRA-14055: Index redistribution breaks SASI index

    During index redistribution process, a new view is created.
    During this creation, old indexes should be released.
    
    But, new indexes are ""attached"" to the same SSTable as the old indexes.
    
    This leads to the deletion of the last SASI index file and breaks the index.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ludovic-boutros/cassandra fix/CASSANDRA-14055

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/174.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #174
    
----
commit 532ed86090c27e51c745c57678cd19ff4b606a0c
Author: lboutros@flatironsjouve.com <lboutros@flatironsjouve.com>
Date:   2017-11-20T14:39:41Z

    CASSANDRA-14055: Index redistribution breaks SASI index

----
;;;","22/Nov/17 14:52;lboutros;It seems to be a bit more complex.
There is a case that I'm not able to reproduce in a unit test which breaks as well SASI index, but I'm able to reproduce it in debug mode (and in production of course :( ).;;;","23/Nov/17 11:46;lboutros;I've updated the PR with a more complex unit test and a better fix.

I will add another patch here for reference.;;;","23/Nov/17 14:29;githubbot;Github user ludovic-boutros commented on the issue:

    https://github.com/apache/cassandra/pull/174
  
    I have added a flag in order to keep index files in the index release function.
    This way I can prevent deletions.
    
    Any advise on the patch ?
;;;","30/Nov/17 21:12;jjirsa;[~ifesdjeen] are you still reviewing SASI patches or do we need to find someone else?;;;","07/Dec/17 17:02;lboutros;Any news on this ? We are currently using the patch in production, but that would be better to use an official version of C* :). 

[~ifesdjeen], do you need some help ?
;;;","07/Dec/17 20:34;ifesdjeen;Sorry for not getting to it. I'm currently a bit overloaded with other things, I hope to get to it next week. If anyone has capacity for it - just assign yourself as a reviewer!;;;","31/Jan/18 10:05;lboutros;[~ifesdjeen] do you think you will have some time to review this patch soon ? I really would like to see it included in 3.11.2 :D

Thx.;;;","31/Jan/18 23:25;jwest;Hi [~lboutros],

One of the original authors of SASI here. I've been taking a look at this issue and your patch. Using the provided test against the {{cassandra-3.11}} branch (fc3357a00e2b6e56d399f07c5b81a82780c1e143), I see three different failure cases – two related directly to this issue and one tangentially related. More details on those below. With respect to this issue in particular, the three scenarios cause the test to fail because {{IndexSummaryManager}} ends up creating a new {{View}} where {{oldSSTables}} and {{newIndexes}} have overlapping values. This occurs because the {{IndexSummaryManager}} may ""update"" (re-open) an {{SSTableReader}} for an index already in the view. I believe this is unique to {{IndexSummaryManager}} and I am able to make your tests pass* without your patch by ensuring that there is no overlap between {{oldSStables}} and {{newIndexes}} (favoring {{newIndexes}}). Your patch looks to do this as well, though the approach is a bit different.

One thing I am curious about in your patch is the {{keepFile}} changes to {{SSTableIndex#release}}. Generally, this concerns me because it seems to be working around improper reference counting rather than correcting the reference counting itself. Also, while using the provided test, I am unable to hit a case where the condition {{obsolete.get() || sstableRef.globalCount() == 0}} is true. I see the file missing in the {{View}} but not on disk itself. Could you elaborate a bit more on the need for this change and your use of the {{keepFile}} flag?

The three failure scenarios I see using the provided test are:
h5. 8 keys returned - sequential case

In this scenario, at the time when the query that fails runs, the {{View}} is missing the most recently flushed sstable. As mentioned previously, this is because the intersection of {{oldSSTables}} and {{newIndexes}} is non-empty. This can be fixed* by ensuring nothing in {{newIndexes}} is in {{oldSSTables}}. I call this the sequential case because the compaction that occurs during the test completes before the index summary redistribution begins to create a new {{View}}. This is also addressed by your patch.
h5. 8 keys returned - race case

This scenario is similar to the previous one but has the additional issue of triggering improper {{SSTableIndex}} reference counting. From the perspective of the provided test, the failure scenario is the same and the fix* is as well. The issue occurs because of a race between compaction and index redistribution's creation of new {{View}} instances. This causes redistribution to create two {{View}} instances, the first of which is thrown away due to a failed compare and swap. The problem is the side-effects (calling {{SSTableIndex#release}}) have occurred already inside the creation of the garbage {{View}}, causing the reference count for the index to drop below 0. I see this issue as a separate one from this ticket and have filed [CASSANDRA-14207|https://issues.apache.org/jira/browse/CASSANDRA-14207]. It is not fixed by the previously mentioned change and while I haven't checked in detail, I don't think the provided patch addresses this either.
h5. 0 keys returned

This scenario is similar to the first but there are three threads involved in the race: the compaction, the flushing of the last memtable, and the index redistribution. In this case, the end result is an empty {{View}}, which leads to no keys being returned since the system thinks there are no indexes to search. This is fixed* by what I mentioned previously and occurs because index redistribution re-opens both sstables in the original {{View}} instead of just one. It is also addressed by your patch. 

 

I am curious if you see any other failure scenarios besides these three and, in particular, if you can elaborate on and provide examples of the issues you see regarding the files being missing on disk and the need for the {{keepFile}} change.

\* While this fix makes the provided test pass I am still verifying its correct from the reference counting perspective.;;;","01/Feb/18 11:04;lboutros;Hi [~jrwest],

first, thank you for reviewing this patch.
 I will try to give answers to your questions.

Your global analysis is correct. The idea of this patch was to change as few things as possible.
 I do not see any other failure scenarios currently.
 We are using this patch in production with success since the end of november.

Regarding the {{keepFile}} change, with my last patch, I can reproduce the file deletion with the {{forceFlush}} boolean set to {{true}}.

You can just add a conditional breakpoint with {{keepFile && (obsolete.get() || sstableRef.globalCount() == 0)}} in the {{release}} function.
 It will stop on each attempt of index redistribution with {{forceFlush}} active (second part of the test).

With my limited knowledge of the global code, I did not see any issue in the reference counting process with my patch.
 But again, I'm quite new with this code :).;;;","09/Feb/18 14:51;lboutros;Hi [~jrwest],

by any chance, did you have some time to work on this ? :)

Thx in advance.;;;","09/Feb/18 19:55;jwest;Hi [~lboutros],

My apologies for the delay. I am waiting on internal review of my version of the patch so you can take a look. I believe this patch accomplishes the same thing but with less changes and doesn't affect the reference counting in SSTableIndex. I hope to have it posted for your review and testing early next week.;;;","12/Feb/18 08:30;lboutros;Hi [~jrwest],

no problem. I would be glad to review your patch. It will be a good way to learn :).;;;","12/Feb/18 23:54;jwest;Hi [~lboutros],

Attached my patch for your review and testing. If you could verify this does the right thing in your environments that would be especially helpful since I have been unable to replicate the deleted file issue – I only see the sstables removed from the SASI View.

The gist of the patch is to ensure the intersection of {{oldSStables}} and {{newIndexes}} is always empty. Your patch was doing the same by not checking {{oldSSTables}} in the second for-loop, but this approach doesn't require the changes to {{SSTableIndex#release}}.

I re-used your test but removed the version that runs with the data entirely in-memory since that won't be affected by index redistribution.;;;","13/Feb/18 12:52;jasobrown;For sanity sake, I'm running the utests on [~jrwest]'s patch:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14055-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14055-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14055-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14055-trunk]|
||
;;;","13/Feb/18 13:08;jasobrown;[~jrwest] patch does not compile on trunk. can you take a look?

{noformat}
build-test:
    [javac] Compiling 587 source files to /orig/opt/dev/cassandra/build/test/classes
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:946: error: cannot find symbol
    [javac]         int minIndexInterval = store.metadata.params.minIndexInterval;
    [javac]                                              ^
    [javac]   symbol:   variable params
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:955: error: cannot find symbol
    [javac]             store.metadata.minIndexInterval(minIndexInterval);
    [javac]                           ^
    [javac]   symbol:   method minIndexInterval(int)
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:961: error: cannot find symbol
    [javac]         store.metadata.minIndexInterval(minIndexInterval);
    [javac]                       ^
    [javac]   symbol:   method minIndexInterval(int)
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 3 errors
{noformat};;;","13/Feb/18 17:57;lboutros;Hi [~jrwest],

thx for your patch. The main difference between the two patches is that you do not release old {{SSTableIndex}} objects at all.

This way you do not remove the index file. On the other hand, don't you think you will have an issue in the reference count ? I'm not sure but in my understanding, the global count will never go down to zero. But perhaps am I wrong.




 

 ;;;","14/Feb/18 17:06;jwest;[~jasobrown], sorry for the trunk issues. The way {{TableMetadata}} is accessed/stored was changed and the test will need to be modified as a result. Will post a separate patch for trunk. 

[~lboutros], In my testing, the primary issue I saw was that files were removed from the SASI {{View}} that shouldn't be. The test writes 5 sstables (with sequence numbers 1-4 & 6) and during the test a compaction typically happens (that generates a sstable with generation 5 from sstables 1-4). The final SASI {{View}} when the queries are performed should contain either (1-4, 6) or (5, 6)*. The test fails by returning 8 keys instead of 10 when the SASI {{View}} ends up containing only sstable 5 or by returning 0 keys instead of 10 when the SASI {{View}} ends up empty. 

The issue occurs when index redistribution completes. Depending on the interleaving* of events (the memtable flush, compaction, and redistribution), redistribution re-opens sstable 6, and sometimes re-opens sstable 5. This results in an {{SSTableListChangedNotification}}, which in turn results in the creation of a new {{View}},  where {{added=[6]}} (or {{added=[5,6]}}) and {{removed=[6]}} (or {{removed=[5,6]}}). The SASI {{View}} was written assuming these two sets were disjoint, which is why any reader in {{oldSSTables}} caused the index to be closed. This is incorrect in both cases because sstables 5 and 6 are indeed the active data files (5 contains keys 0-8, and 6 contains keys 9 & 10). 

Regarding the ref counting, we want to maintain one reference to sstables 5 & 6 via their SSTableIndex instance but we’ve created a second reference and one needs to be closed. This is ensured by the {{newView.containsKey(sstable.descriptor)}} part of the conditional (so we are still indeed calling {{#release()}} on one instance). As I am writing this, however, I am realizing we want to keep a reference to the newer index, which references the newer SSTable instance and my patch does the opposite — keeping the old instance. I will post an updated patch along with my trunk patch for internal review, but the gist is to change the order we iterate over the old view and new indexes to favor new index instances.

NOTE: I've ignored https://issues.apache.org/jira/browse/CASSANDRA-14207 above

*I've found a few other interleavings by using another machine, but the general issue is the same.;;;","14/Feb/18 17:32;lboutros;[~jrwest], 

??As I am writing this, however, I am realizing we want to keep a reference to the newer index, which references the newer SSTable instance and my patch does the opposite — keeping the old instance. I will post an updated patch along with my trunk patch for internal review, but the gist is to change the order we iterate over the old view and new indexes to favor new index instances.??

That was the point of my initial patch. But I aggree, if we can increment the global ref count with the new index before releasing the old one and therefore prevent the index file deletion, that would be better.;;;","19/Feb/18 23:39;jwest;[~lboutros]/[~jasobrown], some updates:

 
 I have attached two new patches. One for trunk and one of 3.11. Unfortunately, the test changes in trunk don't work well on 3.11 so we can't have one patch. The primary changes in this patch are to change the order we iterate over the indexes to ensure we retain the newer instance of {{SSTableIndex}} and thus the newer {{SSTableReader}}. I also changed the code to clone the {{oldSSTables}} collection since its visible outside the {{View}} constructor. 
||3.11||Trunk||
|[branch|https://github.com/jrwest/cassandra/tree/14055-jrwest-3.11]|[branch|https://github.com/jrwest/cassandra/tree/14055-jrwest-trunk]|
|[utests|https://circleci.com/gh/jrwest/cassandra/tree/14055-jrwest-3.11]|[utests|https://circleci.com/gh/jrwest/cassandra/tree/14055-jrwest-trunk]|

NOTE: same utests are failing on [trunk|https://circleci.com/gh/jrwest/cassandra/25] and I'm still working on getting dtests running with my CircleCI setup. 

 

Also, I spoke with some colleagues including [~beobal] and [~krummas] about the use of {{sstableRef.globalCount()}} to determine when to delete the SASI index file. I've come to the conclusion that its use at all is wrong because it represents the number of references to the instance, not globally. Given index summary redistribution, this isn't a safe assumption. Looking back at the original SASI patches, I am not sure why it got merged this way. The [patches|https://github.com/xedin/sasi/blob/master/src/java/org/apache/cassandra/db/index/sasi/SSTableIndex.java#L120] used {{sstable.isMarkedCompacted()}} but the [merged code|https://github.com/apache/cassandra/commit/72790dc8e34826b39ac696b03025ae6b7b6beb2b#diff-4873bb6fcef158ff18d221571ef2ec7cR124] used {{sstableRef.globalCount()}}. Fixing this is a larger undertaking, so I propose we split that work into a separate ticket and focus this one on SASI's failure to account for index redistribution in the {{View}}. The work covered by the other ticket would entail either a) deleting the SASI index files as part of {{SSTableTidier}} or by moving {{SSTableIndex}} to use {{Ref}} and implementing a tidier specific to it.;;;","20/Feb/18 08:50;lboutros;[~jrwest],

I understand your changings and your explanations on the reference counting issue.
It looks good to me.

Thank you.;;;","21/Feb/18 17:12;jwest;[~lboutros] Great! Thanks for taking a look. I've created https://issues.apache.org/jira/browse/CASSANDRA-14248. ;;;","22/Feb/18 17:19;lboutros;[~jrwest] do you have an idea when this will be committed ?;;;","22/Feb/18 18:02;jwest;[~lboutros] its in [~jasobrown]'s queue to give it one more review but I hope next week.;;;","12/Apr/18 09:25;lboutros;Any news on this ? Thx.;;;","14/May/18 19:48;jasobrown;Sorry for the delay in getting this reviewed. Patch lgtm, and committed as sha {{ab8348c578c0bb2d3baefaf387b4d9bc67f4c861}}. Thanks, [~lboutros] and [~jrwest]!;;;","18/May/18 11:53;lboutros;Thank you [~jasobrown].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many cqlsh_copy_tests are busted,CASSANDRA-14050,13118715,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefania,mkjellman,mkjellman,15/Nov/17 22:35,07/Mar/23 11:52,14/Jul/23 05:56,09/Apr/20 12:47,4.0,4.0-alpha4,,,,,Legacy/Testing,,,,,0,,,,,"Many cqlsh_copy_tests are busted. We should disable the entire suite until this is resolved as these tests are currently nothing but a waste of time.

test_bulk_round_trip_blogposts - cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest
test_bulk_round_trip_blogposts_with_max_connections - cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest
test_bulk_round_trip_default - cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest

Error starting node3.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-S9NfIH
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '256',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2546, in test_bulk_round_trip_blogposts
    stress_table='stresscql.blogposts')
  File ""/home/cassandra/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2451, in _test_bulk_round_trip
    self.prepare(nodes=nodes, partitioner=partitioner, configuration_options=configuration_options)
  File ""/home/cassandra/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 115, in prepare
    self.cluster.populate(nodes, tokens=tokens).start(wait_for_binary_proto=True)
  File ""/home/cassandra/env/local/lib/python2.7/site-packages/ccmlib/cluster.py"", line 423, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node3.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-S9NfIH\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '256',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""",,Gerrrr,jay.zhuang,jjirsa,mbyrd,mkjellman,sasrira,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 18:28:53 UTC 2020,,,,,,,,,,"0|i3mu9b:",9223372036854775807,,,,,,,,,Gerrrr,,,Normal,,5.0,,,https://github.com/apache/cassandra-dtest/commit/da8abe3cab3fc186a6cfb2e3771f647a0dac120e,,,,,,,,,"[https://github.com/apache/cassandra-dtest/pull/62]

The dependency of the dtests to cqlshlib formatting.py has been removed by running {{SELECT *}} queries with cqlsh rather than the driver. This avoids applying the formatting manually. In some cases, where formatting was trivial and the number of rows was significant, the driver is still used and the formatting is done manually (normally a conversion to a string was sufficient). This was done to overcome cqlsh paging. Although we can disable paging, parsing a large number of rows could cause memory or speed problems.",,,,,"22/Nov/17 15:20;sasrira;Can any contributor with Assign permissions, assign this JIRA to me? My username is 'sasrira' (Sam Sriramadhesikan);;;","03/Apr/20 19:11;stefania;[~sasrira] are you still working on this? If not, [~Gerrrr] and I would like to take over. We would like to fix these tests before merging CASSANDRA-15679.

The reason for the failures is that the _cqlsh copy tests.py_ links to _cqlshlib/formatting.py_. It needs this in order to apply the identical formatting used by cqlsh and determine if the data obtained via {{self.session.execute(""SELECT * FROM testtuple"")}} matches the data in the csv files.

Since cqlshlib on trunk supports both python 3 and python 2, then the cqlsh copy tests work for trunk. But for older branches that only support python 2, the tests no longer work.

So to fix the tests we would need to make cqlshlib support both python 2 and python 3, at least as far as _formatting.py_. There is a problem with this approach though: this code is mostly tested via dtests, which only support python 3 (I assume this is the case because of the dependency on Python 3) and therefore, how would we know if we broke anything for Python 2? Maybe we could run the dtests from before the migration to python 3, hoping that they still work.

Another approach would be to copy formatting.py into the dtests repo for the older branches but this is quite ugly.

Lastly, there is the option of removing the dependency to _formatting.py_. I think we could try replacing {{self.session.execute(""SELECT * FROM testtuple"")}} with the equivalent cqlsh command and see if that works.

;;;","07/Apr/20 18:28;stefania;I've submitted a [patch|https://github.com/apache/cassandra-dtest/pull/62] based on the third approach, see documentation plan.

CI runs:

* [2.1|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/25]
* [2.2|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/26]
* [3.0|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/27]
* [3.11|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/23]
* [4.0|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest/24]

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstableloader_with_failing_2i_test - sstable_generation_loading_test.TestSSTableGenerationAndLoading fails: Expected [['k', 'idx']] ... but got [[u'k', u'idx', None]]",CASSANDRA-14037,13118698,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,mkjellman,mkjellman,15/Nov/17 22:24,16/Apr/19 09:29,14/Jul/23 05:56,29/Nov/17 17:53,,,,,,,Legacy/Testing,,,,,0,,,,,"sstableloader_with_failing_2i_test - sstable_generation_loading_test.TestSSTableGenerationAndLoading fails: Expected [['k', 'idx']] ... but got [[u'k', u'idx', None]]

Expected [['k', 'idx']] from SELECT * FROM system.""IndexInfo"" WHERE table_name='k', but got [[u'k', u'idx', None]]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-2My0fh
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/cassandra/cassandra-dtest/sstable_generation_loading_test.py"", line 333, in sstableloader_with_failing_2i_test
    assert_one(session, """"""SELECT * FROM system.""IndexInfo"" WHERE table_name='k'"""""", ['k', 'idx'])
  File ""/home/cassandra/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
'Expected [[\'k\', \'idx\']] from SELECT * FROM system.""IndexInfo"" WHERE table_name=\'k\', but got [[u\'k\', u\'idx\', None]]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-2My0fh\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------'",,ifesdjeen,jkni,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 29 17:53:00 UTC 2017,,,,,,,,,,"0|i3mu5j:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"29/Nov/17 05:52;jkni;One-line patch available [here|https://github.com/jkni/cassandra-dtest/commit/cb77eecfd7fa7a4a584dee46784c543ec9e6e43c]. It looks like this was just an oversight from [CASSANDRA-10857]. Since IndexInfo was originally declared as compact, we need to expect the value column when selecting all columns. This change was already made in one other place in the same test. [~ifesdjeen] to review if interested.;;;","29/Nov/17 14:21;ifesdjeen;+1 [~jkni], I had the same patch locally but I didn't get to submit it. Thank you!;;;","29/Nov/17 17:53;jkni;Thanks for the review. Patch committed to cassandra-dtest as {{fc68a0de8d05082a0a78196695572ff2346179c4}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
counter_leader_with_partial_view_test-novnodes - counter_tests.TestCounters fails,CASSANDRA-14028,13118687,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,mkjellman,mkjellman,15/Nov/17 22:19,30/Jun/21 15:32,14/Jul/23 05:56,30/Jun/21 15:32,NA,,,,,,Legacy/Testing,,,,,0,,,,,"Unexpected error in log, see stdout
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-S1sTss
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/cassandra/cassandra-dtest/dtest.py"", line 599, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-S1sTss\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""",,jkni,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14029,,,,,,,,,,,CASSANDRA-13043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 15:32:15 UTC 2021,,,,,,,,,,"0|i3mu33:",9223372036854775807,,,,,,,,,brandon.williams,,,Normal,,NA,,,https://github.com/apache/cassandra-dtest/commit/a3be2b0a418b96adb8d828afe6b43479ce6485f3,,,,,,,,,,,,,,"27/Nov/17 19:53;jkni;Patch available [here|https://github.com/jkni/cassandra-dtest/commit/a209a6311eb4b050a80d261e8b57176f74b070a4].

There appear to be two different issues here. First, the test manually forced the use of vnodes in cluster creation, which breaks novnode runs. As far as I can tell, there's no inherent incompatibility with novnode configurations, so I removed that behavior. It now passes on both vnode and novnode runs.

Second, this test was introduced for [CASSANDRA-13043], which only fixes the issue in 3.0+. The test fails on 2.2 and fails to run entirely on 2.1 due to missing byteman dependencies. I scoped the test to run on 3.0+.;;;","30/Jun/21 15:32;brandon.williams;We added the vnode mark to this test at some point, but it was still failing on 2.2 so I added the since 3.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"test_simple_strategy_counters - consistency_test.TestAccuracy always fails code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text, age int ) [AND]...)",CASSANDRA-14025,13118683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,mkjellman,mkjellman,15/Nov/17 22:15,16/Apr/19 09:29,14/Jul/23 05:56,16/Nov/17 19:31,,,,,,,Legacy/Testing,,,,,0,,,,,"test_simple_strategy_counters - consistency_test.TestAccuracy always fails code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text,                age int            ) [AND]...)

<Error from server: code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text,                age int            ) [AND]...)"">
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-dYXpHm
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '256',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Testing single dc, counters
dtest: DEBUG: Changing snitch for single dc case
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 704, in test_simple_strategy_counters
    self._run_test_function_in_parallel(TestAccuracy.Validation.validate_counters, [self.nodes], [self.rf], combinations)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 535, in _run_test_function_in_parallel
    self._start_cluster(save_sessions=True, requires_local_reads=requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 147, in _start_cluster
    self.create_tables(session, requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 156, in create_tables
    self.create_users_table(session, requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 178, in create_users_table
    session.execute(create_cmd)
  File ""cassandra/cluster.py"", line 2122, in cassandra.cluster.Session.execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""cassandra/cluster.py"", line 3982, in cassandra.cluster.ResponseFuture.result
    raise self._final_exception
'<Error from server: code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input \'AND\' expecting EOF (... text,                age int            ) [AND]...)"">\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-dYXpHm\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'memtable_allocation_type\': \'offheap_objects\',\n    \'num_tokens\': \'256\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ndtest: DEBUG: Testing single dc, counters\ndtest: DEBUG: Changing snitch for single dc case\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\n--------------------- >> end captured logging << ---------------------'
",,jjirsa,jkni,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 19:31:22 UTC 2017,,,,,,,,,,"0|i3mu27:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"16/Nov/17 17:21;jkni;Patch [here|https://github.com/jkni/cassandra-dtest/commit/eff8d51c9cdd16e39afa871cd69b4df24a5f858a]. This is a trivial fix to table creation syntax. Prior to [CASSANDRA-10857], this table had a {{WITH COMPACT STORAGE}} suffix. With that removed, we need to introduce options with {{WITH}} ourselves.;;;","16/Nov/17 19:02;jjirsa;+1
;;;","16/Nov/17 19:31;jkni;Thanks for the review! Committed as {{d67fd66253ebb8aabc7d90d7ff048a4c9a2ff2cf}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add_dc_after_mv_network_replication_test - materialized_views_test.TestMaterializedViews fails due to invalid datacenter,CASSANDRA-14023,13118679,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,mkjellman,mkjellman,15/Nov/17 22:12,16/Apr/19 09:29,14/Jul/23 05:56,11/Apr/18 12:20,,,,,,,Legacy/Testing,,,,,0,,,,,"add_dc_after_mv_network_replication_test - materialized_views_test.TestMaterializedViews always fails due to:

<Error from server: code=2300 [Query invalid because of configuration issue] message=""Unrecognized strategy option {dc2} passed to NetworkTopologyStrategy for keyspace ks"">
",,marcuse,mkjellman,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 12:11:43 UTC 2018,,,,,,,,,,"0|i3mu1b:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"15/Nov/17 22:13;mkjellman;Looking at the tests, all the keyspaces are created with SimpleStrategy so i don't know how this would ever have worked.;;;","16/Jan/18 10:07;marcuse;https://github.com/krummas/cassandra-dtest/commits/marcuse/14023

a few problems with the test
- we can't add dc2 before it has any nodes
- we need to run queries with LOCAL_ONE to make sure the reads don't spill over to dc1
- we need to run rebuild to actually get data in dc2;;;","09/Feb/18 15:33;marcuse;[~pauloricardomg] do you have time to review this?;;;","05/Apr/18 22:13;pauloricardomg;Sorry for the delay. LGTM, thanks!;;;","10/Apr/18 12:11;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pycodestyle_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh code style errors,CASSANDRA-14021,13118677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,15/Nov/17 22:10,15/May/20 08:00,14/Jul/23 05:56,15/Jan/18 14:10,2.1.20,2.2.12,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,0,cqlsh,,,,"Once we commit CASSANDRA-14020, we'll need to cleanup all of the errors that pycodestyle has found to get the test passing",,jasobrown,jay.zhuang,mbyrd,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14020,,,,,,,,,,,,,,,,,,,,CASSANDRA-10066,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 14:10:51 UTC 2018,,,,,,,,,,"0|i3mu0v:",9223372036854775807,,,,,,,jay.zhuang,,jay.zhuang,,,Normal,,,,,,,,,,,,,,,,,,,"15/Nov/17 22:10;mkjellman;https://github.com/mkjellman/cassandra/commit/4a7ef7b2b8fd7133af418626ada79b2b6696ad85;;;","28/Nov/17 01:26;jay.zhuang;The change looks good to me. Seems like the patch is for trunk. Would you please apply the change to branches since {{cassandra-2.1}}, as the dTest tests the version since 2.1: https://github.com/apache/cassandra-dtest/blob/master/cqlsh_tests/cqlsh_tests.py#L42;;;","15/Jan/18 14:10;jasobrown;While I'm quite sure we won't release another 2.1, I've backported all the way to 2.1 and every version up to trunk (your welcome, [~mkjellman] :D).

 

committed as sha \{{f8d73a3acb00d807d09aa33e1612c89389b18480}}.

 

Thanks, all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pep8_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh: pep8 has been renamed to pycodestyle (GitHub issue #466),CASSANDRA-14020,13118675,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,15/Nov/17 22:07,16/Apr/19 09:29,14/Jul/23 05:56,15/Jan/18 13:22,,,,,,,Legacy/Testing,,,,,0,,,,,"test_pep8_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh always fails due to us catching a informative warning from the pip8 tool.. looks like we just need to swap out the usage

/home/cassandra/env/local/lib/python2.7/site-packages/pep8.py:2124: UserWarning: 

pep8 has been renamed to pycodestyle (GitHub issue #466)
Use of the pep8 tool will be removed in a future release.
Please install and use `pycodestyle` instead.

$ pip install pycodestyle
$ pycodestyle ...

  '\n\n'",,jasobrown,jay.zhuang,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14021,,,,,,,,,,,,,,,,CASSANDRA-14076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 13:22:05 UTC 2018,,,,,,,,,,"0|i3mu0f:",9223372036854775807,,,,,,,jay.zhuang,,jay.zhuang,,,Normal,,,,,,,,,,,,,,,,,,,"15/Nov/17 22:08;mkjellman;https://github.com/mkjellman/cassandra-dtest/commit/0a5a2b1df8fb2d51102822a00ff15bd70f9b4e63;;;","28/Nov/17 00:53;jay.zhuang;+1 for the change.

But we still need to fix the python style issue:
{noformat}
AssertionError: /Users/zjay/ws/cassandra/bin/cqlsh.py:130:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/bin/cqlsh.py:376:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/bin/cqlsh.py:2369:17: E722 do not use bare except'
/Users/zjay/ws/cassandra/bin/cqlsh.py:2374:17: E722 do not use bare except'
/Users/zjay/ws/cassandra/bin/cqlsh.py:2433:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:171:13: E722 do not use bare except'
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:225:13: E722 do not use bare except'
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:2150:17: E741 ambiguous variable name 'l'
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:37:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:146:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:379:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:601:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:611:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:642:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:694:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:854:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:928:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1134:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1153:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1201:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1255:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1318:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1375:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1409:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1558:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1583:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/displaying.py:102:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:48:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:59:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:103:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:211:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:240:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:262:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:276:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:305:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:315:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:351:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:369:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:479:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:525:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:551:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:584:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/pylexotron.py:521:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/wcwidth.py:130:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/wcwidth.py:324:1: E305 expected 2 blank lines after class or function definition, found 1
{noformat};;;","28/Nov/17 01:08;mkjellman;[~jay.zhuang] my bad, i forgot to link the two jira's... the pycodestyle issue is handled in CASSANDRA-14021 (commit available there)...;;;","28/Nov/17 18:35;jay.zhuang;Thanks [~mkjellman]
Fixing [linter_check.sh|https://github.com/apache/cassandra-dtest/blob/master/linter_check.sh#L10] here: CASSANDRA-14076;;;","15/Jan/18 13:22;jasobrown;committed as sha {{0d468af9e2617a4a9083e1e527e3a1731e613fcc}}

Thanks, [~mkjellman] and [~jay.zhuang]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc of read_repair_chance and dclocal_read_repair_chance default values is wrong,CASSANDRA-14016,13118640,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,KurtG,tzach,tzach,15/Nov/17 20:24,15/May/20 08:01,14/Jul/23 05:56,16/Nov/17 19:06,4.0,4.0-alpha1,,,,,Legacy/Documentation and Website,,,,,0,documentation,,,,"In the documentation, default values for 
read_repair_chance is 0.1
dclocal_read_repair_chance is 0.0
source: http://cassandra.apache.org/doc/latest/cql/ddl.html#other-table-options

In the code, the default values are the other way around
{quote}
public static final double DEFAULT_READ_REPAIR_CHANCE = 0.0;
public static final double DEFAULT_DCLOCAL_READ_REPAIR_CHANCE = 0.1;
{quote}
source: https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/TableParams.java#L63-L64",,githubbot,jjirsa,KurtG,tzach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 19:06:41 UTC 2017,,,,,,,,,,"0|i3mtsn:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"16/Nov/17 05:48;githubbot;GitHub user kgreav opened a pull request:

    https://github.com/apache/cassandra/pull/173

    Correct default [dclocal_]read_repair_chance documentation

    CASSANDRA-14016

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kgreav/cassandra 14016

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/173.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #173
    
----
commit d52110f02306b3033b748eee52dffce2525ea9dc
Author: kurt <kurt@instaclustr.com>
Date:   2017-11-16T05:43:55Z

    Correct default [dclocal_]read_repair_chance documentation

----
;;;","16/Nov/17 05:49;KurtG;Thanks. [pr|https://github.com/apache/cassandra/pull/173]
FYI for small doc updates we do just accept a github PR. Feel free to submit them if you find any issues like this one.;;;","16/Nov/17 19:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/173
;;;","16/Nov/17 19:06;jjirsa;Thanks, committed as {[f1f6ed609943cc2908835070a4d1b622759464a8}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Keyspace named ""snapshots"" is empty after service restart",CASSANDRA-14013,13118000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,smiklosovic,kongo2002,kongo2002,13/Nov/17 16:30,22/Jan/23 20:32,14/Jul/23 05:56,17/Jan/23 03:55,4.0.8,4.1.1,5.0,,,,Legacy/Core,Local/Snapshots,,,,0,,,,,"I am posting this bug in hope to discover the stupid mistake I am doing because I can't imagine a reasonable answer for the behavior I see right now :-)

In short words, I do observe data loss in a keyspace called *snapshots* after restarting the Cassandra service. Say I do have 1000 records in a table called *snapshots.test_idx* then after restart the table has less entries or is even empty.

My kind of ""mysterious"" observation is that it happens only in a keyspace called *snapshots*...

h3. Steps to reproduce

These steps to reproduce show the described behavior in ""most"" attempts (not every single time though).

{code}
# create keyspace
CREATE KEYSPACE snapshots WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

# create table
CREATE TABLE snapshots.test_idx (key text, seqno bigint, primary key(key));

# insert some test data
INSERT INTO snapshots.test_idx (key,seqno) values ('key1', 1);
...
INSERT INTO snapshots.test_idx (key,seqno) values ('key1000', 1000);

# count entries
SELECT count(*) FROM snapshots.test_idx;
1000

# restart service
kill <cassandra-pid>
cassandra -f

# count entries
SELECT count(*) FROM snapshots.test_idx;
0
{code}

I hope someone can point me to the obvious mistake I am doing :-)

This happened to me using both Cassandra 3.9 and 3.11.0",,blerer,e.dimitrova,jasobrown,jasonstack,jay.zhuang,jeromatron,kohlisankalp,kongo2002,KurtG,mck,paulo,smiklosovic,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,"smiklosovic opened a new pull request #798:
URL: https://github.com/apache/cassandra/pull/798


   https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/152/


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/20 17:12;githubbot;600","smiklosovic closed pull request #793: CASSANDRA-14013 - fixed data loss in snapshots keyspace after service restart
URL: https://github.com/apache/cassandra/pull/793


;22/Jan/23 20:32;githubbot;600","smiklosovic closed pull request #798: CASSANDRA-14013 - trunk - fixed data loss in snapshots keyspace after service restart
URL: https://github.com/apache/cassandra/pull/798


;22/Jan/23 20:32;githubbot;600",,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,CASSANDRA-16235,CASSANDRA-16251,,CASSANDRA-18168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,smiklosovic,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 03:55:46 UTC 2023,,,,,,,,,,"0|i3mpuf:",9223372036854775807,,,,,,,,,paulo,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/6e6846892a5d7ec2feaf2c35d380ed5975d09517,,,,,,,,,CI and added tests,,,,,"13/Nov/17 17:25;kohlisankalp;Which commit log mode are you using? ;;;","13/Nov/17 18:02;kongo2002;It's the default (which is {{periodic}} if I recall correctly) since I tested with the vanilla configuration.;;;","13/Nov/17 18:32;kongo2002;What additionally throws me off is that similar {{INSERTs}} into another keyspace with the exact same schema and settings do survive every restart without any issues.;;;","13/Nov/17 22:19;jasobrown;[~kongo2002] After you perform the inserts, how long do you wait before bouncing cassandra? If you wait for >= 10 seconds (or whatever {{commitlog_sync_period_in_ms}} is set to in the {{cassandra.yaml}}), do you still have the same problem?

I believe CASSANDRA-13987 addresses the same issue that you are raising here. You can read that ticket for all the gory details.;;;","13/Nov/17 22:31;kongo2002;[~jasobrown] Thanks for the pointer to CASSANDRA-13987 - although I don't think this is the same problem as I do wait for more than 10 seconds indeed. It actually appears that I can pretty much restart the service a couple of times until the table in the *snapshots* keyspace is completely empty. I just tried again on a different machine with the same behavior.;;;","13/Nov/17 23:03;jasobrown;OK, I walked through [~kongo2002]'s example script above on the 3.11 branch, and indeed I am able to reproduce. I tried on 3.0, and I think it did not repro (would need to do it again, tbqh).

I don't have time to dig in for the next few days, but I suspect it's because you named the keyspace ""{{snapshots}}"", and cassandra might be getting confused by trying to clean up any data it thinks is ""snapshot"" data. Especially as you have other keyspaces by other names, and you are not seeing this problem, I'm guessing we have a bug in the handling of subdirectories names ""snapshots"";;;","14/Nov/17 09:53;VincentWhite;I took a look at this today and found the cause of this issue. It is indeed that the name ""snapshots"" is causing confusion as C* tries to retrieve the keyspace and column family names from the file paths here: 


{code:java| title=org.apache.cassandra.io.sstable.Descriptor#fromFilename()}
  else if (cfDirectory.getParentFile().getName().equals(Directories.SNAPSHOT_SUBDIR))
            {
                cfDirectory = cfDirectory.getParentFile().getParentFile();
            }
            cfname = cfDirectory.getName().split(""-"")[0] + indexName;
            ksname = cfDirectory.getParentFile().getName();

{code}

I wrote a quick patch [here|https://github.com/vincewhite/cassandra/commits/14013-test] and would really appreciate some suggestions on improving it (or a different approach). I didn't have a chance to test 3.0.x but the code at least in this area appears to be the same. I should have time to tidy this up and add a test this week.

Cheers,
-vince;;;","28/Nov/17 16:01;jasobrown;[~VincentWhite] Thanks for looking at this. Did you have a chance to clean up/add a test? I can review.

UPDATE: reread your comment, where you are looking for feedback. I will do so later today.;;;","01/Dec/17 00:16;VincentWhite;I've create a patch for 3.0.x and trunk using the same method. I guess it should be safe to work with just absolute paths rather than canonical paths here, I haven't made that change on the 3.x.x patches yet. I also had to fiddle with the unit tests since there is now a dependancy on DatabaseDescriptor and passing in file paths that exist in the configured data directory.

[3.0.x|https://github.com/vincewhite/cassandra/commits/14013-30]
[3.11.x|https://github.com/vincewhite/cassandra/commits/14013-test]
[trunk|https://github.com/vincewhite/cassandra/commits/14013-trunk]

;;;","19/Oct/19 14:50;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[cassandra-3.0_14013|https://github.com/apache/cassandra/compare/trunk...vincewhite:14013-30]|[circleci|https://circleci.com/gh/vincewhite/workflows/cassandra/tree/14013-30]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/59//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/59/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/694//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/694]|
|[cassandra-3.11_14013|https://github.com/apache/cassandra/compare/trunk...vincewhite:14013-test]|[circleci|https://circleci.com/gh/vincewhite/workflows/cassandra/tree/14013-test]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/60//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/60/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/695//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/695]|
|[trunk_14013|https://github.com/apache/cassandra/compare/trunk...vincewhite:14013-trunk]|[circleci|https://circleci.com/gh/vincewhite/workflows/cassandra/tree/14013-trunk]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/61//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/61/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/699//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/699]|
;;;","01/Jan/20 19:22;mck;[~VincentWhite], there's a few unit tests failing here, see {{SSTableLoaderTest}} and {{CQLSSTableWriterTest}}. A fair few dtests look to be failing too (see for example {{snapshot_test}}.;;;","29/Oct/20 17:16;stefan.miklosovic;[~mck]

I wanted to approach this more robustly and I think I am getting there but for some strange reason the bunch of tests are failing. I am not sure what is wrong here as they do not seem to fail locally.

The current solution does not count on the fact that for example there might be also a snapshot / table called ""snapshot"" as well as ""backups"" for example - same for indexes and indexes in backups and snapshots. There might be also a snapshot called ""snapshot"" for a keyspace which is called ""snapshot"" and table which is called ""snapshot"" too and so on ...

That code solves all these issues.

The fact that the original tests were failing was that tests do not use data locations from descriptor but sstables are somewhere in /tmp/ which furtherly complicates things.

PR: [https://github.com/apache/cassandra/pull/798] (there is link to build too).

 

EDIT: some of them do fail locally, I am on it.;;;","30/Oct/20 11:29;mck;bq. The current solution does not count on the fact that for example there might be also a snapshot / table called ""snapshot"" as well as ""backups"" for example - same for indexes and indexes in backups and snapshots. There might be also a snapshot called ""snapshot"" for a keyspace which is called ""snapshot"" and table which is called ""snapshot"" too and so on ...

Sounds good. Thanks. ;;;","04/Nov/20 16:51;blerer;{quote}The current solution does not count on the fact that for example there might be also a snapshot / table called ""snapshots"" as well as ""backups"" for example - same for indexes and indexes in backups and snapshots. There might be also a snapshot called ""snapshot"" for a keyspace which is called ""snapshots"" and table which is called ""snapshots"" too and so on ... {quote}

[~stefan.miklosovic] I do not believe that tables or indexes named {{snapshots}} or {{backups}} are trully a problem because their corresponding directories will have different names.
The directory name for a table named {{snapshots}} is {{snapshots-<TableID>}} and the directory name for an index named {{snapshots}} is {{.snapshots}}.

The {{testKeyspaceTableParsing}} is incorrect because it assumes that a table named {{snapshots}} will result in a directory called {{snapshots}}.  
;;;","04/Nov/20 17:16;blerer;It seems to me that when we hit a directory named {{snapshots}}, it can either be the {{snapshots}} directory or the keyspace directory.
If the directory is the {{snapshots}} directory then we know that its parent will be the table directory and will have a name with the pattern {{<tableName>-<TableID>}}.  By consequence determining if the name is the {{snapshots}} directory or the keyspace directory should be relatively easy.;;;","04/Nov/20 18:21;stefan.miklosovic;[~blerer] I put this together

https://github.com/apache/cassandra/pull/798;;;","04/Nov/20 18:34;stefan.miklosovic;[~blerer] more to it, I have just added that ""tableId"" into test, that is just minor detail, the implementation already copes with that.

Devil is in details, for example, for sstableloader, people might put sstable into /tmp/some/path/mykeyspace/mytable/(data files), and that ""mytable"" will not have any ""id"" on it ... the solution works with both scenarios. Plus this ""path"" might be arbitrary, different from the actual data locations specified in cassandra.yaml etc ... 

What I do not like in particular is that the whole solution feels rather ""spaghetti-like"" (I do not want to offend here anybody). I based my solution on regular expressions.

bq. It seems to me that when we hit a directory named snapshots, it can either be the snapshots directory or the keyspace directory.

And you can have also a snapshot taken which is called ""snapshots"" :) That complicates things ever further. Now what, you have a ""snapshots"" dir where snapshots are and there you might have ""snapshots"" dir which represents the snapshot taken etc etc ... ;;;","05/Nov/20 10:56;blerer;[~stefan.miklosovic]
{quote}for example, for sstableloader, people might put sstable into /tmp/some/path/mykeyspace/mytable/(data files), and that ""mytable"" will not have any ""id"" on it ...{quote} 

{{SSTableLoader}} does not rely on the code of {{Descriptor::fromFilenameWithComponent}} for creating the {{Descriptor}} instances, it has its own mechanism which assume that there will be no {{TableID}}.

{quote}And you can have also a snapshot taken which is called ""snapshots"" That complicates things ever further.{quote}

It does not. I did a quick proof of concept and used your PR test to validate it [here|https://github.com/apache/cassandra/compare/trunk...blerer:CASSANDRA-14013-review]. Of course, the test to check if a directory is a table one or not might be done better.

We know that, outside of this {{snapshots}} keyspace problem, this code work and has been battle tested. By consequence, being pragmatic and pretty paranoiac ;-), it feels safer to me to not reimplement the all thing. Specially if we take into account that we have to backport the fix to 3.11 and 3.0 (not sure for 2.2).  
;;;","05/Nov/20 21:32;stefan.miklosovic;??SSTableLoader does not rely on the code of Descriptor::fromFilenameWithComponent for creating the Descriptor instances, it has its own mechanism which assume that there will be no TableID.??

That is not true, it does, follow the rabbit hole:

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java#L88 ;;;","06/Nov/20 09:23;blerer;{quote}That is not true{quote}

You are right, I should open my eyes properly ;-)

Then unless I am mistaken (again ;-)), we cannot rely on {{DatabaseDescriptor.getAllDataFileLocations()}} as those directories will not be the same as the one in which is stored the input directory for the SSTableLoader.

;;;","06/Nov/20 11:28;blerer;Trying to summarize the problem:
# SSTables used within the C* data directories should be within the data directories returned by {{DatabaseDescriptor.getAllDataFileLocations()}} and the table directories should be in the form {{<table name>-<TableID>}}. In this case the problem come mainly from keyspace being named {{backups}} or {{snapshots}}.
# Files coming from SSTableLoader should be outside of the data directories and the table name should be without the TableID. In this case, keyspaces and tables with a 
{{backups}} or {{snapshots}} name will be having issues.

To be honest, the documentation I found on the SSTableloader is pretty confusing and I imagine that some people might try to use it directly on the C* data directories in which case the table directory will contains the TableID. This case is somehow the same than the {{1.}} above.

[~stefan.miklosovic] As you pointed out there are several scenario that we never tested. {{nodetool snapshot}} with a {{snapshots}} or {{backups}} tag name. SSTableLoader for a {{snapshots}} table (the {{backups}} name was tested by CASSANDRA-16235). The patch should add some tests for those scenarios.
We should also probably test {{nodetool refresh}} with a {{snapshots}} or {{backups}} keyspace.

Pinging [~e.dimitrova] as she was involved in CASSANDRA-16235.;;;","06/Nov/20 14:10;e.dimitrova;??To be honest, the documentation I found on the SSTableloader is pretty confusing and I imagine that some people might try to use it directly on the C* data directories in which case the table directory will contains the TableID. This case is somehow the same than the {{1.}} above.??

To support this, the first time I was reading for the SSTableLoader and trying to use it, I did exactly what you said and got really frustrated :-) 

I will open a ticket to [~lorina@datastax.com] to do her magic :-) ;;;","09/Dec/22 16:42;smiklosovic;[~blerer] [~e.dimitrova]

I finally returned to this ticket. This is branch for trunk (1). I ll backport that to  4.0 and 4.1 after we have 4.1 out to lower the buzz. CI for trunk branch in (1) is in (2). 

I have added tests for exotic names to snapshot tests and their listing. I have also added test for importing sstables (refresh uses that too). I have also added backup / snapshot cases. There is also added cases for parsing sstables with that new format.

The changes introduced here basically simplify this so much that we effectively do not need logic introduced in CASSANDRA-16235 anymore.

(1) https://github.com/instaclustr/cassandra/tree/CASSANDRA-14013
(2) https://app.circleci.com/pipelines/github/instaclustr/cassandra/1657/workflows/41a2840d-4189-4f7e-b0df-63ed8698f1ee;;;","14/Dec/22 17:19;paulo;I think the proposed fix looks reasonable, with the caveat that it will remove support for pre-2.1 backups/snapshots, since the ""-"" separator is not present in pre-2.1 table directories. I don't think this is a big issue since we no longer support pre-2.1 format and there is an easy workaround.

I agree with the sentiment expressed in this ticket that the current approach to extract keyspace/index/table from sstable directories is fragile and needs improvement.

In order to make the extracton of keyspace/table/index more robust, I propose encoding the expected sstable directory structure into the following regex:
{noformat}
{keyspace}/{tableName}-{tableId}[\backups|\snapshots\{tag}][.indexName]/{component}.db
{noformat}
This simplifies the extraction of information from directories to:
{noformat}
    if (regex.matches(fullSstablePath))
    {
        keyspaceName = regex.group(""keyspace"");
        tableName =  regex.group(""tableName"");
        String indexName =  regex.group(""indexName"");
        if (indexName != null)
        {
            tableName = String.format(""%s.%s"", tableName, indexName);
        }
    }
    else if (validateDirs)
    {
        throw invalidSSTable(name, ""cannot extract keyspace and table name; make sure the sstable is in the proper sub-directories"");
    }
{noformat}
The deterministic regex allow to fail-fast when an illegal directory structure is found. Furthermore it can simplify supporting more complex or multiple directory structures if needed.

For example, in order to make tools read sstable in pre-2.1 directory format, we can update the code above to:
{noformat}
    regex =  SSTABLE_DIR_PATTERN;
    if (!regex.matches(fullSstablePath)){
            logger.info(""Regex failed, falling back to legacy sstable format"");
            regex = LEGACY_SSTABLE_DIR_PATTERN;
    }
    if (regex.matches(fullSstablePath))
    {
        keyspaceName = regex.group(""keyspace"");
        tableName =  regex.group(""tableName"");
        String indexName =  regex.group(""indexName"");
        if (indexName != null)
        {
            tableName = String.format(""%s.%s"", tableName, indexName);
        }
    }
    else if (validateDirs)
    {
        throw invalidSSTable(name, ""cannot extract keyspace and table name; make sure the sstable is in the proper sub-directories"");
    }
{noformat}
I came up with the following tentative regex for sstable directory structure:
{noformat}
(?<keyspace>\w+)\/(?<tableName>\w+)-(?<tableId>[0-9a-f]{32})\/(backups\/|(snapshots\/(?<tag>[\w-]+)\/))?(\.(?<indexName>[\w-]+)\/)?(?<component>[\w-]+)\.db
{noformat}
Breaking down into components:
{noformat}
* keyspace: any word ""\w+ ""
* tableName: any word  ""\w+""
* tableId: 32 HEX characters  ""[0-9a-f]{32}""
* optional ""/backups"" or ""/snapshots/{tag}"" pre-directories: ""(backups\/|(snapshots\/(?<tag>[\w-]+)\/))?""
* optional "".{indexName}"" pre-directory: any word with dashes ""(.(?<indexName>[\w-]+)\/)?""
* sstable component {component}.db: any word with dashes {{(?<component>[\w-]+)\.db$}}
{noformat}
The above probably needs validation to check if anything is wrong/missing ^. We can further refine component regex parsing if needed (ie. to extract individual component name for example).

You can give it a try on the regex above on [https://regexr.com/] with the examples below:
{noformat}
/path/to/cassandra/data/dir2/dir5/dir6/ks1/tab1-34234234234234234234234234234234/na-1-big-Index.db
/tmp/some/path/tests/keyspace/table-34234234234234234234234234234234/snapshots/snapshots/.index/nb-3g1m_0nuf_3vj5m2k1125165rxa7-big-Index.db
{noformat}
I implemented the above approach on this commit: [https://github.com/pauloricardomg/cassandra/commit/402e7e3b521d92ed12592daf10a9cdbf47846a60] 

When fixing the tests, I noticed that some examples in the tests were not conforming to the expected directory structure, for example:
{noformat}
/path/to/cassandra/data/dir2/dir5/dir6/ks1/tab1-3424234234324/backups/nb-3g1m_0nuf_3vj5m2k1125165rxa7-big-Index.db
{noformat}
As can be seen, the table id did not have 32 characters that is expected for the table uuid - so I fxed these examples to use UUID ""34234234234234234234234234234234"" instead.

I noticed that {{DescriptorTest.validateNames()}} expects to read Descriptors from sstables not conforming to the expected directory structure, when it's not possible to extract keyspace/table information. I believe this is used by external tools. In other to allow these ""unsafe"" usages by offline tools, I have added a [validateDirs|https://github.com/apache/cassandra/commit/402e7e3b521d92ed12592daf10a9cdbf47846a60#diff-69de3dfecd03ec3ea98d88bef04bf3a2bca2a02b488fd20e677c04ffad322bbdR282] parameter to {{Descriptor.fromFilenameWithComponent}} that throws an error when the directory does not match the expected structure. Tools that do not require fail-behavior can set validateDirs=false, but for online sstable reading this would throw an error when an illegal directory structure is found.

I agree with [~e.dimitrova] suggestion that we should include keyspace/table/index name in a sstable component to avoid needing to parse directory structure to find out this information.

I propose applying the simpler fix on earlier versions (4.x), and the improved regex-based fix on trunk. What do you think?;;;","15/Dec/22 10:03;smiklosovic;That is all fine for me, [~paulo] . If you scroll up enough, there is my original PR which was also trying to base the solution to this problem on regexp (1) but then we kind of abandoned that in favor of more concise and ""less invasive"" patch. What I see is that you managed to still do regexp but it looks way shorter which I am glad for.

What do you consider to be the simpler fix? The one I put together without your stuff on top? If yes, why do you want to have simpler in 4.0 and 4.1 and this new stuff in trunk only? Is not it better to just have one approach committed everywhere? 

 

Also,  SSTableLoaderTest#testLoadingBackupsTable is failing for me locally while testLoadingSnapshotsTable is not. Maybe your patch is not covering something?

(1) https://github.com/apache/cassandra/pull/798/files;;;","15/Dec/22 16:19;paulo;bq. If you scroll up enough, there is my original PR which was also trying to base the solution to this problem on regexp (1) but then we kind of abandoned that in favor of more concise and ""less invasive"" patch. 

Sorry, I missed that.

bq. If yes, why do you want to have simpler in 4.0 and 4.1 and this new stuff in trunk only? Is not it better to just have one approach committed everywhere? 

The simpler fix is less invasive so less riskier for released versions, we probably will no longer touch this code. The regex fix changes the way directories are parsed but is more robust and future-proof so we would adopt that moving forward in trunk, at least until we have the proper fix of encoding keyspace/table/index info in the sstable metadata.

bq. Also,  SSTableLoaderTest#testLoadingBackupsTable is failing for me locally while testLoadingSnapshotsTable is not. Maybe your patch is not covering something?

I haven't checked tests other than \{{DescriptorTest}}, I'll take a look and submit a full CI run.;;;","15/Dec/22 21:24;paulo;The reason why {{SSTableLoaderTest#testLoadingBackupsTable}} was failing was because this test was using the legacy table directory format which does not have {{{}-{uuid{}}}} part and the regex was failing to pick up this case.

On [this commit|https://github.com/pauloricardomg/cassandra/commit/59f42ce2e58f7846355ebb9f3395fceb28c76631], I added the {{./*}} prefix to the regex which made it pick up the case of a ""backups"" table in the legacy directory format without the table uuid. I also updated {{SSTableLoaderTest}} to use the new [table directory format|https://github.com/pauloricardomg/cassandra/commit/59f42ce2e58f7846355ebb9f3395fceb28c76631#diff-7caf9acb4092d0f8da99e009b74cab5832ee797d5c9b191ab2e3b394a533812fR333].

We did not have a test on {{DescriptorTest#testKeyspaceTableParsing}} to pick up the scenario of a legacy ""backups"" table directory and keyspace, when the table id suffix is missing so I added it [here|https://github.com/pauloricardomg/cassandra/commit/59f42ce2e58f7846355ebb9f3395fceb28c76631#diff-f1003299495ea8593e83c86e261e77d7d6d7a57a52c61f52e51e7e816fcdad5eR223].

However, neither the solutions are able to correctly parse a snapshot in a legacy ""backups"" table, for example
{noformat}
/path/to/cassandra/data/dir2/dir5/dir6/backups/backups/snapshots/snapshots/na-1-big-Index.db
{noformat}
Both approaches consider this an sstable on the ""snapshots"" table of the ""snapshots"" directory, and not a snapshot on the ""backups"" table of the ""backups"" keyspace. However I don't think we need to fix for this case as we should no longer support directories in the legacy format moving forward.

I have submitted a CI run for the latest branch [here|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2106/].

[~smiklosovic] if tests look good and you're ok I can prepare patch for earlier versions with the simpler fix, and the regex fix on trunk. Should we do just 4.x or 3.X too?;;;","15/Dec/22 22:31;smiklosovic;_We did not have a test on DescriptorTest#testKeyspaceTableParsing to pick up the scenario of a legacy ""backups"" table directory and keyspace, when the table id suffix is missing so I added it here._

So, we _do_ support that, still, right? In that case, could you add a test in SSTableLoaderTest as it was, that it is loading it just fine without uuid as well? Just same thing as it was before.

When it comes to branches, more branches better it is :D I made the peace with having it in 4.0+, you will have bonus points for anything older. However, having data being lost on this kind of stuff is rather embarrassing in 2022 (almost 2023!)
;;;","19/Dec/22 18:37;paulo;{quote} In that case, could you add a test in SSTableLoaderTest as it was, that it is loading it just fine without uuid as well?
{quote}
done [here|https://github.com/pauloricardomg/cassandra/commit/9cc0f63171c60e927af18eb3256eb63a29916a43].

During a [CI run|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2114/testReport/] of the trunk patch, I realized the original regex was only accepting "".db"" sstable files, so it was failing to correctly parse other extensions (such as .txt or .crc32). So I updated the regex to accept any extension on [this commit|https://github.com/pauloricardomg/cassandra/commit/345222a3e2504a84ef91eb25e35ae23762c34178]. We could make the regex more prescriptive with only supported extensions, but I don't think this is needed for now.

I prepared 4.0/4.1 patches with the less disruptive fix, and the trunk patch with the improved regex-based fix:
|branch||CI||
|[CASSANDRA-14013-4.0|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-4.0]|[#2115|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2115/] (finished)|
|[CASSANDRA-14013-4.1|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-4.1]|[#2121|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2121/] (finished)|
|[CASSANDRA-14013-trunk|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-trunk]|[#2125|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2125/] (running)|

(will update state when CI is finished)


Are you ok with the improved regex fix to trunk [~blerer], while having the simpler fix on 4.x to reduce risk on released versions?;;;","21/Dec/22 07:33;smiklosovic;I think there is a regression in the trunk patch (1)

(1) https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2125/testReport/junit/org.apache.cassandra.io.sstable/LegacySSTableTest/testStreamLegacyCqlTables_compression_2/;;;","04/Jan/23 15:09;smiklosovic;[~paulo] do you plan to take a look at this again? I feel we are super-close to finish line here.;;;","09/Jan/23 10:46;smiklosovic;The problem is that when we are trying to get a descriptor for a legacy sstable, the test is going to find the first file in the dir and it might happen that it will return "".txt"" file. But Descriptor.LEGACY_SSTABLE_DIR_PATTERN is ending on "".db"". We should just do this [https://github.com/pauloricardomg/cassandra/pull/2]

I am running the build for trunk with that PR included here [https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2166/];;;","10/Jan/23 01:08;paulo;Good catch! I've added two test cases on {{DescriptorTest}} for non .db files [on this commit|https://github.com/pauloricardomg/cassandra/commit/d5232cbc225b7d7d7b1adf67bd819dfea0d00b79].

I've incorporated [your commit|https://github.com/pauloricardomg/cassandra/pull/2/commits/d5eb3b69bb4d7262fd19368082dbd466b77e7b90] + the test change [above|https://github.com/pauloricardomg/cassandra/commit/d5232cbc225b7d7d7b1adf67bd819dfea0d00b79] into the trunk branch, rebased and resubmitted CI for all branches:
|branch||CI||
|[CASSANDRA-14013-4.0|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-4.0]|[#2171|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2171/] (running)|
|[CASSANDRA-14013-4.1|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-4.1]|[#2170|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2170/] (running)|
|[CASSANDRA-14013-trunk|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14013-trunk]|[#2169|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2169/] (running)|

After CI looks good for all branches this should be good to go from my side.;;;","12/Jan/23 11:41;smiklosovic;seems good to me but 4.1 build somehow finished in half? (too low number of tests run and it returned as errorneous), I will rerun 4.1 branch.;;;","13/Jan/23 03:04;paulo;Resubmitted 4.1 CI on https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2187/;;;","13/Jan/23 11:10;smiklosovic;looks great! +1. Lets ship this!;;;","17/Jan/23 03:55;paulo;Committed as [6e6846892a5d7ec2feaf2c35d380ed5975d09517|https://github.com/apache/cassandra/commit/6e6846892a5d7ec2feaf2c35d380ed5975d09517] to {{cassandra-4.0}} and merged to [cassandra-4.1|https://github.com/apache/cassandra/commit/c9968a8e95253a23846be1b5f502773a1eeea48d] and [trunk|https://github.com/apache/cassandra/commit/6501f576e7c663d285a4db207e6b3dc7f887e8f4] (regex-based version).



Created CASSANDRA-18168 to serialize keyspace/table metadata on {{StatsMetadata}} component.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SStable ordering by max timestamp in SinglePartitionReadCommand,CASSANDRA-14010,13117962,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jonathan.pellby,jonathan.pellby,13/Nov/17 15:11,15/May/20 08:03,14/Jul/23 05:56,07/Dec/17 22:14,3.0.16,3.11.2,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,,,,,0,correctness,,,,"We have a test environment were we drop and create keyspaces and tables several times within a short time frame. Since upgrading from 3.11.0 to 3.11.1, we are seeing a lot of create statements failing. See the logs below:
{code:java}
2017-11-13T14:29:20.037986449Z WARN Directory /tmp/ramdisk/commitlog doesn't exist
2017-11-13T14:29:20.038009590Z WARN Directory /tmp/ramdisk/saved_caches doesn't exist
2017-11-13T14:29:20.094337265Z INFO Initialized prepared statement caches with 10 MB (native) and 10 MB (Thrift)
2017-11-13T14:29:20.805946340Z INFO Initializing system.IndexInfo
2017-11-13T14:29:21.934686905Z INFO Initializing system.batches
2017-11-13T14:29:21.973914733Z INFO Initializing system.paxos
2017-11-13T14:29:21.994550268Z INFO Initializing system.local
2017-11-13T14:29:22.014097194Z INFO Initializing system.peers
2017-11-13T14:29:22.124211254Z INFO Initializing system.peer_events
2017-11-13T14:29:22.153966833Z INFO Initializing system.range_xfers
2017-11-13T14:29:22.174097334Z INFO Initializing system.compaction_history
2017-11-13T14:29:22.194259920Z INFO Initializing system.sstable_activity
2017-11-13T14:29:22.210178271Z INFO Initializing system.size_estimates
2017-11-13T14:29:22.223836992Z INFO Initializing system.available_ranges
2017-11-13T14:29:22.237854207Z INFO Initializing system.transferred_ranges
2017-11-13T14:29:22.253995621Z INFO Initializing system.views_builds_in_progress
2017-11-13T14:29:22.264052481Z INFO Initializing system.built_views
2017-11-13T14:29:22.283334779Z INFO Initializing system.hints
2017-11-13T14:29:22.304110311Z INFO Initializing system.batchlog
2017-11-13T14:29:22.318031950Z INFO Initializing system.prepared_statements
2017-11-13T14:29:22.326547917Z INFO Initializing system.schema_keyspaces
2017-11-13T14:29:22.337097407Z INFO Initializing system.schema_columnfamilies
2017-11-13T14:29:22.354082675Z INFO Initializing system.schema_columns
2017-11-13T14:29:22.384179063Z INFO Initializing system.schema_triggers
2017-11-13T14:29:22.394222027Z INFO Initializing system.schema_usertypes
2017-11-13T14:29:22.414199833Z INFO Initializing system.schema_functions
2017-11-13T14:29:22.427205182Z INFO Initializing system.schema_aggregates
2017-11-13T14:29:22.427228345Z INFO Not submitting build tasks for views in keyspace system as storage service is not initialized
2017-11-13T14:29:22.652838866Z INFO Scheduling approximate time-check task with a precision of 10 milliseconds
2017-11-13T14:29:22.732862906Z INFO Initializing system_schema.keyspaces
2017-11-13T14:29:22.746598744Z INFO Initializing system_schema.tables
2017-11-13T14:29:22.759649011Z INFO Initializing system_schema.columns
2017-11-13T14:29:22.766245435Z INFO Initializing system_schema.triggers
2017-11-13T14:29:22.778716809Z INFO Initializing system_schema.dropped_columns
2017-11-13T14:29:22.791369819Z INFO Initializing system_schema.views
2017-11-13T14:29:22.839141724Z INFO Initializing system_schema.types
2017-11-13T14:29:22.852911976Z INFO Initializing system_schema.functions
2017-11-13T14:29:22.852938112Z INFO Initializing system_schema.aggregates
2017-11-13T14:29:22.869348526Z INFO Initializing system_schema.indexes
2017-11-13T14:29:22.874178682Z INFO Not submitting build tasks for views in keyspace system_schema as storage service is not initialized
2017-11-13T14:29:23.700250435Z INFO Initializing key cache with capacity of 25 MBs.
2017-11-13T14:29:23.724357053Z INFO Initializing row cache with capacity of 0 MBs
2017-11-13T14:29:23.724383599Z INFO Initializing counter cache with capacity of 12 MBs
2017-11-13T14:29:23.724386906Z INFO Scheduling counter cache save to every 7200 seconds (going to save all keys).
2017-11-13T14:29:23.984408710Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.032687075Z INFO Global buffer pool is enabled, when pool is exhausted (max is 125.000MiB) it will allocate on heap
2017-11-13T14:29:24.214123695Z INFO Token metadata:
2017-11-13T14:29:24.304218769Z INFO Completed loading (14 ms; 8 keys) KeyCache cache
2017-11-13T14:29:24.363978406Z INFO No commitlog files found; skipping replay
2017-11-13T14:29:24.364005238Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.394408476Z INFO Token metadata:
2017-11-13T14:29:24.709411652Z INFO Preloaded 0 prepared statements
2017-11-13T14:29:24.719332880Z INFO Cassandra version: 3.11.1
2017-11-13T14:29:24.719355969Z INFO Thrift API version: 20.1.0
2017-11-13T14:29:24.719359443Z INFO CQL supported versions: 3.4.4 (default: 3.4.4)
2017-11-13T14:29:24.719362103Z INFO Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4)
2017-11-13T14:29:24.766102400Z INFO Initializing index summary manager with a memory pool size of 25 MB and a resize interval of 60 minutes
2017-11-13T14:29:24.778800183Z INFO Starting Messaging Service on /172.17.0.2:7000 (eth0)
2017-11-13T14:29:24.783832188Z WARN No host ID found, created 62452b7c-33ae-40e6-859c-1d7c803aaea8 (Note: This should happen exactly once per node).
2017-11-13T14:29:24.897281778Z INFO Loading persisted ring state
2017-11-13T14:29:24.904217782Z INFO Starting up server gossip
2017-11-13T14:29:25.003802973Z INFO This node will not auto bootstrap because it is configured to be a seed node.
2017-11-13T14:29:25.047674499Z INFO Generated random tokens. tokens are [-6736304773851341012, 3437071596424929702, 4372058337604769145, -306854781937968525, -4419476154597297006, 4339837665480866486, 2052026232731139893, -5761537575805252593, -4477540978357776290, 6263754683045286998, 3670054894619378302, -4326549778810780939, 7187409938161102814, 7030537377703307755, -2757270254308154659, -1953637968902719055, -7235425703069930259, 7123794193321014835, 349308827967095711, 997472983569031481, 992257140226393205, -4045122629441468253, 4149955653388319941, -3690032393349188278, 3528068129562283633, -5057394127379238561, -4944743272177354946, 1371473468273321389, -2771267888257678908, -2379074055482922854, 8800628062632970014, 6016352719444925532, -6458243637210081043, -7131512441131507433, -6135681286390467242, -7886878247827491401, -3964432859204941604, -7124853795154335905, 4536647221115220987, 4518363137218750861, -3945920538919881061, -8569890499152898728, -2228677668104169495, -4004623128783039030, -6849460601197629451, -1787645289665343374, -9004089114738085395, -8444847561386064840, -7719025430480017932, -5020575591450775929, -3535144847803187721, 7252524597471726426, -2582131369519057623, 3737595811793840609, -7248797595897252845, -7065188032269288840, -6731826791431802176, -2970075663731571587, -2619987499373344925, -2698285069650269138, -8589822844420136511, 2658120945314344720, -3710290429036098141, 134530136452862749, 3703742438909992913, 3460544540911930621, 8673891706698173777, 2853177281247015813, 13977464647778584, 2404057737490125388, -6759648287860184451, 744453319830059045, -688104893800828924, 3356383003502762348, 9054641886966810357, 2317130729058165506, -5810663910204725460, 2577132949237273515, 6326216055185945365, 1376570278575995967, 8758101809469842945, -2892126907778256351, -1716283861287440286, 3040640159143123724, 4243935966006505554, -6827972097309863039, 3055912546894309570, -3992773844369808712, -4717007910267923035, -846198401308205724, -3924870907185309086, 1746803312676010060, 6821355560067598541, -5786385588878319458, 3085551110635941848, 7832310180114101987, -9149254679798945822, 3124836728424468300, -100875121723899324, -7606007094353527325, 270256410769436649, -3016541299722946307, 6864985654287583845, 8465468836551135602, 7372808321676939792, -2815261206329145311, -2044219183173664775, -5342853768228072396, 3636940711408324184, -2772742494800447004, -8420993393273439531, -1530882172522252534, 8236427746033013128, -8939749738449264357, -571957476330656311, 6462994120934510138, -2744633996286755268, 1001793370994802364, 6170004027360887596, 383603396273760626, 184737756504479596, -4799447088893889554, 1038205033737034383, 2078124248957773983, -5177819727898656480, 1588469358432181111, 2476693400197902714, 246839957213783595, -7804622995667946321, 3516202677463047183, 7649126752776473673, -3286662198144050257, 2592926684883421936, 6953901594207876325, 8920684239689152479, -2427878301857439455, -6527468054932471540, -4117125961852289967, -2833593154725933249, 2548273043767381234, -814886098184093796, -1113961241682560435, -8364806058670744019, -86067309810855914, -7325813350040495905, -2651532619332818109, -3028501296208600216, 2638649530375347897, -3870517833780069551, 3770751443844709295, -7272035856681375921, -6750394828506790417, 3368553496734537183, 8516129492713951191, 4435960977618718666, 638690551817702460, -7462842134093200053, -7312636473795422279, 3825550639500258186, -490674188267611204, 8488259904981422083, 4436678791994058329, 5971819389544487212, 5777643219857256454, 6295906877222880293, -6635403410495817577, -7125973103119231247, 2275471188158109929, -6554337501188391642, -4759608795508681126, -7655250005358224912, 9106670136441382451, -9080117178764089351, 5094764588972879219, -3599769156391426161, 6116955962236377408, -1734768840951819839, 7826627278264825770, -2624139016757063818, -4122417151587476614, -6757251857390630385, 2099124804383862824, -3162332634454027278, 4826222794133551270, 9122652158513265055, 1734656138981660315, 972980826344778639, -1746779194020635548, -3426944282250211269, -3857828063692993065, 1895243495321867610, -8828035583443240909, -4705856469629722102, -8519546521146945353, -2150150551733933931, 8281585304878501119, -2775028105733898661, 2087277989579187052, -4016777313261130077, 2747128117959922334, -1398884803916585873, 7188260080368469340, -3880993098463994199, 3574665846011083154, 5260683239918360122, 5817587463499837044, 38978473621576635, 2680910834841463710, 6083561971466189055, 7236937177408808074, -3600112532662592989, -4559800196660261967, 8276688045060113438, 5496539762676760591, -2999626688519766687, 8917068693185637310, 2348378561310644717, 7605443413072783308, 5729359499569394810, -782345069306605591, 1165004403533704355, -8301882560002322767, 2008499890787626408, -6211027251975593898, 7406423735628820605, -3204398339633370684, -7917412446164112725, -106645076087724250, -1186720400780396653, -8676089669972641821, -1970508303671183113, -7283082875075535628, -3469652138221449481, -3310949358194646693, 6449384223770405185, -3602652844861890703, -7845236015467185307, -4548809972889727666, -8898627491921139823, 5187965699546741544, 295363921125698104, -8013235493809339368, -6747271362503076577, 1102625310233591704, -2543233385033476145, -6197912327393001665, 118165474822979356, -4838870266722406438, -5797141823778124932, -1506683916229985698, 9139710449103348665, -1571612701117454805, 8031141543284728427, 8472337544063987034, 3222463867738580103, 8210687258187437204]
2017-11-13T14:29:25.092248590Z INFO Create new Keyspace: KeyspaceMetadata{name=system_traces, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[org.apache.cassandra.config.CFMetaData@3bc5ed95[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,flags=[COMPOUND],params=TableParams{comment=tracing sessions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [client command coordinator duration request started_at parameters]],partitionKeyColumns=[session_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[client, command, session_id, coordinator, request, started_at, duration, parameters],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@1a296ffd[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,flags=[COMPOUND],params=TableParams{comment=tracing events, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [activity source source_elapsed thread]],partitionKeyColumns=[session_id],clusteringColumns=[event_id],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[activity, event_id, session_id, source, thread, source_elapsed],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.394141160Z INFO Not submitting build tasks for views in keyspace system_traces as storage service is not initialized
2017-11-13T14:29:25.408584506Z INFO Initializing system_traces.events
2017-11-13T14:29:25.424314845Z INFO Initializing system_traces.sessions
2017-11-13T14:29:25.483133136Z INFO Create new Keyspace: KeyspaceMetadata{name=system_distributed, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}, tables=[org.apache.cassandra.config.CFMetaData@2884b38b[cfId=759fffad-624b-3181-80ee-fa9a52d1f627,ksName=system_distributed,cfName=repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants]],partitionKeyColumns=[keyspace_name, columnfamily_name],clusteringColumns=[id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[status, id, coordinator, finished_at, participants, exception_stacktrace, parent_id, range_end, range_begin, exception_message, keyspace_name, started_at, columnfamily_name],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7fcc80b2[cfId=deabd734-b99d-3b9c-92e5-fd92eb5abf14,ksName=system_distributed,cfName=parent_repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges]],partitionKeyColumns=[parent_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,columnMetadata=[requested_ranges, exception_message, keyspace_name, successful_ranges, started_at, finished_at, options, exception_stacktrace, parent_id, columnfamily_names],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7e500004[cfId=5582b59f-8e4e-35e1-b913-3acada51eb04,ksName=system_distributed,cfName=view_build_status,flags=[COMPOUND],params=TableParams{comment=Materialized View build status, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UUIDType),partitionColumns=[[] | [status]],partitionKeyColumns=[keyspace_name, view_name],clusteringColumns=[host_id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[view_name, status, keyspace_name, host_id],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.598604284Z INFO Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized
2017-11-13T14:29:25.602132560Z INFO Initializing system_distributed.parent_repair_history
2017-11-13T14:29:25.624580018Z INFO Initializing system_distributed.repair_history
2017-11-13T14:29:25.624605811Z INFO Initializing system_distributed.view_build_status
2017-11-13T14:29:25.682205208Z INFO JOINING: Finish joining ring
2017-11-13T14:29:25.808448539Z INFO Create new Keyspace: KeyspaceMetadata{name=system_auth, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[org.apache.cassandra.config.CFMetaData@3c28c0da[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[role],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[salted_hash, member_of, role, can_login, is_superuser],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@2e0f771e[cfId=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d,ksName=system_auth,cfName=role_members,flags=[COMPOUND],params=TableParams{comment=role memberships lookup table, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[role],clusteringColumns=[member],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, member],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@4fabdebb[cfId=3afbe79f-2194-31a7-add7-f5ab90d8ec9c,ksName=system_auth,cfName=role_permissions,flags=[COMPOUND],params=TableParams{comment=permissions granted to db roles, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | [permissions]],partitionKeyColumns=[role],clusteringColumns=[resource],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, resource, permissions],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7103b8de[cfId=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec,ksName=system_auth,cfName=resource_role_permissons_index,flags=[COMPOUND],params=TableParams{comment=index of db roles with permissions granted on a resource, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[resource],clusteringColumns=[role],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, role],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.934019252Z INFO Not submitting build tasks for views in keyspace system_auth as storage service is not initialized
2017-11-13T14:29:25.953887674Z INFO Initializing system_auth.resource_role_permissons_index
2017-11-13T14:29:25.957358898Z INFO Initializing system_auth.role_members
2017-11-13T14:29:25.967935061Z INFO Initializing system_auth.role_permissions
2017-11-13T14:29:25.995449692Z INFO Initializing system_auth.roles
2017-11-13T14:29:26.193856408Z INFO Netty using native Epoll event loop
2017-11-13T14:29:26.247676724Z INFO Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
2017-11-13T14:29:26.247705469Z INFO Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
2017-11-13T14:29:26.309591159Z INFO Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
2017-11-13T14:29:36.275846037Z INFO Created default superuser role 'cassandra'
2017-11-13T14:29:40.333918591Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:40.434399612Z INFO Create new table: org.apache.cassandra.config.CFMetaData@c74a94b[cfId=1572b410-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:43.243928493Z INFO Create new table: org.apache.cassandra.config.CFMetaData@1a0616e9[cfId=171e8f50-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:43.284700491Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:44.706916652Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:44.924446999Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:44.993983743Z INFO Create new table: org.apache.cassandra.config.CFMetaData@7338ccab[cfId=182996b0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:45.078407254Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:46.244137923Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:46.500351100Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:46.575419551Z INFO Create new table: org.apache.cassandra.config.CFMetaData@229f3694[cfId=191b97d0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:46.617101680Z ERROR Unexpected error during query
2017-11-13T14:29:46.617126436Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617130194Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617133358Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617135966Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617138576Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617141018Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617143454Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617145953Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617148372Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617150806Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617153201Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617155595Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617157962Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617160377Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617162787Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617166295Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617168898Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617171389Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617173808Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617184008Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617186971Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617189340Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.617191666Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617193951Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.617196258Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.617198553Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617200927Z ... 20 common frames omitted
2017-11-13T14:29:46.617203114Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.617205382Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617207766Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617210107Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617212462Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617214868Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617217261Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617220404Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617222948Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617225287Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617227589Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.617229894Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.617232175Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.617234514Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617236990Z ... 1 common frames omitted
2017-11-13T14:29:46.621331936Z ERROR Exception in thread Thread[MigrationStage:1,5,main]
2017-11-13T14:29:46.621360645Z java.lang.NullPointerException: null
2017-11-13T14:29:46.621364339Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621373614Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621376363Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621378927Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621381395Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621384992Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621387567Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621390255Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621392722Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
2017-11-13T14:29:46.621395153Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.621397502Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.621399919Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
2017-11-13T14:29:46.621402347Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621404867Z at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
2017-11-13T14:29:46.626625652Z ERROR Unexpected exception during request
2017-11-13T14:29:46.626650886Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626654840Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626658003Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626660570Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626663155Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626665745Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626676412Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626679497Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626682051Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626684610Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626687059Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626689495Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626691956Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626694391Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626696869Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626700811Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626703433Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626705926Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626708464Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626710858Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626713448Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626715868Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.626718281Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626720647Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.626723006Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.626725392Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626727820Z ... 20 common frames omitted
2017-11-13T14:29:46.626730100Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.626735106Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626737800Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626740362Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626742804Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626745273Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626747719Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626750759Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626753445Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626755900Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626758684Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.626761055Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.626763436Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.626765871Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626768418Z ... 1 common frames omitted{code}

Steps to reproduce:
1. Start cassandra
2. Start cqlsh and paste the following in quick succession:
{code:java}
USE system;
DROP KEYSPACE IF EXISTS my_keyspace;
CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
USE my_keyspace;
CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
INSERT INTO schema_version (id, version) values (1, 0);{code}
3. Once fourth time or so , we'll see:
{code:java}
cqlsh:system> CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}
or
{code:java}
cqlsh:my_keyspace> CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}",,blerer,jasonstack,jborgstrom,jeromatron,jjirsa,jjordan,jonathan.pellby,mike_tr_adamson,pauloricardomg,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13776,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 22:14:05 UTC 2017,,,,,,,,,,"0|i3mplz:",9223372036854775807,,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"17/Nov/17 00:48;VincentWhite;I had a look at this and assumed that org.apache.cassandra.schema.SchemaKeyspace#fetchKeyspaceParams() was just not getting any rows returned when it queried the system_schema.keyspaces table. In fact in my testing it was getting a row returned but it only contained the primary key and null's for both other columns. I didn't dig too deep into this but it doesn't seem like it should happen, it's probably worth someone with a more intimate knowledge of the read path taking a look. Also on my machine I could only trigger this exception with multiple clients looping CREATE/DROP commands and it was still relatively rare.  
;;;","29/Nov/17 19:56;jjordan;I just saw this on some tests today as well.  The issue seems to be that the drop is happening concurrently with tables being initialized:

{quote}
2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: 
{quote};;;","06/Dec/17 08:25;jasonstack;| patch  |  test  | dtest|
| [3.0 |https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-14010-3.0?expand=1 ] |
| [3.11 |https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-14010-3.11?expand=1 ] |
| [trunk | https://github.com/apache/cassandra/compare/trunk...jasonstack:CASANDRA-14010-trunk?expand=1] |


It turns out that the query in {{fetchKeyspaceParams()}} gets incomplete data from memtable.

{code}
process:
  0. drop ks with ts1 
  1. apply create ks mutation with t2 (t2>t1)
  2. flush memtables including ""system_schema.keyspaces"" table
  3. select keyspace_name from ""system_schema.keyspaces"" table in {{fetchKeyspaceOnly()}} causing ""defragmenting"" (at the end of SPRC.queryMemtableAndSSTablesInTimestampOrder()) to insert the selected data into memtable
  4. select * from ""system_schema.keyspaces"" table in {{fetchKeyspaceParams()}} getting incomplete data(row with liveness of t2 and deletion of t1, no regular columns) from memtable. First sstable's maxtimestamp is smaller than memtable data's deletion time(drop ks time, t1) because sstables are sorted by maxTS in ascending order and other newer sstables are skipped...

The correct order is descending to eliminate older sstables.
{code}

The patch is to make sure sstables are compared with max-timestamp in descending order...

The reason that it only happened on 3.11 is related to {{queriedColumn in ColumnFilter}} and value skipping added in 3.x.  (a bit complex...)

When no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as empty, thus when processing the query in #3, unselected columns(eg. durable_wirtes, replication) are skipped in Cell.Serializer: helper.canSkipValue().

But in trunk, due to CASSANDRA-7396, when no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as null, thus unselected columns are not skipped, later put into memtable. (lost the benefit of value skipping);;;","06/Dec/17 08:53;jasonstack;If we ignore the complexity of defragmenting, columnfilter, etc... It can be reproduced easily:
{code:title=reproduce}
        createTable(""CREATE TABLE %s (k1 int, v1 int, v2 int, PRIMARY KEY (k1))"");
        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(keyspace(), currentTable());
        cfs.disableAutoCompaction();

        execute(""INSERT INTO %s(k1,v1,v2) VALUES(1,1,1)  USING TIMESTAMP 5"");
        cfs.forceBlockingFlush();

        execute(""INSERT INTO %s(k1,v1,v2) VALUES(1,1,2)  USING TIMESTAMP 8"");
        cfs.forceBlockingFlush();

        execute(""INSERT INTO %s(k1) VALUES(1)  USING TIMESTAMP 7"");
        // deletion 6 shadow sstable-1 with ts=5 ...
        execute(""DELETE FROM %s USING TIMESTAMP 6 WHERE k1 = 1"");

        assertRows(execute(""SELECT * FROM %s WHERE k1=1""), row(1, 1, 2));
{code}
;;;","06/Dec/17 14:41;jasonstack;CI looks good..

;;;","06/Dec/17 15:10;blerer;Thanks for the patch. The fix looks good :-)
Small nit: The unit test can be simplified by using {{disableCompaction()}} instead of {{cfs.disableAutoCompaction()}} and {{flush()}} instead of {{cfs.forceBlockingFlush()}}. ;;;","06/Dec/17 17:07;jjirsa;{{SSTableReader.maxTimestampComparator}} is used in LCS:

{code}
                if (candidates.size() > MAX_COMPACTING_L0)
                {
                    // limit to only the MAX_COMPACTING_L0 oldest candidates
                    candidates = new HashSet<>(ageSortedSSTables(candidates).subList(0, MAX_COMPACTING_L0));
                    break;
                }
...

    private List<SSTableReader> ageSortedSSTables(Collection<SSTableReader> candidates)
    {
        List<SSTableReader> ageSortedCandidates = new ArrayList<>(candidates);
        Collections.sort(ageSortedCandidates, SSTableReader.maxTimestampComparator);
        return ageSortedCandidates;
    }

{code}

Changing it to be oldest first violates at least the comment and the intent. Probably need to introduce a new {{Comparator<SSTableReader>}} like {{maxTimestampComparatorDescending}}

;;;","06/Dec/17 18:01;jjordan;[~jjirsa] CASSANDRA-13776 accidentally changed the definition of the maxTimestampComparator while trying to simplify code.
From CASSANDRA-13776:

{code}
-    public static final Comparator<SSTableReader> maxTimestampComparator = new Comparator<SSTableReader>()
 -    {
 -        public int compare(SSTableReader o1, SSTableReader o2)
 -        {
 -            long ts1 = o1.getMaxTimestamp();
 -            long ts2 = o2.getMaxTimestamp();
 -            return (ts1 > ts2 ? -1 : (ts1 == ts2 ? 0 : 1));
 -        }
 -    };
 +    public static final Comparator<SSTableReader> maxTimestampComparator = (o1, o2) -> Long.compare(o1.getMaxTimestamp(), o2.getMaxTimestamp());
{code}

This is just putting it back like it was before the CASSANDRA-13776.  So this is how it has worked up until 13776 went in.;;;","06/Dec/17 18:21;jjirsa;Ok so LCS is wrong, created  CASSANDRA-14099 to follow up there.
;;;","07/Dec/17 00:54;jasonstack;Thanks for the review.. fix nits and restarted CI.  CI looks good.;;;","07/Dec/17 22:14;pauloricardomg;Committed as {{a9225f90e205a7c2b24a4ad4a32d0961067005b0}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RTs at index boundaries in 2.x sstables can create unexpected CQL row in 3.x,CASSANDRA-14008,13117589,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,10/Nov/17 14:57,18/Jun/19 16:24,14/Jul/23 05:56,12/Dec/17 18:00,3.0.16,3.11.2,,,,,Legacy/Local Write-Read Paths,,,,,1,correctness,,,,"In 2.1/2.2, it is possible for a range tombstone that isn't a row deletion and isn't a complex deletion to appear between two cells with the same clustering. The 8099 legacy code incorrectly treats the two (non-RT) cells as two distinct CQL rows, despite having the same clustering prefix.",,aleksey,jasonstack,jay.zhuang,jeromatron,jjirsa,KurtG,ngaugler,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12144,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 18:00:17 UTC 2017,,,,,,,,,,"0|i3mnbb:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"12/Nov/17 20:09;KurtG;So this kind of sounds like an issue we've been investigating, but we've had trouble finding the clustering that's actually causing the problem. Do you have an example to reproduce, or even just an example row to compare with?

If it's the same problem we've found that in some cases after upgrade to 3.11 the SSTable containing the data will also be corrupt.;;;","12/Nov/17 21:26;jjirsa;It does create an invalid (or similarly broken)  sstable on compaction/upgradesstables, and while I've got a fix for the original problem done, I need to think about the right way to un-break the resulting 3.0 sstable (which should be do-able, based on what I've seen so far).




;;;","13/Nov/17 11:41;aleksey;We can probably generate an sstable that triggers this bug relatively easily for a regression test (nice to have, but won't block the patch on lack of it).

And, as Jeff mentions, it would be nice to find a way to un-break 3.0 sstables where the damage's been done already, in a follow-up JIRA.;;;","01/Dec/17 00:39;jjirsa;The raw patches that fix the bug in LegacyLayout are at 

|| Branch || CI ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-14008] | [!https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-14008.svg?style=svg!|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-14008/] | 
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-14008] | [!https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-14008.svg?style=svg!|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-14008/]| 

I was hoping to actually have a solution to un-breaking the broken 3.0 sstables in the same patch, but it's proving to be more difficult than I anticipated. I haven't yet tried to make some sample sstables for regression tests, I agree it'd be nice to have those. 

Please glance at the code, and I'll work on the regression sstables before committing.;;;","01/Dec/17 11:56;aleksey;+1;;;","11/Dec/17 19:40;jjirsa;[~iamaleksey] can you check both branches for a new regression test please?

Exact commits are https://github.com/jeffjirsa/cassandra/commit/eff1f18fcd80b4860bb5812142d196e94b6ae2a1 and 
https://github.com/jeffjirsa/cassandra/commit/a85befc4fc3c0e44f6751a3e6472afecaefc4b77 ;;;","12/Dec/17 12:09;aleksey;Validated the test, both logically, and by running {{LegacyLayoutTest}} in both branches, with and without the fix reverted, ensuring that it doesn't fail, then fails, respectively.

+1;;;","12/Dec/17 18:00;jjirsa;Thanks Aleksey. Committed as {{4a2b516a3488ab04ee3338e74397b8c6d69e6d43}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlshlib tests fail due to compact table,CASSANDRA-14007,13117404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,jkni,jkni,09/Nov/17 21:55,15/May/20 08:00,14/Jul/23 05:56,22/Feb/18 17:32,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,"The pylib/cqlshlib tests fail on initialization with the error {{SyntaxException: <Error from server: code=2000 \[Syntax error in CQL query\] message=""Compact tables are not allowed in Cassandra starting with 4.0 version."">}}. 

The table {{dynamic_columns}} is created {{WITH COMPACT STORAGE}}. Since [CASSANDRA-10857], this is no longer supported. It looks like dropping the COMPACT STORAGE modifier is enough for the tests to run, but I haven't looked if we should instead remove the table and all related tests entirely, or if there's an interesting code path covered by this that we should test in a different way now. [~ifesdjeen] might know at a glance.",,ifesdjeen,jkni,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 22 17:24:43 UTC 2018,,,,,,,,,,"0|i3mm6n:",9223372036854775807,4.0,,,,,,snazy,,snazy,,,Normal,,,,,,,,,,,,,,,,,,,"13/Nov/17 10:46;ifesdjeen;I've just re-ran all the dtests and they seem to be clean. Or do we run the cqlshlib tests in some other way?..;;;","13/Nov/17 16:19;jkni;Yeah, the cqlshlib tests have their own script to run and don't run as part of dtests. See [https://github.com/apache/cassandra-builds/blob/f0e63d66269f9086c3a0393a24a55577d21b4454/build-scripts/cassandra-cqlsh-tests.sh] for an example of how to run them.;;;","15/Nov/17 10:58;ifesdjeen;The patch is trivial and can be found [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-14007], there was no real reason to have this table as compact and it doesn't change any output in this case.

However, there are still plenty cqllib tests failing from what seems to be a concurrency or capture issue:

{code}
AssertionError: unexpected echo 'select * from ascii_with_special_chars where k in (0, 1,, 2, 3);' instead of 'select * from ascii_with_special_chars where k in (0, 1, 2, 3);'
{code}

Other tests fail with other duplicated characters. However, this should probably be handled outside of scope of this ticket.;;;","22/Feb/18 17:24;snazy;Patch LGTM. One minor nit: change {{CREATE COLUMNFAMILY}} to {{CREATE TABLE}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use SHA256 when building merkle trees,CASSANDRA-14002,13116901,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,08/Nov/17 07:58,15/May/20 08:01,14/Jul/23 05:56,05/Mar/18 07:22,4.0,4.0-alpha1,,,,,,,,,,0,,,,,We should avoid using SHA-2 when building merkle trees as we don't need a cryptographic hash function for this.,,cnlwsu,jasobrown,jay.zhuang,jeromatron,marcuse,mkjellman,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 07:22:56 UTC 2018,,,,,,,,,,"0|i3mj3j:",9223372036854775807,,,,,,,mkjellman,,mkjellman,,,Normal,,,,,,,,,,,,,,,,,,,"08/Nov/17 08:30;marcuse;Patch for this [here|https://github.com/krummas/cassandra/commits/marcuse/nosharepairs] - it replaces the SHA-256 with Murmur3.

Murmur3 is only 128 bit though, so the patch instead hashes every value twice using 2 different murmur3 instances with different seeds to get the same number of bits as SHA-256. The approach used is similar to what guava does in its ConcatenatedHashFunction.

In my tests with semi-wide partitions (~100KB mean partition size) this reduces the time spent building merkle trees with at least 50%.

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/422/
https://circleci.com/gh/krummas/cassandra/173

cc [~mkjellman];;;","08/Nov/17 19:06;mkjellman;awesome! I'm guessing we don't need to worry about the upgrade path here because streaming will be broken between majors anyways, right?;;;","08/Nov/17 19:17;jasobrown;bq. we don't need to worry about the upgrade path here because streaming will be broken between majors anyways

This is correct, but not because ""streaming is broken between majors"". There are no guarantees that an un-upgraded node would be parse to read sstables streamed to it from an upgrade node, as the sstable format may have changed (we do not force/guarantee backward compatibility of the sstable format). Thus, we don't support streaming between major versions (although there's nothing that bluntly states this, at least as far as I've seen).
;;;","08/Nov/17 20:44;mkjellman;So +1 from me. Comments look good and I don't even see any code style related nits. 50% faster is a bigger win than I expected when I saw it in profiling a long long long time ago to be honest... so that's super awesome news! 

Looks like a few repair related dtests did fail but looking at stdout it seems that this is weirdness with the Apache Jenkins instance and ccm not starting instances cleanly and not any fallout from this change.;;;","09/Nov/17 08:27;marcuse;thanks for the review!

bq. 50% faster is a bigger win than I expected when I saw it in profiling
yeah, I think the biggest win is actually that guavas streaming hashers actually buffer the data to be hashed instead of hashing them byte-for-byte: https://github.com/google/guava/blob/v23.3/guava/src/com/google/common/hash/AbstractStreamingHasher.java#L34

I'm rerunning the dtests [here|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/424/] before committing;;;","09/Nov/17 09:17;marcuse;also pushed a new commit after rebasing on latest trunk (upgrade guava to 23.3) if you could have a look [~mkjellman] (https://github.com/krummas/cassandra/commits/marcuse/nosharepairs);;;","27/Feb/18 17:00;mkjellman;+1 to rebase.;;;","05/Mar/18 07:22;marcuse;and committed as {{68b81372cd838808b304d58677cdc86f6ec35ffa}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't send new_metadata_id for conditional updates,CASSANDRA-13992,13116066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,omichallat,omichallat,03/Nov/17 21:35,07/Mar/23 11:52,14/Jul/23 05:56,21/Nov/17 12:17,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"This is a follow-up to CASSANDRA-10786.

Given the table
{code}
CREATE TABLE foo (k int PRIMARY KEY)
{code}
And the prepared statement
{code}
INSERT INTO foo (k) VALUES (?) IF NOT EXISTS
{code}

The result set metadata changes depending on the outcome of the update:
* if the row didn't exist, there is only a single column \[applied] = true
* if it did, the result contains \[applied] = false, plus the current value of column k.

The way this was handled so far is that the PREPARED response contains no result set metadata, and therefore all EXECUTE messages have SKIP_METADATA = false, and the responses always include the full (and correct) metadata.

CASSANDRA-10786 still sends the PREPARED response with no metadata, *but the response to EXECUTE now contains a {{new_metadata_id}}*. The driver thinks it is because of a schema change, and updates its local copy of the prepared statement's result metadata.

The next EXECUTE is sent with SKIP_METADATA = true, but the server appears to ignore that, and still sends the metadata in the response. So each response includes the correct metadata, the driver uses it, and there is no visible issue for client code.

The only drawback is that the driver updates its local copy of the metadata unnecessarily, every time. We can work around that by only updating if we had metadata before, at the cost of an extra volatile read. But I think the best thing to do would be to never send a {{new_metadata_id}} in for a conditional update.",,ifesdjeen,KurtG,omichallat,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 08 19:03:46 UTC 2017,,,,,,,,,,"0|i3mdyn:",9223372036854775807,5.0,,,,,,snazy,,snazy,,,Low,,,,,,,,,,,,,,,,,,,"07/Nov/17 11:23;KurtG;bq. The next EXECUTE is sent with SKIP_METADATA = true, but the server appears to ignore that
I believe this is because METADATA_CHANGED will take precedence. If C* thinks the metadata changed it will set the METADATA_CHANGED flag and the driver should need to update it's metadata. TBH this isn't super clear from the spec but appears to be what the code achieves [here|https://github.com/apache/cassandra/blob/922dbdb658b1693973926026b213153d05b4077c/src/java/org/apache/cassandra/transport/messages/ExecuteMessage.java#L174].

I may have no idea what I'm talking about but I think the simplest solution to 
bq. never send a new_metadata_id in for a conditional update.
would be to simply always use the same digest for any LWT.
I think the following patch achieves this without breaking anything but I haven't confirmed if it actually fixes the driver issue yet. If someone with more understanding of the protocol and what not could have a glance and let me know if this makes sense or point me in the right direction.
[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:13992-trunk];;;","08/Nov/17 18:12;omichallat;bq. If C* thinks the metadata changed it will set the METADATA_CHANGED flag and the driver should need to update it's metadata. TBH this isn't super clear from the spec but appears to be what the code achieves here.
That makes sense, and indeed explains why the server ignores SKIP_METADATA.

I've tested your patch against a driver snapshot. The bound statement executions now always return the same (empty) digest, but the problem is that the initial preparation still returns a non-empty digest. So the driver executes with that initial digest and gets METADATA_CHANGED with the empty digest every time.
The prepare should also return an empty digest, which I think should be done around [here|https://github.com/kgreav/cassandra/blob/fa259fd79ea3e0a0fac8583a54c9c76464a653be/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L444].

cc [~ifesdjeen];;;","09/Nov/17 10:22;ifesdjeen;If we take this patch, I'd definitely constantize the ""empty"" hash, as very least.

Other than that - I think we should add more tests with LWTs (preferably dtests) and check what happens when we actually alter the table. Since in this case it seems what will happen is when table is ALTER'ed, metadata won't update (I haven't checked it though). In the initial patch we've tried always forcing metadata transfer for LWTs. If the patch has the same effect (metadata is transferred every time). I'm not insisting on any particular solution here though.  ;;;","10/Nov/17 02:10;KurtG;So it seems that prepare does return an empty digest, however {{ResultSet.ResultMetadata.EMPTY}} is created with {{compute}} and is thus a thread local digest, which will get a different hash. From what I can see EMPTY is only ever used in one place worth mentioning, and that's in {{org.apache.cassandra.cql3.ResultSet.ResultMetadata#fromPrepared}} (it's also used in {{org.apache.cassandra.transport.Message.Codec#decode}} but only for protocol versions prior to V1), and will be used for anything that's not a {{SELECT}} statement. 

Now, what I'm wondering is there any significant reason the ID for an ""EMPTY"" metadata ID is created as a thread local digest?
If we can change empty to instead use {{MD5Digest.wrap()}} we solve the prepared problem.

[~omichallat] Before I waste a lot of time figuring out how to test this in the driver, can you point me at how you did it?

bq. Since in this case it seems what will happen is when table is ALTER'ed, metadata won't update (I haven't checked it though)
Yes we'll probably need to do something about this. I'll write up some tests first. 
;;;","13/Nov/17 03:26;omichallat;bq. Before I waste a lot of time figuring out how to test this in the driver, can you point me at how you did it?

The driver-side changes are not merged yet, but you can find them in [this pull request|https://github.com/datastax/java-driver/pull/794]. The test that covers this specific scenario is [PreparedStatementInvalidationTest#should_never_update_statement_id_for_conditional_updates_in_modern_protocol|https://github.com/datastax/java-driver/blob/46825e446ae9d5f57baeb4f5f2c1f5fc4b99d972/driver-core/src/test/java/com/datastax/driver/core/PreparedStatementInvalidationTest.java#L182].

To run the test you'll need to install CCM (see some instructions [here|https://github.com/datastax/java-driver/blob/3.3.x/CONTRIBUTING.md#running-the-tests]). Then because you're not testing a released Cassandra version, you'll need to point the test harness to your local working copy with those system properties: {{-Dcassandra.version=4.0.0 -Dcassandra.directory=/path/to/cassandra}}

If you want to do some debugging, the result metadata is handled in {{ArrayBackedResultSet.java}}. Search for ""CASSANDRA-13992"", there is a comment that explains what should be changed if this ticket is fixed.;;;","13/Nov/17 08:01;ifesdjeen;I've composed a version of the patch, to demonstrate my thinking [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13992]. It seems that we can solve this problem without patching the driver. In fact, it might be even better if inner doings of metadata hash are transparent for the driver.

In short, we can always force {{METADATA_CHANGED}} for conditional statements and avoid computing their metadata to make sure it's empty. It's a rough equivalent of making metadata hash random, just simpler to reason about. What do you think about it [~KurtG] [~omichallat];;;","13/Nov/17 17:17;omichallat;[~ifesdjeen] that would work, the driver can treat an empty {{new_metadata_id}} as ""don't update my local copy"". Namely, changing [this line|https://github.com/datastax/java-driver/blob/6eeb8b2193ab5b50b73b0d9a533e775265f11007/driver-core/src/main/java/com/datastax/driver/core/ArrayBackedResultSet.java#L83] to:
{code}
if (newMetadataId != null && newMetadataId.bytes.length > 0) {
{code}
However that feels kind of hacky. Consider how we would have to explain that in the protocol spec:
{quote}
        - <new_metadata_id> is \[short bytes] representing the new, changed resultset
           metadata. The new metadata ID must also be used in subsequent executions of
           the corresponding prepared statement, if any, *except if it is empty*.
{quote}
It would make so much more sense to force {{METADATA_CHANGED}} to *false* for conditional updates, isn't there any way we can do that?;;;","13/Nov/17 19:42;ifesdjeen;[~omichallat] not sure, since {{METADATA_CHANGED}} is just a flag: e.g. if it's set it's {{true}}, otherwise it's {{false}}. Moreover, I think that the default behaviour for LWTs has to be that we _always_ update metadata: there's no way for server to know what was the last metadata on the client (since it depends on the result), the server can't distinguish between the metadata hash inequality caused by {{ALTER}} vs caused by success/non-success LWT result.

Unless I'm missing something, my patch achieves exactly that (also, without any driver changes): it forces the server to _always_ send the metadata. This, combined with the metadata consisting of zeroes can instruct the client that caching metadata is possible, but won't bring anything: new result metadata will just be re-delivered on every call, since it's potentially going to be changing on every request.

I haven't updated spec though. I will, if/when we agree on the behaviour.;;;","13/Nov/17 20:09;omichallat;{{METADATA_CHANGED}} tells the client if it needs to update its local copy of the metadata. For conditional updates, the answer is always no (since the client should never store that information in the first place); that is why I think it's more intuitive to set the flag to false.

To put it another way: if the flag is forced to true, I have to add a condition in the client code ({{newMetadataId.bytes.length > 0}}). My worry is that a client implementation could forget to check that the id is empty, and end up with a sub-optimal behavior (that updates the local metadata unnecessarily each time).

If the flag is absent, conditional updates can be handled like any other statement.

;;;","14/Nov/17 03:51;KurtG;My understanding is that, at the moment, {{METADATA_CHANGED}} will _always_ be set for a conditional update, regardless of whether it's necessary or not. Necessary being defined as the schema has actually changed and the prepared statements need to be updated client side to reflect those schema changes. [~omichallat] is this true? what exactly is ""metadata"" referring to on the driver side, and why is the answer ""always no"" for conditional updates? If there is a change to one of the columns in the update is that going to cause problems if we don't tell the driver that it has changed?

I'm with Olivier that that's a hacky addition to the driver, but if it's not even necessary as per above then simply only passing an empty digest will be sufficient.

I've updated my [branch|https://github.com/apache/cassandra/compare/trunk...kgreav:13992] to reflect this. Note I've changed to using {{MD5Digest#compute}} to calculate an ""empty"" digest. Although it's thread local it will always be the same digest, and this will also solve the initial preparation problem, as it also uses the {{EMPTY}} resultset + metadata.



;;;","14/Nov/17 08:39;ifesdjeen;bq. METADATA_CHANGED tells the client if it needs to update its local copy of the metadata. 

You're right, great point. Sure, I've changed the patch to _always_ send metadata (by avoiding setting {{SKIP_METADATA}} for LWTs) and _never_ send metadata id when statement is LWT (by avoiding setting {{METADATA_CHANGED}} for LWTs)). Technically, {{EMPTY}} part isn't even necessary in that case, but we don't have to calculate it, so why not. 

[~KurtG] to give a bit of context, {{METADATA_CHANGED}} flag is instructing the driver to cache a newly received version of metadata (alongside with a new metadata ID). While {{SKIP_METADATA}} flag is hinting the driver to use already cached metadata from previous responses. If I understood it correctly, what [~omichallat] proposed here was to avoid setting {{METADATA_CHANGED}} flag, so driver wouldn't cache the metadata, _but_ still send a metadata all the time (since it's potentially changing on each request).

bq. I'm with Olivier that that's a hacky addition to the driver

There's no addition to the driver (also, was no addition in the previous version of the patch). The only difference is that we can spare the driver a couple of cycles. Behaviour was right in both cases.

I've pulled in the last version of the driver, added comments and prettified it a bit. If we all agree that this behaviour is correct, can anyone take a short look at it?

The patch can be found:

|[here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13992]|;;;","15/Nov/17 02:16;KurtG;I don't see why special casing LWT is necessary. It couples LWT, {{ModificationStatement}} and {{BatchStatement}} with {{ExecuteMessage}}, which seems a bit messy. It also introduces a weird edge case into the protocol - i.e, 
1. you can't skip metadata for LWT 
2. metadata will never be changed for LWT. 

2 is fine as it's somewhat the goal of the ticket, but when we already have a mechanism to meet this requirement it seems silly to wrap it in another. 1 just adds complexity to the protocol, which imo, is complex enough. It's not even clear in the protocol that SKIP_METADATA will be ignored if the metadata *did* change (we should update this in the patch). I think this can be better solved simply by providing a consistent MD5 for LWT's as my patch already does [here|https://github.com/apache/cassandra/compare/trunk...kgreav:13992]. This removes the need to introduce new imports, and special casing to {{ExecuteMessage}} while achieving the same behaviour, and I think logically is easier to explain.

AFAICT [~ifesdjeen]'s patch only really works because the driver is already set up for it to work. The _java_ driver already checks that {{new_metadata_id != null}} which is logical, but in no way required. It seems to me we're introducing a metadata_id that could potentially be null where a driver might expect it to have a value. I prefer my patch, or better yet, we create an {{EMPTY_DIGEST}} that's an actual MD5 to use so we don't have to rely on {{compute}} every time we want to use an {{EMPTY_DIGEST}};;;","15/Nov/17 06:33;omichallat;[~ifesdjeen] yes, your last patch is fine for me from a client's perspective (I can't really comment on the implementation since I'm not that familiar with the Cassandra codebase).

[~KurtG]:

bq. you can't skip metadata for LWT

Nor should you be able to. You can't safely skip metadata since these statements may return different columns depending on the state of the database. You _have_ to return the metadata every time.

bq. metadata will never be changed for LWT

We don't need it since every response includes the correct metadata.

bq. AFAICT Alex Petrov's patch only really works because the driver is already set up for it to work.

Actually no, it allows the driver to treat LWT as any other statement. Here's roughly what we do:
{code}
// When decoding a PREPARED response
if ! NO_METADATA // Note that LWT always have NO_METADATA
  store result metadata in prepared statement cache
end if

// When decoding a ROWS response:
if NO_METADATA
  look up result metadata in prepared statement cache
else
  decode result metadata from response
  if (METADATA_CHANGED)
    update result metadata in prepared statement cache
  end if
end if
{code}
So unsetting {{METADATA_CHANGED}} for LWT allows me to properly skip the last cache update. If we use a special value of {{new_metadata_id}} instead (like the empty array in the initial patch), then I have to change the last test to {{if (METADATA_CHANGED || new_metadata_id == special_value)}}.
HTH;;;","15/Nov/17 09:35;ifesdjeen;[~snazy] could you take a look at the patch as you're most familiar with the previous version and we've discussed the behaviour for LWTs multiple times?;;;","15/Nov/17 10:10;KurtG;bq. then I have to change the last test to if (METADATA_CHANGED || new_metadata_id == special_value)
In Alex's last version, yes, but if the metadata ID from the prepare and the exec are the same (because they are generated with the same value) this is not the case. You wouldn't have to worry about METADATA_CHANGED being set because a LWT will always have the same ID as the initial preparation. <bikeshedding>;;;","15/Nov/17 10:27;ifesdjeen;I thought we have discussed it and I've tried this idea out and it doesn't solve the problem with actually updating the metadata when performing {{ALTER}}.

This is a problem with many pitfalls, I've done a lot of testing both back when wrote an initial patch and now when we were collaborating on the follow-up and personally believe the proposed patch solves a problem in the best possible way. I'm happy to hear all the alternatives out as long as they're tested out and confirmed to work for all cases and keep consistency with previous versions.;;;","15/Nov/17 15:17;omichallat;[~KurtG]

bq. You wouldn't have to worry about METADATA_CHANGED being set because a LWT will always have the same ID as the initial preparation

But then I would have to compare the ids every time. That costs me a lookup in my client-side prepared statement cache (and there also happens to be an additional volatile read in our current implementation). In contrast, checking METADATA_CHANGED is free since it is already in the response.;;;","16/Nov/17 02:14;KurtG;bq. But then I would have to compare the ids every time. That costs me a lookup in my client-side prepared statement cache (and there also happens to be an additional volatile read in our current implementation). In contrast, checking METADATA_CHANGED is free since it is already in the response.
OK I get it.

bq. I thought we have discussed it and I've tried this idea out and it doesn't solve the problem with actually updating the metadata when performing ALTER.
Just trying to understand things here but from what I can see the patches both provide the same behaviour. Notably, METADATA_CHANGED will never be set for LWT but the metadata will still always be returned to the client. Anyway, you can do it that way if you want seeing as it helps on the java driver side. 

But now that I think about it more and after some testing I don't see how either case works completely for {{ALTER}}. When you say ALTER what are you referring to? AFAICT all you can do is add + drop columns that aren't used in the prepared statement, and atm both patches behave in the exact same way. If you alter any column used in the statement it invalidates the statement and you get an error. What aspects of ALTER are you expecting to work?;;;","16/Nov/17 13:56;snazy;As elaborated above, LWTs definitely need special handling as their result set is (or can be) different for each invocation. Remembering (and evicting) metadata for that result set (which is in the ""ok"" case just one column {{[applied]}} - i.e. not much) is probably not beneficiary. Doing that might be worth another look in a separate ticket at a later point. But I don't expect much from that kind of optimization.

I'm not excited about adding a ""special value"" to indicate that the metadata is empty (neither an empty {{byte[]}} nor the MD5 over an empty {{byte[]}}). Just omitting the metadata flags introduced by CASSANDRA-10786 is sufficient.

Olivier correctly pointed out that (unnecessary) additional processing should be avoided - and I second that. Looking at the {{METADATA_CHANGED}} flag is very cheap - comparing values is more expensive (checking a bit in a CPU register or L1 cache line vs. many dloads). Keeping the performance aspect aside, it also looks cleaner.

Since we do not need those metadata-flags for LWTs, we can just omit those and it ""magically"" works - and that's pretty much what [~ifesdjeen]'s patch does.

I've written a [unit test|https://github.com/snazy/cassandra/commit/fcb221af2dcc74c57e3017b73937365e2226b7d3#diff-d04861816aec1bdaa47b3d6819df1a46R277] that verifies the expected behavior on the protocol level.

+1 on [~ifesdjeen]'s patch. I'd like to see the new unit test being added.
Only change that would be good to have is to move the {{boolean hasConditions()}} function up to {{CQLStatement}} and implement it there as {{public default boolean hasConditions() \{ return false; \} }}. By that you can remove the {{instanceof}} and type casts in the change in {{ExecuteMessage}} and probably also save the {{isLWT}} variable as it would just be a call to {{statement.hasConditions()}}.;;;","21/Nov/17 12:17;ifesdjeen;Thank you for the review! 

Committed to trunk with [7eb915097dc3e34e1bb4ef96e6bd8eb67d574622|https://github.com/apache/cassandra/commit/7eb915097dc3e34e1bb4ef96e6bd8eb67d574622] with added unit test and {{hasConditions}} pulled up to statement.;;;","07/Dec/17 23:37;KurtG;Should we have updated the spec to indicate that those flags will not work with LWT? Seems like it could be quite surprising for new driver developers.;;;","08/Dec/17 12:10;ifesdjeen;We didn't do any client-side changes. As far as I can understand, metadata changes are server-driven. ;;;","08/Dec/17 19:03;omichallat;Yes, with the algorithm I described in [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13992?focusedCommentId=16253022&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16253022], LWTs are handled exactly the same way as regular statements, so I don't think we need to amend the spec.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix native protocol v5 spec for new_metadata_id position in Rows response,CASSANDRA-13986,13115488,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,omichallat,omichallat,01/Nov/17 22:24,16/Apr/19 09:29,14/Jul/23 05:56,08/Nov/17 12:15,,,,,,,,,,,,0,,,,,"There's a mistake in the protocol specification for CASSANDRA-10786. In `native_protocol_v5.spec`, section 4.2.5.2:

{code}
4.2.5.2. Rows

  Indicates a set of rows. The rest of the body of a Rows result is:
    <metadata><rows_count><rows_content>
  where:
    - <metadata> is composed of:
        <flags><columns_count>[<new_metadata_id>][<paging_state>][<global_table_spec>?<col_spec_1>...<col_spec_n>]
{code}
The last line should be:
{code}
        <flags><columns_count>[<paging_state>][<new_metadata_id>][<global_table_spec>?<col_spec_1>...<col_spec_n>]
{code}
That is, if there is both a paging state and a new metadata id, the paging state comes *first*, not second.",,ifesdjeen,omichallat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 08 12:14:45 UTC 2017,,,,,,,,,,"0|i3maef:",9223372036854775807,4.0,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"08/Nov/17 12:14;ifesdjeen;Ninja-committed to {{trunk}} with [65ff3e6d9e15060786fe5fdec92005b9932cab08|https://github.com/apache/cassandra/commit/65ff3e6d9e15060786fe5fdec92005b9932cab08];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a workaround for overly large read repair mutations,CASSANDRA-13975,13112044,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,25/Oct/17 18:09,20/May/21 20:50,14/Jul/23 05:56,13/Nov/17 13:21,3.0.16,3.11.2,,,,,Legacy/Coordination,,,,,0,,,,,"It's currently possible for {{DataResolver}} to accumulate more changes to read repair that would fit in a single serialized mutation. If that happens, the node receiving the mutation would fail, and the read would time out, and won't be able to proceed until the operator runs repair or manually drops the affected partitions.

Ideally we should either read repair iteratively, or at least split the resulting mutation into smaller chunks in the end. In the meantime, for 3.0.x, I suggest we add logging to catch this, and a -D flag to allow proceeding with the requests as is when the mutation is too large, without read repair.",,aleksey,estevezsebastian@gmail.com,jasonstack,jeromatron,jjirsa,paulo,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 20:50:01 UTC 2021,,,,,,,,,,"0|i3lp6f:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"02/Nov/17 15:44;aleksey;A straight-forward change pushed [here|https://github.com/iamaleksey/cassandra/commits/13975-3.0]. Unit test run [here|https://circleci.com/gh/iamaleksey/cassandra/63], dtest run [here|https://builds.apache.org/job/Cassandra-devbranch-dtest/407/].

;;;","09/Nov/17 11:46;samt;LGTM, modulo an exceeding minor nit about inconsistent whitespace at the top of {{sendRepairMutation}}, feel free to fix on commit.;;;","13/Nov/17 13:21;aleksey;Thanks, committed as [f1e850a492126572efc636a6838cff90333806b9|https://github.com/apache/cassandra/commit/f1e850a492126572efc636a6838cff90333806b9] to 3.0 and merged up with 3.11 and trunk.;;;","20/May/21 20:50;paulo;Is there a plan for making this the default behavior? I don't see why it shouldn't in a major version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad prefix matching when figuring out data directory for an sstable,CASSANDRA-13974,13111678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,24/Oct/17 12:36,15/May/20 08:38,14/Jul/23 05:56,29/Nov/19 09:17,3.0.20,3.11.6,4.0,4.0-alpha3,,,Legacy/Core,,,,,0,,,,,"We do a ""startsWith"" check when getting data directory for an sstable, we should match including File.separator",,cscotta,jeromatron,jjirsa,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 29 09:17:41 UTC 2019,,,,,,,,,,"0|i3lmxr:",9223372036854775807,,,,,,,jjirsa,,samt,,,Normal,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/eda5db28e38c8652014cadd15ee49a8b8faacf21,,,,,,,,,"new tests, circleci runs",,,,,"24/Oct/17 12:39;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13943
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/391/
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13943

(yes, branch name is bad, but it is the intended patch);;;","30/Nov/17 21:20;jjirsa;I'll take review on this, but it'll be a bit. If someone beats me to it, I won't mind ([~stefania_alborghetti] or [~bdeggleston] or [~pauloricardomg])
;;;","14/Dec/17 00:42;jjirsa;Code looks good to me, though it no longer applies to 3.11 (or trunk). Circle has some unrelated failures, and dtests have rolled over. I'm good with committing, but would be great if we could get a relatively clean dtest run just to be safe.
;;;","04/Sep/18 11:56;marcuse;seems I totally forgot about this, rebased branch: https://github.com/krummas/cassandra/commits/marcuse/13974-2

tests: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13974-2

this version also fixes an existing issue with {{FileUtils.isContained}} as it also did prefix matching without a separator;;;","04/Sep/18 12:00;marcuse;the isContained fix should probably be committed to 3.0 as well, adding 3.0 to fix versions;;;","18/Nov/18 04:15;cscotta;Related to ""CASSANDRA-14013: Data loss in snapshots keyspace after service restart"", in which a user reported data loss in a keyspace called ""snapshots"";;;","01/Oct/19 09:10;marcuse;[3.0|https://github.com/krummas/cassandra/commits/marcuse/13974-3.0], [tests|https://circleci.com/workflow-run/24d1823c-0fce-4fc8-83be-b0a9d9c72118] (only the FileUtils fix)
[3.11|https://github.com/krummas/cassandra/commits/marcuse/13974-3.11], [tests|https://circleci.com/workflow-run/2c2b5c49-82bd-4495-b9df-2b16efeeb3f8]
[trunk|https://github.com/krummas/cassandra/commits/marcuse/13974-trunk], [tests|https://circleci.com/workflow-run/843ec7b5-67ce-439a-b708-8dab9cfc3b9a];;;","02/Oct/19 12:42;samt;Although most of the changes to 3.11+ aren't applicable to 3.0, the change to {{Directories::getLocationForDisk}} is.

In {{FileUtils::isContained}}, have you considered using {{Path::startsWith}} rather than string wrangling?
 Comparing the strings is probably faster/more efficient, but using paths is a bit clearer. e.g.:

{code}
 Path folderPath = Paths.get(getCanonicalPath(folder));
 Path filePath = Paths.get(getCanonicalPath(file));
 return filePath.startsWith(folderPath);
 {code}

The difference isn't huge though, so I'll leave it to you to decide.;;;","02/Oct/19 13:19;marcuse;thanks, pushed fixes to the branches above, tests running here:

[3.0|https://circleci.com/workflow-run/b588038a-dd03-45bb-bec8-a4ab7801ca1e]
[3.11|https://circleci.com/workflow-run/dd658826-41c0-4d2e-b6c3-1cda1a935444]
[trunk|https://circleci.com/workflow-run/7f922026-26f7-447a-9adb-7f7cf91c1543];;;","02/Oct/19 13:39;marcuse;though, it seems using canonical paths doesn't work: CASSANDRA-5185 - I'll investigate;;;","30/Oct/19 14:51;marcuse;Pushed a fix to the 3.11, trunk branches above which keeps a mapping of canonical data path to data directory - we then use this map when figuring out the data directory for an sstable. The directory in the descriptor is always canonical.;;;","04/Nov/19 14:23;samt;LGTM. There's 1 nit which you could ignore or fix on commit : in the {{Directories}} constructor, the check for old format directories was inlined in the 3.11 branch but in the 4.0 version the {{olderDirectoryExists}} is still there.

There's also a couple of failing dtests against trunk, but I'm having trouble even running those 2 tests locally, if they look ok to you I'm +1. 
;;;","29/Nov/19 09:17;marcuse;thanks, committed - brought back the olderDirectoryExist boolean as it is slightly clearer

tests: [trunk|https://circleci.com/workflow-run/ab2ba85d-f56b-4ccb-859b-57526b6bc0f5] [3.11|https://circleci.com/workflow-run/c95beacd-55f7-49f8-bc69-ee9dc0d48ffb] [3.0|https://circleci.com/workflow-run/7c3ad8c2-88de-462a-be94-0064a6cf413b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SecondaryIndexManagerTest.assert[Not]MarkedAsBuilt produces flaky tests,CASSANDRA-13965,13110326,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,18/Oct/17 15:19,07/Mar/23 11:52,14/Jul/23 05:56,23/Nov/17 18:55,4.0,4.0-alpha1,,,,,Feature/2i Index,Legacy/Testing,,,,0,,,,,"The methods [{{SecondaryIndexManagerTest.assertMarkedAsBuilt}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L554-L557] and [{{SecondaryIndexManagerTest.assertNotMarkedAsBuilt}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L559-L562] produce occasional test failures. 

These methods assume that there aren't any other indexes in the {{system.IndexInfo}} table than those created by the calling test. However, it is possible to find indexes built for other tests (not only {{SecondaryIndexManagerTest}}) that rely on {{CQLTester.afterTest}} to cleanup to drop their created indexes, because this method is asynchronous. So, it is possible to reach the {{SecondaryIndexManagerTest.assert(Not)MarkedAsBuilt}} calls before the indexes created by the previous test have been cleaned up.",,adelapena,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14068,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 23 18:55:09 UTC 2017,,,,,,,,,,"0|i3lezz:",9223372036854775807,5.0,,,,,,snazy,,snazy,,,Low,,,,,,,,,,,,,,,,,,,"18/Oct/17 15:30;adelapena;[Here|https://github.com/apache/cassandra/compare/trunk...adelapena:13965-trunk] is a patch modifying the methods to only verify the presence/absence in {{system.IndexInfo}} table of the index of interest.;;;","22/Nov/17 08:46;snazy;+1

Thanks for the patch!;;;","23/Nov/17 18:55;adelapena;Committed to trunk as [2d2879db7a2fe8b0c25d08f67c81c88454e1527c|https://github.com/apache/cassandra/commit/2d2879db7a2fe8b0c25d08f67c81c88454e1527c].

Thanks for reviewing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tracing interferes with digest requests when using RandomPartitioner,CASSANDRA-13964,13110271,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,18/Oct/17 11:23,15/May/20 08:03,14/Jul/23 05:56,07/Nov/17 16:13,3.0.16,3.11.2,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,Legacy/Observability,,,,0,,,,,"A {{ThreadLocal<MessageDigest>}} is used to generate the MD5 digest when a replica serves a read command and the {{isDigestQuery}} flag is set. The same threadlocal is also used by {{RandomPartitioner}} to decorate partition keys. So in a cluster with RP, if tracing is enabled the data digest is corrupted by the partitioner making tokens for the tracing mutations. This causes a digest mismatch on the coordinator, triggering a full data read on every read where CL > 1 (or speculative execution/read repair kick in).
",,githubbot,jasobrown,jeromatron,rha,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 07 23:12:51 UTC 2017,,,,,,,,,,"0|i3lenr:",9223372036854775807,3.0.15,3.11.1,4.0,,,,jasobrown,,jasobrown,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"18/Oct/17 12:16;samt;Added a second {{ThreadLocal<MessageDigest>}} for exclusive use by {{RandomPartitioner}}. 
New dtest: https://github.com/beobal/cassandra-dtest/tree/13964

||branch||utest||dtest||
|[13964-3.0|https://github.com/beobal/cassandra/tree/13964-3.0]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.0]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/379/]|
|[13964-3.11|https://github.com/beobal/cassandra/tree/13964-3.11]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/380/]|
|[13964-trunk|https://github.com/beobal/cassandra/tree/13964-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-trunk]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/381/]|
;;;","18/Oct/17 13:39;jasobrown;I'm +1 on the patch and dtest. I ran the dtest w/o the patch can confirmed the failure, then with and confirmed success.

The only minor nit I have is on the dtest where you changed the {{prepare}} function. Maybe I'm missing something, but couldn't

{code}
        if not random_partitioner:
            cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
        else:
            if random_partitioner:
                cluster.set_partitioner(""org.apache.cassandra.dht.RandomPartitioner"")
            else:
                cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
{code}

be simplified as 

{code}
        if random_partitioner:
            cluster.set_partitioner(""org.apache.cassandra.dht.RandomPartitioner"")
        else:
            cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
{code}

wrt branches to commit on, for sure we need this on 3.0 and 3.11. We also need it on trunk, but should wait to see what happens with CASSANDRA-13291?;;;","31/Oct/17 19:35;samt;[~jasobrown] thanks for the review & sorry for the delay in getting back to this. 

You're totally right about the dtest, [so I've made it sane as you suggested|https://github.com/beobal/cassandra-dtest/commit/edc48bc965e842628413cfd50a7a21071d7b098a]. As the upstream branches have moved on in since you reviewed, I've rebased & force pushed to trigger CI to run again. The addition of CASSANDRA-13291 to trunk was no issue as both branches took the same approach. Unless you have any concerns, I'll commit once CI is done. Thanks.

||branch||utest||dtest||
|[13964-3.0|https://github.com/beobal/cassandra/tree/13964-3.0]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.0]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/401/]|
|[13964-3.11|https://github.com/beobal/cassandra/tree/13964-3.11]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/402/]|
|[13964-trunk|https://github.com/beobal/cassandra/tree/13964-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-trunk]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/403/]|;;;","31/Oct/17 20:53;jasobrown;lgtm, and thanks for confirming my python skills aren't complete garbage :D;;;","07/Nov/17 13:36;githubbot;GitHub user beobal opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/10

    Add test for digest requests with RandomPartitioner and tracing enabled

    Patch by Sam Tunnicliffe; reviewed by Jason Brown for CASSANDRA-13964
    
    @ptnapoleon: Jason already gave this the once over, but if you have chance I'd appreciate your +1 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/beobal/cassandra-dtest 13964

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/10.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #10
    
----
commit edc48bc965e842628413cfd50a7a21071d7b098a
Author: Sam Tunnicliffe <sam@beobal.com>
Date:   2017-10-17T13:50:25Z

    Add test for digest requests with RandomPartitioner and tracing enabled
    
    Patch by Sam Tunnicliffe; reviewed by Jason Brown for CASSANDRA-13964

----
;;;","07/Nov/17 16:13;samt;The CI was generally good barring a couple of flaky-ish tests which I've checked are passing locally, so I've committed to 3.0 in {{58daf1376456289f97f0ef0b0daf9e0d03ba6b81}} and merged to 3.11 and trunk.;;;","07/Nov/17 23:12;githubbot;Github user beobal closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/10
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SecondaryIndexManagerTest.indexWithfailedInitializationIsNotQueryableAfterPartialRebuild is flaky,CASSANDRA-13963,13110264,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,18/Oct/17 11:04,07/Mar/23 11:52,14/Jul/23 05:56,20/Nov/17 17:17,4.0,4.0-alpha1,,,,,Feature/2i Index,Legacy/Testing,,,,0,,,,,"The unit test [SecondaryIndexManagerTest.indexWithfailedInitializationIsNotQueryableAfterPartialRebuild|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L460-L476] is flaky. Apart from [the CI results showing a 3% flakiness|http://cassci.datastax.com/view/All_Jobs/job/trunk_utest/2430/testReport/org.apache.cassandra.index/SecondaryIndexManagerTest/indexWithfailedInitializationIsNotQueryableAfterPartialRebuild/], the test failure can be locally reproduced just running the test multiple times. In my case, it fails 2-5 times for each 1000 executions.",,adelapena,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 17:16:57 UTC 2017,,,,,,,,,,"0|i3lem7:",9223372036854775807,5.0,,,,,,snazy,,snazy,,,Low,,,,,,,,,,,,,,,,,,,"18/Oct/17 11:48;adelapena;[Here|https://github.com/apache/cassandra/compare/trunk...adelapena:13963-trunk] is a patch solving the problem.

It seems that the call to [{{TestingIndex.shouldFailCreate = false;}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L466] is done right after creating the index configured to fail, without waiting for the finalization of the build task. If we are not lucky the index initalization task can start after disabling the configured fail. In such case, the two calls to {{assertFalse(cfs.indexManager.isIndexQueryable(index))}} can either succeed because the index build task hasn't started yet (not because it has failed), or fail because the task has successfully finished without the configured initialization task failure.

The unit tests usually use [{{CQLTester.waitForIndex}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/CQLTester.java#L709-L737] method to wait for the finalization of index builds. In that case, since we are making the initialization to fail, we can't rely on this method, so the patch adds a new [{{CQLTester.waitForIndexBuilds}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/CQLTester.java#L709-L737] method to wait for the finalization of index build tasks independently of their results and the marking procedure. This method is in {{CQLTester}} instead of {{SecondaryIndexManagerTest}} because I think it's suitable to be used by other tests. ;;;","20/Nov/17 14:12;snazy;+1 on the patch

Your evaluation is correct and checking the number of builds is fine as the {{CREATE INDEX}} doesn't return before the index build is triggered.
Thanks for the patch!;;;","20/Nov/17 17:16;adelapena;Thank for the review :)

Committed to master as [5792b667ecf461a40cc391bc1496287547179c91|https://github.com/apache/cassandra/commit/5792b667ecf461a40cc391bc1496287547179c91];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add yaml flag for disabling MVs, log warnings on creation",CASSANDRA-13959,13109748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,16/Oct/17 17:19,15/May/20 08:04,14/Jul/23 05:56,26/Oct/17 21:07,3.0.16,3.11.2,4.0,4.0-alpha1,,,Feature/Materialized Views,,,,,0,,,,,"As discussed on dev@, we should give operators the option to disable materialized view creation, and log warnings when they're created.

Update - Adding link for posterity: https://lists.apache.org/thread.html/d81a61da48e1b872d7599df4edfa8e244d34cbd591a18539f724796f@%3Cdev.cassandra.apache.org%3E",,aleksey,bdeggleston,jasonstack,jeromatron,jjirsa,KurtG,rha,sbtourist,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 00:50:32 UTC 2017,,,,,,,,,,"0|i3lbg7:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"16/Oct/17 22:11;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/13959-3.0] | [utest|https://circleci.com/gh/bdeggleston/cassandra/138]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13959-3.11]|[utest|https://circleci.com/gh/bdeggleston/cassandra/139]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13959-trunk]|[utest|https://circleci.com/gh/bdeggleston/cassandra/137]|;;;","17/Oct/17 14:25;aleksey;Had a quick conversation about this with [~slebresne]. He raised a point that logging a warning every time we load an MV is unreasonable - and I agree that it is. Can we only keep the warning on creation?

Could add a small dtest (or utest if you so prefer) to show that:
1. Setting the flag to {{false}} really disables creation, and
2. That a client warning is emitted

Aside from this, looks good to me. Maybe give it a week before committing (till next Tue/Wed), to let people with binding -1s have a chance to change their mind since dev@ discussion though.;;;","17/Oct/17 19:50;bdeggleston;removed the warning on schema load and added dtests [here|https://github.com/bdeggleston/cassandra-dtest/tree/13959].

Without thinking about it, I squashed the review fixes. Sorry that makes verifying the changes a bit harder.;;;","19/Oct/17 14:00;aleksey;I would add a link to ML archives for that discussion in NEWS.txt (can be done on commit), but +1 (still, give it time till next Tue/Wed though).;;;","19/Oct/17 21:43;KurtG;Interesting. At least you took the advice to keep it enabled in a patch release. I'm still a bit confused at how you can make claims it's experimental without providing any evidence other then some developers having some ""feelings"". Especially when it's quite doubtful that any of said developers 1. run 3.x, and 2. use MV's. 

I'm really still not sure what benefit you see in saying it's experimental rather than just fixing whatever these problems are that you can't actually describe.

bq. I would add a link to ML archives for that discussion in NEWS.txt
Anyone who reads that discussion isn't going to be convinced some well educated decision was made. They are just going to see a thread which contained a lot of disagreement that went nowhere. 

;;;","20/Oct/17 00:33;jjirsa;{quote}
I'm really still not sure what benefit you see in saying it's experimental rather than just fixing whatever these problems are that you can't actually describe.
{quote}

and:

{quote}
Anyone who reads that discussion isn't going to be convinced some well educated decision was made. 
{quote}

Both addressed by [~benedict] in one paragraph:
 [here|https://lists.apache.org/thread.html/de6b92f62eb93e6f424f6a846177f31980e2f1f8ac5e7bde29550a4f@%3Cdev.cassandra.apache.org%3E]:

{quote}
MVs are by far and away the most complicated feature we have ever delivered. We do not fully understand it, even in theory, let alone can we be sure we have the implementation right.
{quote}

The only way to know all of the problems is to formally model it, and then verify the implementation matches the model. That hasn't happened.

The next best way (which is still insufficient) is to run it at scale and actually hunt for flaws. That hasn't happened. I know it hasn't happened, because if that sort of testing did happen, whoever did it would have necessarily stumbled across all sorts bugs like ( CASSANDRA-13595 , CASSANDRA-13911, CASSANDRA-13880, CASSANDRA-12872 , CASSANDRA-13747 ). 

I *strongly encourage* you to spend time and effort to find and fix the bugs you know about, but until someone can provide some level of confidence that it's safe, we shouldn't let users ASSUME it's safe.

;;;","26/Oct/17 21:07;bdeggleston;committed as {{b8697441d7a051e7ff68def6aa9cf14bd92ace9e}};;;","01/Nov/17 13:17;JoshuaMcKenzie;bq. The only way to know all of the problems is to formally model it, and then verify the implementation matches the model. That hasn't happened.
At the risk of beating a dead horse - by this measure, most of the features in C* should be flagged experimental. I'm all for us having more rigor in terms of proving the correctness of behaviors in a distributed system, but this looks an awful lot like arbitrarily applying that standard to this feature and not to others.;;;","01/Nov/17 13:21;jjirsa;You're cherry-picking sentences [~JoshuaMcKenzie] . That statement is true, but the previous line (quoted from Benedict) and the following paragraph is why it needs to be marked experimental.




;;;","01/Nov/17 13:26;JoshuaMcKenzie;bq. MVs are by far and away the most complicated feature we have ever delivered. We do not fully understand it, even in theory, let alone can we be sure we have the implementation right.
Both claims from Benedict on that email thread are 'citation needed', and I believe what Kurt refers to when he's talking about developer's 'feelings'. To be very clear, I'm not even saying I disagree with Benedict on his *intuition* here, just that we have no precedent for formal analysis whatsoever on things like this and marking a feature experimental that's in use by people in production systems smells knee-jerk. Again, this is largely beating a dead horse from what was on the mailing list so I'm not looking to re-litigate here.

As for the latter paragraph, making the claim that you know nobody is running MV's at scale in the wild is a pretty bold claim, seeing as how we don't have concrete data on the full adoption of the project in the wild as a whole, much less the scale of usage of individual features. So no, my intention wasn't to just cherry-pick sentences, but instead to point out that we're making broadly user-impacting decisions that are not backed by data.;;;","01/Nov/17 13:34;jjirsa;I didn't say nobody was running MVs at scale - I said nobody was doing it AND looking for errors. I'm confident in that assertion because had they done so, they would have found the 5 8099 correctness-impacting bugs I linked, which you would certainly hit as you did range through a table (or view) to check the data matched. Those bugs and ~8 years of intuition are all I need to vote in favor of this.

If you're willing to assert that there exists a competent team running MVs in multiple DCs with multiple racks, writing data into the table with nontrivial volumes and non-trivial patterns (new rows, overwrites, TTLs, range deletes, partition deletes), adding/removing nodes, growing the cluster, shrinking the cluster, running full repair, running incremental repair, killing disks, replacing instances, and then verifying that EVERY SINGLE WRITE *and* EVERY SINGLE DELETE is present with the expected consistency guarantees, by all means, say so and I'll vote to remove the flag in 4.0. 

I'm sure people are running MVs at scale, I just don't believe that they're actively hunting for bugs or trying to prove correctness. But you've got a binding -1 now, if you really believe this is hurting users, you know what to do.
;;;","01/Nov/17 14:41;bdeggleston;bq. this looks an awful lot like arbitrarily applying that standard to this feature and not to others

Do we have other features that, by design, can get themselves into an inconsistent state and can’t be fixed without downtime?;;;","01/Nov/17 14:46;aleksey;Are you guys all bored or something?;;;","02/Nov/17 00:50;KurtG;It seems fair to me that all the people who are adamant about marking MV's experimental should be pushing to fix them by 4.0, instead of trying to absolve themselves of the responsibility by throwing existing users of MV's under the bus. Especially considering these are the people who let MV through in the first place.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException while executing query,CASSANDRA-13949,13108547,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,lrodriguez2002cu,lrodriguez2002cu,11/Oct/17 10:03,15/May/20 08:03,14/Jul/23 05:56,17/Oct/17 13:16,3.11.2,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"While executing a query on a  table contaninig a field with a (escaped) json, the following exception occurs:

java.lang.ArrayIndexOutOfBoundsException: null
        at org.codehaus.jackson.io.JsonStringEncoder.quoteAsString(JsonStringEncoder.java:141) ~[jackson-core-asl-1.9.2.jar:1.9.2]
        at org.apache.cassandra.cql3.Json.quoteAsJsonString(Json.java:45) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.db.marshal.UTF8Type.toJSONString(UTF8Type.java:66) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection.rowToJson(Selection.java:291) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:431) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:417) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]

Find attached the schema of the table, the insertion query with the data provoking the failure, and the failing query.
 ",Setup of 3 servers y using docker image [https://github.com/docker-library/cassandra/blob/ca3c9df03cab318d34377bba0610c741253b0466/3.11/Dockerfile],jasobrown,jjirsa,lrodriguez2002cu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 13:01;jasobrown;13949.png;https://issues.apache.org/jira/secure/attachment/12891472/13949.png","11/Oct/17 09:56;lrodriguez2002cu;insert.cql;https://issues.apache.org/jira/secure/attachment/12891451/insert.cql","11/Oct/17 09:59;lrodriguez2002cu;query.cql;https://issues.apache.org/jira/secure/attachment/12891450/query.cql","11/Oct/17 10:02;lrodriguez2002cu;schema.cql;https://issues.apache.org/jira/secure/attachment/12891449/schema.cql",,,,,,,,,,,,,,,,4.0,jasobrown,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 13:16:41 UTC 2017,,,,,,,,,,"0|i3l4jr:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"11/Oct/17 13:07;jasobrown;I tried to reproduce using your attached scripts (very handy, thanks!) on a local running instance of casandra, but was unable to reproduce the {{NullPointerException}}. I did however, get a very curious output in the {{value}} field: at four point in the large json blob, there are a few Kb of printed null characters {{\x00}}. See the attached image. I hexdump'ed your insert.cql and it looked legit (no stray characters).

I then checked our json libs, and they are quite old - jackson*-1.9.2, committed in 2011! The latest jackson jars are version 1.9.13 on maven central (from 2013). I naively replaced the jars in lib/, and I didn't get the null character output anymore.

[~lrodriguez2002cu] Can you try simply replacing the jars and see if the NPE stops occurring? If it does, then I'll create a more formal patch, run tests, and so on.;;;","11/Oct/17 15:09;lrodriguez2002cu;Yes [~jasobrown], I will try to replace the jars. I did note those characters because at some point it gave a different error:

""llegal unquoted character ((CTRL-CHAR, code 0)): has to be escaped using backslash to be included in string value at [Source: java.io.StringReader@2de1af30; line: 1, column: 1228]"" but as you said there is not evidence that those are inserted.

I didn't try to replace them because I made a simple project with that version (1.9.2) of jackson, trying the encode function failing in this case, but the problem didn't occurr.  This is  the repository in case you or anyone want check it [https://github.com/lrodriguez2002cu/cassandra-issue-tests].

What I will try to setup the an environment  with a docker image and map the libs to a volume so  that the libraries can be replaced easily, and see if this fixes  the problem and maybe see possible impacts  in other parts. ;;;","13/Oct/17 14:03;lrodriguez2002cu;Hi [~jasobrown] I have tested as promised an image with the libraries replaced using a newer (still old 1.9.13) version. You mentioned you did not get the error, It happens if you run again the query requesting the json. With the new libraries version, the issue apparently gets solved, in this repository [https://github.com/lrodriguez2002cu/cassandra-issue-images] I have created docker images with the cql files copied inside and the commands for initializing the database and so on, so that you can see the behavior. 

Thanks for the follow up.

;;;","13/Oct/17 20:28;jasobrown;bq. still old 1.9.13

According to maven central, [1.9.13 is the most current version|http://search.maven.org/#search%7Cga%7C1%7Corg.codehaus.jackson] of jackson.

bq. It happens if you run again the query requesting the json 

I did run it a bunch of times, but if the updated jackson is working for you, let's just move ahead on that. Patch coming shortly.;;;","13/Oct/17 21:08;jasobrown;I've created a simple patch which just updates the jackson jars:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13949-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13949-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/370/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/371/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13949-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13949-trunk]|
;;;","14/Oct/17 11:03;lrodriguez2002cu;Thank you very much!;;;","16/Oct/17 20:24;jjirsa;3.11 patch looks good to me
4.0 patch is obviously the same, though the dtests look pretty messy. Can you run that again.

+1 if / once that trunk dtest comes back clean.

;;;","17/Oct/17 13:07;jasobrown;I ran the [dtests again|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/374] for trunk, and the results are still a bitt messy. I compared with the trunk dtests on apache jenkins and cassci, and they are also having problems on the paging_tests and write_failure_tests. Thus I think this patch is probably safe to apply. Committing shortly.;;;","17/Oct/17 13:16;jasobrown;committed as sha {{bbda20155ae0f3443cbb5fee0659234e81b2e914}}. 

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reload compaction strategies when JBOD disk boundary changes,CASSANDRA-13948,13108507,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,11/Oct/17 06:51,15/May/20 08:03,14/Jul/23 05:56,08/Dec/17 18:51,3.11.2,4.0,4.0-alpha1,,,,Local/Compaction,,,,,1,jbod-aware-compaction,,,,"The thread dump below shows a race between an sstable replacement by the {{IndexSummaryRedistribution}} and {{AbstractCompactionTask.getNextBackgroundTask}}:

{noformat}
Thread 94580: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.util.concurrent.locks.AbstractQueuedSynchronizer$Node, int) @bci=67, line=870 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(int) @bci=17, line=1199 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock() @bci=5, line=943 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleListChangedNotification(java.lang.Iterable, java.lang.Iterable) @bci=359, line=483 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(org.apache.cassandra.notifications.INotification, java.lang.Object) @bci=53, line=555 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.Tracker.notifySSTablesChanged(java.util.Collection, java.util.Collection, org.apache.cassandra.db.compaction.OperationType, java.lang.Throwable) @bci=50, line=409 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.LifecycleTransaction.doCommit(java.lang.Throwable) @bci=157, line=227 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit(java.lang.Throwable) @bci=61, line=116 (Compiled frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit() @bci=2, line=200 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish() @bci=5, line=185 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryRedistribution.redistributeSummaries() @bci=559, line=130 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager.runIndexSummaryRedistribution(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=9, line=1420 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=4, line=250 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries() @bci=30, line=228 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow() @bci=4, line=125 (Interpreted frame)
 - org.apache.cassandra.utils.WrappedRunnable.run() @bci=1, line=28 (Interpreted frame)
 - org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run() @bci=4, line=118 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

{noformat}
Thread 94573: (state = IN_JAVA)
 - java.util.HashMap$HashIterator.nextNode() @bci=95, line=1441 (Compiled frame; information may be imprecise)
 - java.util.HashMap$KeyIterator.next() @bci=1, line=1461 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(org.apache.cassandra.db.lifecycle.View) @bci=20, line=268 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(java.lang.Object) @bci=5, line=265 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.apply(com.google.common.base.Predicate, com.google.common.base.Function) @bci=13, line=133 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.tryModify(java.lang.Iterable, org.apache.cassandra.db.compaction.OperationType) @bci=31, line=99 (Compiled frame)
 - org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(int) @bci=84, line=139 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(int) @bci=105, line=119 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run() @bci=84, line=265 (Interpreted frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

This particular node remain in this state forever, indicating {{LeveledCompactionStrategyTask.getNextBackgroundTask}} was looping indefinitely.

What happened is that sstable references were replaced on the tracker by the {{IndexSummaryRedistribution}} thread, so the {{AbstractCompactionStrategy.getNextBackgroundTask}} could not create the transaction with the old references, and the {{IndexSummaryRedistribution}} could not update the sstable reference in the compaction strategy because {{AbstractCompactionStrategy.getNextBackgroundTask}} was holding the {{CompactionStrategyManager}} lock.",,bradfordcp,dkinder,jeromatron,jjordan,llambiel,marcuse,pauloricardomg,sbtourist,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14082,,,,CASSANDRA-13980,,,,,,CASSANDRA-14103,,,CASSANDRA-10099,CASSANDRA-13943,CASSANDRA-14083,,,,,,,,"01/Dec/17 13:49;pauloricardomg;13948dtest.png;https://issues.apache.org/jira/secure/attachment/12900222/13948dtest.png","01/Dec/17 13:49;pauloricardomg;13948testall.png;https://issues.apache.org/jira/secure/attachment/12900221/13948testall.png","06/Dec/17 13:38;pauloricardomg;3.11-13948-dtest.png;https://issues.apache.org/jira/secure/attachment/12900865/3.11-13948-dtest.png","06/Dec/17 13:38;pauloricardomg;3.11-13948-testall.png;https://issues.apache.org/jira/secure/attachment/12900864/3.11-13948-testall.png","12/Oct/17 22:17;dkinder;debug.log;https://issues.apache.org/jira/secure/attachment/12891830/debug.log","13/Nov/17 08:20;pauloricardomg;dtest13948.png;https://issues.apache.org/jira/secure/attachment/12897298/dtest13948.png","30/Nov/17 00:47;pauloricardomg;dtest2.png;https://issues.apache.org/jira/secure/attachment/12899922/dtest2.png","08/Nov/17 17:04;llambiel;threaddump-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12896686/threaddump-cleanup.txt","03/Nov/17 21:09;llambiel;threaddump.txt;https://issues.apache.org/jira/secure/attachment/12895966/threaddump.txt","03/Nov/17 08:12;llambiel;trace.log;https://issues.apache.org/jira/secure/attachment/12895588/trace.log","06/Dec/17 13:38;pauloricardomg;trunk-13948-dtest.png;https://issues.apache.org/jira/secure/attachment/12900863/trunk-13948-dtest.png","06/Dec/17 13:38;pauloricardomg;trunk-13948-testall.png;https://issues.apache.org/jira/secure/attachment/12900862/trunk-13948-testall.png",,,,,,,,12.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 20:23:18 UTC 2017,,,,,,,,,,"0|i3l4b3:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"11/Oct/17 07:13;pauloricardomg;I think we can get rid of the {{while (true)}} loops on {{*CompactionStrategy.getNextBackgroundTask}} when not able to lock sstables for compaction, and just submit a new background task when receiving any notifications from the tracker to ensure a new compaction will operate on the most updated references.

I also removed the {{CompactionStrategyManager.replaceFlushed}} method because it should no longer be necessary because a new background compaction candidate will be submitted when receiving an {{SSTableAddedNotification}} from the tracker. 

Similarly we no longer need to call {{CompactionManager.instance.submitBackground}} after a {{BackgroundCompactionCandidate}} because it will already be called after receiving an {{SSTableListChangedNotification}}, so we centralize calls to {{CompactionManager.instance.submitBackground}} on {{CompactionStrategyManager.handleNotification}}.

I added a unit test to check that {{*CompactionStrategy.getNextBackgroundTask}} never blocks indefinitely, even when not able to lock sstables in the tracker.

A race there should be pretty unlikely, but in case it happens I logged a warning to detect potential problems if it happens frequently due to some wrong condition: {{""Could not acquire references for compacting SSTables {} which is not a problem per se, unless it happens frequently, in which case it must be reported. Will retry later.""}}.

Patch available [here|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]

Mind having a look [~krummas]?

I submitted internal CI, will post the results here once available.;;;","11/Oct/17 07:17;marcuse;sure, I'll review;;;","12/Oct/17 22:13;dkinder;Just a heads up, I have been seeing these deadlocks happen easily, so I am running your patch [~pauloricardomg] in addition to Marcus's[patch|https://github.com/krummas/cassandra/commits/marcuse/13215] from CASSANDRA-13215.

I do see a large number of ""Could not acquire references for compacting SSTable"" happening, in bursts. Will upload a log file.

I also see some of this:
{noformat}
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:344) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:291) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:148) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.fillTemporaryKey(IndexSummary.java:162) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:1370) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.estimatedKeysForRanges(SSTableReader.java:1326) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:441) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:503) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:124) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:262) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_144]
{noformat}

UPDATE: the node does successfully complete compactions for a while but gradually does fewer and fewer and then stops compacting altogether, even though compactionstats says there are pending compactions:

{noformat}
dkinder@seu-walker-fs01:~$ nodetool compactionstats
pending tasks: 102
- walker.links: 102

dkinder@seu-walker-fs01:~$
{noformat}
It stays this way and periodically prints out some ""Could not acquire references for compacting SSTable"" messages. It is able to compact some more if I restart the node.

tablestats for walker.links:
{noformat}
Keyspace : walker
        Read Count: 16952
        Read Latency: 10.388668062765454 ms.
        Write Count: 277291
        Write Latency: 0.0186555207345352 ms.
        Pending Flushes: 0
                Table: links
                SSTable count: 8507
                SSTables in each level: [73/4, 81/10, 297/100, 2078/1000, 2402, 3635, 0, 0, 0]
                Space used (live): 4902698702556
                Space used (total): 4902698702556
                Space used by snapshots (total): 13788057680993
                Off heap memory used (total): 9835235
                SSTable Compression Ratio: -1.0
                Number of partitions (estimate): 19996043
                Memtable cell count: 360248
                Memtable data size: 36729792
                Memtable off heap memory used: 0
                Memtable switch count: 0
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 360248
                Local write latency: 0.017 ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 8538560
                Bloom filter off heap memory used: 8470504
                Index summary off heap memory used: 1364731
                Compression metadata off heap memory used: 0
                Compacted partition minimum bytes: 43
                Compacted partition maximum bytes: 190420296972
                Compacted partition mean bytes: 247808
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 2
{noformat};;;","17/Oct/17 09:27;marcuse;[~pauloricardomg] did you get those CI results?;;;","19/Oct/17 12:29;pauloricardomg;I think I was able to get to the bottom of this issue with the help of [~dkinder]'s logs

It turns out that the {{ColumnFamilyStore}} (and subsequently the {{CompactionStrategyManager}}) is initialized before gossip is settled, so the node's local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries. 

This happens because {{GossipingPropertyFileSnitch}} fallbacks to the {{FilePropertySnitch}}, so when a node is not found is gossip it will pick the DCs/racks from the {{cassandra-topology.properties}} file, and if it's not defined there it will use the {{DEFAULT}} dc/rack and mess up the local ranges and disk boundary computation.

The log lines below show the following steps:
* {{GossipingPropertyFileSnitch}} falling back to {{PropertyFileSnitch}}
* Compactions on system keyspaces being scheduled on the same disk, while compaction on user keyspaces being run on SSTables from different disks, indicating the disk boundaries were not calculated correctly for NTS keyspaces
* After gossip settles, lot's of {{SSTable from level 0 is not on corresponding level in the leveled manifest}} warnings, indicating the disk boundary layout changed after gossip settled but the compaction strategies were not reloaded with the new layout

{code:none}
INFO  [main] 2017-10-12 14:39:32,928 GossipingPropertyFileSnitch.java:64 - Loaded cassandra-topology.properties for compatibility
DEBUG [CompactionExecutor:26] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea1-af98-11e7-b5a1-57bcefdac924) [/srv/disk6/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18386-big-Data.db:level=0, /srv/disk9/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18387-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:31] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea0-af98-11e7-b5a1-57bcefdac924) [/srv/disk10/cassandra-data/system/peers/mc-1671-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1659-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1656-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1690-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:17] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea8-af98-11e7-b5a1-57bcefdac924) [/srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2625-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2605-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2597-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2617-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:29] 2017-10-12 15:02:09,952 CompactionTask.java:155 - Compacting (fe9a8a00-af98-11e7-b5a1-57bcefdac924) [/srv/disk11/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18474-big-Data.db:level=0, /srv/disk3/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18473-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,866 DiskBoundaryManager.java:69 - Cached ring version 84 != current ring version 86
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,867 DiskBoundaryManager.java:83 - Refreshing disk boundary cache for walker.links
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,888 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path='/srv/disk2/cassandra-data/walke
r/links/mc-6566879-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,892 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path='/srv/disk11/cassandra-data/walk
er/links/mc-6566381-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,939 CompactionTask.java:255 - Compacted (2f095f90-af99-11e7-b5a1-57bcefdac924) 1 sstables to [/srv/disk1/cassandra-data/walker/links/mc-6571348-big
,] to level=4.  271.077MiB to 271.077MiB (~100% of original) in 37,729ms.  Read Throughput = 7.185MiB/s, Write Throughput = 7.185MiB/s, Row Throughput = ~116,380/s.  741 total partitions merged to 
741.  Partition merge counts were {1:741, }
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,941 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
{code}

From the above I draw the following conclusions:
1. We should wait for gossip to settle before starting compactions, since the disk boundaries may be dependent on gossip when any gossiping snitch is used. (done on [this commit|https://github.com/pauloricardomg/cassandra/commit/d5bd9eab53ab423dde024400c299f01e335cdf4c])
2. We should reload the compaction strategies when the disk layout changes, so SSTables are correctly mapped to their corresponding compaction strategies. (done on [this commit|https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21], in addition to testing and some other simplifications on {{CompactionStrategyManager}})
3. A race when acquiring references on {{CompactionStrategyManager.getNextBackgroundTask}} is quite common when there are multiple concurrent compactions, since there will be parallel flushes on multiple disks, so rather than removing the {{while true}} loop to acquire references, I updated the compaction strategies to stop when the current candidate is the same as before, indicating there is a race with the tracker ([commit|https://github.com/pauloricardomg/cassandra/commit/3f971daa48036f14ea7aba7fd1d56150e78415b3]).

I will work on dtests to simulate the above case as well as test disk boundary refreshing, but this [branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] should be ready for a first round of review (cc [~krummas]). I will submit a round of unit and dtests on internal CI and will post the results here when ready.
;;;","19/Oct/17 12:46;marcuse;have not had a close look yet, but did you see CASSANDRA-13215 ?;;;","19/Oct/17 12:49;marcuse;bq. It turns out that the ColumnFamilyStore (and subsequently the CompactionStrategyManager) is initialized before gossip is settled, so the node's local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries.
we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can't delay that until gossip has settled (commitlog replay might have to flush);;;","19/Oct/17 15:54;pauloricardomg;bq. we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can't delay that until gossip has settled (commitlog replay might have to flush)

The reason why the ring boundaries were not computed correctly on startup was because the {{GossipingPropertyFileSnitch}} did not have rack/dc info about all nodes on gossip so it fallback to the sample {{conf/cassandra-rackdc.properties}} file from {{PropertyFileSnitch}} so I created CASSANDRA-13970 to fix that.

In any case, even with that fixed, the {{CompactionStrategyManager}} (CSM) does not reload its compaction strategies when the disk boundaries are updated (either because of range movements, when the node first joins the ring, or a disk breaks) what can cause {{CompactionStrategyManager.getCompactionStrategyIndex}} to return a different sstable->disk assignment to the one currently on the CSM, so some SStables may not be correctly updated in the compaction strategies after being updated/replaced/removed.  Perhaps this is better illustrated by [this test|https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21#diff-f9c882c974db60a710cf1f195cfdb801R95].

So it shouldn't be a problem if on a race with gossip flush writes an SSTable to a wrong disk, as long as after the boundary is updated, the SSTable is placed on the correct compaction strategy and its compaction output will be placed in the correct disk (should probably add a test with this scenario).

Also this is a bit orthogonal to CASSANDRA-13215 - the objective there is to cache the disk boundary computation, while here is to make sure CSM can gracefully handle boundary changes.;;;","19/Oct/17 17:30;marcuse;bq. Also this is a bit orthogonal to CASSANDRA-13215
I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)

bq. So it shouldn't be a problem if on a race with gossip flush writes an SSTable to a wrong disk,
The flushed sstable will have wrong boundaries, it will have tokens that shouldn't be on that disk, does not really matter if the first token is on the correct disk - more tokens might be on the correct disk than not - this is why 6696 didn't reload compaction strategies after boundary changes. I didn't really consider the case from these logs though, where we are completely wrong at startup, but then go back to the correct pre-restart state, then it totally makes sense to reload them, and I agree, we should always do it for consistency.

Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here;;;","01/Nov/17 04:21;pauloricardomg;bq. I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)

Even with that we will probably need to cache the current boundaries on the CSM, to prevent a race where the disk boundaries change in the boundary manager and a flush puts an SSTable in the wrong strategy (due to the index having changed) before the CSM is notified about the boundary change - quite unlikely but still possible.

bq. Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here

Actually the issue in the original ticket description only surfaced because the compaction strategies were not properly reloaded after the disk boundary changes, which is the core issue to solve here, so I will update the ticket description to better reflect that.

I added two new dtests to check that the compaction strategies are being properly reloaded when the disk boundary changes due to bootstrap, decommission and delayed join ([here|https://github.com/pauloricardomg/cassandra-dtest/commit/d53f73419e68eb6925b5baf06824b80d0ccf30b7]) and was able to reproduce the deadlock in current 3.11/trunk.

While debugging these dtests I found that reloading the compaction strategy manager when receiving a notification from the tracker can cause an SSTable to be added twice to the {{LeveledManifest}} (first during re-initialization of the CS and second when processing the SSTableAddedNotification), so I stopped reloading the CSM when receiving a notification from the tracker and added a warning on {{LeveledManifest}} when trying to add an SSTable which is already present ([here|https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0]) - this shouldn't be a problem since we maybeReload the CSM before submitting a new background tasks.

In summary this patch makes the following changes:
1) Reload compaction strategies when JBOD disk boundary changes ([commit|https://github.com/pauloricardomg/cassandra/commit/efb2afb22792a06d83020ac7097154593b9e684d])
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock ([commit|https://github.com/pauloricardomg/cassandra/commit/9fdb8f0fb40954a8ed9570cb568a7084de4c80c5])
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work ([commit|https://github.com/pauloricardomg/cassandra/commit/c524ff724f2ca9e7eed59cb07f81b9211098fb5c])
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS ([commit|https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0])

The CI of the previous version of this patch was successful, so this is ready for review. I submitted another round on 3.11 and trunk with the latest version and will update the results here when ready.

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [trunk patch|https://github.com/pauloricardomg/cassandra/tree/trunk-13948]

Please let me know what do you think.;;;","01/Nov/17 17:02;llambiel;I tried your patch on 3.11.2 and got the following errors:


{code:java}
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,397 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@51f52b91) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@1358582595:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@70a08046) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@632323950:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5d6161ea) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1594052942:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2bd55858) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@230164803:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5b00472f) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@508355616:Memory@[7f6b54130b10..7f6b54136f10) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f5a7829) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1390774416:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@b8594dc) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1913719912:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3ec6a933) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1083770739:Memory@[7f6b5453ff30..7f6b54546330) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@671d48c8) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@496335375:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1611d7bf) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@515635345:Memory@[7f6b540f7dc0..7f6b540fe1c0) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@136db886) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@622788070:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3daa7ad5) to class org.apache.cassandra.io.util.FileHandle$Cleanup@2090103425:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2435c1d) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1348493438:Memory@[7f6b546ebd80..7f6b546f2180) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3e72d475) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1638775104:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,431 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7916a2dc) to class org.apache.cassandra.io.util.FileHandle$Cleanup@715546128:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,446 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@25fe64d) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@238299391:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Data.db was not released before the reference was garbage collected
ERROR [CompactionExecutor:71] 2017-11-01 17:51:36,872 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:71,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.<init>(CompressionMetadata.java:474) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:239) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:163) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:362) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:290) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:179) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:134) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter.realAppend(MaxSSTableSizeWriter.java:98) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:141) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$t
{code}

Since we are heavily impacted by this bug (see also CASSANDRA-13980), I'm ok to test as soon as you've an updated version of the patch.
;;;","01/Nov/17 17:34;marcuse;[~llambiel] is that with the patch posted today (or yesterday depending on your tz)?
;;;","01/Nov/17 17:42;llambiel;Yes [~krummas], build including the latest patch.;;;","01/Nov/17 22:28;pauloricardomg;bq. I tried your patch on 3.11.2 and got the following errors:

Thanks for reporting, can you attach the full debug.log leading to that? What is the compaction strategy of the block table? Was this a one-off error or is it repeating periodically?;;;","01/Nov/17 22:53;llambiel;The strategy is LCS, node with 9 data locations. The error is repeating frequently. I'll attach a debug log tomorrow and make some additional tests.;;;","01/Nov/17 23:08;pauloricardomg;bq.  I'll attach a debug log tomorrow and make some additional tests.

Thanks, since the exception seems related to early re-opening, it would be nice if during your tests you could try setting {{sstable_preemptive_open_interval_in_mb=-1}} on {{cassandra.yaml}} to check if the errors will go away (and return once you re-enable it).;;;","02/Nov/17 20:17;llambiel;Did additional testing and wasn't able to reproduce :-/ 

I'll try the patch on more representative nodes in the coming days and report back any issue.;;;","03/Nov/17 04:12;pauloricardomg;bq. Did additional testing and wasn't able to reproduce :-/ 

this looks similar to CASSANDRA-12743, so I wonder if it's an existing race that showed up due to the large compaction backlog after the deadlock was fixed.

bq. I'll try the patch on more representative nodes in the coming days and report back any issue.

sounds good, if you manage to reproduce it would be nice if you could change the log level of the {{org.apache.cassandra.db.compaction}} package to {{TRACE}}.;;;","03/Nov/17 08:15;llambiel;Ok I was able to reproduce it on one node. I've attached the trace log. It's unfiltered since I didn't managed to filter only to org.apache.cassandra.db.compaction



;;;","03/Nov/17 21:08;llambiel;I've deployed the patch on a few big nodes. I've not seen the error popping up so far.

However I'm still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn't show any error.

{code:java}
CREATE TABLE blobstore.block (
    inode uuid,
    version timeuuid,
    block bigint,
    offset bigint,
    chunksize int,
    payload blob,
    PRIMARY KEY ((inode, version, block), offset)
) WITH CLUSTERING ORDER BY (offset ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy', 'enabled': 'true', 'tombstone_compaction_interval': '60', 'tombstone_threshold': '0.2', 'unchecked_tombstone_compaction': 'false'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 172000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}


{code:java}
Keyspace : blobstore
	Read Count: 97019
	Read Latency: 2.4842547026871027 ms.
	Write Count: 472590
	Write Latency: 0.060107954040500226 ms.
	Pending Flushes: 0
		Table: block
		SSTable count: 43373
		SSTables in each level: [18890/4, 115/10, 198/100, 1905/1000, 9451, 12814, 0, 0, 0]
		Space used (live): 4839933810943
		Space used (total): 4839933815913
		Space used by snapshots (total): 0
		Off heap memory used (total): 3273703284
		SSTable Compression Ratio: 0.9416884172984209
		Number of partitions (estimate): 2925826
		Memtable cell count: 41542
		Memtable data size: 2631688187
		Memtable off heap memory used: 2638649871
		Memtable switch count: 7
		Local read count: 87281
		Local read latency: 2.186 ms
		Local write count: 465591
		Local write latency: 0.124 ms
		Pending flushes: 0
		Percent repaired: 4.01
		Bloom filter false positives: 297882
		Bloom filter false ratio: 0.69198
		Bloom filter space used: 5111208
		Bloom filter off heap memory used: 4764232
		Index summary off heap memory used: 3360917
		Compression metadata off heap memory used: 626928264
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 186563160
		Compacted partition mean bytes: 1797922
		Average live cells per slice (last five minutes): 8.641592920353983
		Maximum live cells per slice (last five minutes): 258
		Average tombstones per slice (last five minutes): 1.0
		Maximum tombstones per slice (last five minutes): 1
		Dropped Mutations: 0

{code}

{code:java}
nodetool compactionstats
pending tasks: 3362
- blobstore.block: 3362
{code};;;","06/Nov/17 13:33;marcuse;[~llambiel] I would guess you also need to apply CASSANDRA-13215 for that many sstables, but I doubt both these apply cleanly now, hopefully we'll get them in real soon;;;","06/Nov/17 14:07;llambiel;Thanks [~krummas] The patches from [~pauloricardomg] already reduced the startup time by at least 10x;;;","07/Nov/17 09:06;pauloricardomg;bq. Ok I was able to reproduce it on one node. I've attached the trace log. It's unfiltered since I didn't managed to filter only to org.apache.cassandra.db.compaction

I wasn't able to track down the root cause of this condition from the logs, but a similar issue was reported on CASSANDRA-12743, so I think this is some kind of race condition showing up due to the amount of concurrent compactions happening and is not a consequence of this fix, so I prefer to investigate this separately. If you still see this issue please feel free to reopen CASSANDRA-12743 with details.

bq. However I'm still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn't show any error.

After having a look at the thread dump, it turns out that my previous patch generated a lock contention between compaction and cleanup, because each removed SSTable from cleanup generated a {{SSTableDeletingNotification}} and my previous patch submitted a new compaction task after each received notification which competed with the next {{SSTableDeletingNotification}} for the {{writeLock}} - making things slow overall, so I updated the patch to only submit a new compaction after receiving a flush notification as it was before, so this should be fixed now. [~llambiel]  would you mind trying the latest version now?

[~krummas] this should be ready for review now, the latest version already got a clean CI run, but I resubmitted a new internal CI run after doing the minor fix above and will update here when ready.

Summary of changes:
1) Reload compaction strategies when JBOD disk boundary changes ([commit|https://github.com/pauloricardomg/cassandra/commit/6cab7e0a31a638cc4a957c4ecfa592035d874058])
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock ([commit|https://github.com/pauloricardomg/cassandra/commit/3ef833d1e56c25f67bc8a3b49acf97b2efdf401d])
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work ([commit|https://github.com/pauloricardomg/cassandra/commit/eaf63dc3d52566ce0c4f91bbfec478305597f014])
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS ([commit|https://github.com/pauloricardomg/cassandra/commit/3e61df70025e704ee0c9d6ee8754ccdd38f5ab6d])

Patches
* [3.11|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [trunk|https://github.com/pauloricardomg/cassandra/tree/trunk-13948]

I wonder if now that CSM caches the disk boundaries we can make the handling of notifications use the readLock instead of the writeLock, to reduce contention when there is a high number of concurrent compactors, do you see any potential problems with this? Even if the notification handling races with getNextBackground task, as long as the individual compaction strategies are synchronized getNextBackground task should get a consistent view of the strategy sstables when there is a concurrent notification from the tracker.;;;","08/Nov/17 17:13;llambiel;Compactions are now processing as expected with your latest patch :)

However I'm facing an issue with nodetool cleanup, dunno if it is related or not.

Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn't show any error. I've attached a thread dump.

I'm happy to open another Jira if it's not related.;;;","08/Nov/17 20:44;pauloricardomg;bq. Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn't show any error. I've attached a thread dump.

this looks similar to CASSANDRA-13362 and unrelated to this issue. looking at the jstack it seems like there is some kind of deadlock on logback, do you have enough disk space in your log dir? did you set your log level back to DEBUG from TRACE level (which can be quite verbose and generate too many log rotations - what may be contributing to the logback problem)?;;;","09/Nov/17 03:55;pauloricardomg;Just a heads up that internal CI looks good for the latest version of the patch (there are a bunch of unrelated failures on trunk). I will rebase this on top of CASSANDRA-13215 after the first round of review.;;;","10/Nov/17 08:29;marcuse;Just had a first pass of the commits, and in general it looks good, I added a few comments on github

In general it feels a bit inconsistent about when it calls the {{CSM#maybeReload()}} - with 13215 in, perhaps we could just always call it as it will be cheap? I'll do a more thorough review of that last commit once it has been rebased on top of 13215;;;","13/Nov/17 03:14;pauloricardomg;bq. Just had a first pass of the commits, and in general it looks good, I added a few comments on github

Thanks for the review! Addressed github comments and rebased this on top of CASSANDRA-13215 on this v2 branch: [3.11-13948-v2|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:3.11-13948-v2]

I forgot to link the dtest branch here last time: [13948-dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/13948].

Will submit a new CI and post results here when ready.

bq. In general it feels a bit inconsistent about when it calls the CSM#maybeReload() - with 13215 in, perhaps we could just always call it as it will be cheap? I'll do a more thorough review of that last commit once it has been rebased on top of 13215

You mean not calling it on {{handleNotification}} or inconsistent use in general across {{CSM}}? Calling {{maybeReload}} on {{handleNotification}} is unnecessary, because if the CSM strategy is reloaded then the notification is no longer necessary - since all added or removed SSTables will already be in the correct place after reload - so we just call it whenever building a new compaction task - perhaps we could try reloading on {{handleNotification}} and avoid delivering the notification if the reload is successful?;;;","13/Nov/17 08:21;pauloricardomg;Testall passed with no failures, and [dtest failures|https://issues.apache.org/jira/secure/attachment/12897298/dtest13948.png] look unrelated.;;;","23/Nov/17 07:59;marcuse;I'm +1 on the code, just a few small comments;

* A few unused imports in CSM
* Could we update the class comment in CSM to cover the locking strategy (when do we need the read / write lock) and when maybeReload is called?;;;","27/Nov/17 19:06;pauloricardomg;Thanks for the review!

After rebasing this on top of CASSANDRA-13215 and addressing your latest comments, I noticed a few things which could be improved and did the following updates:
* Since blacklisting a directory will refresh the disk boundaries, we only need to reload strategies when the disk boundary changes or the table parameters change. To avoid equals comparison every time we call {{maybeReload}}, I moved the {{isOutOfDate}} check from the {{DiskBoundaryManager}} to the {{DiskBoundaries}} object - which is invalidated when there are any boundary changes. ([commit|https://github.com/pauloricardomg/cassandra/commit/662cd063ca2e1c382ba3cd5dc8032b0d3f12683c])
* I thought that it no longer makes sense to expose the compaction strategy index to outside the compaction strategy manager since it's possible to get the correct disk placement directly from {{CFS.getDiskBoundaries}}. This should prevent races when the {{CompactionStrategyManager}} reloads boundaries between successive calls to {{CSM.getCompactionStrategyIndex}}. [This commit|https://github.com/pauloricardomg/cassandra/commit/abd1340b000d4596d71f00e5de8507de967ee7a5] updates {{relocatesstables}} and {{scrub}} to use {{CFS.getDiskBoundaries}} instead, and make {{CSM.getCompactionStrategyIndex}} private.
* I found it a bit hard to reason about when to use {{maybeReload}} to write the documentation and made its use consistent across {{CompactionStrategyManager}} on [this commit|https://github.com/pauloricardomg/cassandra/commit/c0926e99edb1ffdcda16640eda6faf8e78da9e46]) (as you suggested before) along with the documentation. I kept the previous call to {{maybeReload}} from {{ColumnFamilyStore.reload}}, but we could probably avoid this and make {{maybeReload}} private-only as this is being called on pretty much every operation.

It feels like we can simplify this and get rid of these locks altogether (or at least greatly reduce their scope) by encapsulating the disk boundaries and compaction strategies in an immutable object accessed with an atomic reference and pessimistically cancel any tasks with an old placement when the strategies are reloaded. This is a significant refactor of {{CompactionStrategyManager}} so we should probably do it another ticket.

I submitted internal CI with the [latest branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] and will post the results here when ready. I will create a trunk version after this follow-up is reviewed.;;;","28/Nov/17 01:04;pauloricardomg;Canceling patch while I address some test failures, will resubmit when CI is green.;;;","30/Nov/17 00:57;pauloricardomg;There were test failures on:
* testall: {{CompactionsCQLTest.testSetLocalCompactionStrategy}} and {{testTriggerMinorCompactionSTCSNodetoolEnabled}}
* dtest: {{disk_balance_test.TestDiskBalance.disk_balance_bootstrap_test}}

{{testSetLocalCompactionStrategy}} and {{testTriggerMinorCompactionSTCSNodetoolEnabled}} were failing because when the strategy was updated via JMX, these manually set configurations were not surviving the compaction strategy reload - this was not introduced by this, but would also happen in case a directory was blacklisted before. This was fixed [on this commit|https://github.com/pauloricardomg/cassandra/commit/11c9a130d9cb7a6cfc5a039fdf79963f7e779d08]

While investigating why the strategies were reloaded even without a ring change on the tests above, I noticed that {{Keyspace.createReplicationStrategy}} was being called multiple times (on {{Keyspace}} construction and {{setMetadata}}), so I updated to only invalidate the disk boundaries when the replication settings actually change ([here|https://github.com/pauloricardomg/cassandra/commit/8a398a5d0d261178547946ac4e457f9abeb90f18]).

After the fix above, {{disk_balance_bootstrap_test}} started failing with imbalanced disks because the disk boundaries were not being invalidated after the joining node broadcasted its tokens via gossip, so {{TokenMetadata.getPendingRanges(keyspace, FBUtilities.getBroadcastAddress())}} was returning empty during disk boundary creation and causing imbalance. This is not failing on trunk because the double invalidation above during keyspace creation was causing the compaction strategy manager to reload the strategies with the correct ring placement during streaming. The fix to this is to invalidate the cached ring after gossiping the local tokens ([here|https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46]).

This made me realize that when replacing a node with the same address, even though the node is on bootstrap mode, it doesn't have any pending range, because it sets its token to normal state during bootstrap, what will cause its boundaries to not be computed correctly. I added a [dtest|https://github.com/pauloricardomg/cassandra-dtest/commit/8d48b166c9bfce51f9ab6c3abd73dfd4779a7c04] to show this and a [fix|https://github.com/pauloricardomg/cassandra/commit/6efd9cd454ce2fbfd40e592b6aaeda9debdb1c2b].

Finally, I didn't find a good reason to pass {{ColumnFamilyStore}} as argument to {{getDiskBoundaries}}, so I updated it to make it a field instead ([here|https://github.com/pauloricardomg/cassandra/commit/5df0d5ebed67aaae6ef9350d25b602af2a1702cf]).

I submitted internal CI, and testall is green and dtest failures [seem unrelated|https://issues.apache.org/jira/secure/attachment/12899922/dtest2.png]. Setting to patch available as this should be ready for a new round of review now. Thanks!;;;","30/Nov/17 08:00;marcuse;This ticket is getting quite big and very hard to review

Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially [this|https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46] as it involves tokenmetadata.

bq. Finally, I didn't find a good reason to pass ColumnFamilyStore as argument to getDiskBoundaries
we shouldn't leak {{this}} from the CFS constructor, I know we do it with CSM, but that is an ancient leftover that we should probably refactor away

edit: seems we leak this in many places in the cfs constructor, but lets avoid it in this case as it really doesn't hurt passing cfs to the methods;;;","30/Nov/17 13:13;pauloricardomg;bq. This ticket is getting quite big and very hard to review

I tried to make things easier by splitting in different commits, but I agree it became a bit complicated for review.

bq. Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially this as it involves tokenmetadata.

The problem is that some bugs (even though were pre-existing) only started showing up after this, so they have a dependency on this. 

I reorganized [this branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] to keep only things essential to this ticket, created CASSANDRA-14079 and CASSANDRA-14081 with unrelated minor fixes, and will create two follow-up tickets which depend on this.

This should be ready for review now, please let me know if some of the changes are not clear for you and needs better explanation. CI looked clean before the reorganization, but I will resubmit with the essential ticket just to make sure we didn't miss anything:

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/13948];;;","01/Dec/17 13:50;pauloricardomg;CI looks good (unrelated [testall|https://issues.apache.org/jira/secure/attachment/12900221/13948testall.png] and [dtest|https://issues.apache.org/jira/secure/attachment/12900222/13948dtest.png] failures).;;;","04/Dec/17 14:08;marcuse;+1 on the 3.11 patch;;;","04/Dec/17 14:09;marcuse;need to check the trunk patch as well, cancelling ""ready to commit"" (i hope);;;","06/Dec/17 14:35;pauloricardomg;bq. +1 on the 3.11 patch

Thanks for the review!

bq. need to check the trunk patch as well, cancelling ""ready to commit"" (i hope)

The merge went smoothly, most of the conflicts were related to CASSANDRA-9143, so I [updated|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948#diff-f9c882c974db60a710cf1f195cfdb801R113] {{CompactionStrategyManagerTest}} to mark a subset of sstables as repaired and pending repair to make sure sstables are being assigned the correct strategies for repaired and pending repair sstables.

However, there were 2 test failures in the trunk branch after the merge:
1. [testSetLocalCompactionStrategy|https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L163]
2. [disk_balance_bootstrap_test|https://github.com/pauloricardomg/cassandra-dtest/blob/73d7a8e1deb5eab05867d804933621062c2f6762/disk_balance_test.py#L34]

1. was failing [here|https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L175] because {{ALTER TABLE t WITH gc_grace_seconds = 1000}} was causing the manually set compaction strategy to be replaced by the strategy defined on the schema. After investigation, it turned out that the disk boundaries were being invalidated due to the schema reload (introduced by CASSANDRA-9425), and {{maybeReload(TableMetadata)}} was causing the compaction strategies to be reloaded with the schema settings instead of the manually set settings. In order to fix this, I split {{maybeReload}} in the original {{maybeReload(TableMetadata)}}, which should be called externally by {{ColumnFamilyStore}} and only reloads the strategies when the schema table parameters change, and {{maybeReloadDiskBoundaries}} which is used internally and reloads the compaction strategies with the same table settings when the disk boundaries are invalidated [here|https://github.com/apache/cassandra/commit/de5916e7c4f37736d5e1d06f0fc2b9c082b6bb99].

2.since the local ranges are not defined when the bootstrapping node starts, the disk boundaries are empty, but before CASSANDRA-9425 the boundaries were invalidated during keyspace construction ([here|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/Schema.java#L388]), so the correct boundaries were used during streaming. After CASSANDRA-9425  the boundaries were no longer reloaded during keyspace creation ([here|https://github.com/apache/cassandra/blob/4c80eeece37d79f434078224a0504400ae10a20d/src/java/org/apache/cassandra/schema/Schema.java#L138]), so the empty boundaries were used during streaming and the disks were imbalanced. I had exactly the same problem on CASSANDRA-14083 ([here|https://issues.apache.org/jira/browse/CASSANDRA-14083?focusedCommentId=16272918&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16272918]). The solution is to invalidate the disk boundaries after the tokens are set during bootstrap ([here|https://github.com/apache/cassandra/commit/a37bbda45142e1b351908a4ff5196eb08e92082b]).

After these two fixes, the tests were passing (failures seem unrelated - test screenshots from internal CI below):

||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-13948]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:13948]|
|[testall|https://issues.apache.org/jira/secure/attachment/12900864/3.11-13948-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12900862/trunk-13948-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12900865/3.11-13948-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12900863/trunk-13948-dtest.png]|;;;","06/Dec/17 16:35;marcuse;this LGTM, +1, just one comment and want to mention one possible issue I just realized:
{code}
        // If reloaded, SSTables will be placed in their correct locations
        // so there is no need to process notification
        if (maybeReloadDiskBoundaries())
            return;
{code}
so the code above, being run in {{handleListChangedNotification}}, {{handleRepairStatusChangedNotification}}, {{handleDeletingNotification}} and {{handleFlushNotification}} in CSM, should this call be run inside the read lock? My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies. This is handled by the fix you made in CASSANDRA-14079, but checking (or re-checking) with the lock should make sure we avoid the double-adding at all I think. I guess it would need some refactoring since we can't upgrade the read lock to a write lock. This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don't think it will be a problem currently.

trunk comment, feel free to address on commit;
* lets remove the deprecated {{public AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, int gcBefore)}} in CSM (and the {{validateForCompaction}} boolean to {{List<AbstractCompactionTask> getUserDefinedTasks(Collection<SSTableReader> sstables, int gcBefore, boolean validateForCompaction)}});;;","08/Dec/17 18:51;pauloricardomg;bq. My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies.

Good catch, even though unlikely this is indeed a possible scenario.

bq. This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don't think it will be a problem currently.

Agreed, created CASSANDRA-14103 to fix that and I commented a possible fix proposal there.

bq. trunk comment, feel free to address on commit;

Done, and I got too excited and removed it from 3.11 as well by mistake, so reinstated it on this commit: {{16bcbb9256392dc5364f2bd592f45649080935dc}}

Committed as {{25e46f05294fd42c111f2f1d5881082d97c572ea}} to cassandra-3.11 and merge up to master. Thanks for the review!;;;","11/Dec/17 20:23;pauloricardomg;Committed dtest as {{debe3780a4694c978f2516e565e071782dc7b2c8}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw descriptive errors for mixed mode repair attempts,CASSANDRA-13944,13108118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,09/Oct/17 22:22,21/Jan/21 20:19,14/Jul/23 05:56,24/Oct/17 20:56,4.0,4.0-alpha1,,,,,Consistency/Repair,,,,,0,,,,,"We often make breaking changes to streaming and repair between major versions, and don't usually support either in mixed mode clusters. Streaming connections check protocol versions, but repair message handling doesn't, which means cryptic exceptions show up in the logs when operators forget to turn off whatever's scheduling repairs on their cluster. Refusing to send or receive repair messages to/ from incompatible messaging service versions, and throwing a descriptive exception would make it clearer why repair is not working, as well as prevent any potentially unexpected behavior.",,bdeggleston,jeromatron,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 24 20:56:39 UTC 2017,,,,,,,,,,"0|i3l1xr:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"23/Oct/17 21:17;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13944]
[utests|https://circleci.com/gh/bdeggleston/cassandra/135];;;","24/Oct/17 08:06;marcuse;Patch LGTM, but I think the message could be a bit clearer that we don't support mixed version repairs - not sure ""messaging service version"" is something that makes sense to end users. Adding something like ""make sure all nodes involved in the repair are on the same major version"" to the message might help?;;;","24/Oct/17 17:18;bdeggleston;[~krummas] fixed;;;","24/Oct/17 17:21;marcuse;+1;;;","24/Oct/17 20:56;bdeggleston;committed as {{2b507c03c5190c744c5e84d7ca5cf7afa2b5c2ae}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor GcCompactionTest to avoid boxing,CASSANDRA-13941,13107685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,07/Oct/17 06:27,15/May/20 08:01,14/Jul/23 05:56,07/Oct/17 06:31,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"From GH PR #116:  https://github.com/apache/cassandra/pull/116

{quote}
Hello,

I am a graduate student at Oregon State University and as a part of my research and subject CS562 Applied Software Engineering project (Study and Refactoring of Functional Interface in Java), I have done refactoring of the Function<T,Integer> to ToIntFunction. My project deals with the boxing and unboxing of Wrapper class and the primitive data-types. I want to know that whether the open source community is ready to accept these micro-optimizing refactorings or not.

Thank you,
Harsh Thakor
{quote}

Trivial patch, but it's right and low risk.",,jeromatron,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/cassandra/pull/116,,,,,,,,,,9223372036854775807,,,Sat Oct 07 06:31:06 UTC 2017,,,,,,,,,,"0|i3kzrz:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"07/Oct/17 06:31;jjirsa;Committed to 4.0 as {{2ecadc88e4407ebd4b3325f42e0296e1e1dc8944}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mishandling of cells for removed/dropped columns when reading legacy files,CASSANDRA-13939,13107161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Oct/17 08:27,03/Nov/19 10:13,14/Jul/23 05:56,06/Oct/17 14:21,3.0.16,3.11.2,,,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"The tl;dr is that there is a bug in reading legacy files that can manifests itself with a trace looking like this:
{noformat}
Exception (java.lang.IllegalStateException) encountered during startup: One row required, 2 found
java.lang.IllegalStateException: One row required, 2 found
    at org.apache.cassandra.cql3.UntypedResultSet$FromResultSet.one(UntypedResultSet.java:84)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTableTimestamp(LegacySchemaMigrator.java:254)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTable(LegacySchemaMigrator.java:244)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readTables$7(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$126/591203139.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTables(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readKeyspace(LegacySchemaMigrator.java:187)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readSchema$4(LegacySchemaMigrator.java:178)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$123/1612073393.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readSchema(LegacySchemaMigrator.java:178)
{noformat}

The reason this can happen has to do with the handling of legacy files. Legacy files are cell based while the 3.0 storage engine is primarily row based, so we group those cells into rows early in the deserialization process (in {{UnfilteredDeserializer.OldFormatDeserializer}}), but in doing so, we can only consider a row finished when we've either reach the end of the partition/file, or when we've read a cell that doesn't belong to that row.  That second case means that when the deserializer returns a given row, the underlying file pointer may actually not positioned at the end of that row, but rather it may be past the first cell of the next row (which the deserializer remembers for future use). Long story short, when we try to detect if we're logically past our current index block in {{AbstractIterator.IndexState#isPastCurrentBlock(}}), we can't simply rely on the file pointer, which again may be a bit more advanced that we logically are, and that's the reason for the ""correction"" in that method. That correction is really just the amount of bytes remembered but not yet used in the deserializer.

That ""correction"" is sometimes wrong however and that's due to the fact that in {{LegacyLayout#readLegacyAtom}}, if we get a cell for an dropped or removed cell, we ignore that cell (which, in itself, is fine). Problem is that this skipping is done within the {{LegacyLayout#readLegacyAtom}} method but without {{UnfilteredDeserializer.OldFormatDeserializer}} knowing about it. As such, the size of the skipped cell ends up being accounted in the ""correction"" bytes for the next cell we read. Lo and behold, if such cell for a removed/dropped column is both the last cell of a CQL row and just before an index boundary (pretty unlikely in general btw, but definitively possible), then the deserializer will count its size with the first cell of the next row, which happens to also be the first cell of the next index block.  And when the code then tries to figure out if it crossed an index boundary, it will over-correct. That is, the {{indexState.updateBlock()}} call at the start of {{SSTableIterator.ForwardIndexedReader#computeNext}} will not work correctly.  This can then make the code return a row that is after the requested slice end (and should thus not be returned) because it doesn't compare that row to said requested end due to thinking it's not on the last index block to read, even though it genuinely is.

Anyway, the whole explanation is a tad complex, but the fix isn't: we need to move the skipping of cells for removed/dropped column a level up so the deserializer knows about it and don't silently count their size in the next atom size.",,aleksey,gdesalas,jeromatron,slebresne,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15081,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 12:42:45 UTC 2018,,,,,,,,,,"0|i3kwkf:",9223372036854775807,,,,,,,snazy,,snazy,,,Normal,,,,,,,,,,,,,,,,,,,"05/Oct/17 08:38;slebresne;Patch available in the following branches:
| [3.0|https://github.com/pcmanus/cassandra/commits/13939-3.0] | [CircleCI|https://circleci.com/gh/pcmanus/cassandra/7] |
| [3.11|https://github.com/pcmanus/cassandra/commits/13939-3.11] | [CircleCI|https://circleci.com/gh/pcmanus/cassandra/8] |     ;;;","06/Oct/17 12:34;snazy;+1;;;","06/Oct/17 14:21;slebresne;Committed thanks (there were a few failures on the CI builds, but that really doesn't seem related (the code only updates code that is involved in reading old sstables, and none of those tests was doing that)).;;;","23/Mar/18 12:42;gdesalas;Hi Sylvain 

can you help me to understand if this particular issue is solved in version 3.11.1 please?

We are now in version 3.0.9 and planning next week to move to 3.11.1 because of this particular issue

I´ll appreciate your comments

thank you in advance

Regards

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Default repair is broken, crashes other nodes participating in repair (in trunk)",CASSANDRA-13938,13107143,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,zznate,zznate,05/Oct/17 06:06,21/Dec/20 08:08,14/Jul/23 05:56,16/Jan/20 18:34,4.0,4.0-alpha3,,,,,Consistency/Repair,,,,,0,,,,,"Running through a simple scenario to test some of the new repair features, I was not able to make a repair command work. Further, the exception seemed to trigger a nasty failure state that basically shuts down the netty connections for messaging *and* CQL on the nodes transferring back data to the node being repaired. The following steps reproduce this issue consistently.

Cassandra stress profile (probably not necessary, but this one provides a really simple schema and consistent data shape):
{noformat}
keyspace: standard_long

keyspace_definition: |
  CREATE KEYSPACE standard_long WITH replication = {'class':'SimpleStrategy', 'replication_factor':3};
table: test_data

table_definition: |
  CREATE TABLE test_data (
  key text,
  ts bigint,
  val text,
  PRIMARY KEY (key, ts)
  ) WITH COMPACT STORAGE AND
  CLUSTERING ORDER BY (ts DESC) AND
  bloom_filter_fp_chance=0.010000 AND
  caching={'keys':'ALL', 'rows_per_partition':'NONE'} AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.000000 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

columnspec:
  - name: key
    population: uniform(1..50000000) # 50 million records available

  - name: ts
    cluster: gaussian(1..50) # Up to 50 inserts per record

  - name: val
    population: gaussian(128..1024) # varrying size of value data


insert:
  partitions: fixed(1) # only one insert per batch for individual partitions
  select: fixed(1)/1 # each insert comes in one at a time
  batchtype: UNLOGGED

queries:

  single:
    cql: select * from test_data where key = ? and ts = ? limit 1;

  series:
    cql: select key,ts,val from test_data where key = ? limit 10;
{noformat}

The commands to build and run:
{noformat}
ccm create 4_0_test -v git:trunk -n 3 -s
ccm stress user profile=./histo-test-schema.yml ops\(insert=20,single=1,series=1\) duration=15s -rate threads=4
# flush the memtable just to get everything on disk
ccm node1 nodetool flush
ccm node2 nodetool flush
ccm node3 nodetool flush
# disable hints for nodes 2 and 3
ccm node2 nodetool disablehandoff
ccm node3 nodetool disablehandoff
# stop node1
ccm node1 stop
ccm stress user profile=./histo-test-schema.yml ops\(insert=20,single=1,series=1\) duration=45s -rate threads=4
# wait 10 seconds
ccm node1 start
# Note that we are local to ccm's nodetool install 'cause repair preview is not reported yet
node1/bin/nodetool repair --preview
node1/bin/nodetool repair standard_long test_data
{noformat} 

The error outputs from the last repair command follow. First, this is stdout from node1:
{noformat}
$ node1/bin/nodetool repair standard_long test_data
objc[47876]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/bin/java (0x10274d4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1047b64e0). One of the two will be used. Which one is undefined.
[2017-10-05 14:31:52,425] Starting repair command #4 (7e1a9150-a98e-11e7-ad86-cbd2801b8de2), repairing keyspace standard_long with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [test_data], dataCenters: [], hosts: [], previewKind: NONE, # of ranges: 3, pull repair: false, force repair: false)
[2017-10-05 14:32:07,045] Repair session 7e2e8e80-a98e-11e7-ad86-cbd2801b8de2 for range [(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] failed with error Stream failed
[2017-10-05 14:32:07,048] null
[2017-10-05 14:32:07,050] Repair command #4 finished in 14 seconds
error: Repair job has failed with the error message: [2017-10-05 14:32:07,048] null
-- StackTrace --
java.lang.RuntimeException: Repair job has failed with the error message: [2017-10-05 14:32:07,048] null
    at org.apache.cassandra.tools.RepairRunner.progress(RepairRunner.java:122)
    at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:77)
    at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.dispatchNotification(ClientNotifForwarder.java:583)
    at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:533)
    at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:452)
    at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:108)
{noformat}

node1's {{system.log}}:
{noformat}
INFO  [Stream-Deserializer-/127.0.0.2:63069-e0af297f] 2017-10-05 14:32:07,037 StreamResultFuture.java:193 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Session with /127.0.0.2 is complete
INFO  [Stream-Deserializer-/127.0.0.3:63068-eb8f23bc] 2017-10-05 14:32:07,037 StreamResultFuture.java:193 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Session with /127.0.0.3 is complete
ERROR [Streaming-Netty-Thread-5-5] 2017-10-05 14:32:07,037 StreamSession.java:617 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Streaming error occurred on session with peer 127.0.0.3
java.nio.channels.ClosedChannelException: null
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
ERROR [Streaming-Netty-Thread-5-7] 2017-10-05 14:32:07,038 StreamSession.java:617 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Streaming error occurred on session with peer 127.0.0.2
java.nio.channels.ClosedChannelException: null
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
WARN  [Stream-Deserializer-/127.0.0.2:63069-e0af297f] 2017-10-05 14:32:07,038 StreamResultFuture.java:220 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Stream failed
WARN  [Stream-Deserializer-/127.0.0.3:63068-eb8f23bc] 2017-10-05 14:32:07,038 StreamResultFuture.java:220 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Stream failed
WARN  [RepairJobTask:1] 2017-10-05 14:32:07,038 RepairJob.java:176 - [repair #7e2e8e80-a98e-11e7-ad86-cbd2801b8de2] test_data sync failed
ERROR [Stream-Deserializer-/127.0.0.3:7000-48246b87] 2017-10-05 14:32:07,041 StreamSession.java:757 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Remote peer 127.0.0.3 failed stream session.
ERROR [RepairJobTask:1] 2017-10-05 14:32:07,042 RepairSession.java:326 - [repair #7e2e8e80-a98e-11e7-ad86-cbd2801b8de2] Session completed with the following error
org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:88) ~[main/:na]
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-18.0.jar:na]
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:221) ~[main/:na]
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:197) ~[main/:na]
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:488) ~[main/:na]
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:601) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:207) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR [RepairJobTask:1] 2017-10-05 14:32:07,043 RepairRunnable.java:564 - Repair session 7e2e8e80-a98e-11e7-ad86-cbd2801b8de2 for range [(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] failed with error Stream failed
org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:88) ~[main/:na]
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-18.0.jar:na]
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:221) ~[main/:na]
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:197) ~[main/:na]
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:488) ~[main/:na]
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:601) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:207) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
INFO  [RepairJobTask:1] 2017-10-05 14:32:07,045 CoordinatorSession.java:233 - Incremental repair session 7e1a9150-a98e-11e7-ad86-cbd2801b8de2 failed
ERROR [Stream-Deserializer-/127.0.0.2:7000-4b83e3cb] 2017-10-05 14:32:07,045 StreamSession.java:757 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Remote peer 127.0.0.2 failed stream session.
INFO  [AntiEntropyStage:1] 2017-10-05 14:32:07,048 CoordinatorSession.java:233 - Incremental repair session 7e1a9150-a98e-11e7-ad86-cbd2801b8de2 failed
INFO  [AntiEntropyStage:1] 2017-10-05 14:32:07,049 LocalSessions.java:501 - Failing local repair session 7e1a9150-a98e-11e7-ad86-cbd2801b8de2
INFO  [RepairJobTask:1] 2017-10-05 14:32:07,049 RepairRunnable.java:647 - Repair command #4 finished in 14 seconds
{noformat}

node2's {{system.log}} (note the transport shutdowns at the end):
{noformat}
INFO  [AntiEntropyStage:1] 2017-10-05 18:31:52,521 LocalSessions.java:560 - Beginning local incremental repair session LocalSession{sessionID=7e1a9150-a98e-11e7-ad86-cbd2801b8de2, state=PREPARING, coordinator=/127.0.0.1, tableIds=[99d53860-a98d-11e7-9807-39cb3e573e5c], repairedAt=1507181512483, ranges=[(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]], participants=[/127.0.0.1, /127.0.0.2, /127.0.0.3], startedAt=1507181512, lastUpdate=1507181512}
INFO  [CompactionExecutor:224] 2017-10-05 18:31:52,539 CompactionManager.java:642 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] Starting anticompaction for standard_long.test_data on 2/2 sstables
INFO  [CompactionExecutor:224] 2017-10-05 18:31:52,539 CompactionManager.java:664 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] SSTable BigTableReader(path='/Users/zznate/.ccm/4_0_test/node2/data0/standard_long/test_data-99d53860a98d11e7980739cb3e573e5c/na-27-big-Data.db') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting
INFO  [CompactionExecutor:224] 2017-10-05 18:31:52,539 CompactionManager.java:664 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] SSTable BigTableReader(path='/Users/zznate/.ccm/4_0_test/node2/data0/standard_long/test_data-99d53860a98d11e7980739cb3e573e5c/na-26-big-Data.db') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting
INFO  [CompactionExecutor:224] 2017-10-05 18:31:52,547 CompactionManager.java:699 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] Completed anticompaction successfully
INFO  [AntiEntropyStage:1] 2017-10-05 18:31:57,500 Validator.java:292 - [repair #7e2e8e80-a98e-11e7-ad86-cbd2801b8de2] Sending completed merkle tree to /127.0.0.1 for standard_long.test_data
INFO  [Stream-Deserializer-/127.0.0.1:63064-3a39d969] 2017-10-05 18:32:05,417 StreamResultFuture.java:115 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2 ID#0] Creating new streaming plan for Repair
INFO  [Stream-Deserializer-/127.0.0.1:63064-3a39d969] 2017-10-05 18:32:05,418 StreamResultFuture.java:122 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2, ID#0] Received streaming plan for Repair
INFO  [NonPeriodicTasks:1] 2017-10-05 18:32:05,856 StreamResultFuture.java:179 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2 ID#0] Prepare completed. Receiving 1 files(8.136MiB), sending 2 files(42.689MiB)
INFO  [Stream-Deserializer-/127.0.0.1:63064-3a39d969] 2017-10-05 18:32:06,625 StreamResultFuture.java:179 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2 ID#0] Prepare completed. Receiving 1 files(8.136MiB), sending 2 files(42.689MiB)
WARN  [Stream-Deserializer-/127.0.0.1:63066-c7002e89] 2017-10-05 18:32:06,747 CompressedStreamReader.java:112 - [Stream 85d4b790-a98e-11e7-ad86-cbd2801b8de2] Error while reading partition DecoratedKey(-9060243433852736644, 5f1c6c5d747c) from stream on ks='standard_long' and table='test_data'.
ERROR [Stream-Deserializer-/127.0.0.1:63066-c7002e89] 2017-10-05 18:32:06,759 StreamSession.java:617 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Streaming error occurred on session with peer 127.0.0.1
org.apache.cassandra.streaming.StreamReceiveException: java.lang.AssertionError: stream can only read forward.
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:63) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:178) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.AssertionError: stream can only read forward.
        at org.apache.cassandra.streaming.compress.CompressedInputStream.position(CompressedInputStream.java:108) ~[main/:na]
        at org.apache.cassandra.streaming.compress.CompressedStreamReader.read(CompressedStreamReader.java:96) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:58) ~[main/:na]
        ... 4 common frames omitted
INFO  [Stream-Deserializer-/127.0.0.1:63066-c7002e89] 2017-10-05 18:32:06,761 StreamResultFuture.java:193 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Session with /127.0.0.1 is complete
WARN  [Stream-Deserializer-/127.0.0.1:63066-c7002e89] 2017-10-05 18:32:06,762 StreamResultFuture.java:220 - [Stream #85d4b790-a98e-11e7-ad86-cbd2801b8de2] Stream failed
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,765 CassandraDaemon.java:211 - Exception in thread Thread[NettyStreaming-Outbound-/127.0.0.1:1,5,main]
org.apache.cassandra.io.FSReadError: java.nio.channels.ClosedByInterruptException
        at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:133) ~[main/:na]
        at org.apache.cassandra.streaming.compress.CompressedStreamWriter.write(CompressedStreamWriter.java:94) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage.serialize(OutgoingFileMessage.java:111) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:53) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.async.NettyStreamingMessageSender$FileStreamTask.run(NettyStreamingMessageSender.java:324) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [main/:na]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
Caused by: java.nio.channels.ClosedByInterruptException: null
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.8.0_101]
        at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:746) ~[na:1.8.0_101]
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727) ~[na:1.8.0_101]
        at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:129) ~[main/:na]
        ... 12 common frames omitted
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,769 StorageService.java:393 - Stopping gossiper
WARN  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,769 StorageService.java:315 - Stopping gossip by operator request
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,769 Gossiper.java:1527 - Announcing shutdown
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,770 StorageService.java:2202 - Node /127.0.0.2 state jump to shutdown
INFO  [AntiEntropyStage:1] 2017-10-05 18:32:07,049 LocalSessions.java:501 - Failing local repair session 7e1a9150-a98e-11e7-ad86-cbd2801b8de2
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:08,771 StorageService.java:398 - Stopping native transport
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:08,774 Server.java:180 - Stop listening for CQL clients
{noformat}

And node3 {{system.log}} (similar to node2):
{noformat}
INFO  [AntiEntropyStage:1] 2017-10-05 18:31:52,521 LocalSessions.java:560 - Beginning local incremental repair session LocalSession{sessionID=7e1a9150-a98e-11e7-ad86-cbd2801b8de2, state=PREPARING, coordinator=/127.0.0.1, tableIds=[99d53860-a98d-11e7-9807-39cb3e573e5c], repairedAt=1507181512483, ranges=[(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]], participants=[/127.0.0.1, /127.0.0.2, /127.0.0.3], startedAt=1507181512, lastUpdate=1507181512}
INFO  [CompactionExecutor:249] 2017-10-05 18:31:52,542 CompactionManager.java:642 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] Starting anticompaction for standard_long.test_data on 2/2 sstables
INFO  [CompactionExecutor:249] 2017-10-05 18:31:52,543 CompactionManager.java:664 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] SSTable BigTableReader(path='/Users/zznate/.ccm/4_0_test/node3/data0/standard_long/test_data-99d53860a98d11e7980739cb3e573e5c/na-27-big-Data.db') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting
INFO  [CompactionExecutor:249] 2017-10-05 18:31:52,543 CompactionManager.java:664 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] SSTable BigTableReader(path='/Users/zznate/.ccm/4_0_test/node3/data0/standard_long/test_data-99d53860a98d11e7980739cb3e573e5c/na-26-big-Data.db') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting
INFO  [CompactionExecutor:249] 2017-10-05 18:31:52,550 CompactionManager.java:699 - [repair #7e1a9150-a98e-11e7-ad86-cbd2801b8de2] Completed anticompaction successfully
INFO  [AntiEntropyStage:1] 2017-10-05 18:31:57,918 Validator.java:292 - [repair #7e2e8e80-a98e-11e7-ad86-cbd2801b8de2] Sending completed merkle tree to /127.0.0.1 for standard_long.test_data
INFO  [Stream-Deserializer-/127.0.0.1:63063-d6987513] 2017-10-05 18:32:05,817 StreamResultFuture.java:115 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2 ID#0] Creating new streaming plan for Repair
INFO  [Stream-Deserializer-/127.0.0.1:63063-d6987513] 2017-10-05 18:32:05,818 StreamResultFuture.java:122 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2, ID#0] Received streaming plan for Repair
INFO  [NonPeriodicTasks:1] 2017-10-05 18:32:05,866 StreamResultFuture.java:179 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2 ID#0] Prepare completed. Receiving 1 files(8.136MiB), sending 2 files(42.679MiB)
INFO  [Stream-Deserializer-/127.0.0.1:63063-d6987513] 2017-10-05 18:32:06,622 StreamResultFuture.java:179 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2 ID#0] Prepare completed. Receiving 1 files(8.136MiB), sending 2 files(42.679MiB)
WARN  [Stream-Deserializer-/127.0.0.1:63067-6347c9a8] 2017-10-05 18:32:06,759 CompressedStreamReader.java:112 - [Stream 85d3f440-a98e-11e7-ad86-cbd2801b8de2] Error while reading partition DecoratedKey(-9060243433852736644, 5f1c6c5d747c) from stream on ks='standard_long' and table='test_data'.
ERROR [Stream-Deserializer-/127.0.0.1:63067-6347c9a8] 2017-10-05 18:32:06,773 StreamSession.java:617 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Streaming error occurred on session with peer 127.0.0.1
org.apache.cassandra.streaming.StreamReceiveException: java.lang.AssertionError: stream can only read forward.
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:63) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:178) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.AssertionError: stream can only read forward.
        at org.apache.cassandra.streaming.compress.CompressedInputStream.position(CompressedInputStream.java:108) ~[main/:na]
        at org.apache.cassandra.streaming.compress.CompressedStreamReader.read(CompressedStreamReader.java:96) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:58) ~[main/:na]
        ... 4 common frames omitted
INFO  [GossipStage:1] 2017-10-05 18:32:06,774 Gossiper.java:1040 - InetAddress /127.0.0.2 is now DOWN
INFO  [Stream-Deserializer-/127.0.0.1:63067-6347c9a8] 2017-10-05 18:32:06,775 StreamResultFuture.java:193 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Session with /127.0.0.1 is complete
WARN  [Stream-Deserializer-/127.0.0.1:63067-6347c9a8] 2017-10-05 18:32:06,775 StreamResultFuture.java:220 - [Stream #85d3f440-a98e-11e7-ad86-cbd2801b8de2] Stream failed
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,778 CassandraDaemon.java:211 - Exception in thread Thread[NettyStreaming-Outbound-/127.0.0.1:1,5,main]
org.apache.cassandra.io.FSReadError: java.nio.channels.ClosedByInterruptException
        at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:133) ~[main/:na]
        at org.apache.cassandra.streaming.compress.CompressedStreamWriter.write(CompressedStreamWriter.java:94) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage.serialize(OutgoingFileMessage.java:111) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:53) ~[main/:na]
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.async.NettyStreamingMessageSender$FileStreamTask.run(NettyStreamingMessageSender.java:324) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [main/:na]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
Caused by: java.nio.channels.ClosedByInterruptException: null
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.8.0_101]
        at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:746) ~[na:1.8.0_101]
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727) ~[na:1.8.0_101]
        at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:129) ~[main/:na]
        ... 12 common frames omitted
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,781 StorageService.java:393 - Stopping gossiper
WARN  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,781 StorageService.java:315 - Stopping gossip by operator request
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,781 Gossiper.java:1527 - Announcing shutdown
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:06,782 StorageService.java:2202 - Node /127.0.0.3 state jump to shutdown
INFO  [AntiEntropyStage:1] 2017-10-05 18:32:07,049 LocalSessions.java:501 - Failing local repair session 7e1a9150-a98e-11e7-ad86-cbd2801b8de2
ERROR [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:08,782 StorageService.java:398 - Stopping native transport
INFO  [NettyStreaming-Outbound-/127.0.0.1:1] 2017-10-05 18:32:08,785 Server.java:180 - Stop listening for CQL clients
{noformat}

The final state of the cluster after running this repair command:
{noformat}
$ ccm node1 nodetool status

Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  8.62 MiB   1            100.0%            ffe7466b-2937-4322-a388-cca1819f6513  rack1
DN  127.0.0.2  44.54 MiB  1            100.0%            e374f662-1da5-477d-b1fb-173b8311c4a9  rack1
DN  127.0.0.3  44.53 MiB  1            100.0%            d8d99bd6-4b9f-4510-a4c3-62951be1b4d2  rack1
{noformat}",,aleksey,alourie,aweisberg,bdeggleston,bradfordcp,dimitarndimitrov,djoshi,G-Ashok,jasobrown,jasonstack,jay.zhuang,jeromatron,jjirsa,jolynch,KurtG,Lerh Low,mbyrd,pauloricardomg,sbtourist,tcooke,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14394,CASSANDRA-15212,,,,,,,,,,,,,,,,,,,,"25/Jan/18 13:51;jasobrown;13938.yaml;https://issues.apache.org/jira/secure/attachment/12907692/13938.yaml","25/Jan/18 13:51;jasobrown;test.sh;https://issues.apache.org/jira/secure/attachment/12907693/test.sh",,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,Availability -> Process Crash,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 18:33:51 UTC 2020,,,,,,,,,,"0|i3kwgf:",9223372036854775807,4.0,,,,,,djoshi,,djoshi,,,Critical,,4.0-alpha,4.0-alpha1,,"[602a5eef177ac65020470cb0fcf8d88d820ab888|https://github.com/apache/cassandra/commit/602a5eef177ac65020470cb0fcf8d88d820ab888]",,,,,,,,,,,,,,"05/Oct/17 16:24;jasobrown;[~zznate] thanks for detailed steps to create the error. I am able to reproduce, and am digging in now.;;;","24/Jan/18 19:10;aweisberg;This reproduced in CircleCI. [https://circleci.com/gh/aweisberg/cassandra/781#tests/containers/70]

Calling position is fine as long as it never goes backwards. It's going backwards because the list of file locations to read is overlapping or out of order (not clear). I traced this back and things seem to be correct and order preserving up until potentially [https://github.com/aweisberg/cassandra/blob/da3e0e091bc2e1819b8839784cc8156533f3cb88/src/java/org/apache/cassandra/streaming/StreamSession.java#L325]

At that point {{Ranges.normalize()}} is called and that should fix this issue because it should merge overlapping ranges and sort them. It's possible the bug is in that method. One easy way to check this might be log the ranges, run it in CircleCI until it reproduces, take the logged ranges and run them through {{Ranges.normalize()}} and see what happens.

I think I'll do that before getting to the issue of this error breaking messaging.;;;","25/Jan/18 13:51;jasobrown;[~aweisberg] thanks for looking at this, I haven't had a chance to yet. I do, however, have a script to repro based on [~zznate]'s description and will attach.;;;","25/Jan/18 17:34;aweisberg;That's not it. It's deserializing a row and blowing past where it expects to go and then trying to rewind deserialization for the next range.;;;","04/Jun/18 04:07;Lerh Low;Here's another stacktrace that may help - I've also been getting these while testing trunk in EC2. The steps I use are the same: 
 - Disable hintedhandoff
 - Take out 1 node
 - Run stress for 10 mins, then run repair

It will error out and the nodes also end up in a bizarre situation with gossip that I will have to stop the entire cluster and then start them up one at a time (in a rolling restart they still won't be able to sort themselves out). Sometimes it errors with {{stream can only read forward}} (as above and in the JIRA), but here's another stacktrace that has also showed up several times in some of the failed nodes:
{code:java}
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: ERROR [Stream-Deserializer-35.155.140.194:39371-28daf76d] 2018-05-31 02:07:24,445 StreamingInboundHandler.java:210 - [Stream channel: 28daf76d] stream operation from 35.155.140.194:39371 failed
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 1711542017
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at net.jpountz.util.ByteBufferUtils.checkRange(ByteBufferUtils.java:20)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at net.jpountz.util.ByteBufferUtils.checkRange(ByteBufferUtils.java:14)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:48)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.compress.LZ4Compressor.uncompress(LZ4Compressor.java:162)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.db.streaming.CompressedInputStream.decompress(CompressedInputStream.java:163)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.db.streaming.CompressedInputStream.reBuffer(CompressedInputStream.java:144)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.db.streaming.CompressedInputStream.reBuffer(CompressedInputStream.java:119)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:144)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.util.RebufferingInputStream.readPrimitiveSlowly(RebufferingInputStream.java:108)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.util.RebufferingInputStream.readShort(RebufferingInputStream.java:164)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.util.RebufferingInputStream.readUnsignedShort(RebufferingInputStream.java:170)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.io.util.TrackedDataInputPlus.readUnsignedShort(TrackedDataInputPlus.java:139)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:367)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:377)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.db.streaming.CassandraStreamReader$StreamDeserializer.newPartition(CassandraStreamReader.java:199)
May 31 02:07:24 ip-10-0-18-230 cassandra[6034]: at org.apache.cassandra.db.streaming.CassandraStreamReader.writePartition(CassandraStreamReader.java:172){code}
I get the feeling they may be related but I'm not sure...I can open a different Jira for this if you like, but otherwise hope it may point out more clues as to what is going on :/ ;;;","10/Jun/18 13:00;jasobrown;I think I've figured out why the {{stream can only read forward}} assert fails. I think it's mishandling of {{CompressedInputStream#current}} [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/streaming/CompressedInputStream.java#L133]. All the other read lengths seem fine so I think it's just a miscalculation of {{current}}. Fortunately, {{current}} doesn't have too much value, expect when figuring out the base offset when {{reBuffer}} is called from the parent class. If you naively comment out the assert that fails, the stream can be deserialized successfully, and data can be read. Thus, I think it's something simple and I should have a fix shortly.

wrt [~Lerh Low]'s report, that's a bit different. I don't know how a failure in streaming could cause ""a bizarre situation with gossip"", which really isn't explained. Either way, there's an {{ArrayIndexOutOfBoundsException}} problem somewhere. [~Lerh Low]: can you open a separate ticket, and include as many steps to repro as possible, including schema? ;;;","11/Jun/18 15:42;jasobrown;The problem is that when {{CompressedInputStream#position()}} is called, the new position might be in the middle of a buffer. We need to remember that offset, and subtract that value when updating {{current}} in {{#reBuffer(boolean)}}. The resaon why is that those offset bytes get double counted on the first call to {{#reBuffer()}} after {{#position()}} as we add the {{buffer.position()}} to {{current}}. {{current}} already accounts for those offset bytes when {{#position()}} was called.

Patch below fixes the problem:
||13938||
|[branch|https://github.com/jasobrown/cassandra/tree/13938]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13938]|

Also, since I was in this code, I've cleaned it up a bit:
 - renamed {{currentOffset}} to {{streamOffset}}
 - reorganized static and member fields at top of class, and added more comments.
 - capture a reference to the background thread that will execute {{CompressedInputStream#Reader}}, so we can close it properly.
 - made {{close()}} more responsible for shutting down/cleaning up resources
 - cleaned up {{decompress()}} by moving the CRC validation into a sub-method (got rid of a needless local boolean)

I also have a dtest based on [~zznate]'s repro steps and schema (slightly modified as we no longer support COMPACT_STORAGE), and it produced the error every time (before the patch). I also tried [~pauloricardomg] 's dtest from CASSANDRA-14394, but it did not repro for me. I'm happy with either dtest (or both), but would definitely like to include one of them.
||13938-dtest||
|[jasobrown dtest|https://github.com/jasobrown/cassandra-dtest/tree/13938]|
|[paulo dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/CASSANDRA-14394]|;;;","13/Jul/18 12:11;dimitarndimitrov;{quote}The problem is that when {{CompressedInputStream#position()}} is called, the new position might be in the middle of a buffer. We need to remember that offset, and subtract that value when updating {{current}} in {{#reBuffer(boolean)}}. The resaon why is that those offset bytes get double counted on the first call to {{#reBuffer()}} after {{#position()}} as we add the {{buffer.position()}} to {{current}}. {{current}} already accounts for those offset bytes when {{#position()}} was called.
{quote}
[~jasobrown], isn't that equivalent (although a bit more complex) to just setting {{current}} to the last reached/read position in the stream when rebuffering? (i.e. {{current = streamOffset + buffer.position()}}).

I might be missing something, but the role of {{currentBufferOffset}} seems to be solely to ""align"" {{current}} and {{streamOffset}} the first time after a new section is started. Then {{current += buffer.position() - currentBufferOffset}} expands to {{current = -current- + buffer.position() + streamOffset - -current- }} which is the same as {{current = streamOffset + buffer.position()}}. After that first time, {{current}} naturally follows {{streamOffset}} without the need of any adjustment, but it seems more natural to express this as {{streamOffset + buffer.position()}} instead of the new expression or the old {{current + buffer.position()}}. To me, it's also a bit more intuitive and easier to understand (hopefully it's also right in addition to intuitive :)).

The equivalence above would hold true if {{current}} and {{streamOffset}} don't change their value in the meantime, but I think this is ensured by the well-ordered sequential fashion in which the decompressing and the offset bookkeeping functionality of {{CompressedInputStream}} happen in the thread running the corresponding {{StreamDeserializingTask}}.
 * The aforementioned well-ordered sequential fashion seems to be POSITION followed by 0-N times REBUFFER + DECOMPRESS, where the first REBUFFER might not update {{current}} with the above calculation in case {{current}} is already too far ahead (i.e. the new section is not starting within the current buffer).;;;","11/Sep/18 05:01;jasobrown;[~dimitarndimitrov], Thanks for your comments, and apologies for the late response.

While your proposed simplification indeed clarifies the logic, unfortunately it doesn't resolve the bug (my dtest still fails - this is due to the need to reset a 'some' value, like the currentBufferOffset, after rebufferring). However, your observation about simplifying this patch (in particular eliminate {{currentBufferOffset}} made me reconsider the needs of this class. Basically, we just need to correctly track the streamOffset for the current buffer, and that's all. When I ported this clas from 3.11, I over-complicated the offsets and counters into the first version of this class (committed with CASSANDRA-12229), and then confused it again (while resolving the error) with the first patch.

In short: as long as I correctly calculate streamOffset, that should satisfy the needs for the class. Thus, I eliminated both {{current}} and {{currentBufferOffset}}, and the result is clearer and correct.

I've pushed a cleaned up branch (which has been rebased to trunk). Please note that, as with the first patch, the majority of this patch is refactoring to clean up the class in general. I've also updated my dtest patch as my version required a stress profile (based on [~zznate]'s original) to be committed, as well. (Note: my dtest branch also includes [~pauloricardomg]'s patch, but, as before, I'm unable to get that to fail on trunk.);;;","22/Oct/18 13:29;alourie;[~jasobrown] I've been testing both trunk and your branch in a simple repair scenario and it's still failing. The scenario I'm working with is:

1. Start a cluster
2. Load the cluster for 10 minutes
3. Stop one node and load it for an additional 30 minutes
4. Clear the hints
5. Start the stopped node and let it resync with others for a couple of minutes.
6. Start the repairs on the previously stopped node.

Repairs crash on the other nodes (on 2 nodes in my 3-node test cluster) with the following error:

{code}
Oct 22 13:10:54 ip-10-0-13-111 cassandra[5927]: INFO  [AntiEntropyStage:1] 2018-10-22 13:10:54,716 Validator.java:417 - [repair #9c38dd00-d5fb-11e8-ac32-316a9d8f8d32] Sending completed merkle tree to 35.162.15.68:7000 for alex.test2
Oct 22 13:16:02 ip-10-0-13-111 cassandra[5927]: INFO  [AntiEntropyStage:1] 2018-10-22 13:16:02,594 StreamingRepairTask.java:72 - [streaming task #9c38dd00-d5fb-11e8-ac32-316a9d8f8d32] Performing streaming repair of 7382 ranges with 35.162.15.68:7000
Oct 22 13:16:02 ip-10-0-13-111 cassandra[5927]: INFO  [AntiEntropyStage:1] 2018-10-22 13:16:02,981 StreamResultFuture.java:89 - [Stream #9fe63820-d5fc-11e8-8a2b-3555ba61a619] Executing streaming plan for Repair
Oct 22 13:16:02 ip-10-0-13-111 cassandra[5927]: INFO  [AntiEntropyStage:1] 2018-10-22 13:16:02,981 StreamSession.java:287 - [Stream #9fe63820-d5fc-11e8-8a2b-3555ba61a619] Starting streaming to 35.162.15.68:7000
Oct 22 13:16:02 ip-10-0-13-111 cassandra[5927]: INFO  [AntiEntropyStage:1] 2018-10-22 13:16:02,987 StreamCoordinator.java:259 - [Stream #9fe63820-d5fc-11e8-8a2b-3555ba61a619, ID#0] Beginning stream session with 35.162.15.68:7000
Oct 22 13:16:03 ip-10-0-13-111 cassandra[5927]: INFO  [Stream-Deserializer-35.162.15.68:7000-0b32ed63] 2018-10-22 13:16:03,783 StreamResultFuture.java:178 - [Stream #9fe63820-d5fc-11e8-8a2b-3555ba61a619 ID#0] Prepare completed. Receiving 6 files(215.878MiB), sending 17 files(720.317MiB)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]: WARN  [Stream-Deserializer-35.162.15.68:60292-be7cb6ee] 2018-10-22 13:16:04,355 CassandraCompressedStreamReader.java:110 - [Stream 9fe63820-d5fc-11e8-8a2b-3555ba61a619] Error while reading partition DecoratedKey(-9088115514873584734, 646572706865616435393632373436) from stream on ks='alex' and table='test2'.
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]: ERROR [Stream-Deserializer-35.162.15.68:60292-be7cb6ee] 2018-10-22 13:16:04,362 StreamingInboundHandler.java:213 - [Stream channel: be7cb6ee] stream operation from 35.162.15.68:60292 failed
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]: java.lang.AssertionError: stream can only read forward.
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.db.streaming.CompressedInputStream.position(CompressedInputStream.java:108)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.db.streaming.CassandraCompressedStreamReader.read(CassandraCompressedStreamReader.java:93)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:74)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:49)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:177)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
Oct 22 13:16:04 ip-10-0-13-111 cassandra[5927]:         at java.lang.Thread.run(Thread.java:748)
{code}


The data is created as follows:

{code:sql}
CREATE KEYSPACE IF NOT EXISTS alex with replication = { 'class': 'NetworkTopologyStrategy', 'alourie': 3 };
CREATE TABLE IF NOT EXISTS alex.test2 (                                                                                                                                                                                       
    part text,                                                                                                                                                                                                                          
    clus int,                                                                                                                                                                                                                           
    data text,                                                                                                                                                                                                                          
    PRIMARY KEY (part, clus)                                                                                                                                                                                                            
); 
CREATE MATERIALIZED VIEW alex.mv AS                                                                                                                                                                                           
    SELECT *                                                                                                                                                                                                                            
    FROM alex.test2                                                                                                                                                                                                                     
    WHERE part IS NOT NULL AND clus IS NOT NULL                                                                                                                                                                                         
    PRIMARY KEY (clus, part)                                                                                                                                                                                                            
    WITH CLUSTERING ORDER BY (part ASC);
{code}

So, basically speaking, the repairs don't work in trunk. ;;;","29/Aug/19 21:51;jolynch;I might have cycles to tackle this shortly, if someone else has cycles first please take it.;;;","29/Aug/19 22:48;djoshi;[~jolynch] I have assigned this to you. Thanks for volunteering :);;;","17/Sep/19 20:21;jjirsa;Is this still broken? 
;;;","17/Sep/19 20:23;djoshi;Yes;;;","19/Dec/19 17:00;aleksey;As multiple folks have noticed, {{current}} tracking is indeed busted. And, also, as noticed, we don’t really need to track {{current}} - it can always be inferred from the current chunk offset into the partition position, plus position in the current buffer. So what we need to track is only that offset of the actual buffer itself.

Tracking it is actually rather trivial, since there are two distinct cases when we move to the next chunk:

1. A {{reBuffer()}} call upon exhaustion of the previous buffer, while still not done with the current partition position range. In this case we are moving to the adjacent compressed chunk, and we should bump offset by uncompressed chunk length, which is a fixed value for a given session;
2. We are skipping to the next partition position range, via {{position(long position)}} call; it might be in the current chunk, or it might skip 1..n compressed chunks. In either case, the new offset and buffer position are derived from the new {{position}} argument only

There is another bug in the current implementation: if the compressed buffer exceeds length of {{info.parameters.chunkLength()}} (if data was poorly compressible, for example, and upon compression blowed up in size instead of shrinking), then read code in {{Reader#runMayThrow()}} wouldn’t be able to read that chunk fully into the temporary byte array.

Speaking of that array: the slow-path copy happens to be the one we use, and it involves a redundant copy into this temporary array, followed by a copy of that array into the destination {{ByteBuffer}}. That can be trivially eliminated by adding a {{readFully(ByteBuffer)}} method to {{RebufferingInputStream}}.

I’ve also realised that for no good reason we have an extra thread whose only job is to read the chunks off input into chunk-size byte buffers and put the on a queue for {{CompressedInputStream}} to later consume. There is absolutely no reason for that and the complexity it introduces. It’s not the place to handle prefetching, nor does it make the input stream non-blocking, nor is it an issue if it were, given that streaming utilises a separate event loop group from messaging.

It’s also very much unnecessary to allocate a whole new direct {{ByteBuffer}} for every chunk. A single {{ByteBuffer}} for the compressed chunk, reused, is all we need.

Also, we’ve had no test coverage for {{min_compress_ratio}}, introduced by CASSANDRA-10520.

And, one last thing/nit: while looking at the write side, I spotted some unnecessary garbage creation in {{CassandraCompressedStreamWriter#getTransferSections()}} when extending current section, that can and should be easily avoided. Also the use of {{SSTableReader.PartitionPositionBounds}} class in place of {{Pair}}, when we did that refactor recently, is semantically incorrect: {{PartitionPositionBounds}} as a class represents partition bound in the uncompressed stream, and not chunk bounds in the compressed one.

I’ve addressed all these points and a bit more in [this branch|https://github.com/iamaleksey/cassandra/commits/13938-4.0].;;;","16/Jan/20 03:56;djoshi;Hi [~aleksey], Overall the code looks good. Minor nits only. Feel free to make changes on commit.

- {{CompressedInputStream}} - could you pull the resizing multiplier (1.5) out as a constant? I think its used in multiple locations.
- {{CompressedInputStream::chunkBytesRead}} can be package private.
- {{RebufferingInputStream}} - Line 106, the word 'length' has a typo in comment.

+1;;;","16/Jan/20 18:33;aleksey;Thanks [~djoshi]!

Addressed the nits and committed as [602a5eef177ac65020470cb0fcf8d88d820ab888|https://github.com/apache/cassandra/commit/602a5eef177ac65020470cb0fcf8d88d820ab888].

CI results [here|https://circleci.com/workflow-run/43acfc46-861a-478f-adf7-f2d145d43446] and [here|https://circleci.com/workflow-run/99a21fd3-cfc8-49bb-99a8-0b904a20669a]. The existing failures are unrelated and known. One that was suspiciously close is {{org.apache.cassandra.distributed.test.FailingRepairTest}} in both 8 and 11 in-JVM tests, but, again, it's not related, and also reliably passes locally. I will be looking into it, though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexes and UDTs creation should have IF NOT EXISTS on its String representation,CASSANDRA-13935,13107041,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stefan.miklosovic,Kani,Kani,04/Oct/17 19:38,03/Jan/21 17:02,14/Jul/23 05:56,16/Sep/20 14:59,3.0.23,3.11.9,4.0,4.0-beta3,,,Feature/2i Index,Legacy/CQL,,,,0,,,,,"I came across something that bothers me a lot. I'm using snapshots to backup data from my Cassandra cluster in case something really bad happens (like dropping a table or a keyspace).

Exercising the recovery actions from those backups, I discover that the schema put on the file ""schema.cql"" as a result of the snapshot has the ""CREATE IF NOT EXISTS"" for the table, but not for the indexes.

When restoring from snapshots, and relying on the execution of these schemas to build up the table structure, everything seems fine for tables without secondary indexes, but for the ones that make use of them, the execution of these statements fail miserably.

Here I paste a generated schema.cql content for a table with indexes:

CREATE TABLE IF NOT EXISTS keyspace1.table1 (
	id text PRIMARY KEY,
	content text,
	last_update_date date,
	last_update_date_time timestamp)
	WITH ID = f1045fc0-2f59-11e7-95ec-295c3c064920
	AND bloom_filter_fp_chance = 0.01
	AND dclocal_read_repair_chance = 0.1
	AND crc_check_chance = 1.0
	AND default_time_to_live = 8640000
	AND gc_grace_seconds = 864000
	AND min_index_interval = 128
	AND max_index_interval = 2048
	AND memtable_flush_period_in_ms = 0
	AND read_repair_chance = 0.0
	AND speculative_retry = '99PERCENTILE'
	AND caching = { 'keys': 'NONE', 'rows_per_partition': 'NONE' }
	AND compaction = { 'max_threshold': '32', 'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' }
	AND compression = { 'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor' }
	AND cdc = false
	AND extensions = {  };
CREATE INDEX table1_last_update_date_idx ON keyspace1.table1 (last_update_date);

I think the last part should be:

CREATE INDEX IF NOT EXISTS table1_last_update_date_idx ON keyspace1.table1 (last_update_date);

// edit by Stefan Miklosovic

PR: https://github.com/apache/cassandra/pull/731

I have added UDTs as part of this patch as well.","Ubuntu 16.04.2 LTS
java version ""1.8.0_144""
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)",adelapena,blerer,Kani,KurtG,snazy,stefan.miklosovic,varung,,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request #731:
URL: https://github.com/apache/cassandra/pull/731


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Aug/20 15:58;githubbot;600","smiklosovic opened a new pull request #739:
URL: https://github.com/apache/cassandra/pull/739


   … clause


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Sep/20 08:23;githubbot;600","adelapena commented on a change in pull request #731:
URL: https://github.com/apache/cassandra/pull/731#discussion_r487841349



##########
File path: test/unit/org/apache/cassandra/cql3/validation/entities/UFJavaTest.java
##########
@@ -796,4 +801,55 @@ public void testAllNativeTypes() throws Throwable
                            ""AS 'return 0;'"");
         }
     }
+
+    @Test
+    public void testUDFToCqlString()
+    {
+        UDFunction function = UDFunction.create(new FunctionName(""my_ks"", ""my_function""),
+                                                Arrays.asList(ColumnIdentifier.getInterned(""column"", false)),
+                                                Arrays.asList(UTF8Type.instance),
+                                                Int32Type.instance,
+                                                false,
+                                                ""java"",
+                                                ""return 0;"");
+
+        Assert.assertTrue(function.toCqlString(true, true).contains(""CREATE FUNCTION IF NOT EXISTS""));
+        Assert.assertFalse(function.toCqlString(true, false).contains(""CREATE FUNCTION IF NOT EXISTS""));

Review comment:
       Nice tests. Not related to the changes introduced in this ticket but, since we are adding a specific test for the `toCqlString` method for the first time, we could also easily check the remaining argument for the sake of completeness. The argument is not used, so we could just verify that it doesn't have any effect:
   ```
   Assert.assertEquals(function.toCqlString(true, true), function.toCqlString(false, true));
   Assert.assertEquals(function.toCqlString(true, false), function.toCqlString(false, false));
   ```
   The same could be done in `testUDAToCqlString`. Feel free to ignore if you think it doesn't add any value.

##########
File path: test/unit/org/apache/cassandra/cql3/validation/entities/UFJavaTest.java
##########
@@ -796,4 +801,55 @@ public void testAllNativeTypes() throws Throwable
                            ""AS 'return 0;'"");
         }
     }
+
+    @Test
+    public void testUDFToCqlString()
+    {
+        UDFunction function = UDFunction.create(new FunctionName(""my_ks"", ""my_function""),
+                                                Arrays.asList(ColumnIdentifier.getInterned(""column"", false)),
+                                                Arrays.asList(UTF8Type.instance),

Review comment:
       Nit: these could use `Collections.singletonList` instead of `Arrays.asList`.

##########
File path: test/unit/org/apache/cassandra/cql3/validation/entities/UFJavaTest.java
##########
@@ -796,4 +801,55 @@ public void testAllNativeTypes() throws Throwable
                            ""AS 'return 0;'"");
         }
     }
+
+    @Test
+    public void testUDFToCqlString()
+    {
+        UDFunction function = UDFunction.create(new FunctionName(""my_ks"", ""my_function""),
+                                                Arrays.asList(ColumnIdentifier.getInterned(""column"", false)),
+                                                Arrays.asList(UTF8Type.instance),
+                                                Int32Type.instance,
+                                                false,
+                                                ""java"",
+                                                ""return 0;"");
+
+        Assert.assertTrue(function.toCqlString(true, true).contains(""CREATE FUNCTION IF NOT EXISTS""));
+        Assert.assertFalse(function.toCqlString(true, false).contains(""CREATE FUNCTION IF NOT EXISTS""));
+    }
+
+    @Test
+    public void testUDAToCqlString() throws Throwable
+    {
+        // we have to create this function in DB otherwise UDAggregate creation below fails
+        String stateFunctionName = createFunction(KEYSPACE, ""int,int"",
+                                                  ""CREATE OR REPLACE FUNCTION %s(state int, val int)\n"" +
+                                                  ""    CALLED ON NULL INPUT\n"" +
+                                                  ""    RETURNS int\n"" +
+                                                  ""    LANGUAGE java\n"" +
+                                                  ""    AS $$\n"" +
+                                                  ""        return state + val;\n"" +
+                                                  ""    $$;"");
+
+        // Java representation of state function so we can construct aggregate programmatically
+        UDFunction stateFunction = UDFunction.create(new FunctionName(KEYSPACE, stateFunctionName.split(""\\."")[1]),
+                                                     Arrays.asList(ColumnIdentifier.getInterned(""state"", false),
+                                                                   ColumnIdentifier.getInterned(""val"", false)),
+                                                     Arrays.asList(Int32Type.instance, Int32Type.instance),
+                                                     Int32Type.instance,
+                                                     true,
+                                                     ""java"",
+                                                     ""return state + val;"");
+
+        UDAggregate aggregate = UDAggregate.create(Arrays.asList(stateFunction),
+                                                   new FunctionName(KEYSPACE, ""my_aggregate""),
+                                                   Arrays.asList(Int32Type.instance),

Review comment:
       Nit: this could use `Collections.singletonList` instead of `Arrays.asList`.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/20 11:32;githubbot;600","smiklosovic commented on a change in pull request #731:
URL: https://github.com/apache/cassandra/pull/731#discussion_r487845386



##########
File path: test/unit/org/apache/cassandra/cql3/validation/entities/UFJavaTest.java
##########
@@ -796,4 +801,55 @@ public void testAllNativeTypes() throws Throwable
                            ""AS 'return 0;'"");
         }
     }
+
+    @Test
+    public void testUDFToCqlString()
+    {
+        UDFunction function = UDFunction.create(new FunctionName(""my_ks"", ""my_function""),
+                                                Arrays.asList(ColumnIdentifier.getInterned(""column"", false)),
+                                                Arrays.asList(UTF8Type.instance),
+                                                Int32Type.instance,
+                                                false,
+                                                ""java"",
+                                                ""return 0;"");
+
+        Assert.assertTrue(function.toCqlString(true, true).contains(""CREATE FUNCTION IF NOT EXISTS""));
+        Assert.assertFalse(function.toCqlString(true, false).contains(""CREATE FUNCTION IF NOT EXISTS""));

Review comment:
       added thanks 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/20 11:38;githubbot;600","smiklosovic closed pull request #731:
URL: https://github.com/apache/cassandra/pull/731


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/20 11:40;githubbot;600","smiklosovic closed pull request #739:
URL: https://github.com/apache/cassandra/pull/739


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/20 11:40;githubbot;600","smiklosovic closed pull request #740:
URL: https://github.com/apache/cassandra/pull/740


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/20 11:40;githubbot;600",,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/17 13:24;Kani;13935-3.0.txt;https://issues.apache.org/jira/secure/attachment/12890516/13935-3.0.txt","05/Oct/17 13:30;Kani;13935-3.11.txt;https://issues.apache.org/jira/secure/attachment/12890517/13935-3.11.txt","05/Oct/17 13:39;Kani;13935-trunk.txt;https://issues.apache.org/jira/secure/attachment/12890519/13935-trunk.txt",,,,,,,,,,,,,,,,,3.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 20:16:36 UTC 2020,,,,,,,,,,"0|i3kvtr:",9223372036854775807,3.0.14,3.11.0,,,,,,,adelapena,blerer,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/277d83961c7332941a9339bf890dbe0c89029ae3,,,,,,,,,The patches modify the unit tests to verify the new behavior. ,,,,,"04/Oct/17 19:39;Kani;This seems to happen with CUSTOM INDEXES too;;;","05/Oct/17 13:25;Kani;Patch for cassandra version 3.0 attached, You can also see changes on my forked repository: https://github.com/javiercanillas/cassandra/tree/13935-3.0;;;","05/Oct/17 13:33;Kani;Patch for cassandra version 3.11 attached (although you can fast-foward the 3.0 patch to 3.11), You can also see changes on my forked repository: https://github.com/javiercanillas/cassandra/tree/13935-3.11;;;","05/Oct/17 13:34;Kani;Attached you may find patches. git comment does not include reviewer, feel free to change it. See submitted files comments if you prefer checking code from my public forked repository;;;","05/Oct/17 13:40;Kani;Patch for cassandra trunk version attached  (although you can fast-foward the 3.0 patch), You can also see changes on my forked repository: https://github.com/javiercanillas/cassandra/tree/13935-trunk;;;","16/Mar/18 04:03;KurtG;Thanks Javier, change seems straightforward enough to me. I've kicked off unit tests.

|[3.0|https://circleci.com/gh/kgreav/cassandra/142]|[3.11|https://circleci.com/gh/kgreav/cassandra/143]|[trunk|https://circleci.com/gh/kgreav/cassandra/144]|;;;","06/Jun/18 03:07;KurtG;Looks like this breaks {{indexWithfailedInitializationIsNotQueryableAfterPartialRebuild}} on trunk which didn't exist on 3.11. Need to check if it's important. Error was:
{code}
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.index.SecondaryIndexManagerTest.indexWithfailedInitializationIsNotQueryableAfterPartialRebuild(SecondaryIndexManagerTest.java:475)
{code};;;","06/Jun/18 22:47;Kani;[~KurtG] where are you testing this? I might try and fix the problem. Tell me which branch please.;;;","12/Jun/18 02:27;KurtG;[~Kani] that was on the trunk unit tests I had run above, but it appears flaky after multiple re-runs. I've rebased the trunk patch and kicked off the tests again [utests|https://circleci.com/gh/kgreav/workflows/cassandra/tree/13935-trunk]
;;;","28/Aug/20 13:13;stefan.miklosovic;Same holds for UDTs when a snapshot is taken, there is not IF NOT EXISTS there.;;;","28/Aug/20 13:20;stefan.miklosovic;I ll hijack this issue due to inactivity, feel free to raise your voice if you want to still finish this.;;;","31/Aug/20 16:00;stefan.miklosovic;PR: https://github.com/apache/cassandra/pull/731

I have added UDTs as well. I have consolidated all ""CQL toString"". The default behaviour is as it was and it is not changed, this just improves the output for cases like snapshots.;;;","31/Aug/20 16:01;stefan.miklosovic;[~snazy] would you please make a review of this code? I saw you as the author of SchemaCQLHelper as part of CASSANDRA-14825;;;","02/Sep/20 18:26;stefan.miklosovic;[~blerer] would you mind to review please? I need 2 reviewers so one more would be awesome!;;;","03/Sep/20 08:20;blerer;[~stefan.miklosovic] sure. I should be able to do that next week. ;;;","07/Sep/20 09:51;blerer;The patch for 4.0 looks good to me. I think we would need one for 3.0 and 3.11.;;;","07/Sep/20 12:02;snazy;Sorry, I'll probably not have time to review.;;;","08/Sep/20 08:23;stefan.miklosovic;PR for 3.11 https://github.com/apache/cassandra/pull/739;;;","08/Sep/20 11:21;stefan.miklosovic;PR for 3.0 https://github.com/apache/cassandra/pull/740;;;","08/Sep/20 11:22;stefan.miklosovic;[~blerer] please review again, is there anything else I can do in this matter?;;;","08/Sep/20 11:31;blerer;Thanks [~stefan.miklosovic]. The code looks good.
I will run CI on your branches.

[~adelapena] would you have sometime to do a second review?  ;;;","08/Sep/20 13:59;blerer;| [3.0 CI|https://ci-cassandra.apache.org/job/Cassandra-devbranch/13/] | [3.11 CI|https://ci-cassandra.apache.org/job/Cassandra-devbranch/8/] | [4.0 CI|https://ci-cassandra.apache.org/job/Cassandra-devbranch/9/]|
;;;","09/Sep/20 08:49;blerer;The 3.11 and 4.0 branches CI looks good. Something weird occured with the 3.0. Relaunching it.;;;","10/Sep/20 07:49;blerer;CI results looks good. +1 on my side
;;;","11/Sep/20 12:17;adelapena;[~stefan.miklosovic] [~blerer] Changes look good to me too, +1. The only thing to mention is that, if I'm right, for UDF/UDA in trunk we don't seem to have any call passing {{true}} to the new {{ifNotExists}} argument in {{toCqlString}}, nor even in tests. I don't think that's a problem, given how simple the change is.

 ;;;","11/Sep/20 12:49;blerer;Thanks [~adelapena]. It is a valid point.

[~stefan.miklosovic] could you add a test for that in {{UFJavaTest}}?;;;","11/Sep/20 21:15;stefan.miklosovic;[~blerer] Test added in UFJavaTest.;;;","11/Sep/20 22:04;stefan.miklosovic;I havent found any way how to test that in 3.11. I could not find a place where we are producing CQL from UDF nor UDA. There is just a ColumnFamilyCQLHelper (something like that) and it does not dump functions nor aggregates. Aggregates and functions are not associated with a CF but with a keyspace and there isnt any ""toCQL"" on a function (hence aggregate) either. UDF and UDA are not part of CQL schema dump upon taking a snapshot as the original goal of this ticket was not to dump them anyway, just indexes are udts.;;;","16/Sep/20 05:44;stefan.miklosovic;[~blerer] I believe everything is in place. Are you ok with merging?;;;","16/Sep/20 07:37;blerer;Sorry, [~stefan.miklosovic]. I was busy with something else. I will commit today.
;;;","16/Sep/20 14:59;blerer;Patch committed into branch cassandra-3.0 at 277d83961c7332941a9339bf890dbe0c89029ae3 and merged into cassandra-3.11 and trunk;;;","16/Sep/20 20:04;stefan.miklosovic;Hi [~blerer],

there are three branches, trunk, 3.11 and 3.0, each branch does something different. Please resolve this.

https://github.com/apache/cassandra/pull/731 this is for trunk

https://github.com/apache/cassandra/pull/739 this is for 3.11

https://github.com/apache/cassandra/pull/740 this is for 3.0

My comments from 8th September are mentioning them.

Each branch is special for each Cassandra branch.
;;;","16/Sep/20 20:16;stefan.miklosovic;Sorry by bad. It IS there, as merge commit. That is very confusing.

Thank you very much.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle mutateRepaired failure in nodetool verify,CASSANDRA-13933,13106871,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,marcuse,marcuse,04/Oct/17 06:31,15/May/20 08:03,14/Jul/23 05:56,07/Feb/18 08:13,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,,,0,lhf,,,,See comment here: https://issues.apache.org/jira/browse/CASSANDRA-13922?focusedCommentId=16189875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16189875,,jjordan,KurtG,marcuse,mychal,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-3.0.txt;https://issues.apache.org/jira/secure/attachment/12909160/CASSANDRA-13933-3.0.txt","04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-3.11.txt;https://issues.apache.org/jira/secure/attachment/12909161/CASSANDRA-13933-3.11.txt","04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-trunk.txt;https://issues.apache.org/jira/secure/attachment/12909162/CASSANDRA-13933-trunk.txt",,,,,,,,,,,,,,,,,3.0,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 08:13:31 UTC 2018,,,,,,,,,,"0|i3kusf:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"31/Jan/18 15:29;sumanth.pasupuleti;[~krummas] I would like to work on submitting a patch for this. Would you be able to assign this to me, and mark the status to be InProgress? I am not sure if I have access to do it.;;;","31/Jan/18 15:34;marcuse;also note CASSANDRA-14201;;;","01/Feb/18 01:18;sumanth.pasupuleti;[~krummas] I am done with my changes and should be ready to submit the commit for review tomorrow. However, your commit (thanks for pointing me to 14201) [https://github.com/krummas/cassandra/commit/1489a5b66f60c733aaa1749d2d2ad36457e21824] would have merge conflicts with my changes. Let me know if I should wait for your changes to be in trunk, and submit changes on top of yours.;;;","01/Feb/18 12:26;marcuse;no it is fine, I can rebase on top of your patch;;;","01/Feb/18 23:38;sumanth.pasupuleti;[~krummas] Submitted the patch. Please let me know if you have any comments.

Ended up going with an info log as against warn log (where I can potentially log the exception details). Curious to know your thoughts.;;;","02/Feb/18 15:27;marcuse;Thanks for the patch [~sumanth.pasupuleti]

I think this looks good in general, small comments;
* I don't think we should declare {{throws CorruptSSTableException}} since it is a {{RuntimeException}}
* we could remove the {{throws IOException}} from {{markAndThrow}} (and its callers) now since we always throw {{CorruptSSTableException}} instead

We need to apply this to 3.0, 3.11 and trunk, can you provide patches?;;;","02/Feb/18 16:31;jjordan;This is just a drive by comment, but please be wary of changing the exception signature of exposed functions in 3.x, as depending on what is involved here it could break external plugin's/extensions.  (Probably not relevant here since this is in the repair path, which I don't think we have any extension points in, but just like to remind people of that.);;;","02/Feb/18 20:21;sumanth.pasupuleti;Thanks [~krummas] . Will be incorporating the comments and submitting patches for 3.0 and 3.11 too.

[~jjordan] this is good to know, especially for a newbie in this community like me. Will keep this in mind. Thank you!;;;","04/Feb/18 21:35;sumanth.pasupuleti;[~krummas] attached patches for 3.0, 3.11 and trunk;;;","07/Feb/18 08:13;marcuse;committed as {{7885a703d6dae8c3c6e5a6af632c6a23342593fc}}

note that I decided to keep the method signatures in 3.0 and 3.11 to avoid breaking anyone depending on this, the {{throws IOException}} is removed in trunk

thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid grabbing the read lock when checking LCS fanout and if compaction strategy should do defragmentation,CASSANDRA-13930,13106677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,03/Oct/17 11:32,15/May/20 08:06,14/Jul/23 05:56,10/Oct/17 11:00,3.11.2,4.0,4.0-alpha1,,,,Local/Compaction,,,,,0,,,,,"We grab the read lock when checking whether the compaction strategy benefits from defragmentation, avoid that.",,eduard.tudenhoefner,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 10 11:00:55 UTC 2017,,,,,,,,,,"0|i3ktlj:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"03/Oct/17 11:32;marcuse;https://github.com/krummas/cassandra/commits/marcuse/defrag
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/356/
https://circleci.com/gh/krummas/cassandra/137;;;","07/Oct/17 07:05;jjirsa;Patch looks good. Dtests look good, -circleci won’t load for me- circle looks good. 

What do you think about a similar fix for fanout? Maybe not worth the effort, since it's not in as hot of a path, and it's more likely to change than defrag? 

+1 to commit ;;;","09/Oct/17 08:35;marcuse;bq. What do you think about a similar fix for fanout? 
makes sense, at least it doesn't hurt (until we want to have LCS change fanout dynamically or something)

pushed up a new commit to the same branch and rerunning tests
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/363/
https://circleci.com/gh/krummas/cassandra/148;;;","09/Oct/17 17:45;jjirsa;lgtm if dtests are happy (expect it should be fine). 
;;;","09/Oct/17 18:57;eduard.tudenhoefner;changes LGTM. Looks like majority of the failed dtests are because they couldn't clone the repo.;;;","10/Oct/17 11:00;marcuse;and committed as {{f3cf1c019e0298dd04f6a0d7396b5fe4a93e6f9a}}, thanks! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTree$Builder / io.netty.util.Recycler$Stack leaking memory,CASSANDRA-13929,13106635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,tsteinmaurer,tsteinmaurer,03/Oct/17 07:58,13/Oct/20 12:34,14/Jul/23 05:56,08/Jun/18 19:12,3.11.3,,,,,,Legacy/Core,,,,,2,,,,,"Different to CASSANDRA-13754, there seems to be another memory leak in 3.11.0+ in BTree$Builder / io.netty.util.Recycler$Stack.

* heap utilization increase after upgrading to 3.11.0 => cassandra_3.11.0_min_memory_utilization.jpg
* No difference after upgrading to 3.11.1 (snapshot build) => cassandra_3.11.1_snapshot_heaputilization.png; thus most likely after fixing CASSANDRA-13754, more visible now
* MAT shows io.netty.util.Recycler$Stack as top contributing class => cassandra_3.11.1_mat_dominator_classes.png
* With -Xmx8G (CMS) and our load pattern, we have to do a rolling restart after ~ 72 hours

Verified the following fix, namely explicitly unreferencing the _recycleHandle_ member (making it non-final). In _org.apache.cassandra.utils.btree.BTree.Builder.recycle()_
{code}
        public void recycle()
        {
            if (recycleHandle != null)
            {
                this.cleanup();
                builderRecycler.recycle(this, recycleHandle);
                recycleHandle = null; // ADDED
            }
        }
{code}

Patched a single node in our loadtest cluster with this change and after ~ 10 hours uptime, no sign of the previously offending class in MAT anymore => cassandra_3.11.1_mat_dominator_classes_FIXED.png

Can' say if this has any other side effects etc., but I doubt.",,bradfordcp,carlo_4002,cscetbon,hkroger,jasobrown,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,kkierer,KurtG,laxmikant99,Lerh Low,norman,ostefano,sayap,szhou,tjake,tsteinmaurer,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15430,,,CASSANDRA-9766,,,,,,,,,,"03/Oct/17 07:59;tsteinmaurer;cassandra_3.11.0_min_memory_utilization.jpg;https://issues.apache.org/jira/secure/attachment/12890116/cassandra_3.11.0_min_memory_utilization.jpg","05/Oct/17 08:24;tsteinmaurer;cassandra_3.11.1_NORECYCLE_memory_utilization.jpg;https://issues.apache.org/jira/secure/attachment/12890495/cassandra_3.11.1_NORECYCLE_memory_utilization.jpg","03/Oct/17 08:03;tsteinmaurer;cassandra_3.11.1_mat_dominator_classes.png;https://issues.apache.org/jira/secure/attachment/12890120/cassandra_3.11.1_mat_dominator_classes.png","03/Oct/17 08:07;tsteinmaurer;cassandra_3.11.1_mat_dominator_classes_FIXED.png;https://issues.apache.org/jira/secure/attachment/12890121/cassandra_3.11.1_mat_dominator_classes_FIXED.png","03/Oct/17 08:02;tsteinmaurer;cassandra_3.11.1_snapshot_heaputilization.png;https://issues.apache.org/jira/secure/attachment/12890119/cassandra_3.11.1_snapshot_heaputilization.png","05/Feb/18 07:53;tsteinmaurer;cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png;https://issues.apache.org/jira/secure/attachment/12909189/cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png","23/Feb/18 10:27;tsteinmaurer;cassandra_heapcpu_memleak_patching_test_30d.png;https://issues.apache.org/jira/secure/attachment/12911703/cassandra_heapcpu_memleak_patching_test_30d.png","13/Feb/18 06:35;jay.zhuang;dtest_example_80_request.png;https://issues.apache.org/jira/secure/attachment/12910331/dtest_example_80_request.png","13/Feb/18 06:41;jay.zhuang;dtest_example_80_request_fix.png;https://issues.apache.org/jira/secure/attachment/12910333/dtest_example_80_request_fix.png","13/Feb/18 05:09;jay.zhuang;dtest_example_heap.png;https://issues.apache.org/jira/secure/attachment/12910320/dtest_example_heap.png","14/Feb/18 12:46;tsteinmaurer;memleak_heapdump_recyclerstack.png;https://issues.apache.org/jira/secure/attachment/12910578/memleak_heapdump_recyclerstack.png",,,,,,,,,11.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 08 19:11:41 UTC 2018,,,,,,,,,,"0|i3ktc7:",9223372036854775807,3.11.0,3.8,,,,,jasobrown,,jasobrown,,,Normal,,3.11.0,,,,,,,,,,,,,,,,,"03/Oct/17 11:47;tjake;The recycler is meant to cache objects for reuse. By nulling the handler you are effectively invalidating the cache every time. 

I don't think this is a leak but perhaps we should limit this cache to hold less items. (see the recycler constructor);;;","03/Oct/17 12:09;tsteinmaurer;[~tjake]: Thanks for the feedback about invalidating the cache. Not sure what actually is cached here, but without nulling the reference, I do see e.g. ~ 770K BTree$Builder instances on the heap. Infinite caching still sounds like a memory leak to me, but that's nitpicking now. :-)

Thanks again.;;;","03/Oct/17 13:39;tjake;The default for recycler is 32k instances per thread. So perhaps change this to 8192 per thread and see if that makes a difference.;;;","03/Oct/17 16:53;tsteinmaurer;I can try a different max value, but what is supposed to be cached here and what area should be suffering without the cache? The reason why I'm asking is, that in our 9 node cluster, a single node is patched with the discussed change. I don't see any difference in CPU usage, GC, request latency etc., thus potentially looking at the wrong metrics.;;;","03/Oct/17 17:27;tjake;This was part of CASSANDRA-9766 and addresses one of the main allocation culprits during streaming.;;;","04/Oct/17 06:11;tsteinmaurer;Ok, if this sort of caching is entirely only useful during streaming, this somehow explains why I do not see any difference between the nodes here, cause they process regular business and no repair, bootstrapping etc, thus I can't comment on the performance gain with the cache (possibly this has been tested in CASSANDRA-9766 anyway).

If the cache is useful for streaming, it still would definitely make sense IMHO, to make the max capacity configurable (with a somewhat useful small default value) via a system property, cassandra.yaml or whatever place is preferred here, cause ~ 2,4G+ heap usage for this sort of cache at -XmX8G does not make sense or perhaps the majority of Cassandra users don't scale out with smaller sized machines anymore.

As you have mentioned *per thread*. I guess we are talking about the number of threads serving client requests, aka e.g. {{native_transport_max_threads}}? If so, there should be somewhere a clear pointer in a comment, documentation etc., that heap usage is directly related to number of threads * configurable max size;;;","05/Oct/17 08:27;tsteinmaurer;Patched our 9 node loadtest cluster with disabled recycling. Flat AVG heap utilization in a time-frame of 24hrs. => cassandra_3.11.1_NORECYCLE_memory_utilization.jpg

I don't see any negative impact for normal operations (haven't tested streaming) running without this cache, so this is probably a cost vs. benefit discussion, but e.g. in relation to other in-memory data structures for speeding up things (key cache, bloom filter), this kind of cache is questionable, IMHO.
;;;","13/Oct/17 12:20;tsteinmaurer;Stable for a week now in our 9 node (m4.xlarge) loadtest cluster from a heap perspective with entirely disabling the recycler cache.

Any ideas if we can expect something in context of this ticket for 3.11.2? Thanks!

;;;","11/Dec/17 13:36;tsteinmaurer;Yet another ping after 2 months of silence and the issue still being unassigned. Is this something which will be handled in the 3.11 series? Thanks!;;;","11/Dec/17 15:31;mshuler;You may have some better success getting eyes on this JIRA by attaching a proper patch for the 3.11 branch (and a separate trunk patch, if merge up isn't clean). You can do this with a simple text file attachment of the diff, or linking to a github branch.

Bonus points for adding a test that reproduces the problem/fix, as well as getting test run through circleci.

This JIRA can be assigned to yourself as the author and you can set the status to ""Patch Available"" when there's an actual patch/branch to review. Then you can possibly seek out a reviewer on the dev@ mailing list, if it's urgent.

It looks like there are currently 115 Patch Available tickets, so it may still take some time, but getting this set up for someone else to review would be a great step in getting the process rolling further than just a comment.
https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20AND%20status%20%3D%20%22Patch%20Available%22;;;","11/Dec/17 15:33;mshuler;I set the fix version to {{3.11.x}} to indicate this is intended for the 3.11 series for you.;;;","22/Jan/18 19:32;tsteinmaurer;[~mshuler], my patch would look like as mentioned in the description, but I'm pretty sure it will get rejected, cause [~tjake] mentioned that this all has been added to cache something, especially useful for streaming, if I remember correctly. Due to lack of knowledge about the inner workings here, I can't provide anything more useful. Our loadtest environment is running a locally patched build with the nulling approach.;;;","25/Jan/18 20:54;jjirsa;I chatted offline with Scott (who is far more familiar with the netty project than I am), and he noted that they've fixed a few recent bugs in that area of the code that COULD resolve leaks. I'm not confident this is actually a leak, vs just the recycler working as intended (but needing to be tuned), but perhaps we can consider bumping netty anyway (at least in trunk this is an easy decision, but it'd be interesting to find if netty 4.1.20 fixes the behavior you see in unpatched 3.11.1 [~tsteinmaurer] ).

 ;;;","25/Jan/18 21:46;tsteinmaurer;[~jjirsa], thanks for the follow-up. Looks like a simple netty jar file replacement works, at least Cassandra starts up without any exceptions. I have now 1 of 9 nodes running with 3.11.1 unpatched + Netty 4.1.20.;;;","25/Jan/18 21:51;jjirsa;h3. [netty-4.0.55|https://github.com/netty/netty/releases/tag/netty-4.0.55.Final] may be a better test, since 3.11.1 is running netty 4.0.44 now, 4.0.55 would be on the same major and appears to have many of the similar fixes to Recycler.;;;","25/Jan/18 21:59;tsteinmaurer;Makes sense. Restarted the single node with Netty 4.0.55 now.;;;","31/Jan/18 13:05;tsteinmaurer;[~jjirsa], had 3.11.1 with Netty 4.0.55 on one node in parallel with 3.11.1 incl. ""nulling patch"" and Netty 4.0.44 on other nodes running over the weekend. Unpatched combo with Netty 4.0.55 unfortunately still shows an AVG heap utilization increase over 72hrs, while the patched show a constant AVG heap usage.;;;","31/Jan/18 21:16;zznate;If [~norman] is around, would be great to get his input on whether this might be a leak or tuning issue with recycler. Thanks for your patience and willingness to test this out [~tsteinmaurer]. ;;;","31/Jan/18 23:14;jay.zhuang;Hi [~tsteinmaurer], as [~tjake] suggested, would you please try to reduce the Recycler Capacity Per Thread ([Recycler.java:64|https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/Recycler.java#L64]), just restart the node with JVM option:
{noformat}
-Dio.netty.recycler.maxCapacityPerThread=1024
{noformat}

;;;","01/Feb/18 08:23;norman;yeah 4.0.55.Final should have the ""fix"" as well:

 

[https://github.com/netty/netty/commit/b386ee3eaf35abd5072992d626de6ae2ccadc6d9#diff-23eafd00fcd66829f8cce343b26c236a]

 

That said maybe there are other issues. Would it be possible to share a heap-dump ?;;;","01/Feb/18 13:30;tsteinmaurer;[~jay.zhuang], will let the node (1 out of 9) - previously, manually upgraded to Netty 4.0.55 - running with your mentioned Netty JVM option over the weekend. Thanks.;;;","01/Feb/18 15:18;norman;[~tsteinmaurer] what about a heap dump ? Is this something you could provide ?;;;","05/Feb/18 07:57;tsteinmaurer;I have attached a new heap utilization chart for our 9 node loadtest environment.
 !cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png|width=300! 

The marked node is running with the following configuration:
* Cassandra 3.11.1 public release
* Netty 4.0.55 (manually upgraded from 4.0.44 by simply replacing the jar file in cassandra/lib)
* -Dio.netty.recycler.maxCapacityPerThread=1024

This still results in a slow increase in AVG heap utilization

The other 8 nodes are running with 3.11.2 built from source with my recycleHandle ""nulling"" patch.

[~norman], I need to internally check if we need to do that via some sort of NDA then. Will report back. Thanks.;;;","13/Feb/18 06:20;jay.zhuang;I think it's just caching the largest tree ever built ({{[BTree.java:907|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/btree/BTree.java#L907]}}) and getting bigger and bigger.

Here is an example with a large number of {{IN}} in a CQL, which causes high heap usage for {{$Stack}} :
{noformat}
session.execute_async(""select id from test.users where id in (%s)"" % ','.join(map(str, range(100000))))
{noformat}
Here is a dtest to reproduce that: {{[large_in_test.py|https://github.com/cooldoger/cassandra-dtest/blob/13929/large_in_test.py]}}
 {{$Stack}} uses 25% 10M heap: {{10M = 10 (request num) * 100K (element) * 8 (obj size)}}
 !dtest_example_heap.png! 
 We could increase heap size and then increase request number, element number gradually to make {{$Stack}} to 80%+ heap usage. And I'm sure there're other use cases could build large BTree. Seems the {{$Stack}} is unable to release until the {{Thread}} is released and by using {{ThreadPoolExecutor}}, the thread is not released.

I'd like to propose the following patch which cleans large BTree builder ({{> 1k elements}}) while recycling:
|Branch|uTest|
|[13929-3.11|https://github.com/cooldoger/cassandra/tree/13929-3.11]|[!https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11]|;;;","13/Feb/18 06:41;jay.zhuang;After increasing the heap size from 1G to 4G, it could support 80 such requests at the same time ({{[large_in_test.py:12|https://github.com/cooldoger/cassandra-dtest/blob/13929/large_in_test.py#L12]}}), then the {{$Stack}} uses about 70% heap:
 !dtest_example_80_request.png! 

Tried the patch, it did fix the problem:
 !dtest_example_80_request_fix.png! ;;;","14/Feb/18 07:51;jjirsa;[~tsteinmaurer] - available to test?

Patch looks straight forward to me, but any volunteers to review? Given its impact, probably deserves a more thorough review than I have time for. 


;;;","14/Feb/18 07:52;norman;[~jay.zhuang] I would be interested what is contained in the `Stack` itself;;;","14/Feb/18 12:50;tsteinmaurer;[~norman], a concrete example for a *single* Recycler$Stack instance from our loadtest.
!memleak_heapdump_recyclerstack.png!

Looks like we have a lot of largish 32K object arrays, which seem to be empty as shallow heap = retained heap for this object arrays. So, a quick look on the patch of [~jay.zhuang], this might help. [~jjirsa], will re-patch a single node and report back after a considerable uptime.;;;","14/Feb/18 13:12;norman;[~tsteinmaurer] yeah thanks I see... So looks more like a misusage for me then a netty bug. Cassandra may also consider to configure the `Recycler` with a more sane default value for this use-case (via the constructor).;;;","19/Feb/18 20:57;jay.zhuang;Seems like the performance is better without {{Recycler}}. Here is microbench test result to build a BTree with and without {{Recycler}} ([13929-3.11-perf|https://github.com/cooldoger/cassandra/tree/13929-3.11-perf]) (The score is operation per ms, higher is better)
{noformat}
     [java] Benchmark                      (dataSize)          (treeBuilder)   Mode  Cnt      Score       Error   Units
     [java] BTreeBuildBench.buildTreeTest           1  treeBuilderRecycleAdd  thrpt    6  23112.102 ? 17522.471  ops/ms
     [java] BTreeBuildBench.buildTreeTest           1         treeBuilderAdd  thrpt    6  46275.541 ? 60422.458  ops/ms
     [java] BTreeBuildBench.buildTreeTest           2  treeBuilderRecycleAdd  thrpt    6  23588.176 ? 16372.260  ops/ms
     [java] BTreeBuildBench.buildTreeTest           2         treeBuilderAdd  thrpt    6  42838.298 ? 25339.870  ops/ms
     [java] BTreeBuildBench.buildTreeTest           5  treeBuilderRecycleAdd  thrpt    6  24358.111 ? 24339.382  ops/ms
     [java] BTreeBuildBench.buildTreeTest           5         treeBuilderAdd  thrpt    6  60074.551 ? 47329.418  ops/ms
     [java] BTreeBuildBench.buildTreeTest          10  treeBuilderRecycleAdd  thrpt    6  21412.578 ?  6072.160  ops/ms
     [java] BTreeBuildBench.buildTreeTest          10         treeBuilderAdd  thrpt    6  50862.304 ? 30597.546  ops/ms
     [java] BTreeBuildBench.buildTreeTest          20  treeBuilderRecycleAdd  thrpt    6  15871.754 ?  5036.739  ops/ms
     [java] BTreeBuildBench.buildTreeTest          20         treeBuilderAdd  thrpt    6  33699.725 ? 10857.366  ops/ms
     [java] BTreeBuildBench.buildTreeTest          40  treeBuilderRecycleAdd  thrpt    6   5168.225 ?  1212.571  ops/ms
     [java] BTreeBuildBench.buildTreeTest          40         treeBuilderAdd  thrpt    6   8806.838 ?  7736.485  ops/ms
     [java] BTreeBuildBench.buildTreeTest         100  treeBuilderRecycleAdd  thrpt    6   2114.218 ?   639.589  ops/ms
     [java] BTreeBuildBench.buildTreeTest         100         treeBuilderAdd  thrpt    6   3213.333 ?   486.126  ops/ms
     [java] BTreeBuildBench.buildTreeTest        1000  treeBuilderRecycleAdd  thrpt    6    335.523 ?   101.230  ops/ms
     [java] BTreeBuildBench.buildTreeTest        1000         treeBuilderAdd  thrpt    6    386.678 ?   333.534  ops/ms
     [java] BTreeBuildBench.buildTreeTest       10000  treeBuilderRecycleAdd  thrpt    6     35.644 ?    32.171  ops/ms
     [java] BTreeBuildBench.buildTreeTest       10000         treeBuilderAdd  thrpt    6     44.250 ?     8.180  ops/ms
     [java] BTreeBuildBench.buildTreeTest      100000  treeBuilderRecycleAdd  thrpt    6      3.073 ?     2.165  ops/ms
     [java] BTreeBuildBench.buildTreeTest      100000         treeBuilderAdd  thrpt    6      4.651 ?     4.137  ops/ms
{noformat}
And I think the performance gain for CASSANDRA-9766 is not because of the {{Recycler}} for BTree builder.

With {{Recycler}}, it does reduce the GC by reserving the memory. But for P95 and sometimes P99, {{noRecycler}} is still better. Here is the test result with percentiles (score is time per operation, so smaller is better):
{noformat}
     [java] Benchmark                                            (dataSize)          (treeBuilder)    Mode       Cnt          Score       Error  Units
     [java] BTreeBuildBench.buildTreeTest                                 1  treeBuilderRecycleAdd  sample   9244739       1435.033 ?   111.857  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             1  treeBuilderRecycleAdd  sample                   92.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             1  treeBuilderRecycleAdd  sample                  638.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             1  treeBuilderRecycleAdd  sample                 1688.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             1  treeBuilderRecycleAdd  sample                 2292.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             1  treeBuilderRecycleAdd  sample                 4544.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            1  treeBuilderRecycleAdd  sample                13792.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           1  treeBuilderRecycleAdd  sample               124233.984              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             1  treeBuilderRecycleAdd  sample            104202240.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 1         treeBuilderAdd  sample   9334292        880.361 ?    99.171  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             1         treeBuilderAdd  sample                   74.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             1         treeBuilderAdd  sample                  177.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             1         treeBuilderAdd  sample                  557.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             1         treeBuilderAdd  sample                 2376.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             1         treeBuilderAdd  sample                 9248.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            1         treeBuilderAdd  sample                19200.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           1         treeBuilderAdd  sample                92489.050              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             1         treeBuilderAdd  sample             66256896.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 2  treeBuilderRecycleAdd  sample   9066713       1655.971 ?   127.345  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             2  treeBuilderRecycleAdd  sample                  101.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             2  treeBuilderRecycleAdd  sample                  756.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             2  treeBuilderRecycleAdd  sample                 1952.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             2  treeBuilderRecycleAdd  sample                 2684.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             2  treeBuilderRecycleAdd  sample                 5696.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            2  treeBuilderRecycleAdd  sample                16272.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           2  treeBuilderRecycleAdd  sample               189176.730              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             2  treeBuilderRecycleAdd  sample            124387328.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 2         treeBuilderAdd  sample   9816463       1264.312 ?   161.134  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             2         treeBuilderAdd  sample                   91.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             2         treeBuilderAdd  sample                  209.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             2         treeBuilderAdd  sample                  579.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             2         treeBuilderAdd  sample                 2096.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             2         treeBuilderAdd  sample                 8104.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            2         treeBuilderAdd  sample                17088.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           2         treeBuilderAdd  sample               227162.522              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             2         treeBuilderAdd  sample             96731136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 5  treeBuilderRecycleAdd  sample   8811507       2044.981 ?   142.626  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             5  treeBuilderRecycleAdd  sample                  126.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             5  treeBuilderRecycleAdd  sample                  934.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             5  treeBuilderRecycleAdd  sample                 2384.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             5  treeBuilderRecycleAdd  sample                 3120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             5  treeBuilderRecycleAdd  sample                 5808.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            5  treeBuilderRecycleAdd  sample                17280.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           5  treeBuilderRecycleAdd  sample               312320.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             5  treeBuilderRecycleAdd  sample            117702656.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 5         treeBuilderAdd  sample   9992328       1044.948 ?   131.496  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             5         treeBuilderAdd  sample                   90.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             5         treeBuilderAdd  sample                  260.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             5         treeBuilderAdd  sample                  607.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             5         treeBuilderAdd  sample                 2156.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             5         treeBuilderAdd  sample                 8384.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            5         treeBuilderAdd  sample                17856.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           5         treeBuilderAdd  sample                98048.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             5         treeBuilderAdd  sample            132382720.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                10  treeBuilderRecycleAdd  sample   8757941       1896.943 ?   116.106  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            10  treeBuilderRecycleAdd  sample                  144.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            10  treeBuilderRecycleAdd  sample                  917.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            10  treeBuilderRecycleAdd  sample                 2152.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            10  treeBuilderRecycleAdd  sample                 2980.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            10  treeBuilderRecycleAdd  sample                 6648.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           10  treeBuilderRecycleAdd  sample                17056.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          10  treeBuilderRecycleAdd  sample               260050.739              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            10  treeBuilderRecycleAdd  sample             60030976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                10         treeBuilderAdd  sample   9456081       1066.351 ?   109.690  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            10         treeBuilderAdd  sample                  127.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            10         treeBuilderAdd  sample                  333.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            10         treeBuilderAdd  sample                  534.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            10         treeBuilderAdd  sample                 1884.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            10         treeBuilderAdd  sample                 7264.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           10         treeBuilderAdd  sample                17760.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          10         treeBuilderAdd  sample               123570.150              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            10         treeBuilderAdd  sample             92274688.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                20  treeBuilderRecycleAdd  sample   8754165       2601.277 ?   131.160  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            20  treeBuilderRecycleAdd  sample                  246.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            20  treeBuilderRecycleAdd  sample                 1696.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            20  treeBuilderRecycleAdd  sample                 3680.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            20  treeBuilderRecycleAdd  sample                 4472.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            20  treeBuilderRecycleAdd  sample                 6864.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           20  treeBuilderRecycleAdd  sample                18720.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          20  treeBuilderRecycleAdd  sample               229269.350              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            20  treeBuilderRecycleAdd  sample            166461440.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                20         treeBuilderAdd  sample  10323487       1596.081 ?   150.936  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            20         treeBuilderAdd  sample                  230.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            20         treeBuilderAdd  sample                  627.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            20         treeBuilderAdd  sample                 1019.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            20         treeBuilderAdd  sample                 2500.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            20         treeBuilderAdd  sample                 7784.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           20         treeBuilderAdd  sample                19584.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          20         treeBuilderAdd  sample               176973.414              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            20         treeBuilderAdd  sample            103546880.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                40  treeBuilderRecycleAdd  sample   8413786       6104.243 ?   195.964  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            40  treeBuilderRecycleAdd  sample                  674.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            40  treeBuilderRecycleAdd  sample                 4136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            40  treeBuilderRecycleAdd  sample                 7528.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            40  treeBuilderRecycleAdd  sample                 8960.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            40  treeBuilderRecycleAdd  sample                12976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           40  treeBuilderRecycleAdd  sample                31008.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          40  treeBuilderRecycleAdd  sample              2341360.845              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            40  treeBuilderRecycleAdd  sample             79560704.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                40         treeBuilderAdd  sample   8477065       4486.574 ?   230.760  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            40         treeBuilderAdd  sample                  699.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            40         treeBuilderAdd  sample                 2156.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            40         treeBuilderAdd  sample                 4312.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            40         treeBuilderAdd  sample                 5376.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            40         treeBuilderAdd  sample                 9520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           40         treeBuilderAdd  sample                28032.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          40         treeBuilderAdd  sample              4583153.664              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            40         treeBuilderAdd  sample            163840000.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                               100  treeBuilderRecycleAdd  sample   7535010      11631.027 ?   225.736  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00           100  treeBuilderRecycleAdd  sample                 1826.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50           100  treeBuilderRecycleAdd  sample                 7504.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90           100  treeBuilderRecycleAdd  sample                16960.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95           100  treeBuilderRecycleAdd  sample                21408.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99           100  treeBuilderRecycleAdd  sample                31808.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999          100  treeBuilderRecycleAdd  sample                88576.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999         100  treeBuilderRecycleAdd  sample             10534912.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00           100  treeBuilderRecycleAdd  sample            100270080.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                               100         treeBuilderAdd  sample   7564546      10905.697 ?   235.164  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00           100         treeBuilderAdd  sample                 1636.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50           100         treeBuilderAdd  sample                 7280.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90           100         treeBuilderAdd  sample                14416.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95           100         treeBuilderAdd  sample                18976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99           100         treeBuilderAdd  sample                32512.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999          100         treeBuilderAdd  sample                63040.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999         100         treeBuilderAdd  sample              8708838.195              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00           100         treeBuilderAdd  sample             60293120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                              1000  treeBuilderRecycleAdd  sample   3117183     101556.600 ?   688.145  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00          1000  treeBuilderRecycleAdd  sample                15136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50          1000  treeBuilderRecycleAdd  sample                80256.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90          1000  treeBuilderRecycleAdd  sample               169728.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95          1000  treeBuilderRecycleAdd  sample               206848.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99          1000  treeBuilderRecycleAdd  sample               295424.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999         1000  treeBuilderRecycleAdd  sample               573440.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999        1000  treeBuilderRecycleAdd  sample             16580608.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00          1000  treeBuilderRecycleAdd  sample             79953920.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                              1000         treeBuilderAdd  sample   3635046      81844.338 ?   695.009  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00          1000         treeBuilderAdd  sample                17120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50          1000         treeBuilderAdd  sample                55936.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90          1000         treeBuilderAdd  sample               133120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95          1000         treeBuilderAdd  sample               168448.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99          1000         treeBuilderAdd  sample               227584.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999         1000         treeBuilderAdd  sample               956367.872              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999        1000         treeBuilderAdd  sample             20004709.990              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00          1000         treeBuilderAdd  sample             75235328.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                             10000  treeBuilderRecycleAdd  sample    396416     806300.548 ?  5006.312  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00         10000  treeBuilderRecycleAdd  sample               178176.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50         10000  treeBuilderRecycleAdd  sample               578560.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90         10000  treeBuilderRecycleAdd  sample              1439744.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95         10000  treeBuilderRecycleAdd  sample              2101248.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99         10000  treeBuilderRecycleAdd  sample              2367488.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999        10000  treeBuilderRecycleAdd  sample             16711680.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999       10000  treeBuilderRecycleAdd  sample             27893461.811              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00         10000  treeBuilderRecycleAdd  sample             69468160.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                             10000         treeBuilderAdd  sample    544398     586738.267 ?  3652.668  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00         10000         treeBuilderAdd  sample               180992.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50         10000         treeBuilderAdd  sample               467456.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90         10000         treeBuilderAdd  sample               898048.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95         10000         treeBuilderAdd  sample              1052672.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99         10000         treeBuilderAdd  sample              1515520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999        10000         treeBuilderAdd  sample             16334848.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999       10000         treeBuilderAdd  sample             28628756.070              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00         10000         treeBuilderAdd  sample             75759616.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                            100000  treeBuilderRecycleAdd  sample     44146    7258758.048 ? 85405.244  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00        100000  treeBuilderRecycleAdd  sample              1724416.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50        100000  treeBuilderRecycleAdd  sample              5701632.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90        100000  treeBuilderRecycleAdd  sample             11796480.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95        100000  treeBuilderRecycleAdd  sample             15056896.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99        100000  treeBuilderRecycleAdd  sample             28868608.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999       100000  treeBuilderRecycleAdd  sample             61781180.416              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999      100000  treeBuilderRecycleAdd  sample             72159775.949              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00        100000  treeBuilderRecycleAdd  sample            153092096.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                            100000         treeBuilderAdd  sample     50573    6333830.725 ? 79209.874  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00        100000         treeBuilderAdd  sample              1839104.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50        100000         treeBuilderAdd  sample              4587520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90        100000         treeBuilderAdd  sample             10764288.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95        100000         treeBuilderAdd  sample             16629760.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99        100000         treeBuilderAdd  sample             28737536.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999       100000         treeBuilderAdd  sample             57427755.008              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999      100000         treeBuilderAdd  sample             92259640.934              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00        100000         treeBuilderAdd  sample            117833728.000              ns/op
{noformat}

So I would suggest removing {{Recycler}} (similar to [dc9ed46|https://github.com/apache/cassandra/commit/dc9ed463417aa8028e77e91718e4f3d6ea563210#diff-6aa10752b68c93ed354d642f9bdbe814L27]).;;;","22/Feb/18 17:59;jay.zhuang;Hi [~tjake], any suggestion on that? Should we remove {{Recycler}} or just reduce the large builder array size?;;;","22/Feb/18 22:10;tjake;I'd prefer to reduce the array size vs removing but you have been at this for so long I'm starting to think it's not worth keeping around.  I'd like to re-setup the test I had to check the improvement of CASSANDRA-9766 and see how much of a impact it has.;;;","23/Feb/18 02:55;jay.zhuang;I tested the change with {{LongStreamingTest}}, seems no impact with/without {{Recycler}}:
{noformat}
With Recycler
    [junit] ERROR [main] 2018-02-22 16:57:56,189 SubstituteLogger.java:250 - Writer finished after 22 seconds....
    [junit] ERROR [main] 2018-02-22 16:58:16,480 SubstituteLogger.java:250 - Finished Streaming in 20.29 seconds: 23.66 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:58:35,921 SubstituteLogger.java:250 - Finished Streaming in 19.44 seconds: 24.69 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:59:25,719 SubstituteLogger.java:250 - Finished Compacting in 49.80 seconds: 19.44 Mb/sec

No Recycler
    [junit] ERROR [main] 2018-02-22 16:50:48,255 SubstituteLogger.java:250 - Writer finished after 22 seconds....
    [junit] ERROR [main] 2018-02-22 16:51:08,209 SubstituteLogger.java:250 - Finished Streaming in 19.95 seconds: 24.06 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:51:27,624 SubstituteLogger.java:250 - Finished Streaming in 19.41 seconds: 24.72 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:52:16,900 SubstituteLogger.java:250 - Finished Compacting in 49.28 seconds: 19.48 Mb/sec
{noformat}

Here is the patch, please review:
| Branch | uTest |
| [13929-3.11|https://github.com/cooldoger/cassandra/tree/13929-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11] |
| [13929-trunk|https://github.com/cooldoger/cassandra/tree/13929-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13929-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-trunk] |;;;","23/Feb/18 09:32;norman;Just a general comment.... The recycler only makes sense to use if creating the object is considered very expensive and or if you create / destroy a lot of these very frequently. Which means usually thousands per second.  So if this is not the case here I think it completely reasonable to not use the Recycler at all... As I have no idea really about the use-case I am just leave this here as general comment :);;;","23/Feb/18 10:37;tsteinmaurer;The following does not include the latest patches from Feb 22, but shows last 30d on a single node (m4.2xlarge, Xmx12G, CMS) out of our 9 node loadtest environment including various tests/patches we have applied.
 !cassandra_heapcpu_memleak_patching_test_30d.png|width=1280!
 * Blue line => AVG heap utilization
 * Orange line => AVG CPU utilization (not really related as usually compaction is overlaying anything else most likely)

Following timelines in the chart:
||Timeframe||Deployment||Comment/Result||
|Jan 25 - Feb 1|Cassandra 3.11 public + Netty 4.0.55|(!) Heap utilization increase|
|Feb 1 - Feb 6|Cassandra 3.11 public + Netty 4.0.55 + limiting Netty capacity per Thread|(!) Heap utilization increase|
|Feb 6 - Feb 14|Cassandra 3.11 public + Netty 4.0.55 + my recycleHandle = null patch|(/) Heap utilization stable|
|Feb 14 - Feb 23|Cassandra 3.11 public + Netty 4.0.55 + *without* recycleHandle = null patch + first [~jay.zhuang] patch from Feb 13|(/) Heap utilization stable, but slightly increased to previous|

Very high-level (although from the field) compared to [~jay.zhuang] tests and benchmarks, but possibly useful for a decision process, hopefully being included in 3.11.3. Thanks guys!;;;","05/Mar/18 05:02;jay.zhuang;Hi [~tjake], are you interested in reviewing the patch? The trunk uTest failure is because of CASSANDRA-14119.;;;","05/Jun/18 21:45;cscetbon;Hey guys, any news on that issue ?;;;","07/Jun/18 13:44;jasobrown;On the whole, this looks pretty good. I'm running tests locally now, and will run in circleci shortly. 

[~jay.zhuang] There's one thing I'm wondering: in {{#reuse{}}}, we do not explicitly null out the {{values}} array, like we used to do in {{#cleanup()}}:

{code}
    Arrays.fill(values, null);
{code}

While every access of {{values}} seems safe, and uses the {{count}} to ensure proper sizing of any built {{Object[]}}, I'd like to be over-cautious and null out the array to prevent the possibility of data leak wherein a reuse of Builder erroneously gets extra data from the last use. This error doesn't seem like it's possible in the current code, but I'd like to future-proof this - it would be a complete nightmare to debug. 

wdyt?

;;;","07/Jun/18 14:07;jasobrown;[~tsteinmaurer] if you've been running with some version of this patch (where recycling is not used), do you see any degradation in streaming?;;;","07/Jun/18 15:49;jasobrown;running [~jay.zhuang]'s patch with a few minor cleanups:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13929-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13929-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13929-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13929-trunk]|
||
;;;","07/Jun/18 16:17;tsteinmaurer;[~jasobrown], sorry I don't have any streaming benchmarks before/after.;;;","07/Jun/18 16:34;jasobrown;[~tsteinmaurer] ok, not a problem. As long as you don't see streaming getting obviously slower (1 hour -> 2 hours, for example), I'll take that as a data point.;;;","07/Jun/18 18:06;jay.zhuang;Thanks [~jasobrown] for the review and fix.

Yes, null out the {{values}} array before reusing is a good practice. +1 on that. Seems the dTest is failed for 3.11 branch, but I don't think it's caused by this patch. Please let me know if I could commit.;;;","08/Jun/18 12:45;jasobrown;[~jay.zhuang] +1;;;","08/Jun/18 19:11;jay.zhuang;Thanks [~jasobrown] again for the review. Committed as [{{ed5f834}}|https://github.com/apache/cassandra/commit/ed5f8347ef0c7175cd96e59bc8bfaf3ed1f4697a].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove initialDirectories from CFS,CASSANDRA-13928,13106622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,03/Oct/17 06:46,15/May/20 08:06,14/Jul/23 05:56,15/Dec/17 12:49,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"The initialDirectories added in CASSANDRA-8671 is quite confusing and I don't think it is needed anymore, it should be removed",,eduard.tudenhoefner,jjordan,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 15 12:49:53 UTC 2017,,,,,,,,,,"0|i3kt9r:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"03/Oct/17 06:47;marcuse;cc [~eduard.tudenhoefner];;;","03/Oct/17 06:50;marcuse;It also makes implementing CASSANDRA-13215 less straight-forward;;;","03/Oct/17 14:27;jjordan;[~krummas] why do you think it is not needed anymore?  How does a compaction strategy control where things are created without it?;;;","03/Oct/17 14:38;marcuse;bq. How does a compaction strategy control where things are created without it?
[~jjordan] they probably shouldn't right now, and I don't think this code is used anymore.;;;","03/Oct/17 15:01;jjordan;(y);;;","03/Oct/17 17:03;eduard.tudenhoefner;[~krummas] yes I think it's safe to remove the *initialDirectories* stuff.;;;","24/Oct/17 06:31;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13928

rebuilding unit tests here:
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13928
dtests look bad but similar to other branches:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/383/;;;","12/Dec/17 22:52;pauloricardomg;LGTM, can you just rebase (now that CASSANDRA-13948 is in) and submit a new test run? Feel free to commit when CI is clean.;;;","13/Dec/17 09:02;marcuse;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/461/
https://circleci.com/gh/krummas/cassandra/195;;;","15/Dec/17 12:49;marcuse;thanks, committed, CI errors look unrelated;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Header only commit logs should be filtered before recovery,CASSANDRA-13918,13105867,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,28/Sep/17 21:12,15/May/20 08:05,14/Jul/23 05:56,29/Sep/17 22:39,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,,0,,,,,"Commit log recovery will tolerate commit log truncation in the most recent log file found on disk, but will abort startup if problems are detected in others. 

Since we allocate commit log segments before they're used though, it's possible to get into a state where the last commit log file actually written to is not the same file that was most recently allocated, preventing startup for what should otherwise be allowable incomplete final segments.

Excluding header only files on recovery should prevent this from happening.",,bdeggleston,jjirsa,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 22:39:28 UTC 2017,,,,,,,,,,"0|i3komv:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"28/Sep/17 22:33;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/13918-3.0]|[utest|https://circleci.com/gh/bdeggleston/cassandra/127]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/342/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13918-3.11]|[utest|https://circleci.com/gh/bdeggleston/cassandra/126]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/343/]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13918-trunk]|[utest|https://circleci.com/gh/bdeggleston/cassandra/128]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/344/]|;;;","29/Sep/17 18:10;samt;LGTM. Minuscule nit: there's an unused import in {{CommitLogReader}} on the 3.11 branch.;;;","29/Sep/17 18:11;bdeggleston;Thanks, will fix on commit;;;","29/Sep/17 21:36;bdeggleston;utests look good on all branches, dtests look good on 3.0 & 3.11. trunk dtests aborted, but looks like it was due to an existing issue.;;;","29/Sep/17 22:39;bdeggleston;committed as {{95839aae2fde28fa29b16741de6bd52c0697843f}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COMPACT STORAGE queries on dense static tables accept hidden column1 and value columns,CASSANDRA-13917,13105742,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Gerrrr,ifesdjeen,ifesdjeen,28/Sep/17 12:40,02/Oct/20 12:42,14/Jul/23 05:56,06/Apr/20 09:50,3.0.21,3.11.7,,,,,Legacy/Core,,,,,0,lhf,,,,"Test for the issue:

{code}
    @Test
    public void testCompactStorage() throws Throwable
    {
        createTable(""CREATE TABLE %s (a int PRIMARY KEY, b int, c int) WITH COMPACT STORAGE"");
        assertInvalid(""INSERT INTO %s (a, b, c, column1) VALUES (?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'));
        // This one fails with Some clustering keys are missing: column1, which is still wrong
        assertInvalid(""INSERT INTO %s (a, b, c, value) VALUES (?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'));       
        assertInvalid(""INSERT INTO %s (a, b, c, column1, value) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'), ByteBufferUtil.bytes('b'));
        assertEmpty(execute(""SELECT * FROM %s""));
    }
{code}

Gladly, these writes are no-op, even though they succeed.

{{value}} and {{column1}} should be completely hidden. Fixing this one should be as easy as just adding validations.",,Gerrrr,ifesdjeen,jjirsa,slebresne,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/19 10:14;Gerrrr;13917-3.0-testall-13.12.2019;https://issues.apache.org/jira/secure/attachment/12988774/13917-3.0-testall-13.12.2019","16/Jan/20 09:37;Gerrrr;13917-3.0-testall-16.01.2020;https://issues.apache.org/jira/secure/attachment/12991097/13917-3.0-testall-16.01.2020","27/Nov/18 11:03;Gerrrr;13917-3.0-testall-2.png;https://issues.apache.org/jira/secure/attachment/12949663/13917-3.0-testall-2.png","20/Nov/19 10:12;Gerrrr;13917-3.0-testall-20.11.2019.png;https://issues.apache.org/jira/secure/attachment/12986317/13917-3.0-testall-20.11.2019.png","16/Jan/20 09:37;Gerrrr;13917-3.0-upgrade-16.01.2020;https://issues.apache.org/jira/secure/attachment/12991098/13917-3.0-upgrade-16.01.2020","22/Nov/18 16:51;Gerrrr;13917-3.0.png;https://issues.apache.org/jira/secure/attachment/12949215/13917-3.0.png","13/Dec/19 10:14;Gerrrr;13917-3.11-testall-13.12.2019;https://issues.apache.org/jira/secure/attachment/12988773/13917-3.11-testall-13.12.2019","16/Jan/20 09:37;Gerrrr;13917-3.11-testall-16.01.2020.png;https://issues.apache.org/jira/secure/attachment/12991099/13917-3.11-testall-16.01.2020.png","27/Nov/18 11:03;Gerrrr;13917-3.11-testall-2.png;https://issues.apache.org/jira/secure/attachment/12949662/13917-3.11-testall-2.png","20/Nov/19 10:12;Gerrrr;13917-3.11-testall-20.11.2019.png;https://issues.apache.org/jira/secure/attachment/12986318/13917-3.11-testall-20.11.2019.png","16/Jan/20 09:37;Gerrrr;13917-3.11-upgrade-16.01.2020.png;https://issues.apache.org/jira/secure/attachment/12991100/13917-3.11-upgrade-16.01.2020.png","22/Nov/18 16:51;Gerrrr;13917-3.11.png;https://issues.apache.org/jira/secure/attachment/12949214/13917-3.11.png",,,,,,,,12.0,Gerrrr,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 21:36:06 UTC 2020,,,,,,,,,,"0|i3knvj:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Low,,3.0.19,,,https://github.com/apache/cassandra/commit/0c54cc98595b4879c9a634737674fd36fd1c46d0,,,,,,,,,Unit tests,,,,,"13/Oct/17 16:14;Gerrrr;If the table is created with COMPACT STORAGE and a single primary key, e.g.

{noformat}
cqlsh:k> CREATE TABLE t1 (a int PRIMARY KEY, b int, c int) WITH COMPACT STORAGE;
{noformat}

We get the behavior from the tests:

{noformat}
cqlsh:k> INSERT INTO t1 (a,b,c,column1) VALUES (1,1,1,'a');
cqlsh:k> select * from t1;

 a | b | c
---+---+---

(0 rows)
{noformat}

Corresponding CFMMetaData and the column definition kinds during the {{INSERT}}:
{noformat}
cfm:
isCompactTable() => true
isStaticCompactTable() => true

Column definitions:
a.kind=PARTITION_KEY
b.kind=STATIC
c.kind=STATIC
column1.kind=CLUSTERING
value.kind=REGULAR
{noformat}

Also, if the table contains a column with a name {{column1}}, the hidden column will be called {{column2}}:

{noformat}
cqlsh:k> CREATE TABLE t2 (a int PRIMARY KEY, b int, c int, column1 text) WITH COMPACT STORAGE;
cqlsh:k> INSERT INTO t2 (a,b,c,column1, column2, value) VALUES (1,1,1,'a','a',0xbb);
cqlsh:k> select * from t2;

 a | b | c | column1
---+---+---+---------

(0 rows)
{noformat}

If the table is created with COMPACT STORAGE and a compound primary key, it works as expected:

{noformat}
cqlsh:k> CREATE TABLE t3 (a int, b int, c int, PRIMARY KEY (a, b)) WITH COMPACT STORAGE;
cqlsh:k> INSERT INTO t3 (a,b,c,column1) VALUES (1,1,1,'a');
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Undefined column name column1""
cqlsh:k> INSERT INTO t3 (a,b,c,column1,value) VALUES (1,1,1,'a',0xff);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Undefined column name column1""
cqlsh:k> INSERT INTO t3 (a,b,c,value) VALUES (1,1,1,0xff);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Undefined column name value""
{noformat}

Corresponding CFMMetaData during the {{INSERT}}:

{noformat}
cfm.isCompactTable() => true
cfm.isStaticCompactTable() => false
{noformat}

h4. Solution

In {{UpdateStatement.prepareInternal}} when the CFM is {{StaticCompactTable}} check that the columns to be updated are not {{CLUSTERING}} or {{REGULAR}}. if this is the case - ""hide"" the columns by returning the error ""Undefined column name"".

Branches:

* [3.0.15|https://github.com/Gerrrr/cassandra/tree/13917-3.0.15]
* [3.11.1|https://github.com/Gerrrr/cassandra/tree/13917-3.11.1];;;","30/Nov/17 21:07;jjirsa;[~ifesdjeen] are you able to review this as the reporter?;;;","12/Nov/18 09:40;ifesdjeen;The patch looks good, modulo indentation in tests. Also, I would list all unmatching columns in the error message instead of just a single in case someone would try to repair query by repairing one column after another.

Another thing, to my best memory, we have actual definitions listed [here|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/CFMetaData.java#L138], which we probably should use. I realise that this does not change much semantically but still might be less error-prone.

Lastly, it'd be great to rebase both patches.;;;","22/Nov/18 16:55;Gerrrr;Thanks for feedback! I fixed indentation in tests, changed the check to report all unmatching columns at once and rebased both branches.

I did not use {{superCfKeyColumn}} and {{superCfValueColumn}} because they are null in the test case.

Created new branches:
* https://github.com/Gerrrr/cassandra/tree/13917-3.0
* https://github.com/Gerrrr/cassandra/tree/13917-3.11

CI results:
* [13917-3.0 | https://issues.apache.org/jira/secure/attachment/12949215/13917-3.0.png]
* [13917-3.11 | https://issues.apache.org/jira/secure/attachment/12949214/13917-3.11.png]
;;;","23/Nov/18 11:23;ifesdjeen;After looking a bit more, it might be that the issue is not fully fixed: 

{code}
# Still throws 'Range deletions are not supported for specific columns'
DELETE value FROM %s WHERE a = 1

# Still throws 'Invalid identifier column1 for deletion (should not be a PRIMARY KEY part)'
DELETE column1 FROM %s WHERE a = 1

# Still works:
DELETE FROM %s WHERE a = 1 and column1 = 'b'

# Still works:
SELECT value, column1 FROM %s
{code}

You're right it's not supercolumn cf, so we have only [compact value|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/CFMetaData.java#L124] that we can use.

After thinking a bit more, I'd try and explore more places where {{getColumnDefinition}} is used in order to understand better the surface of the problem and maybe even try looking [here|https://github.com/apache/cassandra/blob/cassandra-3.11/test/unit/org/apache/cassandra/cql3/validation/operations/DropCompactStorageThriftTest.java] to learn more about compact storage. 

If you have any questions about compact storage, I'm happy to help either on IRC or here.;;;","27/Nov/18 11:04;Gerrrr;Thanks for the review and suggestions! I updated the patches.

CI results:
* [13917-3.0|https://issues.apache.org/jira/secure/attachment/12949663/13917-3.0-testall-2.png]
* [13917-3.11|https://issues.apache.org/jira/secure/attachment/12949662/13917-3.11-testall-2.png]
 ;;;","20/Nov/19 10:14;Gerrrr;I rebased the branches and re-ran tests. 13917-3.0 has 1  timed out test - {{org.apache.cassandra.db/ScrubTest/testScrubCorruptedCounterRow}} that seems unrelated, 13917-3.11 passed.

Branches:
* [13917-3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...Gerrrr:13917-3.0]
* [13917-3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...Gerrrr:13917-3.11]

CI results:
*  [13917-3.0|https://jira.apache.org/jira/secure/attachment/12986317/13917-3.0-testall-20.11.2019.png]
*  [13917-3.11|https://jira.apache.org/jira/secure/attachment/12986318/13917-3.11-testall-20.11.2019.png];;;","28/Nov/19 12:03;ifesdjeen;Thank you for the patch [~Gerrrr]. I think the patch is overall good, but I was a bit skeptical about {{hiddenColumns}} set and its creation depending on {{isStaticCompactTable}}, which has lead me to consider other cases, such as:

{code}
        createTable(""CREATE TABLE %s (a int, b int, PRIMARY KEY(a, b)) WITH COMPACT STORAGE"");
        execute(""INSERT INTO %s (a, b, value) VALUES (?, ?, ?)"", 1, 1, null); // this should not work
{code}

I did check some thrift-created tables, but in the most other things work the same way with and without compact storage. You can get some more information and (hopefully) an exhaustive set of compact storage table examples in {{DropCompactStorageThriftTest}}. 

In short, we should make sure that non-static compact tables also work as expected while not breaking dense tables that actually have the value column specified. This means that we should probably take a slightly different approach for validation and check for hidden columns depending on the table configuration.

Supercolumn families seem to work fine; but I also think we probably can skip adding tests for those.;;;","13/Dec/19 10:20;Gerrrr;Thank you for your review [~ifesdjeen]!

As you suggested, I have slightly changed the approach. Now dense and sparse tables are considered separately. In case of dense tables we only need to add a compact value column if it has an empty type [1]. In case of sparse tables we also need to make sure that the table is not super in order not break thrift integration tests.

Branches:
* [13917-3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...Gerrrr:13917-3.0]
* [13917-3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...Gerrrr:13917-3.11]

CI results:
* [13917-3.0|https://jira.apache.org/jira/secure/attachment/12986317/13917-3.0-testall-13.12.2019.png]
* [13917-3.11|https://jira.apache.org/jira/secure/attachment/12986318/13917-3.11-testall-13.12.2019.png]

{{org.apache.cassandra.cql3.validation.operations.TTLTest.testCapWarnExpirationOverflowPolicy}} failure at the patch for 3.11 looked suspicious, but it did not reproduce locally on the same branch, so I believe that it is a CI glitch and not related to the patch.

1. Besides debugging on my own, I found this article on [Migrating from compact storage|https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/dropCompactStorage.html] that confirmed my assumptions about cases when {{column1}} and {{value}} columns are added, and when {{value}} type is empty.



;;;","16/Dec/19 09:28;ifesdjeen;Thank you for the patch. This looks good for the most part. The only thing where I'm still thinking is if it's worth to move a check on {{isHiddenColumn}} to {{getColumnDefinition}}? This way we don't have to special-case modification statement check for hidden columns and all other places where column definition is null. ;;;","07/Jan/20 12:21;Gerrrr;Good idea! I moved {{isHiddenColumn}} check to {{getColumnDefinition}}.;;;","08/Jan/20 16:12;ifesdjeen;Thank you for the patch! Committed to 3.0 with [0c54cc98595b4879c9a634737674fd36fd1c46d0|https://github.com/apache/cassandra/commit/0c54cc98595b4879c9a634737674fd36fd1c46d0], and merged up to [3.11|https://github.com/apache/cassandra/commit/9635e55f73598c0f7aece56d5e603198ca464fcc] and trunk (with {{-s ours}});;;","10/Jan/20 17:37;slebresne;Re-opening because this patch breaks things badly. Did you run the tests after the move of {{isHiddenColumn}} to {{getColumnDefinition}}? Because if so, our tests aren't very good.

The committed version, that move {{isHiddenColumn}} to {{getColumnDefinition}}, means the compact column is invisible internally, which is wrong, it must be accessible internally.

Concretely, a simple test that creates a 2ndary index, wait for it to be built, and then restart the node will fail on restart with
{noformat}
java.lang.RuntimeException: Unknown column value during deserialization
	at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:353) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:496) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:365) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:544) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_152]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_152]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_152]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_152]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83) [main/:na]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_152]
{noformat}
because the {{SystemKeyspace.BUILT_INDEXES}} table is compact, so when it tries to open the header of the sstables for this table, the {{getColumnDefinition}} returns {{null}} for the {{value}} column, even though it obviously exists and should be returned.
;;;","16/Jan/20 09:44;Gerrrr;I did run unit tests after all changes; should have run upgrade tests as well.

Here are the fixup patches that use the hidden column logic only for statements:

Branches:
* [13917-fixup-3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...Gerrrr:13917-fixup-3.0]
* [13917-fixup-3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...Gerrrr:13917-fixup-3.11]

CI results:
* [13917-fixup-3.0-testall|https://issues.apache.org/jira/secure/attachment/12991097/13917-3.0-testall-16.01.2020]
* [13917-fixup-3.0-upgrade|https://issues.apache.org/jira/secure/attachment/12991098/13917-3.0-upgrade-16.01.2020]
* [13917-fixup-3.11-testall|https://issues.apache.org/jira/secure/attachment/12991099/13917-3.11-testall-16.01.2020.png]
* [13917-fixup-3.11-upgrade|https://issues.apache.org/jira/secure/attachment/12991100/13917-3.11-upgrade-16.01.2020.png];;;","17/Jan/20 10:39;ifesdjeen;[~slebresne] you are right. In fact, I've found several more places where this could cause a problem. What we should've done initially is just add a separate method for CQL layer, and let the rest of the calls (e.g., all internal stuff) to use the old one. Committed version of the patch also caused problems in mixed version environments which was caught by one of the upgrade in-jvm dtests. Also, things like thrift and compact storage compatibility layer was impacted in a way, too. 

I'm still thinking if {{RowUpdateBuilder}} should be using CQL columns or all columns. I'd argue that CQL only (e.g., no hidden), since this is also how we query them for the most part. We don't really those hidden column even on internal paths.

I've made a patch and have added a couple more tests that validates the situations that weren't covered previously. Thanks to [CASSANDRA-15506], we can also make sure that upgrade tests are going to be running. 

|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13917-followup]|[CI|https://circleci.com/gh/ifesdjeen/cassandra/tree/CASSANDRA-13917-followup]|
|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13917-followup-3.11]|[CI|https://circleci.com/gh/ifesdjeen/cassandra/tree/CASSANDRA-13917-followup-3.11]|;;;","22/Jan/20 20:05;Gerrrr;[~ifesdjeen] I reviewed your patch. It covers some cases that I missed and has better abstraction via {{getColumnDefinitionForCQL}}. +1 to merge your version!;;;","22/Jan/20 21:36;ifesdjeen;Committed a follow-up patch to 3.0 with [b907dc9689dd04ebae1f765570401d1f20a88ebd|https://github.com/apache/cassandra/commit/b907dc9689dd04ebae1f765570401d1f20a88ebd], and merged up to [3.11|https://github.com/apache/cassandra/commit/7cd0e92dcac8e3423f2eae08069bd5ebf6a3e236], and trunk (with {{-s ours}}).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException thrown by UPI.Serializer.hasNext() for some SELECT queries,CASSANDRA-13911,13105387,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,27/Sep/17 11:17,16/Apr/19 09:29,14/Jul/23 05:56,30/Sep/17 10:47,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,,,,,"Certain combinations of rows, in presence of per partition limit (set explicitly in 3.6+ or implicitly to 1 via DISTINCT) cause {{UnfilteredPartitionIterators.Serializer.hasNext()}} to throw {{IllegalStateException}} .

Relevant code snippet:

{code}
// We can't answer this until the previously returned iterator has been fully consumed,
// so complain if that's not the case.
if (next != null && next.hasNext())
    throw new IllegalStateException(""Cannot call hasNext() until the previous iterator has been fully consumed"");
{code}

Since {{UnfilteredPartitionIterators.Serializer}} and {{UnfilteredRowIteratorSerializer.serializer}} deserialize partitions/rows lazily, it is required for correct operation of the partition iterator to have the previous partition fully consumed, so that deserializing the next one can start from the correct position in the byte buffer. However, that condition won’t always be satisfied, as there are legitimate combinations of rows that do not consume every row in every partition.

For example, look at [this dtest|https://github.com/iamaleksey/cassandra-dtest/commits/13911].

In case we end up with a following pattern of rows:

{code}
node1, partition 0 | 0
node2, partition 0 |   x x
{code}

, where {{x}} and {{x}} a row tombstones for rows 1 and 2, it’s sufficient for {{MergeIterator}} to only look at row 0 in partition from node1 and at row tombstone 1 from node2 to satisfy the per partition limit of 1. The stopping merge result counter will stop iteration right there, leaving row tombstone 2 from node2 unvisited and not deseiralized. Switching to the next partition will in turn trigger the {{IllegalStateException}} because we aren’t done yet.

The stopping counter is behaving correctly, so is the {{MergeIterator}}. I’ll note that simply removing that condition is not enough to fix the problem properly - it’d just cause us to deseiralize garbage, trying to deserialize a new partition from a position in the bytebuffer that precedes remaining rows in the previous partition.",,aleksey,ifesdjeen,jeromatron,jjirsa,jjordan,mshuler,samt,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 30 10:47:49 UTC 2017,,,,,,,,,,"0|i3klpj:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"29/Sep/17 11:37;aleksey;Branches with fixes here: [3.0|https://github.com/iamaleksey/cassandra/commits/13911-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13911-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13911-4.0].

3.11 needs an extra dtest to show that the change in {{DataResolver}} is necessary (using {{PER PARTITION LIMIT}} > 1), working on it. Tests are running.;;;","29/Sep/17 13:12;samt;+1;;;","30/Sep/17 10:47;aleksey;Thanks.

[3.0 utests|https://circleci.com/gh/iamaleksey/cassandra/43] have the usual unrelated failures (mostly MV schema races). [3.11 utests|https://circleci.com/gh/iamaleksey/cassandra/44] have the common {{CommitLogSegmentManagerTest}} failure, and [4.0 utests|https://circleci.com/gh/iamaleksey/cassandra/45] have an unrelated {{StreamTransferTaskTest}} failure + the usual {{ViewFilteringTest}} timeouts. Basically baseline.

[3.0 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/347/] only have the usual suspects (also we need to do something about {{Could not do a git clone}} issue already).  [3.11 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/348/] and [4.0 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/349/] are a bit more noisy but nothing related fails for a good reason. The flakiness and timeouts on jenkins are pretty bad overall though.

Committed as [1efdf330e291a41cd8051e0c1195f75b5d352370|https://github.com/apache/cassandra/commit/1efdf330e291a41cd8051e0c1195f75b5d352370] to 3.0 and merged with 3.11 and trunk. Dtest committed as [51ad68ec45c7a40de1c51b31651632f2e87ceaa4|https://github.com/apache/cassandra-dtest/commit/51ad68ec45c7a40de1c51b31651632f2e87ceaa4].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve TRUNCATE performance with many sstables,CASSANDRA-13909,13105327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,27/Sep/17 07:33,15/May/20 08:00,14/Jul/23 05:56,02/Oct/17 07:42,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,,0,,,,,"Truncate is very slow in 3.0, mostly due to {{LogRecord.make}} listing all files in the directory for every sstable.",,jay.zhuang,jeromatron,marcuse,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 02 07:42:22 UTC 2017,,,,,,,,,,"0|i3klcf:",9223372036854775807,,,,,,,stefania,,stefania,,,Normal,,,,,,,,,,,,,,,,,,,"27/Sep/17 07:38;marcuse;patch [here|https://github.com/krummas/cassandra/commits/marcuse/bulkobsolete] - it adds a bulk obsoletion for truncate
dtests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/340/
utests: https://circleci.com/gh/krummas/cassandra/120

[~Stefania] do you have time to review this?

edit: my small benchmarks show that this reduces truncate time on a 1600sstable node from 60s to 6s;;;","28/Sep/17 01:11;stefania;Sure, I can review it.;;;","28/Sep/17 06:37;stefania;LGTM - I only found 3 nits, see comments [here|https://github.com/krummas/cassandra/commit/9ea54dc47f75d0296dde4f74fd6f4d3de392f991].

There seem to be timeouts in both the dtests (they timed out on manual_join_test) and the unit tests. I doubt very much that they are related to this patch but the main [builds|https://builds.apache.org/view/A-D/view/Cassandra/] are not affected by timeouts (although they did not run in almost a month). I've verified that the unit test that timed out passes locally.

+1 to commit if you are sure that these timeouts are happening on other branches too.


;;;","28/Sep/17 06:47;marcuse;Thanks for the review

Seems I linked the wrong dtest build, running them again [here|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/341] before committing

I have seen the unit test failure before, so that shouldn't be caused by this;;;","29/Sep/17 01:19;stefania;14 failures in the dtests, but they all failed 14+ times in older builds too, mostly are due to {{Could not do a git clone}}. 

I think we can merge this.;;;","29/Sep/17 07:17;marcuse;Didn't merge cleanly to 3.11/trunk, so running dtests on those branches before merging
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/345/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/346/;;;","02/Oct/17 07:42;marcuse;Thanks for the review, didn't see anything suspicious in the test results, committed as {{b32a9e6452c78e6ad08e371314bf1ab7492d0773}} to 3.0 and merged up;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly close StreamCompressionInputStream to release any ByteBuf,CASSANDRA-13906,13105265,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,26/Sep/17 23:38,15/May/20 08:00,14/Jul/23 05:56,03/Oct/17 19:24,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"When running dtests for trunk (4.x) that perform some streaming, sometimes a {{ByteBuf}} is not released properly, and we get this error in the logs (causing the dtest to fail):

{code}
ERROR [MessagingService-NettyOutbound-Thread-4-2] 2017-09-26 13:42:37,940 Slf4JLogger.java:176 - LEAK: ByteBuf.release() was not called before it's garbage-collected. Enable advanced leak reporting to find out where the leak occurred. To enable advanced leak reporting, specify the JVM option '-Dio.netty.leakDetection.level=advanced' or call ResourceLeakDetector.setLevel() See http://netty.io/wiki/reference-counted-objects.html for more information.
{code}
",,aweisberg,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 19:24:00 UTC 2017,,,,,,,,,,"0|i3kkyv:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"26/Sep/17 23:38;jasobrown;The cause of the leaked {{ByteBuf}} was due to a wrapping class not being closed, and thus never had the chance to call {{release()}} on the {{ByteBuf}}.

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/netty-leak]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/338/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/netty-leak]|

I ran one of the failing dtests (repair_tests/incremental_repair_test.py:TestIncRepair.multiple_repair_test) on my laptop, and without the patch could reproduce within 2-3 runs. With the patch, I ran it fifty time and could not reproduce.

[~aweisberg] Would you mind reviewing?;;;","29/Sep/17 16:34;aweisberg;So to bikeshed this all to hell. Wouldn't the most idiomatic way to do this be to have TrackedDataInputPlus implement Closable and use try with resources?

In fact try with resources will let you declare all the Closeable things in the same try block and clean up if one of the things you are using to wrap throws in it's constructor preventing the wrapped resource from leaking.;;;","29/Sep/17 16:49;jasobrown;bq. Wouldn't the most idiomatic way to do this be to have {{TrackedDataInputPlus}} implement {{Closable}} and use try with resources

I thought about that when I was fixing this, but  {{TrackedDataInputPlus}} wraps a {{DataInput}}, which does not declare a {{#close()}} method. In a {{TrackedDataInputPlus#close()}} I could check if the wrapped instance implements {{Closeable}} and invoke it if it does. wdyt?

There are a couple of other uses of  {{TrackedDataInputPlus}} ({{HintMessage}}, {{IndexedEntry}} called on load saved cache path), but they should not be affected by {{TrackedDataInputPlus}} implementing {{Closeable}} as the are not allocated via try-with-resources. 

Note: if we choose to make this change, which is reasonable, I can also cleanup {{CompressedStreamReader}} to also allocate {{TrackedDataInputPlus}} in a try-with-resources - it has the same concerns as {{StreamReader}} that you raised. Branch coming shortly ...;;;","29/Sep/17 17:02;aweisberg;If you implement Closable it's going to cause warnings any place that doesn't use try with resources. Given the other usages I think what you have done is probably fine although you can still switch to try with resources just for {{StreamCompressionInputStream}}.;;;","29/Sep/17 18:01;jasobrown;OK, I pushed up a fresh branch using try-with-resources for both {{StreamReader}} and {{CompressedStreamReader}}. utests and dtests have been been kicked off, as well.;;;","29/Sep/17 19:55;aweisberg;Is releasing references to a buffer using the {{refCnt()}} a good idea? Isn't that kind of abusing the idiom of reference counting by not counting?

Otherwise looks good.

;;;","29/Sep/17 22:24;jasobrown;bq. Isn't that kind of abusing the idiom of reference counting by not counting?

That is true to a degree, but I'm never sure if any code would, in some broken way, become executed twice and fail on the refCnt decrement and mask some other problem. I could set the buffer to null after a simple call to {{release()}}. wdyt?;;;","02/Oct/17 22:27;aweisberg;I agree reference counting in Java is fraught due to the lack of Destructors and other plumbing.
 
So do we always expect the refcnt to be one or some known number? Then we should assert that at runtime and take an error path if it's not true (after releasing the resources) or maybe just do rate limited logging.

If use after free is a concern we might want to make sure the buffer is set to null so we get an NPE instead of a segfault. Then the  code incorrectly using the buffer will signal an error as well.;;;","03/Oct/17 18:29;aweisberg;+1;;;","03/Oct/17 18:58;jasobrown;[~aweisberg] and I discussed offline, and I'll revert the overly cautious (and perhaps incorrect) change around that refCnt.;;;","03/Oct/17 19:24;jasobrown;committed as sha {{982ab93a2f8a0f5c56af9378f65d3e9e430000b9}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly close netty channels when a stream session ends,CASSANDRA-13905,13105221,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,26/Sep/17 19:31,15/May/20 08:05,14/Jul/23 05:56,29/Sep/17 16:36,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,,0,,,,,"Netty channels in stream sessions were not being closed correctly. TL;DR I was using a lambda that was not executing as it is lazily evaluated. This was causing a {{RejectedExecutionException}} at the end of some streaming-related dtests",,aweisberg,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 16:36:04 UTC 2017,,,,,,,,,,"0|i3kkp3:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"26/Sep/17 19:35;jasobrown;Minor patch here:

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/netty-RejectedExecutionException]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/336/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/netty-RejectedExecutionException]|

utest errors are unrelated, and number of dtest failures are lower (and none caused by {{RejectedExecutionEception}})

[~aweisberg] or [~pauloricardomg] : Would one of you mind reviewing?;;;","29/Sep/17 16:17;aweisberg;+1 to the fix.

If you wanted to stick with the Streamisms you could do Collectors.toList() after the map and then wait on futures for that. Collectors are the don't be lazy step in streams. Although if you aren't being lazy I'm not sure the syntax for streams is really all that much clearer.;;;","29/Sep/17 16:36;jasobrown;bq. Collectors are the don't be lazy step in streams

Yeah that's where I messed up originally :).

committed as sha {[ebefc96a8fe63aca5f324984f7f3147f10218643}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix buffer length comparison when decompressing in netty-based streaming,CASSANDRA-13899,13104716,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,pauloricardomg,pauloricardomg,25/Sep/17 10:16,15/May/20 08:00,14/Jul/23 05:56,04/Oct/17 07:05,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Streaming a single partition with ~100K rows fails with the following exception:

{noformat}
ERROR [Stream-Deserializer-/127.0.0.1:35149-a92e5e12] 2017-09-21 04:03:41,237 StreamSession.java:617 - [Stream #c2e5b640-9eab-11e7-99c0-e9864ca8da8e] Streaming error occurred on session with peer 127.0.0.1
org.apache.cassandra.streaming.StreamReceiveException: java.lang.RuntimeException: Last written key DecoratedKey(-1000328290821038380) >= current key DecoratedKey(-1055007227842125139)  writing into /home/paulo/.ccm/test/node2/data0/stresscql/typestest-482ac7b09e8d11e787cf85d073c
8e037/na-1-big-Data.db
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:63) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:178) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{noformat}

Reproduction steps:
 * Create CCM cluster with 2 nodes
* Start only first node, disable hinted handoff
 * Run stress with the attached yaml: {{tools/bin/cassandra-stress ""user profile=largepartition.yaml n=10K ops(insert=1) no-warmup -node whitelist 127.0.0.1 -mode native cql3 compression=lz4 -rate threads=4 -insert visits=FIXED(100K) revisit=FIXED(100K)""}}
* Start second node, run repair on {{stresscql}} table - the exception above will be thrown.

I investigated briefly and haven't found anything suspicious. This seems to be related to CASSANDRA-12229 as I tested the steps above in a branch without that and the repair completed successfully. I haven't tested with a smaller number of rows per partition to see at which point it starts to be a problem.

We should probably add a regression dtest to stream large partitions to catch similar problems in the future.",,jasobrown,jasonstack,jeromatron,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10520,,,,,,,,,,,,,"25/Sep/17 10:17;pauloricardomg;largepartition.yaml;https://issues.apache.org/jira/secure/attachment/12888815/largepartition.yaml",,,,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 04 07:06:46 UTC 2017,,,,,,,,,,"0|i3khl3:",9223372036854775807,4.0,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"25/Sep/17 10:18;pauloricardomg;(cc [~jasobrown]);;;","26/Sep/17 01:06;jasobrown;[~pauloricardomg] Thanks for the scripts and instructions - I was able to reproduce. Will have a patch hopefully in a day or two;;;","26/Sep/17 21:32;jasobrown;This turned out to be a simple, stupid bug - i used the wrong length to compare against when translating from CASSANDRA-10520.

Patch here:

||13899||
|[branch|https://github.com/jasobrown/cassandra/tree/13899]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/337/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13899]|

[~pauloricardomg] Do you mind reviewing?
;;;","27/Sep/17 01:21;jasobrown;tbh, I don't think it's large partitions that necessarily triggered this bug; streaming just transfers the bytes of sstable Data file naively. I think it was something in the dataset and the way it compresses that exposed the bug. Thus, I'm not sure how to proceed with a dtest, especially one based on [~pauloricardomg]'s submitted patch (as highly useful as it is!).;;;","28/Sep/17 03:07;pauloricardomg;bq. tbh, I don't think it's large partitions that necessarily triggered this bug; streaming just transfers the bytes of sstable Data file naively. I think it was something in the dataset and the way it compresses that exposed the bug. Thus, I'm not sure how to proceed with a dtest, especially one based on Paulo Motta's submitted patch (as highly useful as it is!).

Good point, I managed to reproduce this with 1k entries with the same YAML by setting {{chunk_length_in_kb=1}}. Maybe it's possible to reproduce with even fewer entries and the default generator with a large payload size and a very low {{chunk_length_in_kb}}. Would you mind trying that?

The patch looks good, but would be nice to have a regression dtest for this, since it's pretty subtle to catch.;;;","28/Sep/17 13:44;jasobrown;[~pauloricardomg] Good call on dropping the {{chunk_length_in_kb}} down to 1 for the dtest. I can trigger the error on trunk with it and a much lower insert count in under 80 seconds total. I've turned it into a [dtest|https://github.com/jasobrown/cassandra-dtest/tree/13899]. Please review and let me know what you think.;;;","03/Oct/17 19:26;jasobrown;[~pauloricardomg] do you think you can finish up the review on this soonish?;;;","04/Oct/17 02:57;pauloricardomg;I am away this week but this is simple enough so I managed to have a quick look during vacation. ;)

bq. I can trigger the error on trunk with it and a much lower insert count in under 80 seconds total. I've turned it into a dtest.

Awesome, dtest looks good to me - verified that it fails without the patch and passes with it - thanks! Marking as ready to commit.;;;","04/Oct/17 07:05;jasobrown;Holy crap - I didn't know you were vacationing. Sorry about that, but thanks soo much for the review!!

committed as sha {{d080a73723d9aa402507c1ae04eef92ff6d44948}} to the cassandra repo, and as sha {{6ea3964d18b54b4e23b6e7ebf63ca42080e8404b}} to the dtests repo.;;;","04/Oct/17 07:06;jasobrown;updated the ticket title to more accurately describe the problem;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool compact and flush fail with ""error: null""",CASSANDRA-13897,13104479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stefania,corydoras,corydoras,23/Sep/17 07:57,16/Apr/19 09:29,14/Jul/23 05:56,23/Oct/17 02:05,3.11.2,,,,,,Legacy/Local Write-Read Paths,,,,,1,,,,,"{{nodetool flush}} and {{nodetool compact}} return an error message that is not clear. This could probably be improved. Both of my two nodes return this error.

{{nodetool flush}} Will return this error the first 2-3 times you invoke it, then the error temporarily disappears. {{nodetool compress}} always returns this error message no matter how many times you invoke it.

I have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error. 

{noformat}
cass@s5:~/apache-cassandra-3.11.0$ nodetool compact
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.<init>(ChunkCache.java:222)
	at org.apache.cassandra.cache.ChunkCache.wrap(ChunkCache.java:175)
	at org.apache.cassandra.io.util.FileHandle$Builder.maybeCached(FileHandle.java:412)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:381)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:331)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:333)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:318)
	at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:322)
	at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:370)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:733)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)

{noformat}
","* Apache Cassandra 3.11.0
* Linux 4.4.0-92-generic #115-Ubuntu SMP Thu Aug 10 09:04:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
* jdk1.8.0_144",blambov,cburroughs,corydoras,enriqueg,jjirsa,snazy,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 02:05:22 UTC 2017,,,,,,,,,,"0|i3kg4n:",9223372036854775807,3.11.0,3.11.1,,,,,snazy,,snazy,,,Low,,3.11.0,,,,,,,,,,,,,,,,,"09/Oct/17 03:46;jjirsa;Second report of this on 3.11.0 on the user list

https://lists.apache.org/thread.html/37730e41d91110710935e82eb06a9a7c6d720f21443351d416194f89@%3Cuser.cassandra.apache.org%3E;;;","16/Oct/17 03:22;corydoras;I can confirm upgrading to 3.11.1 does not resolve the problem. Also, the error is occurring with `nodetool compact` and I think it is not successfully completing the compaction in some or all cases.

Every time I restart a node I have to wipe the commit logs to get the node to boot, and when I do this data sometimes disappears and deleted data sometimes reappears;;;","16/Oct/17 03:36;corydoras;Here is my config (git diff -U0) against default 3.11.1 configuration


{code:java}
+++ b/cassandra-topology.properties
@@ -18,16 +18,2 @@
-192.168.1.100=DC1:RAC1
-192.168.2.200=DC2:RAC2
-
-10.0.0.10=DC1:RAC1
-10.0.0.11=DC1:RAC1
-10.0.0.12=DC1:RAC2
-
-10.20.114.10=DC2:RAC1
-10.20.114.11=DC2:RAC1
-
-10.21.119.13=DC3:RAC1
-10.21.119.10=DC3:RAC1
-
-10.0.0.13=DC1:RAC2
-10.21.119.14=DC3:RAC2
-10.20.114.15=DC2:RAC2
+10.1.1.125=dc1:rack1
+10.1.1.241=dc2:rack1
@@ -36 +22 @@
-default=DC1:r1
+default=dc1:rack1
@@ -41 +27 @@ default=DC1:r1
-fe80\:0\:0\:0\:202\:b3ff\:fe1e\:8329=DC1:RAC3
+#fe80\:0\:0\:0\:202\:b3ff\:fe1e\:8329=DC1:RAC3


{code}

And 


{code:java}
diff --git a/cassandra.yaml b/cassandra.yaml
index e847e54..cd30717 100644
--- a/cassandra.yaml
+++ b/cassandra.yaml
@@ -10 +10 @@
-cluster_name: 'Test Cluster'
+cluster_name: 'mycluster'
-          - seeds: ""127.0.0.1""
+          - seeds: ""10.1.1.125""
-listen_address: localhost
+listen_address: 10.1.1.125
-rpc_address: localhost
+rpc_address: 10.1.1.125
-# broadcast_rpc_address: 1.2.3.4
+broadcast_rpc_address: 10.1.1.125
-endpoint_snitch: SimpleSnitch
+endpoint_snitch: GossipingPropertyFileSnitch
{code}
;;;","18/Oct/17 01:52;stefania;The assertion that fails is {{assert Integer.bitCount(chunkSize) == 1;}}, the caching rebufferer is requiring a power of two for its chunks. However chunk sizes derive from buffer sizes, which are multiples of page cache sizes (4096) and, for example, 4096 * 3 is not a power of two. We should prepare a patch. Meanwhile, as a workaround, you can use mmap to read data, which will bypass the chunk cache, by setting {{disk_access_mode: mmap}} in the yaml.;;;","18/Oct/17 02:46;corydoras;Thanks, that was the pointer in the right direction to get me over the line on fixing it for my install. I left that setting at {{auto}}, but updated which version of Java I am using. Apparently I was using a 32bit version, so switching to a 64 bit version apparently enabled mmap.

I switched from:


{code:java}
cass:~$ java -version
java version ""1.8.0_144""
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) Server VM (build 25.144-b01, mixed mode)
{code}

To


{code:java}
cass:~$ java -version
java version ""1.8.0_151""
Java(TM) SE Runtime Environment (build 1.8.0_151-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)

{code}
;;;","18/Oct/17 02:54;stefania;bq. so switching to a 64 bit version apparently enabled mmap.

That's correct. {{disk_access_mode: auto}} defaults to standard on 32-bit and mmap on 64-bit, from {{DatabaseDescriptor}}:

{code}
        if (conf.disk_access_mode == Config.DiskAccessMode.auto)
        {
            conf.disk_access_mode = hasLargeAddressSpace() ? Config.DiskAccessMode.mmap : Config.DiskAccessMode.standard;
            indexAccessMode = conf.disk_access_mode;
            logger.info(""DiskAccessMode 'auto' determined to be {}, indexAccessMode is {}"", conf.disk_access_mode, indexAccessMode);
        }
{code}
;;;","18/Oct/17 07:55;stefania;Patch for 3.11 is available [here|https://github.com/stef1927/cassandra/tree/13897-3.11]. It rounds up to the next power of two since we cap at 64 KB. 

CircleCI was having github access problems, so I've launched the CI jobs on our internal servers.
;;;","18/Oct/17 08:44;stefania;[~blambov], can the {{CachingRebufferer}} be changed to only require a multiple of 4096 rather than a power of two? [~snazy] and myself are worried that neither rounding up nor rounding down to a power of two feels too right as in both cases we may end up being quite far from the initial value derived from the data and the disk optimization strategy. As far as I have understood, the rebufferer requires a power of two to calculate the page position, can it not align to 4096 multiples instead?;;;","18/Oct/17 08:56;blambov;The cache will also take buffers from the buffer pool of that size. If the size does not divide 64k, space will be wasted in each buffer, and this could cause buffer management problems. If we don't change the pool to gracefully handle this, I think it's safer to just round the size.;;;","19/Oct/17 02:00;stefania;Thanks for the explanation [~blambov], it looks like we should continue rounding to powers of two for the chunk cache. 

I've extended the [patch|https://github.com/stef1927/cassandra/tree/13897-3.11] to either round up or down [~snazy]. This can be configured via a yaml property that it only documented in config, not in the actual yaml file. This property is called {{file_cache_round_up}} and by default it will be false when the disk optimization strategy is ssd, therefore rounding down, and it will be true when it is set to spinning, therefore rounding up only for spinning disks.
;;;","20/Oct/17 15:48;snazy;+1 (assuming CI looks good)
One nit: make the new Config field a primitive instead of a boxed one.
;;;","23/Oct/17 02:05;stefania;Thanks for the review!

Committed to 3.11 as {{5b23054f10f4d6553e8dacbf53bd59e552f2a031}} and merged into trunk.

bq. make the new Config field a primitive instead of a boxed one.

It cannot be a primitive because we need {{== null}} to determine if the user set this value or not.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TriggerExecutor ignored original PartitionUpdate,CASSANDRA-13894,13104226,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,22/Sep/17 09:08,15/May/20 08:06,14/Jul/23 05:56,22/Sep/17 14:46,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"Since 3.0, the [TriggerExecutor.execute(PartitionUpdate)|https://github.com/jasonstack/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/triggers/TriggerExecutor.java#L82-L89] will only return augmented mutation, ignoring original update..

[Test|https://github.com/jasonstack/cassandra/commit/eb28844035242c4cb73c5148254f34672f2325da]to reproduce.


",,jasonstack,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 15:54:27 UTC 2017,,,,,,,,,,"0|i3kekv:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"22/Sep/17 09:28;jasonstack;| source |
| [trunk|https://github.com/jasonstack/cassandra/commits/trigger-missing-update] |
| [3.11|https://github.com/jasonstack/cassandra/commits/trigger-missing-update-3.11] |
| [3.0|https://github.com/jasonstack/cassandra/commits/trigger-missing-update-3.0] |

CI is running.

changes:  concatenate original PartitionUpdate with augmented updates.;;;","22/Sep/17 12:59;jasonstack;CircleCI passed. Failed some tests Internal CI , but seems not related, mostly on repair/sasi..;;;","22/Sep/17 14:45;pauloricardomg;Good catch! Fix LGTM. Committed as {{51e6f2446e71c8bd2ce89480b7d30d5b9ed1546e}} and merge up to cassandra-3.11 and trunk. Thanks!;;;","22/Sep/17 15:54;jasonstack;Thanks for reviewing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fromJson(null) throws java.lang.NullPointerException on Cassandra,CASSANDRA-13891,13104006,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,eribeiro,marcel@lisemarie.com,marcel@lisemarie.com,21/Sep/17 16:15,16/Mar/22 11:36,14/Jul/23 05:56,05/Apr/18 13:24,2.2.13,3.0.17,3.11.3,4.0,4.0-alpha1,,Legacy/CQL,,,,,0,,,,,"Basically, {{fromJson}} throws a {{java.lang.NullPointerException}} when NULL is passed, instead of just returning a NULL itself. Say I create a UDT and a table as follows:
{code:java}
create type type1
(
id int,
name text
);

create table table1
(
id int,
t FROZEN<type1>,

primary key (id)
);{code}
And then try and insert a row as such:

{{insert into table1 (id, t) VALUES (1, fromJson(null));}}

I get the error: {{java.lang.NullPointerException}}

This works as expected: {{insert into table1 (id, t) VALUES (1, null);}}

Programmatically, one does not always know when a UDT will be null, hence me expecting {{fromJson}} to just return NULL.",Cassandra 3.11,blerer,eribeiro,jasobrown,jjirsa,marcel@lisemarie.com,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #216:
URL: https://github.com/apache/cassandra/pull/216


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:36;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/18 21:05;eribeiro;CASSANDRA-13891.patch;https://issues.apache.org/jira/secure/attachment/12907184/CASSANDRA-13891.patch",,,,,,,,,,,,,,,,,,,1.0,eribeiro,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 05 13:24:00 UTC 2018,,,,,,,,,,"0|i3kd7z:",9223372036854775807,3.11.0,3.9,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"22/Jan/18 21:16;eribeiro;Hi [~marcel@lisemarie.com], and [~jdarcy], I am still finding my way again after a long period of absence from Cassandra dev, so I would like to see if I am missing something here. Thanks!;;;","23/Jan/18 22:16;jjirsa;Setting assignee and patch-availab.e

 ;;;","23/Jan/18 23:44;eribeiro;Thanks! :);;;","24/Jan/18 16:53;blerer;Thanks for the patch, it looks good to me.
Could you run CI on a 2.2 patched branch?
If the tests run fine I am +1. ;;;","26/Jan/18 22:05;eribeiro;Hi [~blerer],

Thanks for taking your time to look at this patch.

I have setup a 2.2 patched branch: [https://github.com/eribeiro/cassandra/tree/13891-2.2]

I have never used CircleCI before (the project is using this to build and test C* right?), so I am lost about how to build and run the CI on my 2.2 patched branch.

I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?

PS: Running {{JsonTest}} on both cassandra-2.2 and 13891-2.2 branches: it threw some errors I didn't see when the patch was 3.9+

 

Thanks again! ;;;","30/Jan/18 13:51;blerer;{quote}I have never used CircleCI before (the project is using this to build and test C* right?), so I am lost about how to build and run the CI on my 2.2 patched branch.

I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?{quote}

Unfortunately, I cannot help you here. I always used our internal CI.;;;","30/Jan/18 17:21;jjirsa;{quote}
I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?
{quote}

Once you create a new account, you put the circle yml into the new branch, and on push, circle should see you push to the new branch and build it.

If you make the account before you push, it probably won't try to build the new branch. Just re-push to trigger the build.
;;;","03/Apr/18 12:39;jasobrown;ftr, this error only occurs in 3.11 and trunk. Thus, I've ported [~eribeiro]'s patches to those branches and run tests.

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13891-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13891-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13891-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13891-trunk]|
||

Failing tests are unrelated and were previously unhealthy.

committed as sha {{28bd6c2a031e76b725dd773b949070962555698a}}. Thanks!
;;;","03/Apr/18 16:57;eribeiro;Hey, [~jasobrown], thank you very much. I was going to do this today, but you beat me to it. :) Thanks again and sorry for taking too long to look again at this issue.

PS: fyi, the first incarnation of this patch was adressed to trunk, but I was asked to port it to 2.2 first. 

 

Cheers!;;;","04/Apr/18 13:11;blerer;As [~eribeiro] pointed it out we also need patches for 2.2 and 3.0.;;;","04/Apr/18 13:49;jasobrown;[~blerer] The bug didn't happen on 2.2 or 3.0 when I tried it.

[~eribeiro] Can you confirm if the bug happens on 2.2 and 3.0? If so, I'll backport the patch.;;;","04/Apr/18 13:53;blerer;[~jasobrown] I did not tested it on 2.2 and 3.0. I just assumed that we had this bug since the begining. Nevertheless, I would prefer if we added the unit tests to all the versions.;;;","04/Apr/18 13:54;eribeiro;Sure! Gonna try to repro on 2.2 and 3.0 for double check, no problem. Thanks again, Jason!;;;","04/Apr/18 13:57;jasobrown;bq. I would prefer if we added the unit tests to all the versions.

This is fair enough. I'll wait for the results of [~eribeiro]'s testing and then do the backporting of the required parts.;;;","04/Apr/18 14:07;blerer;Thanks.;;;","04/Apr/18 17:24;eribeiro;[~jasobrown], the second test I created below fails with {{{{NullPointerException}}}}, but everything passes after the little fix. FYI, the test that fails on 2.2 and 3.0 is below:


{code:java}
execute(""INSERT INTO %s (k, frozenmapval) VALUES (?, fromJson(?))"", 0, null);
assertRows(execute(""SELECT k, frozenmapval FROM %s WHERE k = ?"", 0), row(0, null));{code}
 

I have open the following PRs targeting 2.2 and 3.0, respectively. I don't have access to a CI so it would be nice if you could run those tests. :)

[https://github.com/apache/cassandra/pull/215] 

[https://github.com/apache/cassandra/pull/216]

Oh, and let me know if I missed something, please. 

Thanks!;;;","05/Apr/18 13:24;jasobrown;+1 to backporting to 2.2 and 3.0. committed as sha {{2e5e11d66d41038bee8d2f81eb013f735d233def}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StrictLiveness for view row is not handled in AbstractRow,CASSANDRA-13883,13102947,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,18/Sep/17 12:41,15/May/20 08:02,14/Jul/23 05:56,25/Sep/17 06:44,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Materialized Views,,,,,0,,,,,"In {{AbstractRow.hasLiveData(nowInSecond)}}, it doesn't handle {{strictLiveness}} introduced in CASSANDRA-11500. The {{DataLimits}} counts the expired view row as live data and then the expired view row is purged in {{Row.purge()}}. When query with limit, we will get less data.

{code:title=test to reproduce}
    @Test
    public void testRegularColumnTimestampUpdates() throws Throwable
    {
        createTable(""CREATE TABLE %s ("" +
                    ""k int PRIMARY KEY, "" +
                    ""c int, "" +
                    ""val int)"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_rctstest"", ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"");

        updateView(""UPDATE %s SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s SET c = ? WHERE k = ?"", 1, 0);
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(1, 0, 1));

        updateView(""TRUNCATE %s"");

        updateView(""UPDATE %s USING TIMESTAMP 1 SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET c = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 2 SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 4 SET c = ? WHERE k = ?"", 2, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET val = ? WHERE k = ?"", 2, 0);

        // FIXME no rows return
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest limit 1""), row(2, 0, 2));
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(2, 0, 2));
    }
{code}",,jasonstack,KurtG,pauloricardomg,sbtourist,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 25 09:08:19 UTC 2017,,,,,,,,,,"0|i3k6pj:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"20/Sep/17 01:52;jasonstack;| source | unit | dtest |
| [trunk|https://github.com/apache/cassandra/compare/trunk...jasonstack:CASSANDRA-13883-trunk?expand=1]| [passed|https://circleci.com/gh/jasonstack/cassandra/627] |   repair_tests.repair_test.TestRepair.dc_parallel_repair_test
repair_tests.repair_test.TestRepair.dc_repair_test
repair_tests.repair_test.TestRepair.local_dc_repair_test
repair_tests.repair_test.TestRepair.simple_parallel_repair_test
repair_tests.repair_test.TestRepair.thread_count_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test
disk_balance_test.TestDiskBalance.disk_balance_decommission_test
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space|
| [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-13883-3.11?expand=1] | [passed|https://circleci.com/gh/jasonstack/cassandra/625] | upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-13883-3.0?expand=1] | [passed|https://circleci.com/gh/jasonstack/cassandra/628]| global_row_key_cache_test.TestGlobalRowKeyCache.functional_test
repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test |
| [dtest|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:CASSANDRA-13883?expand=1] |

CI looks good, failure seems unrelated.

{code}
Changes:
    1. Change {{AbstractRow.hasLiveData}} to check {{enforceStrictLiveness}}:  if livenessInfo is not live and enforceStrictLiveness, then there is not live data.
    2. For SPRC.group, use the first command to get {{enforceStrictLiveness}} since each command should be the same except for key.
{code};;;","25/Sep/17 06:44;pauloricardomg;Oh, it seems like this was in the original version of CASSANDRA-11500, and missed during the final refactor. Good catch!

Patch LGTM, committed as {{68bdf45477417c97fa6ed3840eee39b8390fd678}} on cassandra-3.0 and merged up to trunk, and cassandra-dtest commit as {{2a1ce8450d1876c3df58ea7e85d352c428de2ca2}}. Thanks!;;;","25/Sep/17 09:08;jasonstack;Thanks for reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix short read protection for tables with no clustering columns,CASSANDRA-13880,13102665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,15/Sep/17 19:18,16/Apr/19 09:29,14/Jul/23 05:56,19/Sep/17 10:21,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,,,,,"CASSANDRA-12872 fixed counting replica rows, so that we do now fetch more than one extra row if necessary.

Fixing the issue caused consistency_test.py:TestConsistency.test_13747 to start failing, by exposing a bug in the way we handle empty clusterings.

When {{moreContents()}} asks for another row and {{lastClustering}} is {{EMPTY}}, the response again (and again) contains the row with {{EMPTY}} clustering.

SRP assumes it’s a new row, counts it as one, gets confused and keeps asking for more, in a loop, again and again.

Arguably, a response to a read command with the following non-inclusive {{ClusteringIndexFilter}}:

{code}
command.clusteringIndexFilter(partitionKey).forPaging(metadata.comparator, Clustering.EMPTY, false);
{code}

... should return nothing at all rather than a row with an empty clustering.

Also arguably, SRP should not even attempt to fetch more rows if {{lastClustering == Clustering.EMPTY}}. In a partition key only column
we shouldn’t expect any more rows.

This JIRA is to fix the latter issue on SRP side - to modify SRP logic to short-circuit execution if {{lastClustering}} was an {{EMPTY}} one instead of querying pointlessly for non-existent extra rows.",,aleksey,benedict,jasonstack,jjirsa,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 19 10:21:34 UTC 2017,,,,,,,,,,"0|i3k4zz:",9223372036854775807,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"18/Sep/17 16:35;aleksey;Branches with fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13880-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13880-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13880-4.0]. An isolated dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13880].

The issue only triggers with a per partition limit set, doesn't with a regular limit. I am investigating why, but whatever the reason is, this fix is wanted/needed anyway - it does save us a pointless roundtrip in many scenarios.;;;","18/Sep/17 19:57;benedict;+1;;;","19/Sep/17 10:21;aleksey;Thanks.

CircleCI run for [3.0|https://circleci.com/gh/iamaleksey/cassandra/34] has a bunch of annoying MV timeout issues again, and [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/326/testReport/] have those annoying git clone issues, again, but everything seems alright otherwise.

Committed to 3.0 as [35e32f20ba8cba6cbfb1bee4252c0edd8684cdb1|https://github.com/apache/cassandra/commit/35e32f20ba8cba6cbfb1bee4252c0edd8684cdb1] and merged with 3.11 and trunk. Dtest committed as [163f82c2db0e86d4dd8f312b291ccd094891b986|https://github.com/apache/cassandra-dtest/commit/163f82c2db0e86d4dd8f312b291ccd094891b986].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool setcachecapacity behaves oddly when cache disabled,CASSANDRA-13874,13102371,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bereng,rustyrazorblade,rustyrazorblade,14/Sep/17 20:17,27/May/22 19:25,14/Jul/23 05:56,02/Sep/21 09:14,3.11.12,4.0.1,4.1,4.1-alpha1,,,Legacy/Core,Local/Config,,,,0,lhf,user-experience,,,"If a node has row cache disabled, trying to turn it on via setcachecapacity doesn't issue an error, and doesn't turn it on, it just silently doesn't work.",,bereng,blerer,jeromatron,mychal,rustyrazorblade,shaurya10000,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/17 23:03;mychal;13874-trunk.txt;https://issues.apache.org/jira/secure/attachment/12894864/13874-trunk.txt",,,,,,,,,,,,,,,,,,,1.0,bereng,,,,,,,,,,,,,,,,,,,Low Hanging Fruit,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 30 09:04:39 UTC 2021,,,,,,,,,,"0|i3k36v:",9223372036854775807,,,,,,,rustyrazorblade,,bereng,blerer,,Low,,3.11.x,,,https://github.com/apache/cassandra/commit/957c6264ef97909a043a70b96cf896b1feb0f204,,,,,,,,,See PR,,,,,"19/Jul/21 05:31;bereng;Apologies I moved status unintentionally. [~rustyrazorblade] I see little movement, are you busy and do you mind I take over moving this forward?;;;","19/Aug/21 09:25;bereng;I reworded the message a bit [here|https://github.com/apache/cassandra/pull/1154] and fired CI. [~vane] do you fancy trying out 4.0 and trunk PRs?

Edit: [CI|https://app.circleci.com/pipelines/github/bereng/cassandra?branch=CASSANDRA-13874-3.11] LGTM. I will be OOO next week in case you ping;;;","20/Aug/21 14:47;blerer;The ticket is about the behavior of nodetool when we call {{setcachecapacity}} on a disabled cache. By consequence, I think it will be good to also have a nodetool test that check that scenario. ;;;","20/Aug/21 14:48;blerer;Moving the patch to open.;;;","30/Aug/21 07:58;bereng;[~blerer] you're right, I had forgotten there was some nodetool junit capabilities in 3.11. Does [this|https://github.com/apache/cassandra/pull/1154/commits/a4df8975ab9dd25a3c0b00d660d5eef7919e4e56] help? Please let me know so I can put up the rest of PRs.;;;","30/Aug/21 09:04;blerer;Thanks [~bereng]. Your fix look good to me.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ref bug in Scrub,CASSANDRA-13873,13102367,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,tjake,tjake,14/Sep/17 19:59,15/May/20 08:00,14/Jul/23 05:56,11/Dec/17 08:08,2.2.12,3.0.16,3.11.2,4.0,4.0-alpha1,,Legacy/Tools,,,,,0,,,,,"I'm hitting a Ref bug when many scrubs run against a node.  This doesn't happen on 3.0.X.  I'm not sure if/if not this happens with compactions too but I suspect it does.

I'm not seeing any Ref leaks or double frees.

To Reproduce:

{quote}
./tools/bin/cassandra-stress write n=10m -rate threads=100
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
{quote}

Eventually in the logs you get:
WARN  [RMI TCP Connection(4)-127.0.0.1] 2017-09-14 15:51:26,722 NoSpamLogger.java:97 - Spinning trying to capture readers [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-32-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-31-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-29-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-27-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-26-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-20-big-Data.db')],
*released: [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db')],* 

This released table has a selfRef of 0 but is in the Tracker
",,aleksey,jasonstack,jeromatron,jjirsa,jjordan,jkni,KurtG,marcuse,razi.khaja@gmail.com,tjake,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 05 08:20:13 UTC 2018,,,,,,,,,,"0|i3k35z:",9223372036854775807,3.10,3.11.0,4.0,,,,jkni,,jkni,,,Normal,,,,,,,,,,,,,,,,,,,"14/Sep/17 20:10;tjake;I should probably mention this happens when we cancel compactions.  In 3.0 they would just wait till previous runs finished, now we cancel them.;;;","19/Oct/17 02:46;jkni;It looks like this situation can occur when referencing canonical sstables. As far as I can tell, the issue reproduces only when we have an sstable in a lifecycle transaction with no referencers other than its selfref. If the lifecycle transaction updates this sstable, we'll put a new instance of the sstable reader in the tracker. This causes no problems when getting live sstables, but the canonical sstables can also include sstable readers from the compacting set. In this case, the sstable reader that got updated will still be in the compacting set, but we won't be able to reference it when we try to select and reference canonical sstables, since its instance tidier has run when its last ref was released in the lifecycle transaction. Note that the global tidier doesn't run, since the updated sstable reader is still referenced. With the reproduction provided above in the multiple scrub, the scrubs will eventually proceed once the lifecycle transaction finishes, since it will put an updated sstablereader in the tracker. If there is a situation where a lifecyce transaction needed to select canonical sstables to proceed, this could cause a deadlock.

I pushed a branch at [c13873-2.2|https://github.com/jkni/cassandra/commit/ba70f70d97f648037e742a16bfdf1c8002d2be9c] that implements the simplest fix I can think of. The patch references the original sstables involved in a lifecycle transaction when we create the transaction, releasing these references whenever we do postCleanup or cancel an sstable reader from a transaction. I merged this forward and tests came back clean on all active branches. I'm not sure if there is some existing mechanism that should cover this case - maybe [~krummas] knows from reviewing [CASSANDRA-9699]?;;;","19/Oct/17 11:03;marcuse;bq. the scrubs will eventually proceed once the lifecycle transaction finishes,
but shouldn't cancelling the scrub-compaction also finish the txn?

I'll take review unless you had someone else lined up - -is this still ""critical"" after your analysis [~jkni]-? Edit: yup, looks like it;;;","19/Oct/17 14:05;jkni;You're correct that cancelling will also finish the txn and allow operations to select and reference canonical sstables. In the specific repro that Jake provided, which is the case of multiple scrubs over the same cfs (an admittedly somewhat artificial case), we'll try to select and reference canonical sstables in the snapshot before cancelling the original scrub compaction, so the new scrubs will hang until the original scrub finishes.

That'd be great if you could review. I'm admittedly very unfamiliar with this part of the code, so I expect my initial patch is a rough sketch of the eventual solution.

As far as criticality goes, I could go either way. I know of no situations that this causes data loss or permanent deadlocks at this time, but it can potentially cause operations referencing canonical sstables to hang for long periods of time.;;;","19/Oct/17 14:32;marcuse;looks like this is a problem way back in 2.1 as well and my fix in CASSANDRA-10829 only made the window where this can happen smaller;;;","20/Oct/17 12:47;marcuse;correction, there is only a tiny race just as we finish up compactions in 2.1 - not worth fixing at this point;;;","20/Oct/17 15:03;marcuse;The problem seems to be that we don't grab a reference before starting the compaction everywhere - we do it in [CompactionTask|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java#L177], but not during [cleanup|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1095-L1098] for example.

[~jkni] I think we should do your fix (and also remove the external ref-grabbing in CompactionTask), but maybe only in trunk? It feels a tiny bit safer to not touch {{LifecycleTransaction}}/{{CompactionTask}} in older versions and instead take the ref outside where we need it, but I'm not entirely convinced, wdyt?
;;;","20/Oct/17 15:17;jkni;+1 - I'd like to introduce as few changes as possible to older versions here. That combination sounds good to me. Do you want to prepare the patch for older versions or would you like me to?;;;","24/Oct/17 12:26;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13873
https://github.com/krummas/cassandra/commits/marcuse/13873-3.0
https://github.com/krummas/cassandra/commits/marcuse/13873-3.11

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/387/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/388/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/389/

https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.0
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.11;;;","29/Nov/17 13:39;marcuse;ping [~jkni] - should we port the trivial patch to trunk as well and get this in? We could improve it in another ticket I guess;;;","30/Nov/17 00:56;jkni;Sorry for the latency here - my fault. The patch looks good to me. I considered a few other cases where a similar problem might exist. It seems to me the same issue could exist in in the Splitter/Upgrader, but since they're offline, I don't know what future changes would require another operation to reference canonical sstables in parallel. I also don't see anything in anticompaction grabbing a ref; am I missing something there?

The patches look good for existing cases. Unfortunately, I let the dtests age out before taking a closer look, but I can rerun them after you look at the question above. I'm +1 to merging the relatively trivial patches through to trunk and opening a ticket to improve it later. As you've seen, I don't have a huge amount of bandwidth for this right now, so I'd rather not delay a definite improvement with only the promise of a better one. Thanks for the patience.;;;","30/Nov/17 08:40;marcuse;bq. Splitter/Upgrader, but since they're offline
yeah, don't think we need it here, we don't open early when doing offline operations and we should have no concurrent operations referencing sstables

bq. I also don't see anything in anticompaction grabbing a ref;
looks like we do it [here|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/service/ActiveRepairService.java#L565-L584]

I'll prepare the patches and start the dtests;;;","30/Nov/17 09:00;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13873
https://github.com/krummas/cassandra/commits/marcuse/13873-3.0
https://github.com/krummas/cassandra/commits/marcuse/13873-3.11
https://github.com/krummas/cassandra/commits/marcuse/13873-trunk

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/444/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/445/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/446/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/447/

https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.0
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.11
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-trunk;;;","01/Dec/17 03:38;jkni;Thanks for the patches and CI. Both your remarks look correct to me; frankly, I have no idea how I missed that in anticompaction.

Test results look good for the most part. There's a few flaky unit tests on 3.0/3.11 that appear to have failed the same way before the patch, pass for me locally, and appear to be at the limits of CircleCI's timeouts/resources. The 2.2 dtests timed out, so it seems worthwhile to trigger those again just in case. The only unusual failures on 3.0 dtests are a bunch of tests where Jolokia failed to attach for JMX. I'm not sure if this is a known environmental problem on ASF dtests, but I was unable to reproduce this elsewhere.

Overall, +1 to the patch for me, and this looks good to merge if none of the test issues I raised above worry you.;;;","08/Dec/17 15:22;marcuse;finally got the 2.2 tests run properly and the failures look unrelated: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/456/

I'll get this committed (on monday..);;;","08/Dec/17 23:08;jkni;Great. Thanks!;;;","11/Dec/17 08:08;marcuse;and committed as {{3cd2c3c4ea4286562b2cb8443d6173ee251e6212}}, thanks;;;","11/Jan/18 13:47;tsteinmaurer;Any chance to get this back-ported also to 2.1.x? We are having a lots of 2.1 nodes out there in production with no immediate plan to upgrade, but e.g. we can't have a combination of {{nodetool cleanup}} (e.g. after extending the cluster with additional nodes) and an hourly cron job taking snapshots. See linked issue CASSANDRA-11155. Thanks.;;;","05/Feb/18 05:41;KurtG;[~tsteinmaurer] I couldn't reproduce the issue on my end. I suspect you need at least a few large SSTables to trigger the issue. Can you try building from [this branch|https://github.com/kgreav/cassandra/tree/13873-2.1] which is just latest 2.1 with the simple backport on top and seeing if it fixes the issue?;;;","05/Feb/18 08:20;tsteinmaurer;[~KurtG], thanks for the follow-up. Unfortunately, I don't have a larger environment available to give your back-port a try at the moment, cause we have work-arounded the issue for our scaled out clusters beyond our replication factor by disabling and then re-enabling the hourly cron job taking snapshots.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractTokenTreeBuilder#serializedSize returns wrong value when there is a single leaf and overflow collisions,CASSANDRA-13869,13102346,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jwest,jwest,jwest,14/Sep/17 19:05,15/May/20 08:04,14/Jul/23 05:56,15/Sep/17 21:10,3.11.1,4.0,4.0-alpha1,,,,Feature/SASI,,,,,0,,,,,In the extremely rare case where a small token tree (< 248 values) has overflow collisions the size returned by AbstractTokenTreeBuilder#serializedSize is incorrect because it fails to account for the overflow collisions. ,,cscotta,jasobrown,jjirsa,jwest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/17 17:17;jwest;0001-Fix-AbstractTokenTreeBuilder-serializedSize-when-the.patch;https://issues.apache.org/jira/secure/attachment/12887388/0001-Fix-AbstractTokenTreeBuilder-serializedSize-when-the.patch","15/Sep/17 17:17;jwest;attb-serialized-size-bug-test.patch;https://issues.apache.org/jira/secure/attachment/12887387/attb-serialized-size-bug-test.patch",,,,,,,,,,,,,,,,,,2.0,jwest,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 21:10:27 UTC 2017,,,,,,,,,,"0|i3k313:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"15/Sep/17 17:18;jwest;Attached two patches. attb-serialized-size-bug-test.patch is patch that can be applied to trunk illustrate the issue with a failing test. The other is the fix against trunk and some improved testing. ;;;","15/Sep/17 17:20;jasobrown;||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13869-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13869-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/322/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/321/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13869-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13869-trunk]|
;;;","15/Sep/17 21:10;jasobrown;+1. committed as sha {{436761ed6afbfdc333c6642f6b4418d44712a0b7}}

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Safely handle empty buffers when outputting to JSON,CASSANDRA-13868,13102326,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,14/Sep/17 17:40,15/May/20 08:03,14/Jul/23 05:56,15/Sep/17 11:30,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,0,,,,,"As discussed in CASSANDRA-13734, when {{Selection#rowToJSON}} encounters an empty {{ByteBuffer}}, a {{BufferUnderflowException}} can be thrown. At a minimum let's be defensive and not throw an error.",,jasobrown,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 11:30:29 UTC 2017,,,,,,,,,,"0|i3k2wn:",9223372036854775807,,,,,,,samt,,samt,,,Low,,2.2.8,,,,,,,,,,,,,,,,,"14/Sep/17 18:00;jasobrown;Patch here

||2.2||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13868-2.2]|[branch|https://github.com/jasobrown/cassandra/tree/13868-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13868-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13868-trunk]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-2.2]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-3.0]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-trunk]|
;;;","15/Sep/17 11:30;samt;Thanks [~jasobrown], I took the liberty of committing to cassandra-2.2 in {{a8e2dc52409ce0dc7476d60b3adee34c547f0b14}} and merging to 3.0/3.11/trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clock-dependent integer overflow in tests CellTest and RowsTest,CASSANDRA-13866,13102102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jkni,jkni,jkni,13/Sep/17 23:27,15/May/20 08:00,14/Jul/23 05:56,15/Sep/17 21:40,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Testing,,,,,0,,,,,"These tests create timestamps from Unix time, but this is done as int math with the result stored in a long. This means that if the test is run at certain times, like 1505177731, corresponding to Tuesday, September 12, 2017, 12:55:31, the test can have two timestamps separated by a single second that reverse their ordering when multiplied by 1000000, such as 1505177731 -> 2147149504 and 1505177732 -> -2146817792. This causes a variety of test failures, since it changes the reconciliation order of these cells.

Note that I've tagged this as trivial because the problem is in the manual construction of timestamps in the test; I know of nowhere  that we make this mistake with real data.",,jjirsa,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 21:40:29 UTC 2017,,,,,,,,,,"0|i3k1j3:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"13/Sep/17 23:48;jkni;Trivial patches for [3.0|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-3.0], [3.11|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-3.11], and [trunk|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-trunk]. The 3.0 patch merges forward cleanly.;;;","14/Sep/17 00:00;jjirsa;lgtm.
;;;","15/Sep/17 21:40;jkni;Thanks! Committed to 3.0 branch as {{d79fc9a2258d10e8a54fd4136d5544e10ad3ddda}} and merged forward.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCS ignores compaction thresholds in L0 STCS,CASSANDRA-13861,13101308,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,11/Sep/17 16:04,15/May/20 08:00,14/Jul/23 05:56,15/Sep/17 20:58,4.0,4.0-alpha1,,,,,Local/Compaction,,,,,0,,,,,min max compaction thresholds are hard coded to 4 and 32,,cnlwsu,githubbot,jeromatron,jjirsa,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 20:58:08 UTC 2017,,,,,,,,,,"0|i3jwqn:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"11/Sep/17 16:08;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/150

    Use compaction threshold for STCS in L0 for CASSANDRA-13861

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13861

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/150.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #150
    
----
commit ed584ec70a9f1889a317405aa869758145c3c6a5
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-09-11T16:07:40Z

    Use compaction threshold for STCS in L0 for CASSANDRA-13861

----
;;;","11/Sep/17 16:54;jjirsa;+1 (for trunk only). 

We should be good citizens and run through circleci , just to be sure we don't have any surprises (I know it's a trivial patch, but wouldnt want to break the build or something with missing semicolon). Can you queue that up or do you need me to?
;;;","15/Sep/17 15:37;cnlwsu;https://circleci.com/gh/clohfink/cassandra/39;;;","15/Sep/17 20:58;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/150
;;;","15/Sep/17 20:58;jjirsa;Committed as {{85b74ca5f53b1285b65a0909168a326ab1ac650a}} , thanks!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when closing stream sessions (4.0),CASSANDRA-13852,13100540,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,07/Sep/17 18:14,15/May/20 08:00,14/Jul/23 05:56,07/Sep/17 23:47,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"bootstrap_test:TestBootstrap.manual_bootstrap_test is hanging due to a race condition that can occur when waiting for a streaming COMPLETE message and the remote side has already closed the connection. We do not check to see if we still have remaining bytes buffered for consumption in {{RebufferringByteBufDataInputPlus}}, but always go ahead and throw an {{EOFException}}. We should consume the bytes, then throw an {{EOFException}} on subsequent calls.",,bdeggleston,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13858,CASSANDRA-13839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 23:47:07 UTC 2017,,,,,,,,,,"0|i3jryv:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"07/Sep/17 18:15;jasobrown;This was discovered via bootstrap_test:TestBootstrap.manual_bootstrap_test hanging when running dtests;;;","07/Sep/17 18:18;jasobrown;Patch to fix this problem here:

||hang||
|[branch|https://github.com/jasobrown/cassandra/tree/bootstrap-dtest-hang]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/287]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/bootstrap-dtest-hang]|

There are other dtests problems happening, and I""m on those now, as well.;;;","07/Sep/17 23:29;jasobrown;This fix should also resolve the hanging in system_auth_ks_is_alterable_test (auth_test.TestAuth);;;","07/Sep/17 23:35;bdeggleston;+1;;;","07/Sep/17 23:47;jasobrown;committed as sha {{83822d12d87dcb3aaad2b1e670e57ebef4ab1c36}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow existing nodes to use all peers in shadow round,CASSANDRA-13851,13100446,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,07/Sep/17 12:36,29/Jul/22 17:45,14/Jul/23 05:56,17/Apr/18 17:16,3.11.3,4.0,4.0-alpha1,,,,Local/Startup and Shutdown,,,,,0,,,,,"In CASSANDRA-10134 we made collision checks necessary on every startup. A side-effect was introduced that then requires a nodes seeds to be contacted on every startup. Prior to this change an existing node could start up regardless whether it could contact a seed node or not (because checkForEndpointCollision() was only called for bootstrapping nodes). 

Now if a nodes seeds are removed/deleted/fail it will no longer be able to start up until live seeds are configured (or itself is made a seed), even though it already knows about the rest of the ring. This is inconvenient for operators and has the potential to cause some nasty surprises and increase downtime.

One solution would be to use all a nodes existing peers as seeds in the shadow round. Not a Gossip guru though so not sure of implications.

",,daniel.cranford,jasobrown,jasonstack,jay.zhuang,jeromatron,jjirsa,KurtG,paulo,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 29 17:45:22 UTC 2022,,,,,,,,,,"0|i3jrdz:",9223372036854775807,3.6,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"15/Sep/17 15:15;samt;Seeds have a secondary purpose in gossip, to help shorten convergence time (though this is a little contentious, see CASSANDRA-9206). 
If a node is restarted and cannot contact any seeds (especially if the configured seeds have been removed from the cluster), it's probably a good thing that operators are aware of that. ;;;","15/Sep/17 17:00;jjirsa;Agree with [~KurtG] here that this is a regression. We should be able to start if all the seeds are down. Imagine doing a full cluster bounce (either all nodes at once, or one of each replica set), there's a pretty good chance you'll end up in a weird state trying to order restart based on seeds first, which is not ideal.;;;","18/Sep/17 05:08;KurtG;Certainly agree we should make operators aware of that. Let's most definitely not do that by surprise ""you can't start C*"". Those are the worst types of surprise. Just needs a warning on startup that no live seeds are configured IMO.
I'll probably have a crack at this after NGCC, not going to assign myself to it yet though in case someone else wants to have at it.;;;","01/Nov/17 10:26;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|

Current changes pass through whatever peers exist in system.peers to the shadow round and will only use them if the first round of gossips to the seeds fail. So it will still try the seeds first, it's just if the shadow round goes longer than 5 seconds will peers be included. This allows a node to start if only peers are alive.
It also allows a node that doesn't need to bootstrap to start, even if it couldn't contact any peers or seeds. 

From what I can tell this works but open to ideas on other test cases/obvious things I've missed.

FWIW this also passed all the bootstrap dtests, if that means anything.

edit: updated dtest to actually fail when there is a timeout, rather than throwing an error.;;;","30/Nov/17 21:16;jjirsa;Who wants to review a gossip patch? [~jasobrown] or [~jkni], you two have touched it most recently?
;;;","01/Dec/17 13:28;jasobrown;I can add it to my (ever growing) queue, but no guarantees on promptness.;;;","14/Dec/17 20:31;samt;This looks pretty reasonable, but the thing that concerns me is that the 'starting unsafely' path can be reached in 2 ways:
1) when a new node comes up and has no seeds (other than itself) configured, and no persisted ring info. 
2) when a non-bootstrapping node starts up with seeds and/or peers it *could* contact, but it's unable to reach any of them within RING_DELAY.

I'm not worried about the former case, it is necessary for starting a single node cluster (or the first node in a multi node cluster if starts are staggered) and is also existing behaviour post CASSANDRA-10134.
The latter case though could lead to a regression of CASSANDRA-10134. If a new node comes up with the same address as an existing down node, and it is not bootstrapping (because it already bootstrapped, auto_bootstrap: false, or it's present in its own seed list) it will take over that address if it cannot perform the shadow round i.e. if partitioned.

bq. Imagine doing a full cluster bounce

There is already a mechanism in place to work around a whole cluster bounce. If the whole cluster is restarted, all seeds would be in their own shadow rounds. They report this to the any node contacting them and if all seeds are found to be in that state, the node exits its own shadow round and continues to start. In large clusters, you might have to increase RING_DELAY to give all seeds a chance to enter their shadow round before other nodes start to time theirs out.

Extending the shadow round targets to include known peers works well for the scenario where all seeds are down for a longer period, so I'm +1 on that. I just think that to avoid regression of 10134 we still need to hard fail if a node coming up is not able to contact any seeds *or* peers at all (unless case 1 above applies).

nits:
* 1 arg version of checkForEndpointCollision is unused
* Having 2 log statements at the end of doShadowRound is a bit meaningless, as in both cases we proceed even though the state of the cluster is unknown.
;;;","18/Dec/17 04:01;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/51]|

Updated the patch so that a node that's not a seed won't be able to start if it can't contact any peers. TBH I don't think this is the best solution as it's still in part a regression on the old behaviour and requires you startup seeds first in a full cluster bounce, but if anyone still sees that as a major issue it can be done in a separate ticket as it's likely a bit of refactoring.

dtest has also been updated.;;;","03/Jan/18 16:09;samt;I'm +1 on this latest version, though it occurs to me that there is something else we could do to help full cluster bounces that are done in one shot (per-replica set or otherwise partial bounces will now proceed ok).

Failure to receive an ack within RING_DELAY will terminate the shadow round, fatally for a node not in it's own seed list. So if we make non-seeds remain in the SR for longer than seeds, (e.g. for RING_DELAY * 2), then as long as a single seed is contactable, startup should be able to proceed.
 
e.g. all peers have nodes 1, 2 & 3 configured as seeds, but 2 & 3 have failed. If the cluster is completely stopped and restarted, node1 will exit its SR after RING_DELAY and be available to ack the other nodes' syn requests. Once other, non-seeds start to come up, they will also now ack shadow round syns. 
This would increase startup times for a full bounce when some seeds are failing/missing, but in ""normal"" circumstances it would have no impact. 
It wouldn't help if all of the seeds 1, 2 & 3 were down during a full bounce, but I'd consider that tradeoff acceptable.
;;;","17/Jan/18 02:05;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/54]|

Added another quick dtest to make sure that if a non-seed starts it will still join as long as the seed is started within {{RING_DELAY * 2}}.;;;","14/Mar/18 03:09;KurtG;[~beobal] have you had a chance to review the latest? I've rebased both dtests and patch and also uploaded a branch for trunk (now that it no longer merges).
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[trunk|https://github.com/apache/cassandra/compare]|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|
|[utests|https://circleci.com/gh/kgreav/cassandra/135]|[utests|https://circleci.com/gh/kgreav/cassandra/137]|;;;","14/Mar/18 12:51;samt;[~KurtG] sorry, I totally missed the last update, I'll try my best to look at it today.;;;","14/Mar/18 17:21;samt;Thanks for adding the extra delay for non-seeds, that all LTGM so I've kicked off some dtest runs on CircleCI. If they look good I'll commit everything and update here.
|3.11|[circleci workflow|https://circleci.com/workflow-run/b931896e-6ebb-4513-8f0c-7b6bb8486ee0]|
|trunk|[circleci workflow|https://circleci.com/workflow-run/36423e6e-1763-4274-b923-d78d2947a812]|

There were a couple of tweaks I made to the dtest, lmk if those are ok with you: [https://github.com/beobal/cassandra-dtest/commit/94dee685e10f8819752d7c27533ba8f59b7acb9b]

One small request for future stuff, it makes reviewing iterations on patches much easier if you don't rebase/squash once the review is ongoing. Additional commits + periodically merging from the base branch makes it a lot simpler to pick out the incremental changes during the course of a review. Thanks!;;;","15/Mar/18 02:56;KurtG;Ah sure thing. Sorry about that.

dtest changes LGTM but looks like {{test_startup_non_seed_with_peers}} is failing on trunk (unrelated to changes). I'll take a look.;;;","04/Apr/18 10:42;KurtG;[~beobal] Sorry for the delay, went on holidays and just got around to fixing the test. Turns out it was failing because the log message changed ever so slightly after CASSANDRA-7544 by removing the leading slash on the IP address. I also applied your dtest changes to my branch and made some minor formatting changes.

|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/150]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:13851-trunk]|[utests|https://circleci.com/gh/kgreav/cassandra/148]|
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|;;;","12/Apr/18 02:57;KurtG;ping [~beobal]. keen on getting this in before something breaks it again. :);;;","12/Apr/18 07:06;samt;[~KurtG] ack. I did see the update, but have been a bit snowed under this past week. I'll get it done by Monday, latest.;;;","17/Apr/18 17:16;samt;Thanks [~KurtG], latest CI (after rebases) looks good committed so I've to cassandra-3.11 in {{28ccf3fe3989d9d80063fe4d4bb048efe471936b}} and merged to trunk. 
I'll get the dtest committed directly.;;;","18/Apr/18 00:04;KurtG;Thanks heaps [~beobal]!;;;","21/Jul/22 16:27;daniel.cranford;This is still an *undocumented* regression in the definition of a ""seed"" node. A node *will not start* unless it can contact at least one seed node which is a detail that still hasn't made it into the [documentation|https://cassandra.apache.org/doc/latest/cassandra/faq/index.html#what-are-seeds] ;;;","21/Jul/22 16:57;brandon.williams;Please open a ticket for that to be added to the documentation.;;;","22/Jul/22 09:55;samt;bq. A node will not start unless it can contact at least one seed node 

This is not true. In fact, it was the very point of this JIRA to fix that regression, which was introduced in CASSANDRA-10134. If a node cannot contact any seeds during startup, it will expand the set of nodes contacted in the shadow round to include all known peers. So only if no peers at all can be reached will the node fail to start (and even then, only if the node is not a seed itself).

{code}
            
{code:java}
while (true)
{
    if (slept % 5000 == 0)
    { // CASSANDRA-8072, retry at the beginning and every 5 seconds
        logger.trace(""Sending shadow round GOSSIP DIGEST SYN to seeds {}"", seeds);

        for (InetAddressAndPort seed : seeds)
            MessagingService.instance().send(message, seed);

        // Send to any peers we already know about, but only if a seed didn't respond.
        if (includePeers)
        {
            logger.trace(""Sending shadow round GOSSIP DIGEST SYN to known peers {}"", peers);
            for (InetAddressAndPort peer : peers)
                MessagingService.instance().send(message, peer);
        }
        includePeers = true;
    }

    Thread.sleep(1000);
    if (!inShadowRound)
        break;

    slept += 1000;
    if (slept > shadowRoundDelay)
    {
        // if we got here no peers could be gossiped to. If we're a seed that's OK, but otherwise we stop. See CASSANDRA-13851
        if (!isSeed)
            throw new RuntimeException(""Unable to gossip with any peers"");

        inShadowRound = false;
        break;
    }
}
{code}
;;;","22/Jul/22 15:02;daniel.cranford;[~samt], sorry, I miss-spoke. I appreciate the material improvement in behavior this ticket has provided. What I intended to say was
{quote}A node will note start unless it can contact a seed node *or* another node not also performing the shadow round{quote}

Background: my operations guys routinely perform a full cluster bounce to ensure everything is starting from a clean state. Up until Cassandra 3.6 this worked fine. Unfortunately, due to the details of our hardware, sometimes nodes take longer to come up than usual (eg 5 minutes instead of 30 seconds). If the slow nodes happen to be the seed node/nodes, it is game over - the cluster will not start.

The only way my ops guys were able to figure out how to resolve this was to give me the stack trace of the error, which I had to correlate with the source code and use `git blame` to find CASSANDRA-10134 and this ticket. I would not consider a bug tracker to be appropriate documentation for the semantics of a seed node, especially when the public docs state
{quote}The ring can operate or boot without a seed; however, you will not be able to add new nodes to the cluster.{quote}

My ops guys have worked around this behavior by begrudgingly setting `cassandra.allow_unsafe_joins=true` - an undocumented workaround I found by inspecting the source code. After we upgraded from 3.9 to 3.11, I was eager to see if this ticket allowed us to remove the workaround. Unfortunately it does not, since a full cluster bounce will still fail since only seed nodes and nodes not themselves in the shadow round can release a node from the shadow round.

If anything, the error message in this version is worse, since it is now incorrect. 
{code:java}
if (!isSeed)
    throw new RuntimeException(""Unable to gossip with any peers"");
{code}

actually, the node was unable to gossip with any seeds and any peers not themselves in the shadow round. Peers may be alive but themselves trapped in the shadow round.;;;","22/Jul/22 16:45;samt;The first thing I would say is that if you routinely shutdown the entire cluster, you're going to experience unavailability. It _shouldn't_ be necessary to periodically/preemptively bounce nodes, but I appreciate that sometimes a restart is the simplest solution. If that's the case, is there a reason why it couldn't be done via a rolling bounce? That's something that many ops teams do regularly when deploying configuration changes and the like.

An alternative to `cassandra.allow_unsafe_joins` would be to bump `RING_DELAY`. As described in [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13851?focusedCommentId=16309845&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16309845], after that period, seed nodes will exit the SR whether they receive a response or not and so will be able to respond to the non-seed nodes, which remain in the SR for `RING_DELAY * 2`.

Lastly, I'd have to respectfully disagree with you about the accuracy of the error message. Just because the node was able to send and receive messages with some peer(s), it was unable to gossip with them due to constraints inherent in the gossip contract. i.e. Nodes in SR don't respond to peers SR requests. Perhaps we should expand on that though to include some further info on how to remedy the situation (or at least where to look).
;;;","22/Jul/22 16:52;daniel.cranford;Respectfully, the shadow round isn't documented at all (outside the source code) and gossip is barely documented. My ops guys are going to see `unable to gossip with peers` and assume there's a network issue preventing a node from talking to any of their peers and not ""all my peers are also stuck in this undocumented thing called the shadow round"";;;","22/Jul/22 18:12;daniel.cranford;I hope it is clear that I/we don't care that the behavior of seeds has changed. But rather that this behavior change was not made public (it is ""hidden"" in source code and bug trackers) and took us a significant amount of skilled man-hours to track down what had changed and why and what our potential workarounds were. Just a simple update to the seed docs would have helped us immensely.;;;","29/Jul/22 17:45;daniel.cranford;[~brandon.williams] ticket added. cf CASSANDRA-17786;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipStage blocks because of race in ActiveRepairService,CASSANDRA-13849,13100389,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slapukhov,tvdw,tvdw,07/Sep/17 08:02,15/May/20 08:05,14/Jul/23 05:56,09/Nov/17 01:05,3.0.16,3.11.2,4.0,4.0-alpha1,,,Cluster/Gossip,,,,,0,patch,,,,"Bad luck caused a kernel panic in a cluster, and that took another node with it because GossipStage stopped responding.

I think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces :

{noformat}
""Thread-24004"" #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.CountDownLatch$Sync)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
    at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
    - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:211)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)                                                                                                           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipTasks:1"" #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:306)
    at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.Gossiper.access$800(Gossiper.java:67)
    at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:187)
    at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipStage:1"" #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)                                                                                          at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.service.ActiveRepairService.onRestart(ActiveRepairService.java:744)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1049)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)                                                                                       at java.lang.Thread.run(Thread.java:748)
{noformat}

iow, org.apache.cassandra.service.ActiveRepairService.prepareForRepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. Gossip will at the same time try to mark the node as down, but it requires that same lock :)",,bdeggleston,jasonstack,jeromatron,jkni,sbtourist,slapukhov,tvdw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/17 12:13;slapukhov;CAS-13849.patch;https://issues.apache.org/jira/secure/attachment/12893522/CAS-13849.patch","30/Oct/17 17:21;slapukhov;CAS-13849_2.patch;https://issues.apache.org/jira/secure/attachment/12894797/CAS-13849_2.patch","01/Nov/17 10:37;slapukhov;CAS-13849_3.patch;https://issues.apache.org/jira/secure/attachment/12895158/CAS-13849_3.patch",,,,,,,,,,,,,,,,,3.0,slapukhov,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 01:05:08 UTC 2017,,,,,,,,,,"0|i3jr1b:",9223372036854775807,3.0.14,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"23/Oct/17 12:14;slapukhov;Proposed patch for the CAS-13849;;;","23/Oct/17 12:14;slapukhov;Idea is to replace intrinsic lock (synchronized) everywhere in ActiveRepairService with ReentrantLock, and release it before start waiting on prepareLatch in prepareForRepair.;;;","24/Oct/17 13:18;spod;It's not really clear to me why we use a 1 hour timeout for the PrepareMessage callback in first place. The PrepareMessage should just propagate the repair session. If we really have to support such a long timeout period, we'd have to synchronize with any convict notification raised at some point while still waiting for the timeout. But simply lowering the timeout to something like 1 minute would make the gossip blocking behaviour much less likely and only block for at most that very period. ;;;","30/Oct/17 17:21;slapukhov;Patch reducing 1 hour timeout for the PrepareMessage callback to 1 minute.
;;;","30/Oct/17 17:25;slapukhov;Stefan, thank you for your response.

I have tried your idea in our test setting - it fixes the problem for us. I have uploaded this as a patch ([^CAS-13849_2.patch]), could you please take a look?;;;","30/Oct/17 17:56;bdeggleston;I'd agree that the timeout probably doesn't need to be that long. In the meantime, you should probably re-evaluate whether this method actually needs to be synchronized. This method is unsynchronized in trunk, and I think it may be because it was a holdover from more synchronous repair code of the past. Just looking at it briefly in 3.0, the only thing that *might* need synchonization is the call to registerParentRepairSession, everything else if just method local message sending / receiving stuff.;;;","31/Oct/17 16:28;slapukhov;Yes, ActiveRepairService.prepareForRepair is not synchronized in trunk, indeed.

I believe, however, it gives the way to a possible race condition.

In the receiving node org.apache.cassandra.repair.RepairMessageVerbHandler#doVerb method can be processing PREPARE_MESSAGE from coordinator node and run the ActiveRepairService.instance.registerParentRepairSession method. In the same time, receiving node org.apache.cassandra.tools.nodetool.Repair#execute method can also invoke the same ActiveRepairService.instance.registerParentRepairSession method. As a consequence, ActiveRepairService can be registered twice with the Gossiper.instance.register(this), for example.

Maybe it would be safer to have leave ActiveRepairService.prepareForRepair unsynchronized but make registerParentRepairSession synchronized.

What do you think?;;;","31/Oct/17 17:56;bdeggleston;I think that's reasonable, given the stuff that happens in this method, and the relative infrequency that it's called. It would also fix a visibility issue with the registeredForEndpointChanges variable. We should also check that a parent repair session doesn't already exist for the given key before adding a new one.;;;","01/Nov/17 10:39;slapukhov;Please take a look at this patch: [^CAS-13849_3.patch];;;","02/Nov/17 16:32;bdeggleston;Patch looks good. I've merged it up through trunk and started tests here:

|[3.0|https://github.com/bdeggleston/cassandra/tree/13849-3.0] | [utests|https://circleci.com/gh/bdeggleston/cassandra/152] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/408/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13849-3.11] | [utests|https://circleci.com/gh/bdeggleston/cassandra/153] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/409/]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13849-trunk] | [utests|https://circleci.com/gh/bdeggleston/cassandra/154] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/410/] |

I'll commit once the tests are complete, assuming there aren't any problems.;;;","09/Nov/17 01:05;bdeggleston;I ran the trunk unit tests locally and everything passed. The rest of the test failures were either flaky tests, or unrelated tests that passed locally. Committed as {{49edd70740e2efae3681cb79a391369bfb7de02e}}.

Thanks [~slapukhov]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in cqlsh_tests.cqlsh_tests.CqlLoginTest.test_list_roles_after_login,CASSANDRA-13847,13100296,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,jkni,jkni,06/Sep/17 21:02,09/Jul/19 02:15,14/Jul/23 05:56,13/Sep/17 17:05,2.2.11,,,,,,Legacy/Testing,Legacy/Tools,,,,0,test-failure,,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/546/testReport/cqlsh_tests.cqlsh_tests/CqlLoginTest/test_list_roles_after_login

This test was added for [CASSANDRA-13640]. The comments seem to indicated this is only a problem on 3.0+, but the added test certainly seems to reproduce the problem on 2.1 and 2.2. Even if the issue does affect 2.1/2.2, it seems insufficiently critical for 2.1, so we need to limit the test to run on 2.2+ at the very least, possibly 3.0+ if we don't fix the cause on 2.2.

Thoughts [~adelapena]?",,adelapena,jasonstack,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 13 17:04:52 UTC 2017,,,,,,,,,,"0|i3jqgn:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"07/Sep/17 08:04;adelapena;Right, the bug also affects 2.1 and 2.2.

Here is the patch to solve it in 2.2 and the patch for dtests with the {{@since}} tag:
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13847-2.2]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13847]||

Regarding 2.1, I agree this is indeed insufficiently critical, and there's a clear workaround (exiting cqlsh and logging back).;;;","12/Sep/17 09:39;jasonstack;+1 for the change. sorry the overlook last time.;;;","13/Sep/17 17:04;adelapena;Thanks for the review.

Committed to 2.2 as [43e2a107072bc86c0e26bc2036a61a9ad600f213|https://github.com/apache/cassandra/commit/43e2a107072bc86c0e26bc2036a61a9ad600f213].

Dtest patch committed as [3435b0f2121fa4c6099098e562d0bf5f4bd78d5f|https://github.com/apache/cassandra-dtest/commit/3435b0f2121fa4c6099098e562d0bf5f4bd78d5f].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add additional unit tests for batch behavior, TTLs, Timestamps",CASSANDRA-13846,13100252,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,06/Sep/17 18:24,15/May/20 08:06,14/Jul/23 05:56,11/Sep/17 23:01,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"There are some combinations of batch behavior for which there are no unit tests. An example of this is CASSANDRA-13655 , which adds some tests, but not every combination. This ticket will be for additional unit tests around batches / counter batches / batches with TTLs / etc.
",,jasobrown,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 23:01:51 UTC 2017,,,,,,,,,,"0|i3jq6v:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"11/Sep/17 20:43;jjirsa;All code here is in {{test/}} , so not bothering with dtests 

Branch here: https://github.com/jeffjirsa/cassandra/tree/cassandra-13846
CircleCI: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13846
;;;","11/Sep/17 21:31;jasobrown;+1. ;;;","11/Sep/17 23:01;jjirsa;Thanks. Committed to trunk only as {{471835815811d4de42474a3e3899a42cb6d969ce}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian init shadows CASSANDRA_HEAPDUMP_DIR,CASSANDRA-13843,13099842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,ajorgensen,ajorgensen,05/Sep/17 13:42,07/Mar/23 11:52,14/Jul/23 05:56,21/Sep/21 14:27,3.0.26,3.11.12,4.0.2,,,,Legacy/Observability,,,,,0,newbie,patch,,,"The debian init script sets the heap dump file directly using the cassandra users home directory and the -H flag to the cassandra process[1|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/debian/init#L76]. The cassandra heap dump location can also be set in the cassandra-env.sh file using CASSANDRA_HEAPDUMP_DIR. Unfortunately the debian init heap dump location is based off the home directory of the cassandra user and cannot easily be changed. Also unfortunately if you do `ps aux | grep casandra` you can clearly see that the -H flag takes precedent over the value found in cassandra-env.sh. This makes it difficult to change the heap dump location for cassandra and is non-intuitive when the value is set in cassandra-env.sh why the heap dump does not actually end up in the correct place.",,adelapena,ajorgensen,esimfon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12772,CASSANDRA-12849,,,,,,,,,,,,,,,,,,,,"05/Sep/17 15:03;ajorgensen;0001-Remove-debian-init-setting-heap-dump-file.patch;https://issues.apache.org/jira/secure/attachment/12885398/0001-Remove-debian-init-setting-heap-dump-file.patch",,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 21 14:27:13 UTC 2021,,,,,,,,,,"0|i3jnnz:",9223372036854775807,3.11.x,5.0,,,,,,,adelapena,,,Low,,NA,,,https://github.com/apache/cassandra/commit/ea3cca04eba8844a685142d7d3093b1aa58bb4eb,,,,,,,,,,,,,,"06/Sep/17 08:18;esimfon;I would rather solve this issue with CASSANDRA-13006. So instead of letting Cassandra creating it's own heapdumps leave that job to the JVM. ;;;","15/Sep/17 14:10;ajorgensen;[~esimfon] I believe this change is still needed and works in conjunction with the change  you linked. [CASSANDRA-13006|https://issues.apache.org/jira/browse/CASSANDRA-13006] does not solve this issue, it simply disables heap dumps being created automatically. The change I am proposing here is to allow the user to control where the heap dumps are written to. Currently the debian init script shadows the users ability to set the -XX:HeapDumpPath flag at all and will always default to putting them in the cassandra users home directory.;;;","21/Jul/21 15:11;brandon.williams;I took a simpler approach [here|https://github.com/driftx/cassandra/tree/CASSANDRA-13843] that makes the init respect CASSANDRA_HEAPDUMP_DIR so that that becomes the de facto way to specify the location.;;;","21/Sep/21 10:29;adelapena;The approach respecting {{CASSANDRA_HEAPDUMP_DIR}} looks good to me, +1.;;;","21/Sep/21 14:27;brandon.williams;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: batch_test.TestBatch.batchlog_replay_compatibility_?_test,CASSANDRA-13842,13099785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,marcuse,marcuse,05/Sep/17 09:32,16/Apr/19 09:30,14/Jul/23 05:56,07/Sep/17 17:08,,,,,,,Test/dtest/python,,,,,0,dtest,,,,"batch_test.TestBatch.batchlog_replay_compatibility_1_test and batch_test.TestBatch.batchlog_replay_compatibility_4_test are failing:
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_1_test/
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_4_test/",,aleksey,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 17:08:24 UTC 2017,,,,,,,,,,"0|i3jnbb:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"05/Sep/17 11:21;aleksey;The test passes 100% reliably on MacOS, but it does fail for me in my Ubuntu VM just as it does with Jenkins.

Don't yet know why the difference is there, but I'll deal with it.;;;","07/Sep/17 17:06;aleksey;A simple patch [here|https://github.com/iamaleksey/cassandra-dtest/commits/13842].

The problem with the tests is that they don’t take into account the full behaviour of batchlog replay. In particular,
batchlog will *not* replay a batch if it’s younger than 2 times write rpc timeout. The reasoning for this is that
there is a good chance that the batch can still succeed up until that point and be removed by the remote coordinator -
thus avoiding redundant replay. 2 * timeout is there for batched mutations timeout + remove from batchlog mutation
timeout.

One way to fix this would be to introduce a special argument to batchlog replay methods that would ignore the check and
replay everything it has, ignoring that optimisation. But on 3.0 it has correctness implications, since we store last replayed
batch id as the lower bound for the next replay cycle. In 2.2 it’s a bit safer, but we need it to be addressed in both versions -
and I’m not comfortable with making such changes in minor releases.

The other way to fix it is by introducing a deterministic delay to age the batches by 2 * timeout. It adds 20 extra seconds per test,
for a total of 40 seconds, but I don’t see a better way unfortunately. Shouldn’t be flaky however.

P.S. The reason it passes on MacOS and fails on Linux is that the test runs very, very slowly on MacOS. 160+ seconds vs. 30 seconds on Linux.
It naturally has just enough time for the batch to age, so when we force its replay, it is replayed reliably.;;;","07/Sep/17 17:07;jjirsa;+1
;;;","07/Sep/17 17:08;aleksey;Committed as [5893020fd9f2ca783be13a5a0974504632529440|https://github.com/apache/cassandra-dtest/commit/5893020fd9f2ca783be13a5a0974504632529440], thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair will hang,CASSANDRA-13839,13099465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,jasonstack,jasonstack,03/Sep/17 16:46,15/May/20 08:05,14/Jul/23 05:56,09/Sep/17 09:31,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,,0,,,,,"After CASSANDRA-12229(fc92db2b9b56c143516026ba29cecdec37e286bb) merged, repair  may hang forever.

Here is the [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13299-apache] to reproduce. 
bq. nosetests -s -v materialized_views_test.py:TestMaterializedViews.throttled_partition_update_test

looks like some RemoteSyncTask are not completed ",,jasonstack,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13852,,,,,,,,CASSANDRA-12229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 09 09:31:08 UTC 2017,,,,,,,,,,"0|i3jlc7:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"04/Sep/17 04:48;jasonstack;[~jasobrown] when you have time, could you have a look?;;;","09/Sep/17 09:31;jasonstack;This is fixed in CASSANDRA-13852 (Commit: 83822d12d87dcb3aaad2b1e670e57ebef4ab1c36);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hanging threads in BulkLoader,CASSANDRA-13837,13099226,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,01/Sep/17 13:50,15/May/20 08:00,14/Jul/23 05:56,08/Sep/17 10:32,4.0,4.0-alpha1,,,,,,,,,,0,,,,,[~krummas] discovered some threads that were not closing correctly when he fixed CASSANDRA-13836. We suspect this is due to CASSANDRA-8457/CASSANDRA-12229.,,jasobrown,jeromatron,jkni,marcuse,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 10:32:42 UTC 2017,,,,,,,,,,"0|i3jjvb:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"05/Sep/17 15:45;mshuler;This appears to be hanging up the trunk dtest jobs in random tests. We've been looking for a single dtest that might be problematic, but there doesn't seem to be a clear pattern on a particular test.

Any way we can bump the priority on this from Minor?;;;","05/Sep/17 16:25;jkni;I pointed [~mshuler] here prematurely - my bad. It looks like the System.exit uncommented in [CASSANDRA-13836] resolves the hang we were seeing on that test. The remaining hangs seem to be repair-related and unrelated to this ticket.;;;","07/Sep/17 20:56;jasobrown;Patch here:

||13837||
|[branch|https://github.com/jasobrown/cassandra/tree/13837]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/293/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13837]|

Basically, we just need to make the netty {{EventLoopGroup}} threads daemon threads;;;","07/Sep/17 21:09;jasobrown;[~krummas] do you mind reviewing?;;;","08/Sep/17 07:52;marcuse;LGTM, +1 (dtests didn't really run though, but sstableloader exits fine manually)

I just noticed [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/streaming/StreamResultFuture.java#L215] - I doubt that should be WARN? Feel free to remove or change to INFO on commit;;;","08/Sep/17 10:32;jasobrown;committed as sha {{6d4e056c923b7367fc4199ae73dedea893d8f6b8}}. Did a follow up ninja-commit ({{6c0e6693f44753fde87fdec36c648aa0ff02097c}}) to remove that log line [~krummas] pointed out because I wasn't smart enough to remove it on the original commit.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: snapshot_test.py:TestSnapshot.test_basic_snapshot_and_restore,CASSANDRA-13836,13099215,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,01/Sep/17 13:02,15/May/20 08:04,14/Jul/23 05:56,01/Sep/17 13:58,4.0,4.0-alpha1,,,,,,,,,,0,,,,,Looks like sstableloader always tries to use SSL since CASSANDRA-12229 and that makes the dtest hang,,jasobrown,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 01 13:58:20 UTC 2017,,,,,,,,,,"0|i3jjsv:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"01/Sep/17 13:05;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13836

also seems we now again need the System.exit(0) there, we should probably have a look at that as well

[~jasobrown] could you review?;;;","01/Sep/17 13:24;jasobrown;Do we need to uncomment the {{System.exit()}} in {{BulkLoader}}? That was not one the changes from CASSANDRA-12229, but from CASSANDRA-10637 (1.5 years ago)

Otherwise +1;;;","01/Sep/17 13:27;marcuse;I'm guessing CASSANDRA-12229 or CASSANDRA-8457 introduced some non-daemon threads so that we need the System.exit again? sstableloader exits fine before those two went in, but after it hangs;;;","01/Sep/17 13:49;jasobrown;[~krummas] and I discussed offline, and we'll commit his current patch as-is, and open a new ticket for the daemon threads issue. This way we can unblock dtests on trunk.

UPDATE: created CASSANDRA-13837;;;","01/Sep/17 13:58;marcuse;committed as {{fb0e0019e76eb96659904}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed compaction is not captured,CASSANDRA-13833,13098935,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,31/Aug/17 17:39,07/Mar/23 11:52,14/Jul/23 05:56,04/Sep/17 13:05,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,Local/Compaction,,,,,0,,,,,"Follow up for CASSANDRA-13785, when the compaction failed, it fails silently. No error message is logged and exceptions metric is not updated. Basically, it's unable to get the exception: [CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

Here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at java.lang.Thread.run(Thread.java:745)
{noformat}
There're 2 {{FutureTask}} in the call stack, for example {{FutureTask1(FutureTask2))}}, If the call thrown an exception, {{FutureTask2}} sets the status, save the exception and return. But FutureTask1 doesn't get any exception, then set the status to normal. So we're unable to get the exception in:
[CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

2.1.x is working fine, here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[main/:na]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73) ~[main/:na]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264) ~[main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_141]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_141]
{noformat}",,jay.zhuang,marcuse,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 04 13:05:54 UTC 2017,,,,,,,,,,"0|i3jilb:",9223372036854775807,2.2.x,3.0.x,3.11.x,5.0,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"31/Aug/17 20:21;jay.zhuang;Attached the patch to change {{submit()}} to {{execute()}}, as we already created the {{FutureTask}} for {{Runnable}}, {{submit()}} will create one more {{FutureTask}}, which causes nested {{FutureTask}}:
{code}
    public Future<?> submit(Runnable task) {
        if (task == null) throw new NullPointerException();
        RunnableFuture<Void> ftask = newTaskFor(task, null);
        execute(ftask);
        return ftask;
    }
{code}

| branch | utest |
| [13833-2.2|https://github.com/cooldoger/cassandra/tree/13833-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-2.2] |
| [13833-3.0|https://github.com/cooldoger/cassandra/tree/13833-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.0] |
| [13833-3.11|https://github.com/cooldoger/cassandra/tree/13833-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.11] |
| [13833-trunk|https://github.com/cooldoger/cassandra/tree/13833-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-trunk] |;;;","31/Aug/17 20:41;jay.zhuang;[~aweisberg] would you please review? You made the similar change ({{submit() -> execute()}}) in 3.11 and trunk: CASSANDRA-12358

cc [~Stefania];;;","01/Sep/17 07:18;marcuse;nice catch, code LGTM, just a small test fix for 3.11 and trunk: https://github.com/krummas/cassandra/commit/bb9c9e0b685d3b4e76a7b082b46b01a7ed6c8af5

running dtests:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/261/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/262/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/263/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/264/;;;","04/Sep/17 12:27;marcuse;I reran the failures locally and the ones that looked suspicious all pass (except for trunk which looks completely broken now)

I'll get it committed
;;;","04/Sep/17 13:05;marcuse;committed as {{e80ede6d393460f22ee}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyFactoryTest is failing in trunk on MacOS,CASSANDRA-13831,13098839,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,aleksey,aleksey,31/Aug/17 10:36,15/May/20 08:01,14/Jul/23 05:56,07/Sep/17 18:04,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,"Example failure:

{code}
    [junit] Testcase: getEventLoopGroup_EpollWithoutIoRatioBoost(org.apache.cassandra.net.async.NettyFactoryTest):	Caused an ERROR
    [junit] failed to load the required native library
    [junit] java.lang.UnsatisfiedLinkError: failed to load the required native library
    [junit] 	at io.netty.channel.epoll.Epoll.ensureAvailability(Epoll.java:78)
    [junit] 	at io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:53)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:134)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:35)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:58)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:47)
    [junit] 	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:104)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:91)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:68)
    [junit] 	at org.apache.cassandra.net.async.NettyFactory.getEventLoopGroup(NettyFactory.java:175)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_Epoll(NettyFactoryTest.java:187)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_EpollWithoutIoRatioBoost(NettyFactoryTest.java:205)
    [junit] Caused by: java.lang.ExceptionInInitializerError
    [junit] 	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33)
    [junit] 	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:162)
    [junit] 	at org.apache.cassandra.net.async.NettyFactory.<clinit>(NettyFactory.java:94)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_Nio(NettyFactoryTest.java:216)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_NioWithoutIoRatioBoost(NettyFactoryTest.java:211)
    [junit] Caused by: java.lang.IllegalStateException: Only supported on Linux
    [junit] 	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:189)
    [junit] 	at io.netty.channel.epoll.Native.<clinit>(Native.java:61)
{code}

It's obviously caused by epoll being unavailable on MacOS, but the tests should still be made to pass.",,aleksey,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 18:04:44 UTC 2017,,,,,,,,,,"0|i3jhzz:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"31/Aug/17 12:29;jasobrown;pushed a fix for the test:

||13831||
|[branch|https://github.com/jasobrown/cassandra/tree/13831]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13831]|
;;;","04/Sep/17 15:39;aleksey;This makes the tests pass but still shits all over the logs.

{code}
    [junit] WARN  [main] 2017-09-04 16:33:51,193 NettyFactory.java:98 - epoll not availble {}
    [junit] java.lang.ExceptionInInitializerError: null
    [junit] 	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:162) ~[main/:na]
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.<clinit>(NettyFactoryTest.java:65) ~[classes/:na]
    [junit] 	at java.lang.Class.forName0(Native Method) ~[na:1.8.0_144]
    [junit] 	at java.lang.Class.forName(Class.java:264) ~[na:1.8.0_144]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:380) ~[ant-junit.jar:na]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) ~[ant-junit.jar:na]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) ~[ant-junit.jar:na]
    [junit] Caused by: java.lang.IllegalStateException: Only supported on Linux
    [junit] 	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:189) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	at io.netty.channel.epoll.Native.<clinit>(Native.java:61) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	... 8 common frames omitted
{code}

Could you maybe modify {{NativeTransportService.useEpoll()}} to look at the OS first before trying to call {{Epoll.isAvailable()}}, or fix it some other way?;;;","05/Sep/17 11:59;jasobrown;Printing the error is useful on linux to know why epoll might not be available, but I agree it's not necessary on other platforms. Fixing.;;;","05/Sep/17 12:07;jasobrown;rebased and pushed a change that adds an additional check to see if the OS is linux, and only then log that epoll is unavailable.

UPDATE: just saw your comment about {{NativeTransportService}}. Will move the logging there, from {{NettyFactory}};;;","07/Sep/17 12:43;aleksey;kk, just mark as Patch Available when ready.;;;","07/Sep/17 14:01;jasobrown;ahh, sorry, already pushed that change about 10 minutes after the last UPDATE tag. Marking Patch Available now.;;;","07/Sep/17 17:21;aleksey;+1. On commit, can you please fix your recent CHANGES.txt entries in the log to start with upper case, like every other line in the list?;;;","07/Sep/17 18:04;jasobrown;committed as sha {{bab76eba958241bbfca33e9fc6fe663ce230b034}} to trunk. And I've cleaned up the CHANGES entrie3s as per [~iamaleksey]'s request.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't let user drop (or generally break) tables in system_distributed,CASSANDRA-13813,13097898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,slebresne,slebresne,28/Aug/17 07:25,16/Apr/19 09:30,14/Jul/23 05:56,17/Oct/17 12:44,3.0.16,3.11.2,,,,,Legacy/Distributed Metadata,,,,,0,,,,,"There is not currently no particular restrictions on schema modifications to tables of the {{system_distributed}} keyspace. This does mean you can drop those tables, or even alter them in wrong ways like dropping or renaming columns. All of which is guaranteed to break stuffs (that is, repair if you mess up with on of it's table, or MVs if you mess up with {{view_build_status}}).

I'm pretty sure this was never intended and is an oversight of the condition on {{ALTERABLE_SYSTEM_KEYSPACES}} in [ClientState|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/ClientState.java#L397]. That condition is such that any keyspace not listed in {{ALTERABLE_SYSTEM_KEYSPACES}} (which happens to be the case for {{system_distributed}}) has no specific restrictions whatsoever, while given the naming it's fair to assume the intention that exactly the opposite.",,aleksey,jasonstack,jjordan,julienlau,KurtG,mbulman,slebresne,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13954,,CASSANDRA-13812,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 12:44:11 UTC 2017,,,,,,,,,,"0|i3jc8v:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"29/Aug/17 10:53;aleksey;The reason it isn't listed is because we want to allow changing the replication strategy/factor on those keyspaces themselves.

It's just that it's been implemented sub-diligently. So we want reject any changes to the tables (including dropping them), reject dropping of those keyspaces, but allow altering keyspace params. Also reject: creating of new tables in those keyspaces.;;;","29/Aug/17 12:31;slebresne;bq. The reason it isn't listed is because we want to allow changing the replication strategy/factor on those keyspaces themselves.

The name of the set makes it clear what the _intention_ was. What I'm saying is that the code doesn't implement that intention. Currently, you can alter the replication strategy/factor of _all_ system distributed keyspace, even {{system_distributed}}, and I'm reasonably convinced that wasn't the intention. This certainly isn't hard to fix though.

Now with that said, I  happen to not particularly see a terribly good reason to limit which of the system distributed keyspace can have their strategy/factor changed (and again, despite the original code intention, we currently _don't_ have such limit), so instead of simply making the check on {{ALTERABLE_SYSTEM_KEYSPACES}} work as originally intended (and thus starting to forbid changing the RF on {{system_distributed}}), I'd advocate for just removing {{ALTERABLE_SYSTEM_KEYSPACES}} and deal with all those keyspace consistently. Just my 2 cents though.;;;","29/Aug/17 12:50;aleksey;Not having {{system_distributed}} in {{ALTERABLE_SYSTEM_KEYSPACES}} is an oversight from the patch that added {{system_distributed}}.

Had it been added correctly, CASSANDRA-13812 would not be a problem - {{ClientState.preventSystemKSSchemaModification()}} would've forbidden those drops.

The constant is there to enumerate the keyspace whose tables cannot be modified, and that's the purpose it serves. It might not have the best possible name, though, or be ideally documented.

I'd start with adding {{system_distributed}} to {{ALTERABLE_SYSTEM_KEYSPACES}} - that would immediately fix CASSANDRA-13812 and this ticket. Then rename {{ALTERABLE_SYSTEM_KEYSPACES}} to something more descriptive (even {{PARTIALLY_ALTERABLE_SYSTEM_KEYSPACES}} would be a start. And document the intent.

EDIT: As Sylvain reminded me offline, we don't really need {{ALTERABLE_SYSTEM_KEYSPACES}} at all. The intended set of keyspaces in there is duplicated in {{Schema.REPLICATED_SYSTEM_KEYSPACE_NAMES}}, which is what we should be using instead - directly or via {{SchemaConstants.isReplicatedSystemKeyspace()}}.;;;","05/Oct/17 17:41;aleksey;Fixes for [3.0|https://github.com/iamaleksey/cassandra/tree/13813-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/13813-3.11], [4.0|https://github.com/iamaleksey/cassandra/tree/13813-4.0] and a new [dtest|https://github.com/iamaleksey/cassandra-dtest/tree/13813].

Utests and dtests queued up.

[~slebresne] Any chance you could review? I've implemented what we talked about on IRC - and renamed some confusingly named things while at it.;;;","06/Oct/17 10:46;slebresne;So the thing is that due to how the existing code is work, we currently allow anything on the tables of the {{system_distributed}} tables. And while I doubt there is any debate that we shouldn't let use drop those tables, or even drop/rename columns of those tables, some user may have been relying on altering these table _properties_. CASSANDRA-12701 particularly comes to mind here: it's kind of a bug that the repair tables in there don't have TTLs and so user may have legitimately work around it by adding the table TTL themselves. The attached patch will remove the ability for this work-around (to be extra precise, user that have already changed the properties would obviously be fine, it's new user wanting to do it that wouldn't be able to), and this feels uncomfortable to do in minor releases at least.

I guess, if we make sure to commit CASSANDRA-12701 before this, we at least remove one imo legitimate reason to modify those tables params and that would be better. Even without that though, modifying some system tables params isn't an entirely crazy idea (we can debate whether that's truly useful, but it's not crazy) and this remove that ability. Don't get me wrong btw, I know that we currently only allow this for {{system_distributed}} tables and so it's inconsistent with other system keyspace in the first place, and I'm not definitively saying we should allow modifying params on all system tables. I'm just raising the point that the patch removes something that could be considered as useful by some, and doing so in a minor concerns me a little bit.;;;","06/Oct/17 13:08;aleksey;[~slebresne] I don't believe it's reasonable to keep bugs in the codebase just because they mask out other bugs. Leaving this ability is dangerous, as it breaks the assumptions we make about replicated system keyspaces, and the way we alter them. Those who make these changes manually - because we weren't careful enough and haven't shielded them from being able to - might have to pay the price later, when their cluster are unable to pick up our altered defaults, because of our bumped hardcoded timestamps being smaller than the new ones set via {{ALTER TABLE}}.

FWIW, those who are really keen to work around this issue, might as well alter {{system_schema.tables}} manually, and get the desired result. With just as much risk.

Either way, again, I don't believe that keeping the bug in just because it masks the bug in CASSANDRA-12701 is reasonable. So if you are only slightly concerned and don't mind really strongly, I'd prefer to go ahead and fix this issue here and now, before more people shoot themselves in the feet.

;;;","06/Oct/17 14:04;slebresne;bq. I don't believe it's reasonable to keep bugs in the codebase just because they mask out other bugs.

Just to make sure my position is clear, I'm not suggesting we leave this ticket unsolved. What I'm suggesting is that it would be possible to forbid truly harmful changes (droping tables and adding/removing/renaming columns; that part is the real bug to me) while still allowing the altering of table parameters (which actually don't seem that problematic to me on principle). Sure, it requires adding some special casing in {{AlterTableStatement}}, but it's not very complex either.

bq. when their cluster are unable to pick up our altered defaults, because of our bumped hardcoded timestamps being smaller than the new ones set via ALTER TABLE

I'm not sure I understand the problem. If user have manually and knowingly updated some table params, my guess is that they expect (even rely on) future changes to defaults to not override their changes. Isn't the whole point why we've picked 0 for our hardcoded timestamp in fact?

But if there is genunine reason I'm missing that make it dangerous for user to alter those parameters, that's certainly important to  understand.  I'm pretty sure some user _have_ done it (at least to work around CASSANDRA-12701, possibly for other reasons), and if their change is a time-bomb, we kind of owe them to disarm that bomb.

But anyway, to sum it up, the patch lgtm from a technical level, and I'm definitively on board with forbidding clearly harmful changes. The only specific part I'm unsure is the ""altering table parameters"" part. In light of my current understanding, it doesn't look obviously harmful (I can even see it being desirable if I'm being honest), so I'm uncomfortable removing the ability in a minor release.

Happy to gather a few other opinions though and if the concensus is that the patch is fine the way it currently is, no problem, not trying to veto this in any way.;;;","09/Oct/17 10:14;aleksey;bq. I'm not sure I understand the problem. If user have manually and knowingly updated some table params, my guess is that they expect (even rely on) future changes to defaults to not override their changes. Isn't the whole point why we've picked 0 for our hardcoded timestamp in fact?

Right. But the way ALTER works, we serialise the whole table, including all params and all columns, with the new timestamp in {{system_schema.*}} tables. Which makes it impossible for us to change the defaults later, even those that the user didn't modify on purpose. And this isn't something we can change very easily in a minor I'm afraid.

This is why we don't allow altering anything beyond keyspace params, and why this issue is, as it stands, a serious bug, and was never intended to be allowed.;;;","09/Oct/17 10:38;aleksey;FWIW, I'll be the first to admit that the current situation is not ideal. It wasn't me who came up with it, but I share part of the blame - replicated system keyspaces are a bit of a mess, and this has already caused us some issues, and hassle with {{system_auth}}, and it won't be the last.

We can't even fix CASSANDRA-12701 properly in a minor without causing migration mismatch fun. So all things considered, my personal preference would be to shield existing users from causing further issues for themselves by accidentally or intentionally modifying those tables. At least until we have a good answer to these related issues, which I don't :(;;;","09/Oct/17 13:04;slebresne;bq.  But the way ALTER works, we serialise the whole table, including all params and all columns

Good point, I hadn't though about that part. Sad!

So I guess I would agree in principle about shielding user against clearly dysfunctional behaviors. The problem is that in practice I know for a fact that CASSANDRA-12701 has been an issue for some users, where the tables had been growing way too much, to the point that being able to work-around that by setting a TTL manually probably override concerns about hypothetical future changes to defaults not being picked up.

Or to put it another way, none of this is ideal, but I wonder is ""repair history tables regularly grows out of control regularly"" isn't a bigger problem in practice than ""future defaults changes to system tables may not be picked up"". Anyway, again, not opposed on the current patch personally, but uneased by it, so wouldn't mind a few additional opinion to see if it's just me being difficult (which is possible).;;;","09/Oct/17 13:09;aleksey;bq. so wouldn't mind a few additional opinion to see if it's just me being difficult (which is possible).

Oh, you've never been difficult. Neither have I. FWIW I don't feel very strongly about this going to 3.0.x vs. this going to 4.0 only. Worst case I'll just fix this for us internally.

Seeing that neither of us feels really strongly about this, I don't mind getting some opinions from others, either. I'll throw a signal on IRC and hopefully someone will reply. Either way it's not urgent.;;;","09/Oct/17 13:53;jjordan;I am a little concerned about this change not letting anything be updated, but I do understand the reasons, and I can't really see a way around them. Given that an experience person can still get around this restriction by doing inserts into the schema tables, that is probably enough if there are any future bugs to be worked around.  Un-experienced users should not be changing these values by themselves.;;;","10/Oct/17 01:23;KurtG;I think if we can't provide a data model for our tables that works for all scenarios then we need to allow operators to make changes. I've had quite a few occasions where modifying ""system"" tables was necessary, and I'm sure more tables will be introduced that don't work in all scenarios in the future. 

While there is the workaround of just inserting into the system_schema tables that is fraught with peril, and far more likely for them to do something that breaks things. I can't see someone saying ""woops I accidentally DROPped/ALTERed a random column in system_distributed.view_build_status"", but I can definitely see someone trying to insert into system_schema.tables and making mistakes. As soon as we make them replicated we hand over some responsibility to the operator to manage them (not that the non-replicated keyspaces have a history of being perfect though), and I'd expect to be able to change table properties that potentially affect the cluster.

Cassandra already requires you to know what your doing as an operator, this really doesn't increase that expectation. There are a million other bad choices you could make when managing a cluster that would be far more catastrophic (and far more likely). I would like to move away from that, but a lot of that sort of thing requires major changes to fix. As in this case it seems we'll need the capability limitation framework or other major changes to make a reasonable compromise. ;;;","10/Oct/17 09:58;slebresne;bq. an experience person can still get around this restriction by doing inserts into the schema tables

I also just realized that doing so actually avoids the issue we currently have with {{ALTER}} that it rewrites all columns, so it makes it a somewhat better work-around (of course, still a work-around and that don't dispense us for fixing all this more cleanly). My only bother is that while I haven't actually tried it recently, last time I did try updating the schema tables manually, it was annoying because the changes were not automatically picked-up and in fact tended to be overridden, so I had to force a reload in weird ways (altering some other unrelated table in the keyspace, which here would actually be an issue). So it would be nice if we added a JMX call to force reload schema tables from disk to make this easier (should be easy). If we do, I'm warming up to the idea of considering this is the only really safe work-around until we find a better way to deal with all thise.

bq. if we can't provide a data model for our tables that works for all scenarios then we need to allow operators to make changes.

I'm not sure what you are trying to mean by that. If it's a reference to CASSANDRA-12701, then what makes that change problematic is the very same reason why leaving {{ALTER}} working here is problematic. So feel free to suggest a concrete solution to those problems if you have one, but otherwise, I'm not sure how this statement helps make a decision on this issue.

bq. I've had quite a few occasions where modifying ""system"" tables was necessary, and I'm sure more tables will be introduced that don't work in all scenarios in the future.

First, it would be nice if you could be a bit more concrete on those time where it was ""necessary"": which tables, what modification and why what it necessary? We're trying to find the best course of action for a very concrete problem and we are all experienced C* developers: let's be specific.

Second, I'm not sure how to re-conciliate that sentence as a whole to the concrete problem at end. Let's remind that we do _already_ refuse {{ALTER}} on most system tables, and this ticket is not about discussing whether we should allow {{ALTER}} on system tables _in general_. If you want to discuss that, I'm good with it (outside of the fact that we will have to solve the technical gotcha mentioned above) and your arguments seem to really be applied to such change, but please open a separate ticket and let's not divert that one.;;;","10/Oct/17 10:24;aleksey;bq. My only bother is that while I haven't actually tried it recently, last time I did try updating the schema tables manually, it was annoying because the changes were not automatically picked-up and in fact tended to be overridden, so I had to force a reload in weird ways (altering some other unrelated table in the keyspace, which here would actually be an issue). So it would be nice if we added a JMX call to force reload schema tables from disk to make this easier (should be easy). If we do, I'm warming up to the idea of considering this is the only really safe work-around until we find a better way to deal with all thise.

Yep. You either do an unrelated {{ALTER}} (usually {{WITH comment = ...}}) or bounce the node. Also wouldn't mind at all adding the new JMX call, as a companion to {{resetlocalschema}}.;;;","10/Oct/17 10:30;aleksey;[~slebresne] I can/will extend the patch with a new {{reloadlocalschema}} JMX call and a nodetool cmd when/if you warm up to it sufficiently (:

EDIT: Actually, never mind. I'll do it either way, in a separate JIRA, for cleanliness sake, and it's independently useful anyway. Will poke you on it once ready.;;;","13/Oct/17 14:33;aleksey;Created CASSANDRA-13954 for jmx/nodetool work.;;;","16/Oct/17 02:17;KurtG;bq. I'm not sure what you are trying to mean by that.
Not sure how to make this clearer. If we can't provide a one-size-fits-all solution for our system distributed tables, which we can't, we should allow for them to be modified by operators. In CASSANDRA-12701 we can't set a TTL because we can't pick a TTL that's appropriate for all use cases (or compaction strategy for that matter). Users have to pick their own, thus we should allow them to change these properties. So far it only may be that TTL is the one that users need to change, but I'm sure there are potentially others, and I'm sure there will be more in the future.

bq. First, it would be nice if you could be a bit more concrete on those time where it was ""necessary""
Sure, I've had to change TTL as per CASSANDRA-12701 multiple times. I've never bothered changing compaction strategy but obviously that would be beneficial.
I've also had to set up jobs to routinely truncate system_traces. This may sound absurd, but in environments where you don't necessarily have control over clients, it is applicable. Being able to TTL on this keyspace would improve things here.
I've also had to drop tables in system_distributed on occasion when for some reason or another they got corrupt/different ID's across the cluster. This has also occured for system_traces, but obviously had to delete manually from system_schema.
Obviously also the usual system_auth changes, however so far that only relates to modifying RF, but I'll note that we do require dropping tables in that keyspace for conversion to role based access.

bq.  this ticket is not about discussing whether we should allow ALTER on system tables in general.
Yep, I was never talking about system tables in general, just the distributed ones. I thought this would be obvious considering that's what the ticket was about.

I think it's worth pointing out the following as well, because it's been raised that having alterable system_distributed tables is a major defect.
This issue (and related ones) were raised by developers as ""major"" issues.
There appears to be no evidence so far that this issue has actually affected users.
We're rushing to fix it because for some reason, all of a sudden, we've determined that users breaking their own cluster by doing funny things is a huge problem. This is odd to me, considering users routinely break their clusters by running repairs, but I've never heard of anyone breaking their cluster because they altered a system ""distributed"" keyspace.
There appears to be no evidence that you can really do catastrophic damage by altering these tables. If we fixed the fact that they don't get recreated automatically anymore I'm pretty sure any issues created by doing odd changes on the tables (which is very unlikely) would easily be resolved by dropping the tables and restarting a node. On that note, not sure if repair actually will break if the history tables are broken, but as far as MV are concerned, they keep working even if you truncate/drop the tables.
The suggested fix still leaves a workaround to make the changes we'd be blocking here, and this is on purpose. This kind of implies that we're perfectly OK with users making these changes. I mean, if this is an actual issue surely we should also create a ticket ""Don't let user insert garbage into system_schema"".

I'm really not convinced that this is such a major high priority issue that we can't just put it off until 4 and fix it with something more reasonable like the capability limitation framework, while maintaining the current behaviour. 
;;;","17/Oct/17 08:19;slebresne;As suggested earlier, I'm personally +1 on the patch here now that we have CASSANDRA-13954.;;;","17/Oct/17 12:44;aleksey;Thanks. Committed to 3.0 as [9e37967e17a1b223df35c1c7cec4dc4adf0b2d91|https://github.com/apache/cassandra/commit/9e37967e17a1b223df35c1c7cec4dc4adf0b2d91] and merged up with 3.11 and trunk. Dtest committed as [957ae2bc455c56835632193e1aa251495c3724f8|https://github.com/apache/cassandra-dtest/commit/957ae2bc455c56835632193e1aa251495c3724f8].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflows with Amazon Elastic File System (EFS),CASSANDRA-13808,13097643,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,betalb,betalb,25/Aug/17 15:54,15/May/20 08:03,14/Jul/23 05:56,29/Sep/17 07:58,3.11.1,4.0,4.0-alpha1,,,,Legacy/Core,Local/Config,,,,0,,,,,"Integer overflow issue was fixed for cassandra 2.2, but in 3.8 new property was introduced in config that also derives from disk size  {{cdc_total_space_in_mb}}, see CASSANDRA-8844

It should be updated too https://github.com/apache/cassandra/blob/6b7d73a49695c0ceb78bc7a003ace606a806c13a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L484",,betalb,blerer,ifesdjeen,mshuler,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 07:58:27 UTC 2017,,,,,,,,,,"0|i3jaof:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"25/Sep/17 19:44;blerer;I pushed a patch for the problem [here|https://github.com/apache/cassandra/compare/trunk...blerer:13808-3.11].;;;","26/Sep/17 08:19;ifesdjeen;+1, LGTM

For the record, the fix is the same as in [CASSANDRA-13067].;;;","29/Sep/17 07:57;blerer;Thanks for the review.;;;","29/Sep/17 07:58;blerer;Committed into cassandra-3.11 at 0bc45aa46766625698e6e4c47085dfe94766c7df and merged into trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI fix - only collect the xml file from containers where it exists,CASSANDRA-13807,13097635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,25/Aug/17 15:21,15/May/20 08:05,14/Jul/23 05:56,30/Aug/17 11:41,2.1.19,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,CI,,,,,0,CI,,,,"Followup from CASSANDRA-13775 - my fix with {{ant eclipse-warnings}} obviously does not work since it doesn't generate any xml files

Push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers
Test running here: https://circleci.com/gh/krummas/cassandra/86",,eduard.tudenhoefner,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 11:41:40 UTC 2017,,,,,,,,,,"0|i3jamn:",9223372036854775807,,,,,,,eduard.tudenhoefner,,eduard.tudenhoefner,,,Normal,,,,,,,,,,,,,,,,,,,"28/Aug/17 06:50;marcuse;[~eduard.tudenhoefner] could you have a look?;;;","28/Aug/17 15:20;eduard.tudenhoefner;[~krummas] changes lgtm, so +1 on merging;;;","30/Aug/17 11:41;marcuse;I added the same case statement to all branches - just made sure to include the nodes that generate an xml file;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionManager sometimes wrongly determines that a background compaction is running for a particular table,CASSANDRA-13801,13097575,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dimitarndimitrov,dimitarndimitrov,dimitarndimitrov,25/Aug/17 11:51,15/May/20 08:07,14/Jul/23 05:56,12/Dec/17 19:53,2.2.12,3.0.16,3.11.2,4.0,4.0-alpha1,,Local/Compaction,,,,,0,,,,,"Sometimes after writing different rows to a table, then doing a blocking flush, if you alter the compaction strategy, then run background compaction and wait for it to finish, {{CompactionManager}} may decide that there's an ongoing compaction for that same table.
This may happen even though logs don't indicate that to be the case (compaction may still be running for system_schema tables).",,dimitarndimitrov,jjordan,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/17 19:32;dimitarndimitrov;c13801-2.2-testall.png;https://issues.apache.org/jira/secure/attachment/12900929/c13801-2.2-testall.png","06/Dec/17 19:32;dimitarndimitrov;c13801-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12900928/c13801-3.0-testall.png","06/Dec/17 19:32;dimitarndimitrov;c13801-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12900927/c13801-3.11-testall.png","06/Dec/17 19:32;dimitarndimitrov;c13801-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12900926/c13801-trunk-testall.png",,,,,,,,,,,,,,,,4.0,dimitarndimitrov,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 19:53:43 UTC 2017,,,,,,,,,,"0|i3ja9b:",9223372036854775807,3.11.0,,,,,,pauloricardomg,,pauloricardomg,,,Low,,,,,,,,,,,,,,,,,,,"06/Dec/17 19:31;dimitarndimitrov;It turns out that the problem does not necessarily require altering the compaction strategy.
It seems to be rooted in a potential problem with counting the CF compaction requests, that can eventually lead to a skipped background compaction.

The wrong counting can happen if the counting multiset increment [here|https://github.com/apache/cassandra/blob/95b43b195e4074533100f863344c182a118a8b6c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L197] gets delayed and happens after the corresponding counting multiset decrement already happened [here|https://github.com/apache/cassandra/blob/95b43b195e4074533100f863344c182a118a8b6c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L284].

Here are the branches with the proposed changes, as well as a Byteman test that can be used to demonstrate the issue.
testall results look good (3.0 and trunk each have 1 seemingly unrelated, flaky test failing).
dtest results will be added soon.

| [2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...dimitarndimitrov:c13801-2.2] | [testall|^c13801-2.2-testall.png] |
| [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...dimitarndimitrov:c13801-3.0] | [testall|^c13801-3.0-testall.png] |
| [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...dimitarndimitrov:c13801-3.11] | [testall|^c13801-3.11-testall.png] |
| [trunk|https://github.com/apache/cassandra/compare/trunk...dimitarndimitrov:c13801-trunk] | [testall|^c13801-trunk-testall.png] |
;;;","12/Dec/17 19:53;pauloricardomg;Good catch, patch and tests LGTM! I looked the results of the dtests on internal CI and they look good (unrelated failures), can you just attach the screen shots so they are public?

I will commit this to 2.2 even though it is in critical fixes mode because it's pretty simple/safe and the only way to solve it is to restart the node if it ever gets into this nasty state.

I added a comment on [CompactionManager.submitBackground|https://github.com/apache/cassandra/commit/35e6d61361e699908d73c277da7d9ac3390f6e5d#diff-d4e3b82e9bebfd2cb466b4a30af07fa4R159] extracted from CASSANDRA-4310 to explain the reasoning behind keeping track of compacting cfs.

Committed to 2.2 as {{35e6d61361e699908d73c277da7d9ac3390f6e5d}} and merged up to cassandra-3.0, cassandra-3.11 and trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow filtering on non-primary-key base column for MV,CASSANDRA-13798,13097486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,25/Aug/17 03:02,15/May/20 08:06,14/Jul/23 05:56,31/Aug/17 10:43,3.11.1,4.0,4.0-alpha1,,,,Feature/Materialized Views,,,,,0,,,,,"We should probably consider disallow filtering conditions on non-primary-key base column for Materialized View which is introduced in CASSANDRA-10368.

The main problem is that the liveness of view row is now depending on multiple base columns (multiple filtered non-pk base column + base column used in view pk) and this semantic could not be properly support without drastic storage format changes. (SEE CASSANDRA-11500, [background|https://issues.apache.org/jira/browse/CASSANDRA-11500?focusedCommentId=16119823&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16119823])

We should step back and re-consider the non-primary-key filtering feature together with supporting multiple non-PK cols in MV clustering key in CASSANDRA-10226.
",,brstgt,jasonstack,JoshuaMcKenzie,KurtG,pauloricardomg,sbtourist,shaurya10000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11500,CASSANDRA-10368,CASSANDRA-10226,CASSANDRA-13956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 12:11:26 UTC 2017,,,,,,,,,,"0|i3j9pr:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"25/Aug/17 11:08;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13798-trunk] |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13798-3.11] |

The patch is to revert supporting non-pk base column filtering on MV on 3.11 & trunk.  The feature was added in 3.10 and the consequence may have been overlooked. It's better to revert it in 3.x/trunk, before more users start using it.

At the same time, I will work on a proper design (eg. multi-timestamp approach discussed in CASSANDRA-11500 and CASSANDRA-10226) for non-pk base column filtering and multiple non-pk base column in view pk.

[~pauloricardomg] [~brstgt] [~KurtG] what do you think?;;;","25/Aug/17 12:40;JoshuaMcKenzie;bq. The feature was added in 3.10
bq. revert it in 3.x/trunk, before more users start using it.
What, if anything, do we want to do for the theoretical user on 3.10/3.11 that's created an MV w/this schema?;;;","25/Aug/17 14:03;brstgt;Even if the current implementation has known issues, you cannot kill that (or any other) feature just like that. As [~JoshuaMcKenzie] mentioned, how do you treat existing installations + schemas?
If I was affected (I really have to check this) this would either force me to change my schema or to be blocked on updates. Both is not viable if the current solution works for my needs. For example I am not really affected if I have an insert-only payload or if my data does not expire.

What you of course can do: 
Spit our a warning in the logs on bootstrap if the schema is affected and on schema changes that are affected and refer to a JIRA. So one can decide to stay with it or to migrate the schema / model to be not affected any more.

My 2 cents.;;;","25/Aug/17 15:29;pauloricardomg;bq.  If I was affected (I really have to check this) this would either force me to change my schema or to be blocked on updates. Both is not viable if the current solution works for my needs. For example I am not really affected if I have an insert-only payload or if my data does not expire.

It seems from a quick peek at the patch that this just forbids creation of new tables with non-PK conditions, but does not break existing tables. So this just changes the UX to prevent news users from shooting themselves in the foot, but keeps the underlying machinery to support it (which will probably be reused when the feature is fully supported). Perhaps we can even have a hidden system flag to allow table creation if you ""know what you are doing""© and you can guarantee there are no column updates. In any case, we should explain the reasoning behind the revert and maybe expose the system flag for append-only workloads on NEWS.txt.;;;","25/Aug/17 15:48;jasonstack;This ticket is to block creating new table with non-pk base column filtering.

All internal filter related logic are still there for supporting filter conditions on base pk. (existed even before CASSANDRA-10368)

Existing users of 3.10/3.11 with non-pk base column should still be able to successfully upgrade and use the broken non-pk base column filtering.( probably, we should give them an option to avoid filtering as well..). ;;;","28/Aug/17 02:01;KurtG;I think something should be done on 3.11 and blocking the behaviour with a workaround flag as [~pauloricardomg] suggested is probably the best way to go about it. MV's are already causing a lot of issues for people doing dev on 3.11, and I know we have users with this case, that will undoubtedly run into issues sooner or later.

However I'm not convinced about also committing this to 4.0. If we're not committing to fixing it in 4.0 then MV's will be pretty useless until 5.0 if it does take massive storage changes, which could be a long way away. Which will be problematic for people who have already gone down this path, and we'll be missing a much wanted feature for quite a long time. IMO we made the mistake of pushing it out too fast, we should fix it asap.;;;","28/Aug/17 07:38;pauloricardomg;bq. However I'm not convinced about also committing this to 4.0. If we're not committing to fixing it in 4.0 then MV's will be pretty useless until 5.0 if it does take massive storage changes, which could be a long way away. 

The storage changes required to support this are probably a new SSTable format + changes in the binary protocol, so while it requires a major version, it doesn't need to be 5.0, but 4.1 if we are not able to ship this by 4.0.

bq.  Which will be problematic for people who have already gone down this path, and we'll be missing a much wanted feature for quite a long time. IMO we made the mistake of pushing it out too fast, we should fix it asap.

We should definitely fix this ASAP, but adding this to trunk doesn't prevent that in any way, we're just adding this limitation while the problem is not fixed. As a follow-up to this, we should create another ticket to implement this feature properly. Whether that ticket should block the 4.0 release it's up for the community to decide, but on the meantime the code will be correct in not allowing people to use broken functionality.;;;","29/Aug/17 01:48;KurtG;Right OK. If we can do it in 4.1 that's fine. It seems we don't actually have a difference between major and minor then? My (incorrect) assumption was that massive storage changes constituted a major version change, and 4.x would be intended to be minor.


;;;","29/Aug/17 03:52;jasonstack;Updated NEWS.txt and introduced a flag: {{cassandra.mv.filtering.nonkey.columns=false}}.

If users ""know what they are doing"" ,eg. append-only, they could turn on the flag to create MV with non-pk column filtering.
;;;","29/Aug/17 08:07;pauloricardomg;bq. Right OK. If we can do it in 4.1 that's fine. It seems we don't actually have a difference between major and minor then? My (incorrect) assumption was that massive storage changes constituted a major version change, and 4.x would be intended to be minor.

Hmm it depends whether the tick tock model is happening or not after 4.0, I'm not sure what the consensus was with that. But I meant that sstable version changes are normally during majors, and assumed 4.1 would be a major in the non-tick model while 4.0.1 would be a minor (following the previous model). In any case, this is just to leave things at a consistent state in case the community decides to release 4.0 without waiting for this feature to be fully enabled, but doesn't prevent us to removing this limitation if the feature is correctly implemented by 4.0 release timeframe. We will create another ticket to properly enable this feature following this.;;;","31/Aug/17 10:42;pauloricardomg;bq. Updated NEWS.txt and introduced a flag: cassandra.mv.filtering.nonkey.columns=false.

Thanks, the patch LGTM. I just updated the flag to {{cassandra.mv.allow_filtering_nonkey_columns_unsafe}} (added unsafe to make sure it's not safe unless you know what you are doing, and replaced dots for underscores, since dot is usually used to separate the flag component but not the flag name), hope that's OK.

Committed to cassandra-3.11 and merged to trunk as {{425880ffb2e6bd5aeaa509fdbfa553db58b74c43}}. Thank you!

Created CASSANDRA-13832 to re-enable and remove the flag after we have CASSANDRA-13826.;;;","31/Aug/17 12:11;jasonstack;Thanks for reviewing and feedback;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RepairJob blocks on syncTasks,CASSANDRA-13797,13097450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,24/Aug/17 22:15,03/Sep/20 11:56,14/Jul/23 05:56,14/Mar/18 12:56,3.0.15,3.11.1,4.0,4.0-alpha1,,,Consistency/Repair,,,,,0,,,,,"The thread running {{RepairJob}} blocks while it waits for the validations it starts to complete ([see here|https://github.com/bdeggleston/cassandra/blob/9fdec0a82851f5c35cd21d02e8c4da8fc685edb2/src/java/org/apache/cassandra/repair/RepairJob.java#L185]). However, the downstream callbacks (ie: the post-repair cleanup stuff) aren't waiting for {{RepairJob#run}} to return, they're waiting for a result to be set on RepairJob the future, which happens after the sync tasks have completed. This post repair cleanup stuff also immediately shuts down the executor {{RepairJob#run}} is running in. So in noop repair sessions, where there's nothing to stream, I'm seeing the callbacks sometimes fire before {{RepairJob#run}} wakes up, and causing an {{InterruptedException}} is thrown.

I'm pretty sure this can just be removed, but I'd like a second opinion. This appears to just be a holdover from before repair coordination became async. I thought it might be doing some throttling by blocking, but each repair session gets it's own executor, and validation is  throttled by the fixed size executors doing the actual work of validation, so I don't think we need to keep this around.",,bdeggleston,jjirsa,jjordan,KurtG,marcuse,mbulman,slater_ben,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15902,,,,,CASSANDRA-13555,,,,,,,CASSANDRA-14332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 14 16:35:19 UTC 2018,,,,,,,,,,"0|i3j9hr:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"24/Aug/17 22:23;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13797]
[utest|https://circleci.com/gh/bdeggleston/cassandra/104]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/218/];;;","25/Aug/17 14:27;marcuse;+1 (if the tests pass, I restarted the dtests);;;","29/Aug/17 21:54;bdeggleston;let's see if this one finishes: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/231]

Edit: also aborted;;;","11/Sep/17 22:54;bdeggleston;rebased on latest trunk: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/295/];;;","12/Sep/17 15:10;bdeggleston;Adding 3.0 branch & tests since it's also affected.

[3.0|https://github.com/bdeggleston/cassandra/tree/13797-3.0]
[utest|https://circleci.com/gh/bdeggleston/cassandra/117]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/302/];;;","13/Sep/17 12:56;jjordan;If adding 3.0 you also need 3.11;;;","13/Sep/17 22:06;bdeggleston;Got clean utest runs on 3.0, 3.11, and trunk. 3.0 and trunk dtest failures were either also failing in their respective parent branches, or not reproducible locally. Committed to 3.0 as {{e7299c08f940057e8fd4dfa3f24dcc6e0cb5f78d}} and merged up;;;","02/Oct/17 16:35;jjirsa;Please keep an eye on fixvers.
;;;","01/Mar/18 23:17;VincentWhite;Now that we don't wait for the validations of each repair job to finish before moving onto the next one, I don't see anything to stop the repair coordinator from spinning through all the token ranges and effectively triggering all the validations tasks at once, which could be a significant amount of validation compactions on each node depending on your topology and common ranges for that keyspace. I'm also not sure of the overhead of creating all the futures/listeners on the coordinator at once in this case. 

In 3 the validation executor thread pool has no size limit so a new validation is started as soon as a validation request is received. I admit I haven't caught up on the changes to repair in trunk, and while the validation executor pool size is configurable in trunk, its default is still Integer.MAX_VALUE.

I understand this same affect (hundreds of concurrent validations) can still happen if you trigger a repair across a keyspace with a large number of column families but with this change there is no way of avoiding it without using subrange repairs on a single column family (if you have a topology/replication that cant be merged into a small number of common ranges) .
;;;","02/Mar/18 01:04;bdeggleston;[~VincentWhite] are you seeing this happen? It's been a little while since I've thought about this, but I do remember worrying about something like and eventually convincing myself that this runaway validation case you're describing is prevented by something else.;;;","02/Mar/18 03:43;VincentWhite;After upgrading an ~18 node, vnode multi-DC cluster from 3.11.0 to 3.11.1 it started seeing some nodes running hundreds of concurrent validation compactions, rolling back it went back to 1 concurrent validation per CF. I haven't had a chance to reproduce it at that scale but my locale testing show that if I have enough data, or just add a sleep(9999999) to validation compactions to simulate long validations, they continue to accumulate over a few seconds until the repair session has looped through all the common ranges. 

;;;","14/Mar/18 00:22;KurtG;This issue is actually pretty serious for anyone running vnodes and a mid sized cluster. This isn't the first time we've had unbounded validation compactions kicked off simultaneously and it's caused a lot of problems at Instaclustr in the past. We should really fix this by 3.11.3 because it easily causes massive latency spikes whenever a repair kicks off due to validations taking up all the CPU. I'd like a simple revert but that doesn't fix the issue in the description. Don't think this warrants a new ticket so I think reopening this one is in order. [~bdeggleston] [~krummas] WDYT?;;;","14/Mar/18 12:55;jjordan;[~KurtG] when a ticket has already gone out in a release we prefer to open a brand new ticket rather than re-open an existing one, as fix versions get confusing if you re-open and already released ticket.;;;","14/Mar/18 16:35;bdeggleston;Agreed this should be a new ticket. Maybe the right fix is to backport CASSANDRA-13521 with concurrent validations set to something reasonable? I think the incorrect assumption of this ticket that's causing problems was that the validation executor was bounded, which obviously isn't the case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix trigger example on 4.0,CASSANDRA-13796,13097331,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cnlwsu,cnlwsu,cnlwsu,24/Aug/17 14:54,15/May/20 08:05,14/Jul/23 05:56,24/Aug/17 15:36,4.0,4.0-alpha1,,,,,,,,,,0,,,,,{{CFMetadata.cfName}} was moved to {{name}} not {{table}},,cnlwsu,githubbot,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 05:28:18 UTC 2017,,,,,,,,,,"0|i3j8rr:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"24/Aug/17 14:56;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/140

    Fix trigger example for CASSANDRA-13796

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13796

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/140.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #140
    
----

----
;;;","24/Aug/17 15:36;jasobrown;Committed as sha {{2e5847d29bbdd45fd4fc73f071779d91326ceeba}}. Thanks!

This closes #140;;;","30/Aug/17 05:26;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/140
  
    @clohfink  - looks like this was merged in 2e5847d29bbdd45fd4fc73f071779d91326ceeba , can you close?

;;;","30/Aug/17 05:28;githubbot;Github user clohfink closed the pull request at:

    https://github.com/apache/cassandra/pull/140
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix short read protection logic for querying more rows,CASSANDRA-13794,13097299,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,benedict,aleksey,24/Aug/17 12:36,16/Apr/19 09:30,14/Jul/23 05:56,20/Sep/17 16:59,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,Correctness,,,,"Discovered by [~benedict] while reviewing CASSANDRA-13747:

{quote}
While reviewing I got a little suspicious of the modified line {{DataResolver}} :479, as it seemed that n and x were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

This is probably a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value >= 0 before setting to 1.
{quote}",,aleksey,benedict,jasonstack,jeromatron,mshuler,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13747,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 20 16:59:24 UTC 2017,,,,,,,,,,"0|i3j8kn:",9223372036854775807,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"04/Sep/17 19:28;aleksey;Work in progress branch [here|https://github.com/iamaleksey/cassandra/tree/13794-3.0]. Currently missing (new) tests, but I want to get the underlying logic reviewed and approved, first. Would add coverage before committing it.

A short summary of the issue: the code right now has two variables swapped, which ultimately results in us always fetching 1 extra row per short read protection requests, in a blocking manner, making it very inefficient. But upon closer look, there are some other inefficiencies here that can and should be addressed:

1. One of our stop conditions is {{lastCount == counter.counted()}}. It's supposed to abort a short read if our previous attempt to fetch more rows yielded 0 extra rows. It's not incorrect, but is only a special case of the more general scenario: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica, and allows us to abort earlier and more frequently.

2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again, it isn't incorrect, but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only, it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always, and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.

3. Once we've swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error, we'd still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion, for example, the formula would fetch *a lot* of rows {{n * (n - 1)}}, with {{n}} growing exponentially with every attempt.

Upon closer inspection, the formula doesn't make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}}, but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration, but will be diverging further and further with each request. In addition to that, it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition, which can't be true for most workloads.

I couldn't come up with some ideal heuristic that covers all workloads, so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. I'm not completely sure about it, but I welcome any ideas on how to make it better. Either way, anything we do should be significantly more efficient than what we have now.

I've also made some renames, refactorings, and moved a few things around to better understand the code myself, and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method, instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation, so you can see the contrast in arguments.;;;","04/Sep/17 19:29;aleksey;Marking the ticket as {{Patch Available}}, despite its lack of (new) tests, so that it can be reviewed first. Tests will be committed with the rest of the code.;;;","14/Sep/17 19:13;benedict;This patch is great (excepting a couple of extraneous edits).  Love the comments.

+1

I would suggest filing two follow-up tickets to address some short comings with this code path, but they're edge-case, and not straight-forward enough, to not block this merge.

# For some extreme users, 16 rows could be a huge amount of data.  There should probably be some modulation of this lower bound based on known data sizes in the table, or the like.
# Conversely, in an overloaded cluster on which users are commonly performing fairly large-limit reads (say, 1k+ moderate sized rows) of larger partitions, we could find ourselves doubling the amount of work the cluster needs to do; during overload we can expect dropped writes, and a single missing row in any read would trigger a same-sized read.  This could iteratively compound the overload. 
 The best solutions to this problem are probably non-trivial, though a simplish approach might be to use exponential growth, bounded on both sides by a minimum and maximum (perhaps similarly determined from the known data size distribution) - with the query limit being used as the first value if it is small enough.

I _would_ say that (2) is no worse than the status-quo, given the per-request overheads are probably greater than the per-datum overheads in a typical cluster, but CASSANDRA-12872 suggests we haven't been incurring the full overheads of SRP, so we cannot claim that.  I still think it is reasonable to address this in a follow-up ticket, however.;;;","20/Sep/17 16:59;aleksey;Committed to 3.0 as [f93e6e3401c343dec74687d8b079b5697813ab28|https://github.com/apache/cassandra/commit/f93e6e3401c343dec74687d8b079b5697813ab28] and merged with 3.11 and trunk.

Circle run for 3.0 [here|https://circleci.com/gh/iamaleksey/cassandra/39] has two completely unrelated {{CommitLogSegmentManagerTest}} failures, and [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/314/testReport/] here is mostly failures to git clone.

The passing tests include the 3 new dtests added since this JIRA was created. My initial plan was to cover it with proper unit tests, too - similar to read repair tests we have - but doing it properly has proven to be too time consuming. In addition to the tests we have, I did a lot of manual testing (which uncovered a couple more issues - not affecting my branch). But more unit test coverage will be added later - we've budgeted a significant chunk of time on {{DataResolver}} testing alone.

Follow up JIRAs I'll file soonish. Thanks for the review! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTombstoneMarker and PartitionDeletion is not properly included in MV,CASSANDRA-13787,13096892,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,23/Aug/17 02:03,15/May/20 08:02,14/Jul/23 05:56,20/Sep/17 13:20,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Materialized Views,Legacy/Local Write-Read Paths,,,,0,,,,,"Found two problems related to MV tombstone. 

1. Range-tombstone-Marker being ignored after shadowing first row, subsequent base rows are not shadowed in TableViews.

    If the range tombstone was not flushed, it was used as deleted row to shadow new updates. It works correctly.
    After range tombstone was flushed, it was used as RangeTombstoneMarker and being skipped after shadowing first update. The bound of RangeTombstoneMarker seems wrong, it contained full clustering, but it should contain range or it should be multiple RangeTombstoneMarkers for multiple slices(aka. new updates)

-2. Partition tombstone is not used when no existing live data, it will resurrect deleted cells. It was found in 11500 and included in that patch.- (Merged in CASSANDRA-11500)


In order not to make 11500 patch more complicated, I will try fix range/partition tombstone issue here.


{code:title=Tests to reproduce}
    @Test
    public void testExistingRangeTombstoneWithFlush() throws Throwable
    {
        testExistingRangeTombstone(true);
    }

    @Test
    public void testExistingRangeTombstoneWithoutFlush() throws Throwable
    {
        testExistingRangeTombstone(false);
    }

    public void testExistingRangeTombstone(boolean flush) throws Throwable
    {
        createTable(""CREATE TABLE %s (k1 int, c1 int, c2 int, v1 int, v2 int, PRIMARY KEY (k1, c1, c2))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""view1"",
                   ""CREATE MATERIALIZED VIEW view1 AS SELECT * FROM %%s WHERE k1 IS NOT NULL AND c1 IS NOT NULL AND c2 IS NOT NULL PRIMARY KEY (k1, c2, c1)"");

        updateView(""DELETE FROM %s USING TIMESTAMP 10 WHERE k1 = 1 and c1=1"");


        if (flush)
            Keyspace.open(keyspace()).getColumnFamilyStore(currentTable()).forceBlockingFlush();

        String table = KEYSPACE + ""."" + currentTable();
        updateView(""BEGIN BATCH "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 0, 0, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 1, 0, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 0, 1, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 1, 1, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 2, 1, 2) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 3, 1, 3) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 2, 0, 2, 0) USING TIMESTAMP 5; "" +
                ""APPLY BATCH"");

        assertRowsIgnoringOrder(execute(""select * from %s""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
        assertRowsIgnoringOrder(execute(""select k1,c1,c2,v1,v2 from view1""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
    }

    @Test
    public void testExistingParitionDeletionWithFlush() throws Throwable
    {
        testExistingParitionDeletion(true);
    }

    @Test
    public void testExistingParitionDeletionWithoutFlush() throws Throwable
    {
        testExistingParitionDeletion(false);
    }

    public void testExistingParitionDeletion(boolean flush) throws Throwable
    {
        // for partition range deletion, need to know that existing row is shadowed instead of not existed.
        createTable(""CREATE TABLE %s (a int, b int, c int, d int, PRIMARY KEY (a))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_test1"",
                   ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE a IS NOT NULL AND b IS NOT NULL PRIMARY KEY (a, b)"");

        Keyspace ks = Keyspace.open(keyspace());
        ks.getColumnFamilyStore(""mv_test1"").disableAutoCompaction();

        execute(""INSERT INTO %s (a, b, c, d) VALUES (?, ?, ?, ?) using timestamp 0"", 1, 1, 1, 1);
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, 1));

        // remove view row
        updateView(""UPDATE %s using timestamp 1 set b = null WHERE a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));
        // remove base row, no view updated generated.
        updateView(""DELETE FROM %s using timestamp 2 where a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));

        // restor view row with b,c column. d is still tombstone
        updateView(""UPDATE %s using timestamp 3 set b = 1,c = 1 where a=1""); // upsert
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, null));
    }
{code}",,jasonstack,jeromatron,KurtG,sbtourist,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 04:57:24 UTC 2017,,,,,,,,,,"0|i3j62v:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"23/Aug/17 09:46;jasonstack;The first problem is not exactly a MV bug, it's SSTableReader issue of handling multiple slices which is probably only used by MV. The existing code consumes the {{original RangeTombstoneMarker(eg. Deletion@c1=1@10)}} for the first slice to generate a RangeTombstoneMarker(eg. Deletion@c1=1&c1=0@10) with first slice's clustering. So there is no tombstones generated for other slices. The fix is to make sure tombstones are generated for each slices within the range of original RangeTombstoneMarker and do not close the openMarker til reading its paired closeMarker.

The second problem is that if there is no existing live data, empty existing row is given to the ViewUpdateGenerator which may resurrect deleted cells. The fix is to include current-open-deletion into the empty existing row. So view could shadow the deleted cells.


[Draft|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.0] patch base on cassandra-3.0 . This needs to be fixed in 3.0/3.11/trunk.

;;;","05/Sep/17 12:56;slebresne;Sorry for the delay reviewing, but patch lgtm (though, to make extract sure we're on the same page, this appear to only include the fix for the ""first problem"" since the ""second problem"" seems to have been included in CASSANDRA-11500).

Have you run CI on this? Also, if you could provide branches for all branches, that would be amazing (the fix itself should merge up cleanly, but I suspect the tests may require a few minor updates).;;;","05/Sep/17 12:58;jasonstack;Thanks for reviewing.

I will prepare branches for 3.0/3.1/trunk. I have removed the code for second issue since 11500 is merged..;;;","06/Sep/17 15:25;jasonstack;| source |  test  |   dtest |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-trunk] | [passed|https://circleci.com/gh/jasonstack/cassandra/571] | repair_tests.repair_test.TestRepair.simple_parallel_repair_test
repair_tests.repair_test.TestRepair.test_dead_sync_initiator
rebuild_test.TestRebuild.rebuild_ranges_test
repair_tests.repair_test.TestRepair.dc_repair_test
topology_test.TestTopology.simple_decommission_test
disk_balance_test.TestDiskBalance.disk_balance_decommission_test
repair_tests.repair_test.TestRepair.simple_sequential_repair_test
repair_tests.repair_test.TestRepair.thread_count_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space|
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.11] | [passed|https://circleci.com/gh/jasonstack/cassandra/573] |upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.0] | [passed|https://circleci.com/gh/jasonstack/cassandra/572] | rebuild_test.TestRebuild.simple_rebuild_test
global_row_key_cache_test.TestGlobalRowKeyCache.functional_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test|

Failing dtests are either failing on trunk or passed on local.
;;;","20/Sep/17 08:51;slebresne;+1, nothing seems unrelated to this here.;;;","20/Sep/17 13:20;slebresne;Alright, committed, thanks.;;;","21/Sep/17 04:57;jasonstack;thanks for reviewing~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation compactions can cause orphan sstable warnings,CASSANDRA-13786,13096866,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,22/Aug/17 23:00,15/May/20 08:04,14/Jul/23 05:56,12/Sep/17 20:34,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"I've seen LevelledCompactionStrategy occasionally logging: {quote}<sstable_name> from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.""{quote} warnings from a ValidationExecutor thread.

What's happening here is that a compaction running concurrently with the validation is promoting (or demoting) sstables as part of an incremental repair, and an sstable has changed hands by the time the validation compaction gets around to getting scanners for it. The sstable isolation/synchronization done by validation compactions is a lot looser than normal compactions, so seeing this happen isn't very surprising. Given that it's harmless, and not unexpected, I think it would be best to not log these during validation compactions.",,bdeggleston,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 20:34:04 UTC 2017,,,,,,,,,,"0|i3j5xb:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"22/Aug/17 23:01;bdeggleston;I have a fix pushed [here|https://github.com/bdeggleston/cassandra/commits/orphan-sstables], but I don't like that it's changing the compaction strategy interface, anyone have any ideas on alternate solutions?

/cc [~krummas] [~jjirsa]
;;;","22/Aug/17 23:13;jjirsa;Not a review and haven't read the code, but you're just targeting 4.0? Seems like if we're going to break an interface, that's probably the time to do it.
;;;","23/Aug/17 00:49;bdeggleston;Yeah for sure. It just seems like changing the interface is an oversize solution to the minor annoyance of getting periodic warnings in the logs. Maybe we don't need the log entry anymore, or it can be demoted to info, or there's a solution I haven't though of. That said, there probably aren't many 3rd party compaction strategies written against vanilla Cassandra these days.;;;","23/Aug/17 06:39;marcuse;Lets just drop it to DEBUG? It is pretty unclear what a user needs to do to avoid/fix it but keeping it around to make debugging easier might make sense?;;;","24/Aug/17 20:10;bdeggleston;[~kohlisankalp] and I were talking about this, and he mentioned that even if the operator can't do anything to fix, it's still good to know it's happening. I coded up a more ""correct"" solution. [This branch|https://github.com/bdeggleston/cassandra/tree/orphan-sstables2] adjusts how mutating repairedAt is synchronized, and how getScanners is synchronized. So that the scenario causing the warning in this case shouldn't be able to happen.

1. for {{getScanners}}, it moves the organizing of sstables into unrepaired/pending/repaired into the same read locked block that actually gets the scanners
2. adds a {{mutateRepaired}} method to {{CompactionStrategyManager}} that is used to mutate the repaired time and move the sstables between compaction strategies with a write lock held.;;;","28/Aug/17 13:17;marcuse;makes sense, added 2 comments on the gh branch;;;","29/Aug/17 21:47;bdeggleston;Fixed

[trunk|https://github.com/bdeggleston/cassandra/tree/orphan-sstables2]
[utest|https://circleci.com/gh/bdeggleston/cassandra/tree/orphan-sstables2]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/230/];;;","30/Aug/17 07:45;marcuse;restarted dtests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/236/;;;","31/Aug/17 09:09;marcuse;code lgtm, but dtest runs seem to die, restarted again: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/256;;;","11/Sep/17 23:00;bdeggleston;rebased on latest trunk: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/297/];;;","12/Sep/17 20:34;bdeggleston;Got a clean utest run locally, dtest failures were also all failing on trunk.
Committed as {{7d4d1a32581ff40ed1049833631832054bcf2316}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
issue with pycharm datastax cassandra driver,CASSANDRA-13779,13096176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,batchuusa,batchuusa,19/Aug/17 00:25,16/Apr/19 09:30,14/Jul/23 05:56,19/Aug/17 16:27,,,,,,,,,,,,0,,,,,"[Server error] message=""io.netty.handler.codec.DecoderException: org.apache.cassandra.transport.ProtocolException: Invalid or unsupported protocol version: 4""',)})",,batchuusa,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 19 16:27:38 UTC 2017,,,,,,,,,,"0|i3j1pz:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"19/Aug/17 12:46;rha;Hi, this error happens when there is a mismatch between Cassandra version and Driver version. This is not related to PyCharm. Be sure to check [Python DataStax driver compatibility matrix|https://docs.datastax.com/en/developer/driver-matrix/doc/pythonDrivers.html], [Java DataStax driver compatibility matrix|https://docs.datastax.com/en/developer/java-driver/3.3/manual/native_protocol/#compatibility-matrix], etc.

(Note that DataStax Driver has its own bug tracker for [Python|https://datastax-oss.atlassian.net/projects/PYTHON/issues/], [Java|https://datastax-oss.atlassian.net/projects/JAVA/summary], etc.);;;","19/Aug/17 15:20;batchuusa;thanks i will follow up with Datastax.

Sent from my iPhone


;;;","19/Aug/17 16:25;batchuusa;It was not datastax drive, I downloaded from apache website , cassandra-driver-3.11.0 is same driver what datastax have ?;;;","19/Aug/17 16:27;batchuusa;My apologies, I checked and it is related datastax. Closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a field to an UDT can corrupte the tables using it,CASSANDRA-13776,13095594,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,blerer,blerer,blerer,18/Aug/17 09:34,15/May/20 08:04,14/Jul/23 05:56,24/Aug/17 16:33,3.0.15,3.11.1,4.0,4.0-alpha1,,,Local/SSTable,,,,,0,,,,,"Adding a field to an UDT which is used as a {{Set}} element or as a {{Map}} element can corrupt the table.
The problem can be reproduced using the following test case:
{code}
    @Test
    public void testReadAfterAlteringUserTypeNestedWithinSet() throws Throwable
    {
        String ut1 = createType(""CREATE TYPE %s (a int)"");
        String columnType = KEYSPACE + ""."" + ut1;

        try
        {
            createTable(""CREATE TABLE %s (x int PRIMARY KEY, y set<frozen<"" + columnType + "">>)"");
            disableCompaction();

            execute(""INSERT INTO %s (x, y) VALUES(1, ?)"", set(userType(1), userType(2)));
            assertRows(execute(""SELECT * FROM %s""), row(1, set(userType(1), userType(2))));
            flush();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1), userType(2))));

            execute(""ALTER TYPE "" + KEYSPACE + ""."" + ut1 + "" ADD b int"");
            execute(""UPDATE %s SET y = y + ? WHERE x = 1"",
                    set(userType(1, 1), userType(1, 2), userType(2, 1)));

            flush();
            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                           row(1, set(userType(1),
                                      userType(1, 1),
                                      userType(1, 2),
                                      userType(2),
                                      userType(2, 1))));

            compact();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1),
                                  userType(1, 1),
                                  userType(1, 2),
                                  userType(2),
                                  userType(2, 1))));
        }
        finally
        {
            enableCompaction();
        }
    }
{code} 

There are in fact 2 problems:
# When the {{sets}} from the 2 versions are merged the {{ColumnDefinition}} being picked up can be the older one. In which case when the tuples are sorted it my lead to an {{IndexOutOfBoundsException}}.
# During compaction, the old column definition can be the one being kept for the SSTable metadata. If it is the case the SSTable will not be readable any more and will be marked as {{corrupted}}.

If one of the tables using the type has a Materialized View attached to it, the MV updates can also fail with {{IndexOutOfBoundsException}}.

This problem can be reproduced using the following test:
{code}
    @Test
    public void testAlteringUserTypeNestedWithinSetWithView() throws Throwable
    {
        String columnType = typeWithKs(createType(""CREATE TYPE %s (a int)""));

        createTable(""CREATE TABLE %s (pk int, c int, v int, s set<frozen<"" + columnType + "">>, PRIMARY KEY (pk, c))"");
        execute(""CREATE MATERIALIZED VIEW "" + keyspace() + "".view1 AS SELECT c, pk, v FROM %s WHERE pk IS NOT NULL AND c IS NOT NULL AND v IS NOT NULL PRIMARY KEY (c, pk)"");

        execute(""INSERT INTO %s (pk, c, v, s) VALUES(?, ?, ?, ?)"", 1, 1, 1, set(userType(1), userType(2)));
        flush();

        execute(""ALTER TYPE "" + columnType + "" ADD b int"");
        execute(""UPDATE %s SET s = s + ?, v = ? WHERE pk = ? AND c = ?"",
                set(userType(1, 1), userType(1, 2), userType(2, 1)), 2, 1, 1);


        assertRows(execute(""SELECT * FROM %s WHERE pk = ? AND c = ?"", 1, 1),
                       row(1, 1, 2, set(userType(1),
                                        userType(1, 1),
                                        userType(1, 2),
                                        userType(2),
                                        userType(2, 1))));
    }
{code}      ",,aleksey,blerer,christopher.lambert,sbtourist,snazy,spod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14010,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 16:33:30 UTC 2017,,,,,,,,,,"0|i3iy6f:",9223372036854775807,,,,,,,snazy,,snazy,,,Critical,,,,,,,,,,,,,,,,,,,"22/Aug/17 13:32;blerer;I pushed the patches for [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:13776-3.0], [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13776-3.11] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13776-trunk].
I ran the tests on our internal CI and the failing tests look unrelated to the patches.;;;","23/Aug/17 14:01;snazy;Nice work, Benjamin!
The new unit tests cover the issue.

Just a few things:
* We should order on an array (or ArrayList) in {{SerializationHeader.orderByDescendingGeneration()} and use ""static"" comparator instances
* Checks on component count are mussing for tuples and composites in {{AbstractTypeVersionComparator}}
* Test should cover all type ""types"" in {{AbstractTypeVersionComparatorTest}}

Pushed some commits [here|https://github.com/snazy/cassandra/commits/13776-3.0-review].

+1 with the above changes. CI (internal one) looks good.
;;;","24/Aug/17 16:33;blerer;Committed into 3.0 at cf0b6d107bade419dada49a5da40d2579c80ade8 and merged into 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI tests fail because *stress-test* isn't a valid target,CASSANDRA-13775,13095425,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,17/Aug/17 18:40,13/May/20 13:18,14/Jul/23 05:56,18/Aug/17 17:39,2.1.19,2.2.11,3.0.15,,,,Build,CI,,,,0,CI,,,,"*stress-test* was added to CircleCI in CASSANDRA-13413 (2.1+) but the target itself got introduced in CASSANDRA-11638 (3.10).
",,eduard.tudenhoefner,marcuse,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,eduard.tudenhoefner,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 25 15:22:58 UTC 2017,,,,,,,,,,"0|i3ix53:",9223372036854775807,2.1.18,2.2.10,3.0.14,,,,mshuler,,mshuler,,,Normal,,,,,,,,,,,,,,,,,,,"17/Aug/17 18:51;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/13775-21
Test: https://circleci.com/gh/nastra/cassandra/7;;;","17/Aug/17 19:31;mshuler;Looks good - will commit in a few!;;;","17/Aug/17 19:35;mshuler;Commit {{3c0c4620f2}} on cassandra-2.1 and merged up. Thanks, Ed.;;;","18/Aug/17 13:37;marcuse;I think this might still be broken:
{code}
find ./build/test/output/ -iname ""*.xml"" -exec cp {} $CIRCLE_TEST_REPORTS/junit/ \;
find: `./build/test/output/': No such file or directory

find ./build/test/output/ -iname ""*.xml"" -exec cp {} $CIRCLE_TEST_REPORTS/junit/ \; returned exit code 1
{code}

Most will have their circleci configured to 4 containers, so it would make sense to keep using 4. Patch to just move the {{ant eclipse-warnings}} to container 3 here: https://github.com/krummas/cassandra/commits/marcuse/13775-update

and tests running here (note, different branch name, but it was just renamed for clarity): https://circleci.com/gh/krummas/cassandra/78;;;","18/Aug/17 14:45;marcuse;seems eclipse-test doesn't exist in 2.1 either, running {{test-clientutil-jar}} for 2.1 and {{eclipse-test}} for the rest: https://circleci.com/gh/krummas/cassandra/80;;;","18/Aug/17 15:00;mshuler;Thanks for catching these additional details, Marcus - reopening.;;;","18/Aug/17 15:27;eduard.tudenhoefner;[~krummas] your code changes lgtm, so +1 once tests pass;;;","18/Aug/17 16:47;marcuse;added another commit as the test-clientutil-jar did not generate a report, could you have a look [~eduard.tudenhoefner]?;;;","18/Aug/17 16:58;eduard.tudenhoefner;verified locally that report is generated. Waiting for https://circleci.com/gh/krummas/cassandra/81 to finish;;;","18/Aug/17 17:13;eduard.tudenhoefner;[~krummas] the new artifact shows up in https://81-12804511-gh.circle-artifacts.com/3/tmp/circle-junit.Xa1F9EO/junit/TEST-org.apache.cassandra.serializers.ClientUtilsTest.xml so +1 for merging;;;","18/Aug/17 17:20;marcuse;and committed, thanks;;;","18/Aug/17 17:39;eduard.tudenhoefner;Follow up fix was committed as [a3498d5|https://github.com/apache/cassandra/commit/a3498d5eb7b5b5418b32491238524b33f20347dd] to *cassandra-2.1* and merged upstream.;;;","25/Aug/17 15:22;marcuse;created CASSANDRA-13807 as another follow-up;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress writes even data when n=0,CASSANDRA-13773,13095404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,17/Aug/17 17:01,15/May/20 08:04,14/Jul/23 05:56,22/Aug/17 01:40,3.0.15,3.11.1,4.0,4.0-alpha1,,,Tool/stress,,,,,0,,,,,"This is very unintuitive as
{code}
cassandra-stress write n=0 -rate threads=1
{code}
will do inserts even with *n=0*. I guess most people won't ever run with *n=0* but this is a nice shortcut for creating some schema without using *cqlsh*

This is happening because we're writing *50k* rows of warmup data as can be seen below:
{code}
cqlsh> select count(*) from keyspace1.standard1 ;

 count
-------
 50000

(1 rows)
{code}

We can avoid writing warmup data using 
{code}
cassandra-stress write n=0 no-warmup -rate threads=1
{code}

but I would still expect to have *0* rows written when specifying *n=0*.

",,eduard.tudenhoefner,jay.zhuang,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,eduard.tudenhoefner,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 22 01:40:04 UTC 2017,,,,,,,,,,"0|i3ix0f:",9223372036854775807,3.0.14,,,,,,stefania,,stefania,,,Low,,,,,,,,,,,,,,,,,,,"17/Aug/17 17:13;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/CASSANDRA-13773-30
Test: https://circleci.com/gh/nastra/cassandra/12;;;","21/Aug/17 02:55;stefania;The patch LGTM however, whilst it doesn't make sense to perform warmup when n=0, it still changes the behavior seen by the user. Therefore, I am not entirely sure this patch should go into 3.0, any thoughts [~tjake]?

Regarding CI, I think what we care is that dtests using cassandra-stress still work, I don't think cassandra-stress impacts unit tests at all.;;;","21/Aug/17 03:21;stefania;I've started the dtests on our internal CI, if there is a way to run them on CircleCI please launch them and I haven't set it up yet.

I've tested a bit locally as well, and I think we are better off skipping the command entirely when {{n=0}}, not just the warm-up, otherwise {{cassandra-stress write n=0}} (with no rate specified) will loop over different rates and sleep for no good reason whatsoever. So I suggest that we create the schema and then exit, see [here|https://github.com/apache/cassandra/compare/trunk...stef1927:13773-3.0#diff-fd2f2d2364937fcb1c0d73c8314f1418R57].;;;","21/Aug/17 14:50;eduard.tudenhoefner;Skipping the command entirely sgtm;;;","22/Aug/17 01:40;stefania;dtests looked good as well.

I don't think anyone would expect cassandra-stress to write data if n=0, so I've committed it to 3.0 as {{6a1b1f26b7174e8c9bf86a96514ab626ce2a4117}} and merged into 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRepairManager.getNextBackgroundTask throwing IndexOutOfBoundsException,CASSANDRA-13769,13095155,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,16/Aug/17 21:38,15/May/20 07:59,14/Jul/23 05:56,18/Aug/17 17:33,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"If all the repair sessions managed by a PendingRepairManager are can be cleaned up and we call getNextBackgroundTask, we'll try to pull an element out of an empty list and throw an exception.",,bdeggleston,marcuse,mbyrd,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 17:33:07 UTC 2017,,,,,,,,,,"0|i3ivhb:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"16/Aug/17 21:43;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13769]
[utest|https://circleci.com/gh/bdeggleston/cassandra/88];;;","17/Aug/17 11:03;marcuse;+1;;;","18/Aug/17 17:33;bdeggleston;committed as {{c066f126e20180ae0230b7d6a61666152149a79f}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SelectTest.testMixedTTLOnColumnsWide is flaky,CASSANDRA-13764,13094662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jkni,jkni,15/Aug/17 02:21,15/May/20 07:59,14/Jul/23 05:56,29/Aug/17 22:53,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Testing,,,,,0,,,,,"{{org.apache.cassandra.cql3.validation.operations.SelectTest.testMixedTTLOnColumnsWide}} is flaky. This is because it inserts rows and then asserts their contents using {{ttl()}} in the select, but if the test is sufficiently slow, the remaining ttl may change by the time the select is run. Anecdotally, {{testSelectWithAlias}} in the same class uses a fudge factor of 1 second that would fix all the failures I've seen, but it might make more sense to measure the elapsed time in the test and calculate the acceptable variation from that time.",,jasonstack,jjirsa,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 22:53:25 UTC 2017,,,,,,,,,,"0|i3isg7:",9223372036854775807,,,,,,,jkni,,jkni,,,Low,,,,,,,,,,,,,,,,,,,"15/Aug/17 16:06;jkni;This also affects {{SelectTest.testMixedTTLOnColumns}}.;;;","29/Aug/17 19:50;jjirsa;[~slebresne] says he's ok with ninja'ing this in, but in case you ( [~jkni] ) want to review before I do it (I need to push for Circle anyway just to be safe, so this'll sit here until Circle gives me a green run):

3.0: https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13764 (Circle: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13764 )
3.11: https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13764 (Circle:  https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13764 )
trunk: https://github.com/jeffjirsa/cassandra/tree/cassandra-13764 (Circle: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13764 );;;","29/Aug/17 21:47;jkni;The idea looks sound to me - I think you need to remove the TTL column from the select [here|https://github.com/jeffjirsa/cassandra/commit/a1e49db69622de11a996d09105e5ebf3b54c58c3#diff-7f5981228f9d9428fb164aa91316aa85R2976], as you did in {{testMixedTTLOnColumnsWide}}. If you agree, I'm comfortable with you doing that on commit and don't need to rereview if CI looks good.;;;","29/Aug/17 22:53;jjirsa;Thanks Joel.

Committed to 3.0 as {{67ac1496cc9e7d9d15be28b9536e9cdbce42473d}} and merged to 3.11 and trunk WITHOUT adding CHANGES.txt entry, because it's just a test fix and never hit a release (not quite a ninja).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair sessions shouldn't be deleted if they still have sstables,CASSANDRA-13758,13094203,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,11/Aug/17 21:13,15/May/20 08:05,14/Jul/23 05:56,18/Aug/17 17:49,4.0,4.0-alpha1,,,,,,,,,,0,incremental_repair,,,,"The incremental session cleanup doesn't verify that there are no remaining sstables marked as part of the repair before deleting it. Deleting a successful repair session which still has outstanding sstables will cause those sstables to be demoted to unrepaired, creating an inconsistency.

This typically wouldn't be an issue, since we'd expect the sstables to long since have been promoted / demoted. However, I've seen a few ref leak issues which can cause sstables to get stuck. Those have been fixed, but we should still protect against that edge case to prevent inconsistencies caused by future (or currently unknown) bugs.",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 17:49:07 UTC 2017,,,,,,,,,,"0|i3ipnb:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"11/Aug/17 21:27;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13758]

[utest|https://circleci.com/gh/bdeggleston/cassandra/87];;;","17/Aug/17 15:33;marcuse;+1 - but it might make sense to log something if a session is kept because it contains data? Feel free to add that on commit if you agree;;;","18/Aug/17 17:49;bdeggleston;added warning and committed as {{e1a1b80d424e31eeb5805c710ce010953160e3a4}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingHistogram is not thread safe,CASSANDRA-13756,13093973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,xiaxiangzhou,xiaxiangzhou,11/Aug/17 01:45,16/Apr/19 09:30,14/Jul/23 05:56,05/Sep/17 16:58,3.0.15,3.11.1,,,,,,,,,,0,,,,,"When we test C*3 in shadow cluster, we notice after a period of time, several data node suddenly run into 100% cpu and stop process query anymore.

After investigation, we found that threads are stuck on the sum() in streaminghistogram class. Those are jmx threads that working on expose getTombStoneRatio metrics (since jmx is kicked off every 3 seconds, there is a chance that multiple jmx thread is access streaminghistogram at the same time).  

After further investigation, we find that the optimization in CASSANDRA-13038 led to a spool flush every time when we call sum(). Since TreeMap is not thread safe, threads will be stuck when multiple threads visit sum() at the same time.

There are two approaches to solve this issue. 

The first one is to add a lock to the flush in sum() which will introduce some extra overhead to streaminghistogram.

The second one is to avoid streaminghistogram to be access by multiple threads. For our specific case, is to remove the metrics we added.  ",,dikanggu,hkroger,jasobrown,jay.zhuang,jeromatron,jjirsa,jjordan,marcuse,mck,mshuler,weideng,whangsf,xiaxiangzhou,yngwiie,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13718,CASSANDRA-13752,,,,,,,CASSANDRA-13718,CASSANDRA-13752,,CASSANDRA-13038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 16:58:15 UTC 2017,,,,,,,,,,"0|i3io8v:",9223372036854775807,3.0.13,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"17/Aug/17 23:00;jjirsa;Shouldn't need a version for trunk, but [~jasobrown] if you can check me there to be sure that'd be nice (I think in the faster rewrite for trunk, we now [build|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/streamhist/StreamingTombstoneHistogramBuilder.java#L182-L186] a snapshot that is no longer modified on read).
 
|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/221/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/222/] |
;;;","17/Aug/17 23:33;jasobrown;+1 on the changes, and please commit if/when the tests pass.

wrt trunk, yes, I think it should be safe due to the snapshots we create and access from {{StatsMetadata}}.;;;","18/Aug/17 00:33;dikanggu;[~jjirsa], thanks for fixing it!;;;","18/Aug/17 04:55;jjirsa;utests are clean, waiting on dtests. Apparently ~8 of the jenkins slaves are offline, so little bit delayed.;;;","21/Aug/17 18:46;hkroger;Linking to SSTable Corruption ticket which this same bug seems to cause.;;;","21/Aug/17 19:34;hkroger;[~jjirsa] Serialization probably needs to be synchronized somehow as well. Seems like sstable statistics can be corrupted at the moment because of this issue and although I am not sure it looks like that this patch probably doesn't address.;;;","22/Aug/17 10:45;hkroger;Created a branch in https://issues.apache.org/jira/browse/CASSANDRA-13752 for serialization fix.;;;","28/Aug/17 12:58;hkroger;Cassandra cleanup is probably also affected by this.;;;","29/Aug/17 18:56;jjirsa;Pushed new branches up based on what we learned from 13752:


|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/224/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/225/] |


;;;","29/Aug/17 19:18;jasobrown;I'm +1 on the snapshot version of the code. You may want to wait a day to see if [~krummas] has any additional feedback.;;;","30/Aug/17 07:27;marcuse;We should probably remove {{synchronized}} from {{flushHistogram}} and {{equals}} in the builder or make it actually thread safe and add it to the {{update}} methods as well (but the builder should never be updated by several threads so removing it should be ok)

Other than that, +1;;;","05/Sep/17 16:58;jjirsa;Thanks all. Marcus, I've addressed your comments on commit. Committed as {{ef5ac1a4abe4fb5f407c0a24f4bc808932c5d7a2}} to 3.0 and 3.11, and {{merge -s ours}} to trunk (no changes but CHANGES.txt) with Jason and Marcus both listed as reviewers. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: repair_tests.incremental_repair_test:TestIncRepair.consistent_repair_test,CASSANDRA-13755,13093930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,bdeggleston,bdeggleston,10/Aug/17 22:03,16/Apr/19 09:30,14/Jul/23 05:56,10/Aug/17 22:35,,,,,,,,,,,,0,,,,,,,bdeggleston,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 23:25:53 UTC 2017,,,,,,,,,,"0|i3inzj:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"10/Aug/17 22:24;bdeggleston;patch by [~jkni] here: https://github.com/jkni/cassandra-dtest/commit/f55f78b093fc668dc5cc9d1fc72f66dc5a9bf3a6;;;","10/Aug/17 22:35;bdeggleston;committed as {{013efa11f3d7bd2e3f64a4a5a865ff5dad565552}} thanks!;;;","10/Aug/17 23:25;jkni;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTree.Builder memory leak,CASSANDRA-13754,13093846,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,snazy,urandom,urandom,10/Aug/17 16:15,16/Apr/19 09:30,14/Jul/23 05:56,13/Sep/17 17:05,3.11.1,,,,,,Legacy/Core,,,,,1,,,,,"After a chronic bout of {{OutOfMemoryError}} in our development environment, a heap analysis is showing that more than 10G of our 12G heaps are consumed by the {{threadLocals}} members (instances of {{java.lang.ThreadLocalMap}}) of various {{io.netty.util.concurrent.FastThreadLocalThread}} instances.  Reverting [cecbe17|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=cecbe17e3eafc052acc13950494f7dddf026aa54] fixes the issue.","Cassandra 3.11.0, Netty 4.0.44.Final, OpenJDK 8u141-b15",bradfordcp,christianmovi,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,markusdlugi,snazy,tsteinmaurer,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/17 22:14;urandom;Screenshot from 2017-09-11 16-54-43.png;https://issues.apache.org/jira/secure/attachment/12886512/Screenshot+from+2017-09-11+16-54-43.png","13/Sep/17 15:47;urandom;Screenshot from 2017-09-13 10-39-58.png;https://issues.apache.org/jira/secure/attachment/12886890/Screenshot+from+2017-09-13+10-39-58.png","01/Oct/17 11:47;tsteinmaurer;cassandra_3.11.1_Recycler_memleak.png;https://issues.apache.org/jira/secure/attachment/12889883/cassandra_3.11.1_Recycler_memleak.png","02/Oct/17 20:26;tsteinmaurer;cassandra_3.11.1_snapshot_heaputilization.png;https://issues.apache.org/jira/secure/attachment/12890028/cassandra_3.11.1_snapshot_heaputilization.png",,,,,,,,,,,,,,,,4.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 08:09:14 UTC 2017,,,,,,,,,,"0|i3ingv:",9223372036854775807,,,,,,,jjordan,,jjordan,,,Normal,,,,,,,,,,,,,,,,,,,"10/Aug/17 18:08;jjirsa;What version are you on [~urandom] (or really, which version of netty is in the classpath) ? 
;;;","10/Aug/17 21:43;urandom;Cassandra 3.11.0, Netty 4.0.44.Final;;;","31/Aug/17 07:38;markusdlugi;[~jjirsa], we are experiencing the same issue on Cassandra 3.11.0 and Netty 4.0.44. We have some custom triggers which create additional rows when {{INSERT}} s on specific CFs are executed. When inserting a lot of data, the Cassandra would constantly crash with {{OutOfMemoryError}} s. We analyzed a heap dump and came to the same conclusion, which is that the instances of {{FastThreadLocalThread}} were responsible for this behaviour. As a quick workaround, we added a call to {{FastThreadLocal.removeAll()}} to our triggers, which alleviates the problem for us.

It seems that there already was an issue once with {{FastThreadLocal}} s, as described in CASSANDRA-13033, but apparently the change by [~snazy] in CASSANDRA-13034 resurfaced the issue. For a proper fix, I think all {{FastThreadLocalThread}} instances should call {{FastThreadLocal.removeAll()}} when they are done with their work. A quick glance at the code indicates that this affects the classes {{CompressedInputStream}} , {{StreamingInboundHandler}} , {{NamedThreadFactory}} and {{SEPWorker}}.;;;","31/Aug/17 10:03;snazy;Thanks for the investigation, [~markusdlugi], I'm going to check what's happening. All {{Runnables}} should have been decorated with a {{try-finally}} to call {{FastThreadLocal.removeAll()}} to prevent exactly that.;;;","31/Aug/17 10:55;markusdlugi;I see, you are talking about {{NamedTheadFactory.threadLocalDeallocator()}}, right? That should help for any threads created there, but as stated above, {{FastThreadLocalThread}} is also used in some other classes. Heap-wise, the most problematic one for us was the usage in {{SEPWorker}} I think, and this seems to have only been introduced with commit [1e92ce43a5a730f81d3f6cfd72e7f4b126db788a|https://github.com/apache/cassandra/commit/1e92ce43a5a730f81d3f6cfd72e7f4b126db788a] by [~tjake]. Maybe something similar can be done there and in all other classes making use of {{FastThreadLocalThread}}.;;;","31/Aug/17 11:33;snazy;Yea, that {{SEPWorker}} plus a couple more places (see [here|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13754-ftl-leak-3.11?expand=1] and [here|https://github.com/apache/cassandra/compare/trunk...snazy:13754-ftl-leak-trunk?expand=1]). It's actually difficult to tell whether there was really something broken in all these places in the patch, but those changes don't (should not) harm. But if someone introduces a {{FastThreadLocal}} there in the future, it should have no negative impact.
Our CI's currently checking the patches.
[~markusdlugi], do you have any chance to verify the patches?;;;","31/Aug/17 13:53;markusdlugi;[~snazy], I checked out the 13754-ftl-leak-3.11 branch and ran our load test with your patches. Unfortunately, the memory leak is still there, Cassandra first starts GCing like crazy and then throws an {{OutOfMemoryError}} after a while. So it seems like the call to {{FastThreadLocal.removeAll()}} is not properly executed.

Actually, while writing this it just struck me why that is the case. The thread created in {{SEPWorker}} does not stop after it finishes, but is instead returned to the pool and picks up new tasks. So it will never leave the {{run()}} method, and therefore also never execute {{FastThreadLocal.removeAll()}}. So I think we will have to include that call in the {{SEPWorker}} itself.;;;","31/Aug/17 14:05;snazy;[~markusdlugi] do you have a heap dump for me or references to what's actually in these {{FastThreadLocal}} s or which FTLs actually cause this? I suspect these are FTLs created in non-static fields, which probably need a different handling.

Calling {{FastThreadLocal.removeAll()}} after every iteration in {{SEPWorker}} is not a good solution, because that would effectively kill the benefit of all static FTLs.;;;","31/Aug/17 16:24;snazy;Well, yea. Looking at the heap dump, that [~markusdlugi] provided, is looks like the node is ""just"" overloaded with too many and maybe too big writes in combination with a small heap. There are lots of {{BTree$Builder}} instances with live references in their {{Object[] values}} array to {{HeapByteBuffer}} instances, each holding a 1MB {{byte[]}}.
{{BTree$Builder}} instances reset the {{Object[] values}} when finished - i.e. those builders are actively doing something (writes are happening at that time).
TL;DR I don't think this is actually related to the issue that [~urandom] describes.

[~urandom], can you explain what actually what these {{ThreadLocal}} instances referenced?;;;","01/Sep/17 07:41;markusdlugi;[~snazy], I don't think the node is overloaded. I originally thought so as well, so I made a little experiment where I included a cap in our load test limiting the {{INSERT}} s per minute from ~25,000 to ~10,000. As a consequence, the node survived a little longer, but in the end it still died with an {{OutOfMemoryError}} after more data had been inserted. So it's not that there are too many active writes, it's just that the node fails after a certain amount of total writes, which indicates to me that a memory leak is indeed happening.

I also had another look into the heap dump I sent you, and you are correct that the heap is mostly filled with {{BTree$Builder}} instances that still have stuff in their {{values}} array. However, if you look closer, you will notice that for each of these instances, the {{values}} array always contains {{null}} for the first couple of entries, and only after those there is still actual content. For some reason, the actual content always starts at index 28, whereas indices 0 - 27 are {{null}} - not sure if this is a coincidence? But you can also see that for all the {{BTree$Builder}} objects, the {{count}} attribute is 0, which also indicates to me that {{BTree$Builder.cleanup()}} has already run and those are not active writes. This theory is supported by the fact that my little workaround of manually calling {{FastThreadLocal.removeAll()}} actually works, because this means that no other objects except the {{FastThreadLocal}} s still have references to the builders.

Therefore, I think we have two issues here:

# {{SEPWorker}} is never cleaning the {{FastThreadLocal}} s, therefore accumulating references to otherwise dead objects - maybe we can include something to at least remove non-static entries regularly?
# {{BTree$Builder}} seems to have an issue properly cleaning up after building, so the objects referenced by the {{FastThreadLocal}} s of the {{SEPWorker}} threads are very large and thus ultimately lead to the {{OutOfMemoryError}} s;;;","01/Sep/17 10:21;snazy;Your observation regarding {{BTree.Builder.values[]}} seems correct.
However, {{SEPWorker}} must *not* remove the thread locals - it's the intention of these thread-locals to be kept for reuse.;;;","01/Sep/17 12:08;markusdlugi;Your latest patch which resets the entire {{BTree$Builder.values}} array seemed to do the trick, entire load test is now running smoothly. No more crazy GCing and most importantly no {{OutOfMemoryError}} s. Thanks a lot for the fast support and help!;;;","01/Sep/17 14:19;snazy;Given that the FTL changes apparently do not have any influence to the OOM issuse, but look serious enough to fix, I've split them out into CASSANDRA-13838.

Patch for this ticket is reduced to the BTree change.
CI looks good.;;;","01/Sep/17 14:21;jjordan;+1 for just https://github.com/apache/cassandra/commit/2cafd0b6b4bbc5a6ec5726d47d0093bdac3af19c to fix this and splitting out the other changes to a new ticket.;;;","01/Sep/17 14:22;jjordan;and +1 for the patch.;;;","01/Sep/17 17:49;snazy;Committed as [bed7fa5ef8492d1ff3852cf299622a5ad4e0b621|https://github.com/apache/cassandra/commit/bed7fa5ef8492d1ff3852cf299622a5ad4e0b621] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11] and merged to trunk.
;;;","11/Sep/17 22:13;urandom;I apologize for chiming in here so late, but I'm not sure this addresses what I was seeing.  In my dumps, all of the heap is tied up in the {{ThreadLocalMap}} s of instances of {{FastThreadLocalThread}} for _Native-Transport-Requests_, _RequestResponseStage_, _ReadStage_, etc; I think what I was seeing is different than [~markusdlugi].

See the attached screenshot of the dominator tree view in MemoryAnalyzer.

I can make a dump available, but be warned, it is 12G in size.;;;","12/Sep/17 06:29;markusdlugi;[~urandom], I think it might still be the same issue. The threads you mentioned are all created by the {{SEPWorker}} as well, as you can also see in your screenshot where your {{FastThreadLocalThread}} has a reference to an instance of that class. Now I'm not sure whether the actual content of your {{ThreadLocalMap}} s is the same as in my heap dump - in my case, the maps mostly held instances of {{BTree$Builder}} , which then had references to many {{byte[]}} arrays. Maybe you can check if this is the case for you as well?

Other than that, you could also try and see if the patches created by [~snazy] alleviate your issue.;;;","13/Sep/17 15:47;urandom;
{quote}
I think it might still be the same issue. The threads you mentioned are all created by the SEPWorker as well, as you can also see in your screenshot where your FastThreadLocalThread has a reference to an instance of that class. Now I'm not sure whether the actual content of your ThreadLocalMap s is the same as in my heap dump - in my case, the maps mostly held instances of BTree$Builder , which then had references to many byte[] arrays. Maybe you can check if this is the case for you as well?
{quote}

There are no instances of {{BTree}} here, (see new screenshot attached).

{quote}
Other than that, you could also try and see if the patches created by Robert Stupp alleviate your issue.
{quote}

Do you mean [bed7fa5|https://github.com/apache/cassandra/commit/bed7fa5ef8492d1ff3852cf299622a5ad4e0b621]?  I haven't applied that, no, but it doesn't look like I'm leaking anything {{BTree}} so I don't think that would help.;;;","13/Sep/17 17:05;snazy;[~urandom], it's not about the {{BTree.Builder}} instances, it's about what's kept referenced by those - and that is the bunch of {{HeapByteBuffer}} instances, as reported by [~markusdlugi] and fixed by the patch for this ticket. The screenshot you attached, matches the fixed issue. Therefore, I recommend to try the patch or a build from the recent 3.11/trunk branches and test again. Going go resolve this issue. If it's really something else that's causing the issue, I'd prefer to open another ticket, because there is already something committed for this ticket that addresses a very particular issue.;;;","01/Oct/17 11:48;tsteinmaurer;We have deployed Cassandra 3.11.1 (snapshot build from September 25, 2017) into our 9-node loadtest cluster. We still see increasing heap utilization over time with ~ 140 Recycler$Stack instances consuming ~1,8 GB heap. See attached screen (Eclipse memory analyzer screen): cassandra_3.11.1_Recycler_memleak.png;;;","02/Oct/17 20:27;tsteinmaurer;72hrs heap utilization increase with 3.11.1 snapshot build from Sept. 25, 2017 + cluster rolling restart marker => cassandra_3.11.1_snapshot_heaputilization.png;;;","02/Oct/17 21:44;snazy;I'll note that the fixed issue and what you're describing are probably two different things.
It might also be that the combination of recycling the btree-builders _and_ many cells in a partition. This is technically different from what's been fixed.
It would help a lot, if someone can come up with steps (ideally some code) to reproduce the issue as mat screenshots show that something happened but not why.;;;","03/Oct/17 08:09;tsteinmaurer;[~snazy]: Created CASSANDRA-13929 and discussed a potential fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race / ref leak in PendingRepairManager,CASSANDRA-13751,13093247,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,08/Aug/17 17:29,15/May/20 08:04,14/Jul/23 05:56,10/Aug/17 19:03,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"PendingRepairManager#getScanners has an assertion that confirms an sstable is, in fact, marked as pending repair. Since validation compactions don't use the same concurrency controls as proper compactions, they can race with promotion/demotion compactions and end up getting assertion errors when the pending repair id is changed while the scanners are being acquired. Also, error handling in PendingRepairManager and CompactionStrategyManager leaks refs when this happens.",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 19:03:45 UTC 2017,,,,,,,,,,"0|i3ijyn:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"08/Aug/17 17:38;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13751]
[utest|https://circleci.com/gh/bdeggleston/cassandra/76]
[dtest (pending)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/171/];;;","09/Aug/17 08:02;marcuse;+1 if tests succeed (seems the utests failed as well);;;","10/Aug/17 19:03;bdeggleston;Got the utest passing. dtests failures were flaky/succeeding locally. Committed as {{9c3354e3211c6a3f3982e87477e156c29cd9b7ea}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter digests include local data,CASSANDRA-13750,13093241,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,08/Aug/17 17:27,01/Aug/21 12:29,14/Jul/23 05:56,10/Aug/17 22:53,3.0.15,3.11.7,4.0,4.0-alpha1,,,,,,,,0,,,,,"In 3.x+, the raw counter value bytes are used when hashing counters for reads and repair, including local shard data, which is removed when streamed. This leads to constant digest mismatches and repair overstreaming.",,aleksey,bdeggleston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 22:53:53 UTC 2017,,,,,,,,,,"0|i3ijxb:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"08/Aug/17 20:53;bdeggleston;|[trunk|https://github.com/bdeggleston/cassandra/tree/13750-trunk]| [utest|https://circleci.com/gh/bdeggleston/cassandra/81] / [dtest (pending)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/172/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13750-3.11] | [utest|https://circleci.com/gh/bdeggleston/cassandra/80]|
|[3.0|https://github.com/bdeggleston/cassandra/tree/13750-3.0] | [utest|https://circleci.com/gh/bdeggleston/cassandra/79] |;;;","09/Aug/17 10:53;aleksey;If you have legacy pre-2.1 data shards lying around in sstables, this bug will hurt. One minor problem is that for people who don't, there will be a short period of digest mismatches during the minor upgrade from 3.0.prev to 3.0.next, but I don't see a way to avoid it.

+1;;;","10/Aug/17 22:53;bdeggleston;Committed as {{eb6f03c8928e913cb6f9eaa7c9ea9f4501039112}}

Opened/reviewed/committed CASSANDRA-13755 to fix only non-flaky test failure;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AssertionError in short read protection,CASSANDRA-13747,13092855,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,07/Aug/17 11:15,16/Apr/19 09:30,14/Jul/23 05:56,29/Aug/17 11:37,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,Correctness,,,,"{{ShortReadRowProtection.moreContents()}} expects that by the time we get to that method, the global post-reconciliation counter was already applied to the current partition. However, sometimes it won’t get applied, and the global counter continues counting with {{rowInCurrentPartition}} value not reset from previous partition, which in the most obvious case would trigger the assertion we are observing - {{assert !postReconciliationCounter.isDoneForPartition();}}. In other cases it’s possible because of this lack of reset to query a node for too few extra rows, causing unnecessary SRP data requests.

Why is the counter not always applied to the current partition?

The merged {{PartitionIterator}} returned from {{DataResolver.resolve()}} has two transformations applied to it, in the following order:
{{Filter}} - to purge non-live data from partitions, and to discard empty partitions altogether (except for Thrift)
{{Counter}}, to count and stop iteration

Problem is, {{Filter}} ’s {{applyToPartition()}} code that discards empty partitions ({{closeIfEmpty()}} method) would sometimes consume the iterator, triggering short read protection *before* {{Counter}} ’s {{applyToPartition()}} gets called and resets its {{rowInCurrentPartition}} sub-counter.

We should not be consuming iterators until all transformations are applied to them. For transformations it means that they cannot consume iterators unless they are the last transformation on the stack.

The linked branch fixes the problem by splitting {{Filter}} into two transformations. The original - {{Filter}} - that does filtering within partitions - and a separate {{EmptyPartitionsDiscarder}}, that discards empty partitions from {{PartitionIterators}}. Thus {{DataResolve.resolve()}}, when constructing its {{PartitionIterator}}, now does merge first, then applies {{Filter}}, then {{Counter}}, and only then, as its last (third) transformation - the {{EmptyPartitionsDiscarder}}. Being the last one applied, it’s legal for it to consume the iterator, and triggering {{moreContents()}} is now no longer a problem.

Fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13747-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13747-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13747-4.0]. dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13747].",,aleksey,bdeggleston,benedict,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,jkni,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 11:37:22 UTC 2017,,,,,,,,,,"0|i3ihjj:",9223372036854775807,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"07/Aug/17 23:42;aleksey;Stack trace for the triggered assertion:

{code}
java.lang.AssertionError
	at org.apache.cassandra.service.DataResolver$ShortReadProtection$ShortReadRowProtection.moreContents(DataResolver.java:449)
	at org.apache.cassandra.service.DataResolver$ShortReadProtection$ShortReadRowProtection.moreContents(DataResolver.java:412)
	at org.apache.cassandra.db.transform.BaseIterator.tryGetMoreContents(BaseIterator.java:111)
	at org.apache.cassandra.db.transform.BaseIterator.hasMoreContents(BaseIterator.java:101)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:155)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129)
	at org.apache.cassandra.db.transform.FilteredRows.isEmpty(FilteredRows.java:50)
	at org.apache.cassandra.db.transform.Filter.closeIfEmpty(Filter.java:73)
	at org.apache.cassandra.db.transform.Filter.applyToPartition(Filter.java:43)
	at org.apache.cassandra.db.transform.Filter.applyToPartition(Filter.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.service.StorageProxy$SingleRangeResponse.computeNext(StorageProxy.java:2200)
	at org.apache.cassandra.service.StorageProxy$SingleRangeResponse.computeNext(StorageProxy.java:2172)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)
	at org.apache.cassandra.service.StorageProxy$RangeCommandIterator.computeNext(StorageProxy.java:2243)
	at org.apache.cassandra.service.StorageProxy$RangeCommandIterator.computeNext(StorageProxy.java:2210)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:707)
{code};;;","17/Aug/17 08:21;benedict;The patch looks good, but while reviewing I got a little suspicious of the modified line {{DataResolver:479}}, as it seemed that {{n}} and {{x}} were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

-Assuming, now correctly defined, that {{n <= x}}, this also obviates the need for the {{Math.max(x, 1)}} you have introduced.  This must be true, given that we can only have a short-read triggered in the case that we have yielded too few rows, so we must have fewer than we requested (even if other rows we didn't know about were introduced by other peers).-

This is _probably_ a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.  

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value {{>= 0}} before setting to 1.;;;","24/Aug/17 12:38;aleksey;Thanks for the review.

I've decided to tackle the related issue you in a [separate ticket|https://issues.apache.org/jira/browse/CASSANDRA-13794].

Will commit this once tests are happy.;;;","29/Aug/17 11:37;aleksey;Didn’t get a clean CI run, but these are the issues flagged:

On 3.0 and 3.11, {{RemoveTest}} failed due to a port being used by another process. Passed locally, with and without compression.
On 3.0, {{ClientWarningsTest}} timed out. Passed locally.

5 dtest failures in 3.0 with unrelated failure reasons. Unfortunately currently unable to get a clean run on ASF Jenkins.

Committed to 3.0 as [a7cb009f8a3f4d0e0293111bfcfff3d404a37a89|https://github.com/apache/cassandra/commit/a7cb009f8a3f4d0e0293111bfcfff3d404a37a89] and merged with 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better bootstrap failure message when blocked by (potential) range movement,CASSANDRA-13744,13092482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mck,mck,mck,04/Aug/17 13:41,15/May/20 08:01,14/Jul/23 05:56,16/Aug/17 03:11,3.11.1,4.0,4.0-alpha1,,,,,,,,,0,,,,,"The UnsupportedOperationException thrown from {{StorageService.joinTokenRing(..)}} when it's detected that other nodes are bootstrapping|leaving|moving offers no information as to which are those other nodes.

In a large cluster this might not be obvious nor easy to discover, gossipinfo can hold information that takes a bit of effort to uncover. Even when it is easily seen it's helpful to have it confirmed.

Attached is the patch that provides a more thorough exception message to the failed bootstrap attempt.

",,jasobrown,jjirsa,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 16 03:12:19 UTC 2017,,,,,,,,,,"0|i3ifav:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"04/Aug/17 16:43;jjirsa;Patch looks good to me, but before we commit, can you kick off a unit test run with circleci or jenkins, and a dtest run using the ASF jenkins? Just to be sure we don't 
 break any existing tests. 

;;;","05/Aug/17 02:53;mck;ofc [~jjirsa] (have also been looking if there's any tests around this code…);;;","05/Aug/17 04:21;mck;|| branch || testall || dtest ||
| [cassandra-3.11_13744|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/163] |
| [trunk_13744|https://github.com/thelastpickle/cassandra/tree/mck/trunk_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/164] |
;;;","05/Aug/17 13:39;jasobrown;+1. tests looks good except for the 3.11 dtest that self-imploded through no fault of this patch.;;;","07/Aug/17 04:26;mck;…and the trunk dtest was aborted. will add CHANGES.txt and restart both dtest.;;;","07/Aug/17 04:37;mck;Updated CHANGES.txt and triggered dtest rebuild…

|| branch || testall || dtest ||
| [cassandra-3.11_13744|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/165] |
| [trunk_13744|https://github.com/thelastpickle/cassandra/tree/mck/trunk_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/166] |
;;;","07/Aug/17 04:39;mck;[~jjirsa], [~jasobrown], can either of you add yourself to the ""Reviewer"" field so I know who to refer to as reviewer in the commit msg, please.

Dtests are running again.;;;","11/Aug/17 00:59;jjirsa;I'll claim it, not only because I'm first, but because I'm not sure if Jason's +1 carries through on the new tests.
;;;","16/Aug/17 03:12;mck;thanks [~jjirsa], committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAPTURE not easilly usable with PAGING,CASSANDRA-13743,13092403,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,iksaif,iksaif,iksaif,04/Aug/17 08:23,15/May/20 08:06,14/Jul/23 05:56,11/Aug/17 01:03,4.0,4.0-alpha1,,,,,Legacy/Tools,,,,,0,,,,,See https://github.com/iksaif/cassandra/commit/7ed56966a7150ced44c375af307685517d7e09a3 for a patch fixing that.,,cnlwsu,iksaif,jjirsa,rgerard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,iksaif,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 07:19:13 UTC 2017,,,,,,,,,,"0|i3ietj:",9223372036854775807,,,,,,,cnlwsu,,cnlwsu,,,Normal,,,,,,,,,,,,,,,,,,,"04/Aug/17 19:18;cnlwsu;+1, if you make a pull request against trunk a bot will pick it up and update link here.;;;","11/Aug/17 01:03;jjirsa;Thanks guys, committed as {{ed0243954f9ab9c5c68a4516a836ab3710891d5b}}

;;;","17/Aug/17 09:49;rgerard;As a side note, I was reading the commit logs and found that the commit message and changelog badly reference this ticket.
In both, CASSANDRA-13473 is used but this ticket is CASSANDRA-13743
https://github.com/apache/cassandra/commit/ed0243954f9ab9c5c68a4516a836ab3710891d5b;;;","17/Aug/17 19:02;jjirsa;Thanks [~rgerard] - I've updated the [CHANGES log|https://github.com/apache/cassandra/commit/c0dc77ed4fa3b16558ce6f92c4ff076b890afc49] appropriately (but I'm not going to back out the commit to fix it there).

- Jeff;;;","28/Aug/17 07:19;iksaif;Thanks for merging it and fixing it :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Orphan hint file gets created while node is being removed from cluster,CASSANDRA-13740,13092315,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,03/Aug/17 22:04,16/Mar/22 11:08,14/Jul/23 05:56,30/Apr/18 18:06,3.0.17,3.11.3,4.0,4.0-alpha1,,,Consistency/Hints,Legacy/Core,,,,0,,,,,"I have found this new issue during my test, whenever node is being removed then hint file for that node gets written and stays inside the hint directory forever. I debugged the code and found that it is due to the race condition between [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195] and [HintsWriteExecutor.java::closeWriter | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L106]
. 
 
*Time t1* Node is down, as a result Hints are being written by [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195]
*Time t2* Node is removed from cluster as a result it calls [HintsService.java-exciseStore | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L327] which removes hint files for the node being removed
*Time t3* Mutation stage keeps pumping Hints through [HintService.java::write | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L145] which again calls [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215] and new orphan file gets created

I was writing a new dtest for {CASSANDRA-13562, CASSANDRA-13308} and that helped me reproduce this new bug. I will submit patch for this new dtest later.

I also tried following to check how this orphan hint file responds:
1. I tried {{nodetool truncatehints <node>}} but it fails as node is no longer part of the ring
2. I then tried {{nodetool truncatehints}}, that still doesn’t remove hint file because it is not yet included in the [dispatchDequeue | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsStore.java#L53]


Reproducible steps:
Please find dTest python file {{gossip_hang_test.py}} attached which reproduces this bug.

Solution:
This is due to race condition as mentioned above. Since {{HintsWriteExecutor.java}} creates thread pool with only 1 worker, so solution becomes little simple. Whenever we [HintService.java::excise | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L303] a host, just store it in-memory, and check for already evicted host inside [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215]. If already evicted host is found then ignore hints.

Jaydeep",,aleksey,chovatia.jaydeep@gmail.com,githubbot,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #136:
URL: https://github.com/apache/cassandra/pull/136


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:08;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/17 22:01;chovatia.jaydeep@gmail.com;13740-3.0.15.txt;https://issues.apache.org/jira/secure/attachment/12884139/13740-3.0.15.txt","03/Aug/17 22:03;chovatia.jaydeep@gmail.com;gossip_hang_test.py;https://issues.apache.org/jira/secure/attachment/12880316/gossip_hang_test.py",,,,,,,,,,,,,,,,,,2.0,chovatia.jaydeep@gmail.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 18:08:50 UTC 2018,,,,,,,,,,"0|i3ie9z:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"03/Aug/17 22:37;chovatia.jaydeep@gmail.com;Please find patch with this fix attached.;;;","04/Aug/17 19:30;jay.zhuang;Hi [~chovatia.jaydeep@gmail.com] nice catch. One comment about your patch, I don't think it's a good idea to have {{static final Set<UUID> evictedHostIds}} set in {{HintsStore}}, as the HintsStore instance is for one hints file. How about adding a field {{private boolean isEvicted = false;}}? To indicate if the target host is evicted or not.;;;","04/Aug/17 22:11;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] for review comments.

{{HintsStore}} object may get created multiple times for same {{hostId}} as following:

*time t1* removenode thread calls [catalog.exciseStore | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L327 ]
and it removes from [Map<UUID, HintsStore> stores | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsCatalog.java#L41]
*time t2* HintWriter thread calls [HintsStore::get | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsCatalog.java#L85] which creates a new {{HintStore}} object and will write hints again.

For this bug specifically above mentioned scenario is been happening, please let me know your comments.

I think we should atleast move {{static final Set<UUID> evictedHostIds}} from {{HintStore.java}} to {{HintService.java}} which is a singleton?;;;","05/Aug/17 00:13;chovatia.jaydeep@gmail.com;Hi [~jay.zhuang] Please find patch file ""13740-2_3.0.15.txt"" attached with your review comments incorporated.;;;","08/Aug/17 22:10;githubbot;GitHub user jaydeepkumar1984 opened a pull request:

    https://github.com/apache/cassandra/pull/136

    CASSANDRA-13740 orphan hint file get created

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jaydeepkumar1984/cassandra 13740-3.0

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/136.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #136
    
----
commit 993d5891e69f5cda402ae2158e06914f653a644d
Author: Jaydeepkumar Chovatia <chovatia.jaydeep@gmail.com>
Date:   2017-08-03T22:34:26Z

    CASSANDRA-13740 orphan hint file get created

----
;;;","08/Aug/17 22:19;chovatia.jaydeep@gmail.com;Please find pull request against Cassandra-3.0 here: https://github.com/apache/cassandra/pull/136/commits/993d5891e69f5cda402ae2158e06914f653a644d;;;","09/Aug/17 12:24;aleksey;The patch likely works, but I think we can do better.

Some of the issues I have with it:
1. It introduces a dependency on {{HintsService}} and {{StorageService}} to {{HintsWriteExecutor}}
2. It introduces a dependency on {{HintsService}} to {{HintsStore}}

When designing the current iteration of hints I was very careful to design the system in a top-down way without any interleaving that’s avoidable. Each class is a dumb as possible on its own, and as you go up, you just compose dumb classes that by themselves know nothing of layers above them.

As for the problem itself, we do acknowledge that “The worst that can happen if we don't get everything right is a hints file (or two) remaining undeleted.” - comments to {{excise()}}, it’s more of a known limitation than a bug. But of course we can improve on it. What is a problem, however, is the inability to programmatically remove those orphan files via JMX. {{nodetool truncatehints}} should get results no matter what, and it should be fixed.

If we want to deal with the orphans for sure - and I don’t see why not improve this as well - I suggest you do so in a different way. Perhaps as last step of {{excise()}} schedule an task - on {{ScheduledExecutors.optionalTasks}} to clean up any orphans, if any, after some delay.;;;","09/Aug/17 21:56;chovatia.jaydeep@gmail.com;Thanks [~iamaleksey] for the code review. I will change it as per your suggestion and will provide updated patch.;;;","11/Aug/17 23:17;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

I have modified code as per your review comments, please find it attached ""13740-3.0.15.txt""
Also please find same patch here: https://github.com/jaydeepkumar1984/cassandra/commit/173fce0362246595d26b24196d6690223d132d5e

I will create patch for 3.11 as well as will run {{circleci}} after receiving your review comments.

Jaydeep;;;","21/Aug/17 21:01;aleksey;Hey. There are a few [code style|https://wiki.apache.org/cassandra/CodeStyle] issues: we don't use {{final}} for arguments and local variables, brackets go to new lines always. And the patch doesn't wait for {{closeWriter}} future to be completed.

And a more interesting issue is that of the delay. {{RING_DELAY}} doesn't have anything to do with hints. What does is write timeouts, and {{MessagingService}} 's timeout reporter the callbacks expiring map firing - that's where the race ultimately is.

Also, we aren't fixing the issue of {{nodetool truncatehints}} not being able to clean up after we excise.

The more I think about it, the more I'm inclined to just correct that last issue and leave everything else be as is (and also commit your unit tests, thanks for those).;;;","24/Aug/17 16:29;chovatia.jaydeep@gmail.com;Thanks for the review comments, I will change it and send updated one soon.;;;","28/Aug/17 22:00;chovatia.jaydeep@gmail.com;Hi [~tuxslayer]

Isn’t the race condition is as following:

*Thread T1 - Time 1:* Removes {{HintStore}} by calling [HintsService.instance.excise |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2268] but
at this time node has not yet been removed from [tokenMetadata  |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L133]

*Thread T2 - Time 2:* Mutation stage does a lookup to [tokenMetadata  | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageProxy.java#L2617] and finds node as valid hence it dumps hint for it.

*Thread T1 - Time 3:* Now removes node from [tokenMetadata  | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2270] 

Hence I decided it is safer to sleep for {{StorageService.RING_DELAY}} and then schedule optional {{removeOrphanHintFiles}}  task. Please let me know your comments.

I have also incorporated your review comment, please find updated patch attached as well as here: https://github.com/jaydeepkumar1984/cassandra/commit/16d4ab3316ab71a9a3b96ab67944384092be40ca

Jaydeep;;;","11/Sep/17 21:36;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

Can you please review my latest patch?

Jaydeep;;;","13/Sep/17 16:57;aleksey;A bit busy currently, sorry. Will have a look as soon as I can.;;;","06/Mar/18 05:47;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

 A gentle ping. If you have some time then can you please help me close this?

 Jaydeep;;;","15/Mar/18 19:19;aleksey;Sure. So what the current patch is doing is it does excise, and then, in essence, schedules another excise in {{RING_DELAY}}?

In other words, we simply don't trust the immediate excise, and rely on the follow-up one. In that case, to make sure that hints for writes that will time out at some later point get deleted from disk, shouldn't we just say ""alright, let's just delay the first excise, instead of doing the immediate one and then another one""?;;;","15/Mar/18 19:26;aleksey;And the changes to {{deleteAllHints()}} I don't fully understand. I don't think it's the responsibility of the method to close any writers. The contract is (as I understand it) - please remove all written hints files at this point. One problem is that for catalogues that aren't loaded, if some files are remaining, they won't be deleted - but this change doesn't address it. More importantly, I think that with excise fixed, that would be less of a problem and probably not needed to fix..

So let's leave {{deleteAllHints()}} alone. And also leave excise more or less alone, but call it instead at the end of {{StorageProxy.excise()}}, with a delay. Not sure why you picked {{RING_DELAY}} for the delay though.. can you explain? ;;;","16/Mar/18 17:40;chovatia.jaydeep@gmail.com;Thanks [~iamaleksey] for the review.

Reason behind {{RING_DELAY}} is as following, in this fix one thing is clear that we have to delay {{StorageProxy.excise()}} which means we have to put some sleep. So we have two options to put sleep:
 1. Hardcode some random value say for example delay {{StorageProxy.excise()}} for 10 seconds
 OR
 2. Other nodes in the ring will no longer accept writes once they learn that given node is no longer part of the ring. Hence I have used {{RING_DELAY}} which is general delay used at [many places|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/Gossiper.java#L553] and after this delay we can assume ring has stabilized. So my theory is that once ring has stabilized then everyone in the ring would have learnt about node that just left and at this time it is safe to do {{StorageProxy.excise(). }}Please let me know if my understanding is not correct, I can change it to some hardcoded value say 20 seconds.

I will incorporate other code review comments and will send you updated patch soon.;;;","19/Mar/18 22:57;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

Please find updated patch here:
||trunk||3.0||
|[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:13740-trunk?expand=1]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jaydeepkumar1984:CASSANDRA-13740_1?expand=1]|
|[utest|https://circleci.com/gh/jaydeepkumar1984/cassandra/46]|[utest|https://circleci.com/gh/jaydeepkumar1984/cassandra/44]|

Jaydeep;;;","30/Apr/18 18:05;aleksey;Made some changes and committed to 3.0 as [b2f6ce961f38a3e4cd744e102026bf7a471056c9|https://github.com/apache/cassandra/commit/b2f6ce961f38a3e4cd744e102026bf7a471056c9] and merged upwards.

Changes made:
- fixed {{excise()}} to properly handle non-existing stores instead of re-initializing them
- changed the delay to be min rpc timeout + write rpc timeout, which roughly the time you may expect a new hint written after node decom

Thank you for you patience and for the added tests.;;;","30/Apr/18 18:08;chovatia.jaydeep@gmail.com;Thank You!!! [~iamaleksey];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra can't start because of unknown type <usertype> exception,CASSANDRA-13739,13092181,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,Andrey.Khashchin,Andrey.Khashchin,03/Aug/17 12:27,16/Apr/19 09:30,14/Jul/23 05:56,05/Aug/17 15:26,3.7,,,,,,Legacy/Core,Legacy/CQL,,,,0,,,,,"OS: CentOS Linux release 7.2.1511 (Core)
Kernel: 3.10.0-327.36.1.el7.x86_64
Cassandra: v 3.7

Issue:

Cassandra is not able to start after restart of the node with following error in system.log:

ERROR [main] 2017-08-03 12:10:28,633 CassandraDaemon.java:731 - Exception encountered during startup
org.apache.cassandra.exceptions.InvalidRequestException: Unknown type <name of user type>
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawUT.prepare(CQL3Type.java:751) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepare(CQL3Type.java:667) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepareInternal(CQL3Type.java:644) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.lambda$prepare$34(Types.java:313) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_102]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_102]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_102]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_102]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.prepare(Types.java:314) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder.build(Types.java:263) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchTypes(SchemaKeyspace.java:920) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:891) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:869) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:857) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:136) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:126) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:585) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714) [apache-cassandra-3.7.0.jar:3.7.0]

Thank you for your help!",Development,aleksey,Andrey.Khashchin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 05 15:26:50 UTC 2017,,,,,,,,,,"0|i3idgf:",9223372036854775807,3.7,,,,,,,,,,,Normal,,3.7,,,,,,,,,,,,,,,,,"03/Aug/17 13:51;aleksey;Seems like you have dropped a user type that some other types still depend on. It shouldn't happen normally because of validations, but they aren't perfect.

Have you dropped a type recently?

If not, then it might be a corruption issue, and you might have to copy over schema sstables from another node to the failing one.;;;","03/Aug/17 14:47;Andrey.Khashchin;Aleksey, we really have heavy dependencies between several usertypes and tables but as I know there were no direct DDL actions against this particular usertype mentioned in the system.log
At the same time, there was VM redeployment and this fact may lead to some data corruption, indeed.

Do we need to copy sstables related to the custom keyspace (which contains original usertype mentioned in the log) or do we need to copy some system keyspaces (like system_schema) maybe? Unfortunately, we have one node configuration so there are no other nodes in the cluster but there are some similar standalone installations (QA) of Cassandra on other hosts;;;","03/Aug/17 14:53;aleksey;You'd need to copy over system_schema.types sstables. Just make sure the data you are copying over is a superset, so that replacing your node's data wouldn't cause a loss of anything.;;;","03/Aug/17 16:37;Andrey.Khashchin;We copied system_schema.types sstables from another one machine to the failing one, original error disappeared but we have new issue, very similar to previous but related to another one usertype in different custom keyspace  ;;;","05/Aug/17 15:26;Andrey.Khashchin;Original issue was resolved by deleting extra sstables in system_schema.type folder which were generated as result of VM redeployment. Cassandra is able to continue to boot now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load is over calculated after each IndexSummaryRedistribution,CASSANDRA-13738,13092049,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,03/Aug/17 01:23,07/Mar/23 11:52,14/Jul/23 05:56,05/Sep/17 08:50,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,Legacy/Core,,,,,0,,,,," For example, here is one of our cluster with about 500GB per node, but {{nodetool status}} shows far more load than it actually is and keeps increasing, restarting the process will reset the load, but keeps increasing afterwards:
{noformat}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  IP1*****       13.52 TB   256          100.0%            c4c31e0a-3f01-49f7-8a22-33043737975d  rac1
UN  IP2*****       14.25 TB   256          100.0%            efec4980-ec9e-4424-8a21-ce7ddaf80aa0  rac1
UN  IP3*****       13.52 TB   256          100.0%            7dbcfdfc-9c07-4b1a-a4b9-970b715ebed8  rac1
UN  IP4*****       22.13 TB   256          100.0%            8879e6c4-93e3-4cc5-b957-f999c6b9b563  rac1
UN  IP5*****       18.02 TB   256          100.0%            4a1eaf22-4a83-4736-9e1c-12f898d685fa  rac1
UN  IP6*****       11.68 TB   256          100.0%            d633c591-28af-42cc-bc5e-47d1c8bcf50f  rac1
{noformat}

!sizeIssue.png|test!

The root cause is if the SSTable index summary is redistributed (typically executes hourly), the updated SSTable size is added again.",,aleksey,jay.zhuang,jjirsa,KurtG,marcuse,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12964,,,,,,,,,,,,,,,,,,,,,"03/Aug/17 01:28;jay.zhuang;sizeIssue.png;https://issues.apache.org/jira/secure/attachment/12880147/sizeIssue.png",,,,,,,,,,,,,,,,,,,1.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Sep 05 08:47:53 UTC 2017,,,,,,,,,,"0|i3icnj:",9223372036854775807,2.2.x,3.0.x,3.11.0,5.0,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"03/Aug/17 01:53;jjirsa;Are you aware yet if this is a new regression? If so, when was it introduced?
;;;","03/Aug/17 03:38;KurtG;This has been around for a long time - haven't had the opportunity to find out what the exact cause was but this makes sense. I've definitely seen it in 3.7. Pretty sure I've also seen it in 3.0 and 2.1 as well. I don't think it happens in all versions, or at least for some reason it doesn't happen on all clusters.;;;","03/Aug/17 05:56;jay.zhuang;I think the problem has been there for awhile. It only happens when [the IndexSummary is rebuilt|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummaryRedistribution.java#L129], which is triggered by [large read traffic load changing|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummaryRedistribution.java#L202]. I'm able to reproduce it with an unittest. Here is the patch, please review:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [circleci#56|https://circleci.com/gh/cooldoger/cassandra/56] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [circleci#54|https://circleci.com/gh/cooldoger/cassandra/54] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [circleci#51|https://circleci.com/gh/cooldoger/cassandra/51] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/trunk] | [circleci#57|https://circleci.com/gh/cooldoger/cassandra/57] |

Seems branch {{2.1}} don't have this issue.;;;","04/Aug/17 00:31;jay.zhuang;I deployed the change to the cluster which has the problem. Confirmed the issue has been fixed. It has been running for more than 6 hours, so far looks fine.
[~jjirsa] do you mind reviewing the patch?;;;","04/Aug/17 01:40;jjirsa;I'll take review but I'm at least two weeks away from getting to it

If anyone else beats me to it I won't mind
;;;","06/Aug/17 18:57;jay.zhuang;Updated unittest to make it stable:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/13738-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk] |
;;;","27/Aug/17 20:26;jjirsa;[~iamaleksey] - are you willing to take review on this?

;;;","29/Aug/17 12:16;aleksey;[~jjirsa] Not my strongest area of the codebase. Maybe [~krummas] has some spare cycles?;;;","29/Aug/17 12:18;marcuse;sure;;;","30/Aug/17 08:03;marcuse;lgtm

running dtests just to be sure:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/237/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/238/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/239/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/240/;;;","30/Aug/17 15:29;aleksey;[~jay.zhuang] You aren't running all the unit tests, FYI - because there is no way to get a green run currently. You have parallelism set to 1 instead of 4, which skips long-test, test-compression, and stress-test. Should set it to 4 and rerun.;;;","30/Aug/17 17:12;jay.zhuang;[~iamaleksey] Thanks for the reminder, updated setting and rerunning the tests.;;;","01/Sep/17 03:36;jay.zhuang;Yeah, all the builds are failing after the parallelism is set to 4 :(, rebased the code and updated the unittest:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/13738-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk] |
;;;","01/Sep/17 16:16;jay.zhuang;2.2 branch uTest fail for {{ant eclipse-warnings}}, but I'm unable to reproduce it locally:
{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] incorrect classpath: /home/ubuntu/cassandra/build/cobertura/classes
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/db/compaction/CompactionManager.java (at line 853)
     [java] 	ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());
     [java] 	                ^^^^^^^
     [java] Resource 'scanner' should be managed by try-with-resource
     [java] ----------
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java (at line 257)
     [java] 	scanners.add(new LeveledScanner(intersecting, range));
     [java] 	             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: '<unassigned Closeable value>' may not be closed
     [java] ----------
     [java] ----------
     [java] 3. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/tools/SSTableExport.java (at line 315)
     [java] 	ISSTableScanner scanner = reader.getScanner();
     [java] 	                ^^^^^^^
     [java] Resource 'scanner' should be managed by try-with-resource
     [java] ----------
     [java] 3 problems (3 errors)
{noformat}
And for the other test failures, I don't think they're introduced by this patch.;;;","05/Sep/17 08:47;marcuse;failing dtests pass locally

committed as {{4e834c53ca57910e8c4}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node start can fail if the base table of a materialized view is not found,CASSANDRA-13737,13091644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,01/Aug/17 18:37,15/May/20 08:04,14/Jul/23 05:56,08/Aug/17 09:32,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Materialized Views,Legacy/Distributed Metadata,,,,0,,,,,"Node start can fail if the base table of a materialized view is not found, which is something that can happen under certain circumstances. There is a dtest reproducing the problem:
{code}
cluster = self.cluster
cluster.populate(3)
cluster.start()
node1, node2, node3 = self.cluster.nodelist()
session = self.patient_cql_connection(node1, consistency_level=ConsistencyLevel.QUORUM)
create_ks(session, 'ks', 3)

session.execute('CREATE TABLE users (username varchar PRIMARY KEY, state varchar)')

node3.stop(wait_other_notice=True)

# create a materialized view only in nodes 1 and 2
session.execute(('CREATE MATERIALIZED VIEW users_by_state AS '
                 'SELECT * FROM users WHERE state IS NOT NULL AND username IS NOT NULL '
                 'PRIMARY KEY (state, username)'))

node1.stop(wait_other_notice=True)
node2.stop(wait_other_notice=True)

# drop the base table only in node 3
node3.start(wait_for_binary_proto=True)
session = self.patient_cql_connection(node3, consistency_level=ConsistencyLevel.QUORUM)
session.execute('DROP TABLE ks.users')

cluster.stop()
cluster.start()  # Fails
{code}
This is the error during node start:
{code}
java.lang.IllegalArgumentException: Unknown CF 958ebc30-76e4-11e7-869a-9d8367a71c76
	at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:215) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.addView(ViewManager.java:143) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.reload(ViewManager.java:113) ~[main/:na]
	at org.apache.cassandra.schema.Schema.alterKeyspace(Schema.java:618) ~[main/:na]
	at org.apache.cassandra.schema.Schema.lambda$merge$18(Schema.java:591) ~[main/:na]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.lambda$entryConsumer$0(Collections.java:1575) ~[na:1.8.0_131]
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1043) ~[na:1.8.0_131]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.forEach(Collections.java:1580) ~[na:1.8.0_131]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:591) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationTask$1.response(MigrationTask.java:89) ~[main/:na]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[main/:na]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [main/:na]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
{code}",,adelapena,githubbot,jasonstack,jay.zhuang,jjirsa,KurtG,sbtourist,szhou,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 09:31:44 UTC 2017,,,,,,,,,,"0|i3ia5r:",9223372036854775807,,,,,,,tjake,,tjake,,,Normal,,,,,,,,,,,,,,,,,,,"01/Aug/17 21:47;szhou;We had the same issue on 3.0.14 couple of days ago. Looks like somehow the MV data was corrupted and restart of any data would be stuck. Even ""drop MV"" from cqlsh doesn't work (on a different node, before restart) because the base table doesn't exist.;;;","02/Aug/17 09:58;adelapena;Here is a very simple patch that simply ignores (and warns about) the internal addition of materialized views whose base table is unknown:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13737-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13737-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13737-trunk]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13737]
;;;","07/Aug/17 19:02;tjake;+1 assuming clean CI;;;","07/Aug/17 19:44;githubbot;GitHub user adelapena opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/4

    Add dtest for CASSANDRA-13737

    Add test verifying that a schema propagation adding a view over a non existing table doesn't prevent a node from start

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/adelapena/cassandra-dtest 13737

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/4.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4
    
----
commit 250239bf8f3da3e72f3507626a62c21ee51837e9
Author: Andrés de la Peña <a.penya.garcia@gmail.com>
Date:   2017-08-02T08:57:49Z

    Add test verifying that a schema propagation adding a view over a non existing table doesn't prevent a node from start

----
;;;","07/Aug/17 19:45;adelapena;Committed to 3.0 as [918667929f87a2e8e74913fe6d6e5dd137fe4e4f|https://github.com/apache/cassandra/commit/918667929f87a2e8e74913fe6d6e5dd137fe4e4f] and merged to 3.11 and trunk.

Created [this PR|https://github.com/apache/cassandra-dtest/pull/4] for the dtest.
;;;","08/Aug/17 09:31;githubbot;Github user adelapena commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/4
  
    Committed as [959208749d70e5808aec144e87b73e90d56a7f91](https://github.com/apache/cassandra-dtest/commit/959208749d70e5808aec144e87b73e90d56a7f91)
;;;","08/Aug/17 09:31;githubbot;Github user adelapena closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/4
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GH PR #131 - Refactoring to primitive functional interfaces in AuthCache.java,CASSANDRA-13732,13090332,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,26/Jul/17 21:14,15/May/20 08:04,14/Jul/23 05:56,28/Jul/17 00:03,4.0,4.0-alpha1,,,,,Feature/Authorization,,,,,0,,,,,"https://github.com/apache/cassandra/pull/131

Refactor to avoid unnecessary boxing/unboxing in auth cache.
",,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/cassandra/pull/131,https://github.com/apache/cassandra/pull/131,,,,,,,,,,9223372036854775807,,,Fri Jul 28 00:03:38 UTC 2017,,,,,,,,,,"0|i3i23z:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"27/Jul/17 00:14;jjirsa;Assigning to myself, actual author is {{Ameya Ketkar}}

Pushed to CI here: 
Unit tests: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13732
Dtests : https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/149/ (I think, but ASF Jenkins is unhappy right now)
;;;","28/Jul/17 00:03;jjirsa;lgtm, committed as {{a5dff2f79621d7527a3837c0028d2e8b61d16e42}}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropping a table doesn't drop its dropped columns,CASSANDRA-13730,13089965,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasonstack,duarten,duarten,25/Jul/17 18:05,16/Apr/19 09:30,14/Jul/23 05:56,03/Aug/17 13:43,3.0.15,3.11.1,,,,,Legacy/Distributed Metadata,,,,,0,,,,,"I'm not sure if this is intended or not, but currently a table's dropped columns are not dropped when the table itself is dropped:

{noformat}
cqlsh> create keyspace ks WITH replication={ 'class' : 'SimpleStrategy', 'replication_factor' : 1 } ;
cqlsh> use ks;
cqlsh:ks> create table  test (pk text primary key, c1 int);
cqlsh:ks> alter table test drop c1;
cqlsh:ks> drop table test;
cqlsh:ks> select * from system_schema.dropped_columns where keyspace_name = 'ks' and table_name = 'test';

 keyspace_name | table_name | column_name | dropped_time                    | kind    | type
---------------+------------+-------------+---------------------------------+---------+------
            ks |       test |          c1 | 2017-07-25 17:53:47.651000+0000 | regular |  int

(1 rows)
{noformat}

This can have surprising consequences when creating another table with the same name.",,aleksey,jasonstack,jay.zhuang,jeromatron,jjirsa,stoneFang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 13:30:45 UTC 2017,,,,,,,,,,"0|i3hzun:",9223372036854775807,3.11.0,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"25/Jul/17 20:11;aleksey;It's most certainly unintended, and I can confirm that the necessary code is missing from {{SchemaKeyspace.makeDropTableMutation()}} method.;;;","25/Jul/17 20:17;aleksey;It's also really old, going all the way to 2.0. Should fix in 2.1 and upwards.;;;","25/Jul/17 20:30;duarten;Weren't the dropped columns just a map before 3.0?;;;","25/Jul/17 20:35;aleksey;bq. Weren't the dropped columns just a map before 3.0?

Totally was. My memory is apparently garbage. Thanks.;;;","26/Jul/17 00:54;aleksey;The reason nobody noticed before is that it shouldn't affect you, normally. Assuming the clock working even remotely correctly, timestamps of new columns in the new table will all be higher than dropped column times in the deleted table.

You'd be affected if you set timestamps manually to values < time of new table creation.

You'll also be affected by being unable to re-add certain columns to schema - there are some validations in place forbidding you to re-add columns with types incompatible with dropped columns' types (for collections in particular).

It's still a bug though and should be fixed, but is a minor one.

Unassigning myself as it's less impactful as I initially assumed. If nobody else picks it up, I will eventually fix it, once I don't have more important things on my plate.

Feel free to take over. And thanks for spotting it in the first place (:;;;","30/Jul/17 03:15;jasonstack;This might affect backup/restore.. 

1. backup
2. drop column
3. drop table
4. restore schema
5. restore sstable
6. data missing for dropped column;;;","30/Jul/17 08:43;stoneFang;[~jasonstack]
why restore process need to  check system_schema.dropped_columns?
should put system_schema.dropped_table first;;;","31/Jul/17 02:44;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-trunk] | [unit|https://circleci.com/gh/jasonstack/cassandra/295] | irrelevant
 compaction_test.TestCompaction_with_LeveledCompactionStrategy.fanout_size_test
 materialized_views_test.TestMaterializedViews.view_tombstone_test (known)
 bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test(known) |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-3.11] | [unit|https://circleci.com/gh/jasonstack/cassandra/296] | passed dtest |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-3.0] | [unit|https://circleci.com/gh/jasonstack/cassandra/287] | irrelevant
 offline_tools_test.TestOfflineTools.sstableofflinerelevel_test
auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [dtest|https://github.com/riptano/cassandra-dtest/commits/CASSANDRA-13730] | 

clear corresponding dropped_columns entries when table is dropped.;;;","02/Aug/17 17:09;aleksey;[~jasonstack] Looking good, only have one small nit: {{columnName.toString()}} call is redundant, as {{columnName}} is already a string. Should remove that call. Also, my (subjective) preference would be to pass {{DroppedColumn}} and not {{String}} as {{columnName}} to {{dropDroppedColumnFromSchemaMutation()}} method, for consistency with other similar methods in the class.

Unit test and dtest look good to me.;;;","03/Aug/17 04:47;jasonstack;thanks for reviewing. updated 3.0/3.1 branch.;;;","03/Aug/17 10:10;aleksey;Thanks! LGTM, will commit soonish.;;;","03/Aug/17 13:30;aleksey;Committed to 3.0 as [d9eabd3d0cbf1287aa7d01bc23dd8e39c3acf232|https://github.com/apache/cassandra/commit/d9eabd3d0cbf1287aa7d01bc23dd8e39c3acf232] and merged into 3.11 and trunk, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrap streaming failed suspect due to secondary index,CASSANDRA-13726,13089549,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,wavelet,wavelet,24/Jul/17 14:35,16/Apr/19 09:30,14/Jul/23 05:56,25/Jul/17 08:35,,,,,,,,,,,,0,,,,,"Hi,

We have a Cassandra DC with 31 nodes,but it's always failed when adding the 32nd node.
The following is the error log.And we suspect it's due to the secondary index,but it never happens before.Could you please advise?

Thanks
{code}
INFO  [CompactionExecutor:3] 2017-07-24 22:07:40,774 ColumnFamilyStore.java:917 - Enqueuing flush of compactions_in_progress: 149 (0%) on-heap, 20 (0%) off-heap
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:40,775 Memtable.java:347 - Writing Memtable-compactions_in_progress@581635159(0.008KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:40,775 Memtable.java:382 - Completed flushing /data/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-tmp-ka-1159-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=11929032)
INFO  [CompactionExecutor:3] 2017-07-24 22:07:40,796 CompactionTask.java:274 - Compacted 15 sstables to [/data/cassandra/data/ecommercedata/ecommerce_baitiao_bill-c1a7f8d0660511e781e2077cc592d79d/ecommercedata-ecommerce_baitiao_bill-ka-40,].  293,793 bytes to 288,251 (~98% of original) in 108ms = 2.545348MB/s.  1,024 total partitions merged to 1,016.  Partition merge counts were {1:1008, 2:8, }
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,351 ColumnFamilyStore.java:917 - Enqueuing flush of ecommerce_baitiao_record.baitiao_state: 2228978 (0%) on-heap, 6183142 (0%) off-heap
INFO  [CompactionExecutor:3] 2017-07-24 22:07:43,352 ColumnFamilyStore.java:917 - Enqueuing flush of compactions_in_progress: 349 (0%) on-heap, 266 (0%) off-heap
INFO  [MemtableFlushWriter:287] 2017-07-24 22:07:43,353 Memtable.java:347 - Writing Memtable-ecommerce_baitiao_record.baitiao_state@1651345625(4.792MiB serialized bytes, 89948 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:43,353 Memtable.java:347 - Writing Memtable-compactions_in_progress@284311350(0.197KiB serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:43,353 Memtable.java:382 - Completed flushing /data/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-tmp-ka-1160-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=13336501)
INFO  [MemtableFlushWriter:287] 2017-07-24 22:07:43,542 Memtable.java:382 - Completed flushing /data/cassandra/data/ecommercedata/ecommerce_baitiao_record-c94692e012cd11e682501f0d5e9f1e54/ecommercedata-ecommerce_baitiao_record.baitiao_state-tmp-ka-15-Data.db (1.593MiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=13336501)
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,558 SecondaryIndexManager.java:174 - Index build of [ecommerce_baitiao_record.baitiao_state] complete
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,558 StreamResultFuture.java:181 - [Stream #15d5ffa0-7020-11e7-8b09-d510dd9f6293] Session with /xxx.xxx.xxx.xxx is complete
WARN  [StreamReceiveTask:5] 2017-07-24 22:07:43,559 StreamResultFuture.java:208 - [Stream #15d5ffa0-7020-11e7-8b09-d510dd9f6293] Stream failed
ERROR [main] 2017-07-24 22:07:43,561 CassandraDaemon.java:583 - Exception encountered during startup
java.lang.RuntimeException: Error during boostrap: Stream failed
        at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:87) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1166) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:944) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:740) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:617) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:391) [apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:566) [apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:655) [apache-cassandra-2.1.18.jar:2.1.18]
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-16.0.jar:na]
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:209) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:185) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:413) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSession.java:700) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.taskCompleted(StreamSession.java:661) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:179) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
WARN  [StorageServiceShutdownHook] 2017-07-24 22:07:43,564 Gossiper.java:1462 - No local state or state is in silent shutdown, not announcing shutdown
INFO  [StorageServiceShutdownHook] 2017-07-24 22:07:43,564 MessagingService.java:734 - Waiting for messaging service to quiesce
INFO  [ACCEPT-/172.16.135.52] 2017-07-24 22:07:43,565 MessagingService.java:1020 - MessagingService has terminated the accept() thread
INFO  [CompactionExecutor:3] 2017-07-24 22:07:43,571 CompactionTask.java:141 - Compacting [SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-10-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-12-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-11-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-14-Data.db')]
{code}",Cassandra 2.1.18,KurtG,wavelet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 25 23:59:05 UTC 2017,,,,,,,,,,"0|i3hxnr:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"25/Jul/17 00:51;KurtG;You need to check both sides of the stream for errors. Have a look at the corresponding log on ""xxx.xxx.xxx.xxx"" for errors that could explain the failure.;;;","25/Jul/17 08:27;wavelet;Thanks for the reply.the peer side says the  streaming_socket_timeout_in_ms is too low.Although I have set the new node with streaming_socket_timeout_in_ms=86400000

ERROR [STREAM-IN-/172.16.135.52] 2017-07-25 11:55:08,034 StreamSession.java:505 - [Stream #b5594a10-70ec-11e7-b9ad-d510dd9f6293] Streaming socket tim
ed out. This means the session peer stopped responding or is still processing received data. If there is no sign of failure in the other end or a ver
y dense table is being transferred you may want to increase streaming_socket_timeout_in_ms property. Current value is 7200ms.;;;","25/Jul/17 23:59;KurtG;You should set {{streaming_socket_timeout_in_ms=86400000}} on all the nodes. Both sides of the streaming connection need a larger timeout. 7200 is way too low.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary indexes are always rebuilt at startup,CASSANDRA-13725,13089524,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,sbtourist,sbtourist,sbtourist,24/Jul/17 13:06,07/Jul/23 22:58,14/Jul/23 05:56,26/Jul/17 16:41,4.0,4.0-alpha1,,,,,Feature/2i Index,,,,,0,,,,,"Following CASSANDRA-10130, a bug has been introduced that causes a 2i to be rebuilt at startup, even if such index is already built.",,adelapena,githubbot,jeromatron,ricbartm,sbtourist,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #135:
URL: https://github.com/apache/cassandra/pull/135


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:08;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,CASSANDRA-18656,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sbtourist,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 26 16:17:36 UTC 2017,,,,,,,,,,"0|i3hxif:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Critical,,,,,,,,,,,,,,,,,,,"24/Jul/17 13:12;githubbot;GitHub user sbtourist opened a pull request:

    https://github.com/apache/cassandra/pull/135

    CASSANDRA-13725

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sbtourist/cassandra CASSANDRA-13725

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/135.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #135
    
----
commit 915382930a45244d439fd8407407322f6b5fa330
Author: Sergio Bossa <sergio.bossa@gmail.com>
Date:   2017-07-24T13:09:15Z

    Indexes created during column family initialization should not be marked as ""not built"", to avoid rebuilding them needlessly.

----
;;;","24/Jul/17 13:49;sbtourist;This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization, which marks the index as ""not built"" and causes the index initialization task to rebuild it.

Given there's no need to mark the index when a new column family is created (as the index will be ""not built"" by definition and there can't be any concurrent indexing), we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times, i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].

Such solution is implemented in the following patch, with a new dtest verifying it:
|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|

Test runs are in progress on our internal CI and I will report results as soon as they're ready.
;;;","26/Jul/17 12:31;adelapena;Both the patch and the dtest look good to me, and the CI results seem ok, +1.;;;","26/Jul/17 16:07;adelapena;Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c].;;;","26/Jul/17 16:17;adelapena;Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix exception logging that should be consumed by placeholder to 'getMessage()' for new slf4j version,CASSANDRA-13723,13089462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasonstack,jasonstack,jasonstack,24/Jul/17 10:09,15/May/20 07:59,14/Jul/23 05:56,10/Aug/17 06:22,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"The wrong tracing log will fail {{materialized_views_test.py:TestMaterializedViews.view_tombstone_test}} and impact clients.

Current log: {{Digest mismatch: {} on 127.0.0.1}}

Expected log: {{Digest mismatch: org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey....... on 127.0.0.1}}
",,jasobrown,jasonstack,jay.zhuang,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12996,,,,,,,,,,,,,"24/Jul/17 10:19;jasonstack;CASSANDRA-13723.patch;https://issues.apache.org/jira/secure/attachment/12878600/CASSANDRA-13723.patch",,,,,,,,,,,,,,,,,,,1.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 10:05:34 UTC 2017,,,,,,,,,,"0|i3hx4n:",9223372036854775807,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"24/Jul/17 12:47;jasonstack;It only affects trunk by  CASSANDRA-12996

All tests passed except for bootstrap_test(known) in dtest.

;;;","24/Jul/17 13:17;jasobrown;Should we wait until the dust settles on CASSANDRA-12996 before making this change? It seems unfortunate to need to call {{#toString()}} on an object before passing it to {{Tracing}}/{{MessageFormatter}}.

Otherwise patch is trivial and fine with me;;;","24/Jul/17 14:12;jasonstack;thanks for reviewing. let's wait for CASSANDRA-12996 to settle.;;;","25/Jul/17 08:20;spod;I've created ticket  [SLF4J-416|https://jira.qos.ch/browse/SLF4J-416] for this on the SLF4J tracker and it looks like this is a known issue, with an open [PR|https://github.com/qos-ch/slf4j/pull/166] created half a year ago.

Is it really just this one log message that is affected?;;;","25/Jul/17 08:58;jasonstack;the affected range is unknown.. need to go through the codebase. the digest log is luckily being caught by dtest;;;","25/Jul/17 09:29;spod;There seem to be a couple of statements that would have to be fixed, see

{noformat}grep -r logger src/java |grep '{}' |grep ', [e|t])'{noformat}

Running {{FileUtilsTest.testFolderSize}} will show the new behaviour:

{noformat}
ERROR [main] 2017-07-25 11:22:14,389 FileUtils.java:490 - Error while getting build/test/cassandra/data/testFolderSize/i_dont_exist folder size. {}
java.nio.file.NoSuchFileException: build/test/cassandra/data/testFolderSize/i_dont_exist
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[na:1.8.0_112]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[na:1.8.0_112]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[na:1.8.0_112]
...
{noformat}

The logger will not substitute the placeholder and print the hold stack trace instead. ;;;","26/Jul/17 06:51;jasonstack;thanks, I will update the patch this week;;;","27/Jul/17 08:20;jasonstack;if switch to {{e.toString()}} everywhere, C*  may generate many unnecessary String objects(gc) even if logger level is NONE.

By checking:  {{if(logger.isXXXEnable())}} would help, but it's troublesome.

how about using {{e.getMessage()}} instead of {{e.toString()}}?  this will affect driver's trace and server log.;;;","27/Jul/17 09:22;spod;I think we'd just have to change any log messages that intent to print the exception as represented by a log format parameter, e.g. {{logger.debug(""Error! {}"", e)}}. In this case we can simply replace the exception with {{e.getMessage()}}. But we don't have to change log messages that would print the stacktrace,
 e.g. {{logger.debug(""Error!"", e)}}.;;;","07/Aug/17 11:48;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13723] | [unit|https://circleci.com/gh/jasonstack/cassandra/368] | irrelevant
bootstrap_test.TestBootstap.consistent_range_movement_false_with_rf1_should_succeed_test
ttl_test.TestDistributedTTL.ttl_is_respected_on_repair_test
|
| [dtest|https://github.com/riptano/cassandra-dtest/commits/CASSANDRA-13723] | \ | \ |

changed {{logger.debug(""Error! {}"", e)}} (exception consumed by placeholder)  to {{logger.debug(""Error! {}"", e.getMessage())}} for new slf4j version
changed dtest to grep new digest mismatch log in tracing: {{Digest mismatch: Mismatch for key DecoratedKey.....}};;;","10/Aug/17 03:41;jasonstack;[~spodxx@gmail.com] [~jasobrown] could you please review it? thanks;;;","10/Aug/17 06:22;spod;Merged as ba87ab4e954ad2
Thanks!;;;","10/Aug/17 10:05;jasonstack;Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ignore"" option is ignored in sstableloader",CASSANDRA-13721,13089233,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,varuna,syegournov,syegournov,22/Jul/17 11:56,15/May/20 08:04,14/Jul/23 05:56,23/Jul/17 14:29,3.11.1,4.0,4.0-alpha1,,,,Legacy/Tools,,,,,0,patch,,,,"If ignore option is set on the command line sstableloader still streams to the nodes excluded.

I believe the issue is in the [https://github.com/apache/cassandra/blob/dfb90b1458ac6ee427f9e329b45c764a3a0a0c06/src/java/org/apache/cassandra/tools/LoaderOptions.java] - the LoaderOptions constructor does not set the ""ignores"" field from the the ""builder.ignores""",,jasobrown,syegournov,varuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/17 17:48;varuna;CASSANDRA-13721.patch;https://issues.apache.org/jira/secure/attachment/12878508/CASSANDRA-13721.patch",,,,,,,,,,,,,,,,,,,1.0,varuna,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 23 14:34:40 UTC 2017,,,,,,,,,,"0|i3hvpz:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,3.11.0,,,,,,,,,,,,,,,,,"22/Jul/17 17:47;varuna;Yes, constructor does not set the {{ignores}}. I'm providing patch for this. 
Also need to check for other versions.;;;","22/Jul/17 17:49;varuna;fixed constructor of {{LoaderOptions}};;;","23/Jul/17 04:14;varuna;cassandra {{tag >= 3.4}} are having this bug.;;;","23/Jul/17 14:29;jasobrown;[~varuna] nice find. +1

committed to 3.11 and trunk as sha {{c3a19f3554113682b4d37fe6a7f49bc810e5ddf6}}. Thanks!;;;","23/Jul/17 14:34;varuna;Thank you so much!! It was my first patch which got accepted!! :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential AssertionError during ReadRepair of range tombstone and partition deletions,CASSANDRA-13719,13089070,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,21/Jul/17 15:02,16/Apr/19 09:30,14/Jul/23 05:56,24/Aug/17 09:40,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,,,,,"When reconciling range tombstones for read repair in {{DataResolver.RepairMergeListener.MergeListener}}, when we check if there is ongoing deletion repair for a source, we don't look for partition level deletions which throw off the logic and can throw an AssertionError.",,blambov,jjirsa,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 09:40:13 UTC 2017,,,,,,,,,,"0|i3hupz:",9223372036854775807,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"21/Jul/17 15:46;slebresne;Pushed fix on the following branches:
| [3.0|https://github.com/pcmanus/cassandra/tree/13719-3.0] | [3.11|https://github.com/pcmanus/cassandra/tree/13719-3.11] | [trunk|https://github.com/pcmanus/cassandra/tree/13719-trunk]|

There is a first commit that basically only add additional logging when the assertion that this triggers happen. I used it to debug the problem but I think it would be wise to commit it because it's the 2nd time we get an assertion error triggered in that method (the first being CASSANDRA-13719) and I'd rather we have more info to debug if this happens a 3rd time (not that I hope it will). The additional logging doesn't do anything unless an AssertionError is triggered, so it's kind of safe to add, it's just a minor code pollution if this is never useful again, which isn't a big deal.

The 2nd commit is the real fix and includes a unit test;;;","21/Aug/17 12:36;slebresne;Gentle ping that I've just rebased the branches above and this is still good for review. I've tried running CI with CircleCI and [it looks ok|https://circleci.com/gh/pcmanus/cassandra/3#tests/containers/0] (the build is marked failed, but afaict it's just the eclipse-warnings target that failed and from the logs, it's not even clear it's a genuine failure; all unit tests looks good however) though it's the first time I try it and I'm not sure if/how to run dtests yet. ;;;","22/Aug/17 10:05;blambov;Is it not possible to have both a partition deletion repair and a condition suitable for the [{{markerToRepair == null}} branch|https://github.com/apache/cassandra/commit/8900a8dd9c2a66dfb601031f0905edffc427557f#diff-8781f9483cca1cfc87145c767295cc79R360]? E.g. partition deletion with time 10, range tombstone with time 11 between 1 and 10, with the other source having only a range tombstone with time 11 between 2 and 3? Moreover, if the second source includes a range tombstone with time 10 between 4 and 5 we may get trouble from [the other side of that branch|https://github.com/apache/cassandra/commit/8900a8dd9c2a66dfb601031f0905edffc427557f#diff-8781f9483cca1cfc87145c767295cc79R375].

 I'm not sure this can actually happen in practice, but it does not look too hard to fix: I think we need to check that the current deletion is not the same as the partition level deletion repair in all cases that generate tombstones.;;;","23/Aug/17 13:17;slebresne;You are right, those were genuine problems, thanks. I force-pushed an update to the same branches to fix and added an additional unit tests for this particular problem.;;;","23/Aug/17 15:54;blambov;LGTM;;;","24/Aug/17 09:40;slebresne;Alright, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in nodetool upgradesstables,CASSANDRA-13718,13089054,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,hkroger,hkroger,21/Jul/17 14:22,16/Apr/19 09:30,14/Jul/23 05:56,17/Aug/17 23:07,,,,,,,,,,,,0,,,,,"When upgrading from 2.2.8 to Cassandra 3.11 we were able to upgrade all other sstables except 1 file on 3 nodes (out of 4). Those are related to 2 different tables.

Upgrading sstables fails with ConcurrentModificationException.

{code}
$ nodetool upgradesstables
error: null
-- StackTrace --
java.util.ConcurrentModificationException
	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1211)
	at java.util.TreeMap$KeyIterator.next(TreeMap.java:1265)
	at org.apache.cassandra.utils.StreamingHistogram.flushHistogram(StreamingHistogram.java:168)
	at org.apache.cassandra.utils.StreamingHistogram.update(StreamingHistogram.java:124)
	at org.apache.cassandra.utils.StreamingHistogram.update(StreamingHistogram.java:96)
	at org.apache.cassandra.io.sstable.metadata.MetadataCollector.updateLocalDeletionTime(MetadataCollector.java:209)
	at org.apache.cassandra.io.sstable.metadata.MetadataCollector.update(MetadataCollector.java:182)
	at org.apache.cassandra.db.rows.Cells.collectStats(Cells.java:44)
	at org.apache.cassandra.db.rows.Rows.lambda$collectStats$0(Rows.java:102)
	at org.apache.cassandra.utils.btree.BTree.applyForwards(BTree.java:1242)
	at org.apache.cassandra.utils.btree.BTree.apply(BTree.java:1197)
	at org.apache.cassandra.db.rows.BTreeRow.apply(BTreeRow.java:172)
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:97)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:237)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:141)
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:110)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:173)
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:135)
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:65)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:141)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:428)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:315)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:745)
{code}",Cassandra 3.11 on Linux,dikanggu,hkroger,jeromatron,jjirsa,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13756,,,,,,,,,CASSANDRA-13756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 06:55:35 UTC 2017,,,,,,,,,,"0|i3humf:",9223372036854775807,,,,,,,,,,,,Normal,,3.11.0,,,,,,,,,,,,,,,,,"24/Jul/17 12:46;hkroger;This affects also user defined repair on that single file. Probably also other operations?;;;","17/Aug/17 23:07;jjirsa;Resolving this as a dupe of CASSANDRA-13756 , you reported it first, but I arbitrarily chose that one for my github branch name, so going with that. Thanks for the report!
;;;","18/Aug/17 06:55;hkroger;This is good news! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT statement fails when Tuple type is used as clustering column with default DESC order,CASSANDRA-13717,13089042,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,skonto,akichidis,akichidis,21/Jul/17 13:28,15/May/20 08:04,14/Jul/23 05:56,12/Sep/17 21:19,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Core,Legacy/CQL,,,,0,,,,,"When a column family is created and a Tuple is used on clustering column with default clustering order DESC, then the INSERT statement fails. 

For example, the following table will make the INSERT statement fail with error message ""Invalid tuple type literal for tdemo of type frozen<tuple<timestamp, text>>"" , although the INSERT statement is correct (works as expected when the default order is ASC)

{noformat}
create table test_table (
	id int,
	tdemo tuple<timestamp, varchar>,
	primary key (id, tdemo)
) with clustering order by (tdemo desc);
{noformat}
",Cassandra 3.11,akichidis,ifesdjeen,jasobrown,jjirsa,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 13:27;akichidis;example_queries.cql;https://issues.apache.org/jira/secure/attachment/12878350/example_queries.cql","10/Aug/17 01:51;jjirsa;fix_13717;https://issues.apache.org/jira/secure/attachment/12881118/fix_13717",,,,,,,,,,,,,,,,,,2.0,skonto,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 21:19:27 UTC 2017,,,,,,,,,,"0|i3hujr:",9223372036854775807,3.11.0,,,,,,jasobrown,,jasobrown,,,Critical,,,,,,,,,,,,,,,,,,,"21/Jul/17 13:36;akichidis;After some quick debugging I saw that this error is thrown because of the tuple validation on Tuples.java:

{noformat}
        if (!(receiver.type instanceof TupleType))
            throw invalidRequest(""Invalid tuple type literal for %s of type %s"", receiver.name, receiver.type.asCQL3Type());
{noformat}

When the default ordering on tuple is DESC, the *receiver.type* is instance of type *ReversedType* and not *TupleType* . However, I don't have the expertise to know the cause of this.
;;;","21/Jul/17 21:09;jjirsa;There are a bunch of cases in {{Tuples.java}} where we check {{instanceof TupleType}} that are probably wrong in the case that it's Reversed. I suspect we should be checking if the {{baseType}} (which is already public) of the {{ReversedType}} is a Tuple.
;;;","10/Aug/17 01:44;skonto;I have created a patch and verified Jeff's suggestion:
[patch|https://drive.google.com/open?id=0B0SeiqgJaLZvclhmY0N4dEJtUGs]

{noformat}
cqlsh> create keyspace test with replication = {'class':'SimpleStrategy','replication_factor': 1};cqlsh> create table test.test_table ( id int, tdemo tuple<timestamp, varchar>, primary key (id, tdemo) ) with clustering order by (tdemo desc);
cqlsh> insert into test.test_table (id, tdemo) values (1, ('2017-02-03 03:05+0000','Europe'));
cqlsh> select * from test.test_table;
 id | tdemo
----+-----------------------------------------------
  1 | ('2017-02-03 03:05:00.000000+0000', 'Europe')

(1 rows)

{noformat}

What are the next steps for the review (I am new here)?
;;;","10/Aug/17 01:52;jjirsa;Welcome [~skonto] ! Next step would be to assign yourself (I've done that for you), and then hit 'submit patch' to mark the issue as patch available (I've done that for you again).

We typically ask either the contributor (you) or the reviewer (someone who will volunteer, hopefully soon) to push the patch to a github branch and kick off CI (we have it setup to use circleci for unit tests, and a committer can kick off dtests). We typically ask that your patch includes a test case that fails before your patch and succeeds after it's applied, so while this is not a review, I will say that any reviewer should ask you to do that. 

;;;","10/Aug/17 02:37;skonto;Thnx [~jjirsa] ! Here is my branch: https://github.com/skonto/cassandra/tree/cassandra-13717. As for the test case I read the contribution wiki etc... still a bit confused where should I add it? In dtests or just part of the patch?
;;;","10/Aug/17 02:43;jjirsa;I typically prefer keeping them in unit tests (junit tests in the same repo, check out the test/ directory). There should be a section for cql3 tests, and almost certainly a TupleTest within it that you can add a function or two to.
;;;","10/Aug/17 04:41;jjirsa;[~skonto] - do you know which versions need to be fixed? 3.0? 3.11? trunk?

I've kicked off some test builds [here (unit tests)|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13717] and [here (dtest)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/173] - we'll want to do that for each branch that needs this fix (and of course, we'll want to add tests to this fix as well).
;;;","10/Aug/17 11:13;skonto;[~jjirsa] I fixed it for trunk (version 4). I could backport it to 3.11 (version reported) as soon as it is verified that this fix is ok.
Good to know about the test procedure, thanx a lot! I will check the unit tests.;;;","10/Aug/17 12:26;skonto;[~jjirsa] I added a test there in TupleTypeTest, updated the branch. How can I update the patch? 
Should I cancel it and add a new one?;;;","20/Aug/17 20:53;skonto;[~jjirsa] Any update or something I should do?;;;","11/Sep/17 22:21;jasobrown;This bug exists in 3.0, as well - perhaps earlier, but even if it does this doesn't meet the bar of being 'critical' to patch 2.x for.;;;","11/Sep/17 23:03;jasobrown;backported the patch to 3.0 and 3.11, and running tests now:

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13717-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13717-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13717-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/298/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/299/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/300/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-3.0]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-trunk]|
;;;","12/Sep/17 21:19;jasobrown;committed as sha {{a08a816a6a3497046ba75a38d76d5095347dfe95}}.

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move processing of EchoMessage response to gossip stage,CASSANDRA-13713,13088897,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,20/Jul/17 21:10,15/May/20 08:06,14/Jul/23 05:56,29/Mar/18 12:43,4.0,4.0-alpha1,,,,,Legacy/Distributed Metadata,,,,,0,,,,,"Currently, when a node receives an {{EchoMessage}}, is sends a simple ACK reply back (see {{EchoVerbHandler}}). The ACK is sent on the small message connection, and because it is 'generically' typed as {{Verb.REQUEST_RESPONSE}}, is consumed on a {{Stage.REQUEST_RESPONSE}} thread. The proper thread for this response to be consumed is {{Stage.GOSSIP}}, that way we can move more of the updating of the gossip state to a single, centralized thread, and less abuse of gossip's shared mutable state can occur.",,jasobrown,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 12:43:25 UTC 2018,,,,,,,,,,"0|i3htnj:",9223372036854775807,,,,,,,jkni,,jkni,,,Low,,,,,,,,,,,,,,,,,,,"20/Jul/17 21:23;jasobrown;A simple fix available here:

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13713-trunk]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13713-trunk/]|

This incorrect behavior has been around for a long time. However, I'm not sure how far back to apply the change (the change will be the same for back to 2.1). The existing execution of {{Gossiper#realMarkAlive}} is already asynchronous (nothing is dependent on it, per se); the only downside is another (short-lived, once per peer) task to be executed on the gossip stage. I feel that's a tiny price to pay for reducing the number of different threads that can modify the state of gossip.
;;;","26/Jul/17 19:41;jkni;The simple change here seems good - do you think it's worth doing something to map the echo response directly to the gossip stage and gossip connection? This seems like a good fix for the present state, but I think perhaps a different verb or a way to map individual REQUEST_RESPONSE verbs to a stage/connection makes sense on a longer term.;;;","26/Jul/17 21:35;jasobrown;bq. do you think it's worth doing something to map the echo response directly to the gossip stage and gossip connection?

CASSANDRA-13714 :);;;","02/Aug/17 17:30;jkni;Ah - guess I missed that one. LGTM to the change here. I don't have any strong opinions as to where it goes in 2.2+. While we don't have any known problems related to this, we also know bugs can lurk in this subsystem for a long time, and this seems like a safe preventative measure.;;;","29/Mar/18 12:43;jasobrown;Decided to be very cautious and only committed to trunk (4.0) as sha {{674addd03701abf1bce7d6b47978761c00d0a431}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid writetime for null columns in cqlsh,CASSANDRA-13711,13088800,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,20/Jul/17 15:45,15/May/20 08:06,14/Jul/23 05:56,31/Jul/17 22:32,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/CQL,,,,,0,,,,,"From the user list:

https://lists.apache.org/thread.html/448731c029eee72e499fc6acd44d257d1671193f850a68521c2c6681@%3Cuser.cassandra.apache.org%3E

{code}
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm create test -n 1 -s -v 3.0.10
Current cluster is now: test
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm node1 cqlsh
Connected to test at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.10 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.t ( a text primary key, b text );
cqlsh> insert into test.t(a) values('z');
cqlsh> insert into test.t(a) values('w');
cqlsh> insert into test.t(a) values('e');
cqlsh> insert into test.t(a) values('r');
cqlsh> insert into test.t(a) values('t');
cqlsh> select a,b, writetime (b) from test.t;

a | b | writetime(b)
---+------+--------------
z | null | null
e | null | null
r | null | null
w | null | null
t | null | null

(5 rows)
cqlsh>
cqlsh> insert into test.t(a,b) values('t','x');
cqlsh> insert into test.t(a) values('b');
cqlsh> select a,b, writetime (b) from test.t;

 a | b    | writetime(b)
---+------+------------------
 z | null |             null
 e | null |             null
 r | null |             null
 w | null |             null
 t |    x | 1500565131354883
 b | null | 1500565131354883

(6 rows)
{code}

Data on disk:

{code}
MacBook-Pro:~ jjirsa$ ~/.ccm/repository/3.0.14/tools/bin/sstabledump /Users/jjirsa/.ccm/test/node1/data0/test/t-bed196006d0511e7904be9daad294861/mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""z"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 20,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:54.818118Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""e"" ],
      ""position"" : 21
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 44,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:04.288547Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""r"" ],
      ""position"" : 45
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 68,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:08.991417Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""w"" ],
      ""position"" : 69
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 92,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:59.005382Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""t"" ],
      ""position"" : 93
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 120,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:38:51.354883Z"" },
        ""cells"" : [
          { ""name"" : ""b"", ""value"" : ""x"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""b"" ],
      ""position"" : 121
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 146,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:39:03.631297Z"" },
        ""cells"" : [ ]
      }
    ]
  }
]MacBook-Pro:~ jjirsa$
{code}
",,aleksey,bradfordcp,hkroger,jay.zhuang,jjirsa,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 19:05:33 UTC 2017,,,,,,,,,,"0|i3ht27:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"27/Jul/17 01:22;jjirsa;FWIW, this is not a cqlsh bug, repro's on python drivers, and does not repro on 2.1

;;;","27/Jul/17 19:04;jjirsa;|| Branch || Circle || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.0-13711] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13711 ] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/153/] |
| [3.11|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.11-13711] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13711 ] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/154/] |
| [trunk|https://github.com/jeffjirsa/cassandra/commits/cassandra-13711] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13711 ] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/152/] |
;;;","28/Jul/17 06:40;jjirsa;[~blerer] you were in this code recently, interested in reviewing? If not, [~beobal] or [~iamaleksey] ? 
;;;","28/Jul/17 12:09;aleksey;+1;;;","31/Jul/17 22:32;jjirsa;Committed to 3.0 as {{9dc896f4ea51276de4ea76ffca3fd719e0c8b8a1}} and merged 3.11 and 4.0
;;;","29/Aug/17 09:53;slebresne;The unit tests committed with that ticket are pretty fragile because they insert with a TTL of 100 and expect a following read to get the same TTL of 100, but a read give you the remaining TTL (and this is returning with a 1 second precision), so on a slow run or through some timing issue you can easily get a failure with:
{noformat}
junit.framework.AssertionFailedError: Invalid value for row 0 column 3 (ttl(i) of type int), expected <100> but got <99>
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1255)
	at org.apache.cassandra.cql3.validation.operations.SelectTest.testMixedTTLOnColumnsWide(SelectTest.java:4806)
{noformat}
[~jjirsa]: Would you mind having a look at making those tests less flaky? (personally don't mind if you'd rather ninja-fix).;;;","29/Aug/17 19:05;jjirsa;I saw him open it as CASSANDRA-13764 - let me fix it there just so we don't re-open this


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using min_compress_ratio <= 1 causes corruption,CASSANDRA-13703,13088783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,blambov,blambov,blambov,20/Jul/17 15:21,15/May/20 08:06,14/Jul/23 05:56,15/Sep/17 08:38,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"This is because chunks written uncompressed end up below the compressed size threshold. Demonstrated by applying the attached patch meant to improve the testing of the 10520 changes, and running {{CompressedSequentialWriterTest.testLZ4Writer}}.

The default {{min_compress_ratio: 0}} is not affected as it never writes uncompressed.",,blambov,dimitarndimitrov,jasonstack,jeromatron,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/17 15:21;blambov;patch;https://issues.apache.org/jira/secure/attachment/12878193/patch",,,,,,,,,,,,,,,,,,,1.0,blambov,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 08:38:02 UTC 2017,,,,,,,,,,"0|i3hsyf:",9223372036854775807,,,,,,,dimitarndimitrov,,dimitarndimitrov,,,Critical,,,,,,,,,,,,,,,,,,,"21/Jul/17 07:53;blambov;Patch [here|https://github.com/blambov/cassandra/tree/13703].

Changes the max size threshold to be non-inclusive so that {{min_compress_ratio: 1}} can work (this would be a breaking change, but CASSANRDA-10520 is not part of any release yet), and adds an error for values between 0 and 1 exclusive.

Unit tests are clean, dtests have two failures that appear to be flaky on trunk:
{{repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test}}
{{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}};;;","05/Sep/17 13:54;dimitarndimitrov;+1 - from my still largely layman perspective, the change looks good.

A couple of small nits:
* {{CompressedChunkReader.maybeCheckCrc()}} (renamed in this patch to {{shouldCheckCrc()}}) may be removed or at least its visibility could be reduced. It seems to be used solely in CompressedChunkReader.java, lines 158 and 204 in this patch.
* The no-argument / single-argument factory methods for Snappy and LZ4 compressions in CompressionParams.java seem to differ in the values that they default to for min compression ratio and max compressed length.
** For Snappy, if nothing is specified, or only chunk length is specified, a default min compression ratio of 1.1 is used, and therefore max compressed length ends up somewhere roughly around 90% of chunk length.
** For LZ4, if nothing is specified, or only chunk length is specified, a default max compressed length of chunk length is used, and therefore min compression ratio ends up at 1.0 (I'm not sure if a precision error is possible there).

Edit: Of course, take this review with the appropriate rock-sized grain of salt.;;;","06/Sep/17 16:52;blambov;Rebased and updated the branch, reducing the visibility of {{shouldCheckCrc}} and adding a comment and {{VisibleForTesting}} annotations to make it clear why the shorthand methods use inconsistent parameters.;;;","15/Sep/17 08:38;blambov;Tests were fine, committed to trunk as [17a358c2cc2c583c3e0fa046ca8dee6d743ad1c5|https://github.com/apache/cassandra/commit/17a358c2cc2c583c3e0fa046ca8dee6d743ad1c5].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heartbeats can cause gossip information to go permanently missing on certain nodes,CASSANDRA-13700,13088570,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jkni,jkni,jkni,19/Jul/17 21:50,15/May/20 08:01,14/Jul/23 05:56,24/Jul/17 20:29,2.1.19,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,Legacy/Distributed Metadata,,,,,1,,,,,"In {{Gossiper.getStateForVersionBiggerThan}}, we add the {{HeartBeatState}} from the corresponding {{EndpointState}} to the {{EndpointState}} to send. When we're getting state for ourselves, this means that we add a reference to the local {{HeartBeatState}}. Then, once we've built a message (in either the Syn or Ack handler), we send it through the {{MessagingService}}. In the case that the {{MessagingService}} is sufficiently slow, the {{GossipTask}} may run before serialization of the Syn or Ack. This means that when the {{GossipTask}} acquires the gossip {{taskLock}}, it may increment the {{HeartBeatState}} version of the local node as stored in the endpoint state map. Then, when we finally serialize the Syn or Ack, we'll follow the reference to the {{HeartBeatState}} and serialize it with a higher version than we saw when constructing the Ack or Ack2.

Consider the case where we see {{HeartBeatState}} with version 4 when constructing an Ack and send it through the {{MessagingService}}. Then, we add some piece of state with version 5 to our local {{EndpointState}}. If {{GossipTask}} runs and increases the {{HeartBeatState}} version to 6 before the {{MessageOut}} containing the Ack is serialized, the node receiving the Ack will believe it is current to version 6, despite the fact that it has never received a message containing the {{ApplicationState}} tagged with version 5.

I've reproduced in this in several versions; so far, I believe this is possible in all versions.",,aleksey,csplinter,dikanggu,jasobrown,jeromatron,jjirsa,jkni,mbyrd,mshuler,rlow,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 15:54:34 UTC 2017,,,,,,,,,,"0|i3hrn3:",9223372036854775807,3.0.14,,,,,,jasobrown,,jasobrown,,,Critical,,,,,,,,,,,,,,,,,,,"20/Jul/17 14:14;jasobrown;[~jkni] Fantastic debugging here, Joel. We have seen this problem, as well, with missing STATUS and TOKENS entries.

I followed this through, and I believe you are correct. Just to point out (because I had to dig and reason through it), the key problem is (as Joel points out) the shared mutable state of {{HeartBeatState}}. In {{Gossiper.getStateForVersionBiggerThan}}, when the local node is building up the {{Map<ApplicationState, VersionedValue> states}} about itself, if any states are added after the function returns *and* the heartbeat is incremented before serialization, the peer will get the updated heartbeat value but not the updated states (as we the set of states for the local node that we're sending over was already constructed a priori the serialization).

Off the top of my head, I think there are at least two possible ways to fix this:

- clone the {{HeartBeatState}} when constructing the {{EndpointState}} to return from {{Gossiper.getStateForVersionBiggerThan}}. That way it's not referencing mutable heartbeat state.
- execute the {{GossipTask}} on the same thread the we receive the gossip syn/ack/ack2 messages (on the {{Stage.GOSSIP}} thread). That way we force (almost) all references to gossip's stated mutable state into one thread.

The first option is simpler, smaller in scope, and certainly safer.
The second option is has performance implications, especially if the {{GossipTask}} takes a while to execute, then we could start backing up the tasks on the stage. This option, though, has the ""possibility"" of eliminating more of the state race bugs that we seems to continually uncover as time goes on. (Side note: there are still some updates to local Gossip state from the main thread (via {{StorageService}}) at startup, and the response to the {{EchoMessage}} is on the wrong thread, as well.)

Joel, can you share the method of how you are able to reproduce this?;;;","20/Jul/17 14:17;jasobrown;We also might need to make {{HeartBeatState.version}} volatile, but I'm still thinking about it (just adding it here for discussion);;;","20/Jul/17 16:25;jkni;Thanks, Jason! In this case, I agree the first option is safer for this issue. Something like the second likely makes sense eventually, at least as part of a larger audit of correctness issues in gossip. I believe your volatile suggestion is correct.

I don't have a lot of helpful information to reproduce this; it reproduces in larger clusters, particularly with higher latency levels. We can see the effects locally with a few well-timed sleeps in MessagingService, but that isn't terribly representative.

Branches pushed here:
||branch||
|[13700-2.1|https://github.com/jkni/cassandra/tree/13700-2.1]||
|[13700-2.2|https://github.com/jkni/cassandra/tree/13700-2.2]||
|[13700-3.0|https://github.com/jkni/cassandra/tree/13700-3.0]||
|[13700-3.11|https://github.com/jkni/cassandra/tree/13700-3.11]||
|[13700-trunk|https://github.com/jkni/cassandra/tree/13700-trunk]||

There's a somewhat conceptually similar issue when we bump the gossip generation in the middle of constructing a reply - I believe that's the cause in [CASSANDRA-11825], which presents similar problems. I'm choosing to address them separately because they're indeed distinct problems and 11825 requires an additional trigger (enabling and disabling gossip during runtime).;;;","20/Jul/17 17:58;jasobrown;+1;;;","20/Jul/17 17:59;jasobrown;re: CASSANDRA-11825. Yes, shared mutable state strikes again, and thanks for addressing them separately ;);;;","24/Jul/17 20:29;jkni;Thanks! Tests looked good on all branches.

Committed to 2.1 as {{2290c0d4b0c20ce3407ae2c542e580c75a5ab337}} and merged forward through 2.2, 3.0, 3.11, and trunk.;;;","27/Jul/17 18:36;jasobrown;[~jkni] In our internal review of this change, we discovered that that patch can be made incrementally safer. In {{Gossiper#getStateForVersionBiggerThan()}}, we [get the generation|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/Gossiper.java#L899], but it's possible the {{epState.getHeartBeatState()}} could have been swapped out before the next line executes (where we get the version).

Are you ok if I make the following small change to {{Gossiper#getStateForVersionBiggerThan()}}:
{code}
-            int localHbGeneration = epState.getHeartBeatState().getGeneration();
-            int localHbVersion = epState.getHeartBeatState().getHeartBeatVersion();
+            HeartBeatState heartBeatState = epState.getHeartBeatState();
+            int localHbGeneration = heartBeatState.getGeneration();
+            int localHbVersion = heartBeatState.getHeartBeatVersion();
{code}

Basically, just grab a reference the the same `HeartBeatState` that we'll use for both the generation and version. Of course, this does not protect against that `HeartBeatState` instance being mutated, but at least we can be smarter about what we reference from the `epState`.;;;","01/Aug/17 14:32;jkni;I'm not sure of any places in the current codebase where the distinction matters in practice, but the change is cleaner and makes the code more tolerant of changes elsewhere, so +1.;;;","29/Aug/17 15:54;jasobrown;pushed the minor fix as sha {{6b927783ba0777d3dd7c2c3311b246a8dbce5b59}} to 2.1+.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digest mismatch Exception if hints file has UnknownColumnFamily,CASSANDRA-13696,13087482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,17/Jul/17 06:18,15/May/20 08:04,14/Jul/23 05:56,27/Jul/17 00:00,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Core,,,,,0,,,,,"{noformat}
WARN  [HintsDispatcher:2] 2017-07-16 22:00:32,579 HintsReader.java:235 - Failed to read a hint for /127.0.0.2: a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0 - table with id 3882bbb0-6a71-11e7-9bca-2759083e3964 is unknown in file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints
ERROR [HintsDispatcher:2] 2017-07-16 22:00:32,580 HintsDispatchExecutor.java:234 - Failed to dispatch hints file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints: file is corrupted ({})
org.apache.cassandra.io.FSReadError: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:199) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:164) ~[main/:na]
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHints(HintsDispatcher.java:157) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHintsAndAwait(HintsDispatcher.java:139) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:123) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:95) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:268) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:251) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:229) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:208) [main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_111]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNextInternal(HintsReader.java:216) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:190) ~[main/:na]
    ... 16 common frames omitted
{noformat}

It causes multiple cassandra nodes stop [by default|https://github.com/apache/cassandra/blob/cassandra-3.0/conf/cassandra.yaml#L188].

Here is the reproduce steps on a 3 nodes cluster, RF=3:
1. stop node1
2. send some data with quorum (or one), it will generate hints file on node2/node3
3. drop the table
4. start node1

node2/node3 will report ""corrupted hints file"" and stop. The impact is very bad for a large cluster, when it happens, almost all the nodes are down at the same time and we have to remove all the hints files (which contain the dropped table) to bring the node back.
",,aleksey,bradfordcp,ifesdjeen,jasonstack,jay.zhuang,jjirsa,szhou,vinegh,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,Availability -> Cluster Crash,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 06:35:33 UTC 2018,,,,,,,,,,"0|i3hkxb:",9223372036854775807,3.0.14,,,,,,jjirsa,,jjirsa,,,Critical,,,,,,,,,,,,,,,,,,,"17/Jul/17 06:53;jay.zhuang;The problem is because {{[skipBytes()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/io/util/RebufferingInputStream.java#L119]}} will leave some value in the buffer. Before returning, it should call {{input.checkCrc()}} to update the CRC.
The fix is to set {{hint=null}} and use the reminding code to do {{checkCRC()}}. Adding an uTest to reproduce the problem:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#21|https://circleci.com/gh/cooldoger/cassandra/21]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#24|https://circleci.com/gh/cooldoger/cassandra/24]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#23|https://circleci.com/gh/cooldoger/cassandra/23]|

[~jjirsa] would you please review the patch?;;;","17/Jul/17 20:48;jjirsa;Looks pretty serious, I'll try to get to it soon. Short term, your trunk patch doesn't actually compile ( {{CFMetaData}} is gone in trunk), can you fix that and  push that branch? ;;;","18/Jul/17 06:00;jay.zhuang;Sure, updated the test for trunk:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#21|https://circleci.com/gh/cooldoger/cassandra/21]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#24|https://circleci.com/gh/cooldoger/cassandra/24]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#25|https://circleci.com/gh/cooldoger/cassandra/25]|

Yes, I think it's a serious problem that all nodes go down at the same time. And to recover, the user has to delete all hints file (Not sure if there's an easy way to identify the ""corrupted"" hints files, system.log only shows the first one. So for us, we just delete all the hints files.);;;","19/Jul/17 05:23;jjirsa;Hey [~jay.zhuang] - took a look and I don't see any issues. The new tests look good. trunk circleCI run failed due to circleci, but run #26 looks green.

I'd like to glance at it again tomorrow when I'm more awake, but as of right now, it seems very reasonable and correct to me.

;;;","19/Jul/17 06:31;jay.zhuang;Thanks [~jjirsa].
I did more investigation today. Seems it's more serious than I thought. Even there's no down node, ""drop table"" + write traffic, will trigger the problem.
Here is reproduce steps:
1. Create a 3 nodes cluster:
  {{$ ccm create test13696 -v 3.0.14 && ccm populate -n 3 && ccm start}}
2. Send some traffics with cassandra-stress (blogpost.yaml is only in trunk, if you use another yaml file, change the RF=3)
  {{$ tools/bin/cassandra-stress user profile=test/resources/blogpost.yaml cl=QUORUM truncate=never ops\(insert=1\) duration=30m -rate threads=2 -mode native cql3 -node 127.0.0.1}}
3. While the traffic is running, drop table
  {{$ cqlsh -e ""drop table  stresscql.blogposts""}}
*All 3 nodes go down because of ""Digest mismatch Exception"".*

The CRC calculation problem has been there for a long time, but only got exposed after CASSANDRA-13004 because of the MessagingService version bump. In the normal case when the versions are the same, HintsDispatcher uses {{[page.buffersIterator()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L138]}} instead of {{[page.hintsIterator()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L139]}}. {{buffersIterator()}} doesn't need to decode hints, so it won't have the problem.

I think the messagingVersion for the hints file should be updated: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1 so it could dispatch hints in an optimized way. Not sure if we need to check/bump other {{MessagingService.VERSION_30}}s in the 3.0 branch.
cc [~ifesdjeen];;;","19/Jul/17 07:19;jjirsa;cc [~iamaleksey] as well.
;;;","19/Jul/17 11:29;ifesdjeen;Might be we want to use either 30 or 3014 depending on which one is active?

Question: does this happen in mixed version cluster or all the nodes actually have the same protocol version?;;;","19/Jul/17 17:48;jay.zhuang;{quote}
Question: does this happen in mixed version cluster or all the nodes actually have the same protocol version?
{quote}
All the nodes are on the same messagingVersion {{[VERSION_3014|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/net/MessagingService.java#L95]}}
It can be reproduced with a new 3.0.14 cluster.;;;","20/Jul/17 12:20;ifesdjeen;I agree we should also return a correct version from the hints service (as [~jay.zhuang] already mentioned), like [here|https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1] same as we do in commit log descriptor.

This also would make the issue for same-version go away, and since it would make the service to pick a different code path I'd say it's also necessary to include it. 

WRT to the patch itself, might be it's better to just call {{resetCrc}} explicitly and still return null like I did [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13696-3.0#diff-cf15f9cac67d8b2f3e581129d617df16R242]? {{hint}} is a local variable, and setting it and carrying on makes the logic a bit harder to understand. For example, for me it was non-obvious that this boolean method would also do some buffer rewinding / state resetting under the hood. ;;;","20/Jul/17 15:38;jjirsa;Wanted to chat with Aleksey offline about the version implications, so withholding comment on that for a bit, but:


{quote}
might be it's better to just call resetCrc explicitly and still return null like I did here? hint is a local variable, and setting it and carrying on makes the logic a bit harder to understand. For example, for me it was non-obvious that this boolean method would also do some buffer rewinding / state resetting under the hood.
{quote}

I think the current behavior is actually the right thing to do. Simply resetting the CRC state isn't enough, we need to check to see if the CRC matches, because we want to invoke the disk failure policy if we're reading corrupt data, and frankly, a corruption source (bad disk / RAM / etc) that flips bits could cause us to see an invalid CFID, and skipping the corruption test at that point would be the wrong thing to do. A few more comment lines are probably worthwhile, though, since it seems like an easy 'fix' to revert in the future because it's nonobvious. 

;;;","20/Jul/17 16:14;ifesdjeen;bq. Simply resetting the CRC state isn't enough,

True. Re-read the issue description/first comment, now it's quite obvious. Thanks for explanation. 

Adding a comment would be great though!;;;","20/Jul/17 18:35;jay.zhuang;Updated based on the reviews:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#31|https://circleci.com/gh/cooldoger/cassandra/31]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#32|https://circleci.com/gh/cooldoger/cassandra/32]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#30|https://circleci.com/gh/cooldoger/cassandra/30]|

Also, {{resetCrc()}} could also cause CRC mismatch for the next hint read in the same file.

Another question is why dropTable + write traffic will cause hints file, do you think it's an issue? I create a separate ticket to track that: CASSANDRA-13712;;;","24/Jul/17 12:24;aleksey;+1 from me as well.;;;","24/Jul/17 12:33;aleksey;{quote}
The CRC calculation problem has been there for a long time, but only got exposed after CASSANDRA-13004 because of the MessagingService version bump. In the normal case when the versions are the same, HintsDispatcher uses page.buffersIterator() instead of page.hintsIterator(). buffersIterator() doesn't need to decode hints, so it won't have the problem.
I think the messagingVersion for the hints file should be updated: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1 so it could dispatch hints in an optimized way. Not sure if we need to check/bump other {{MessagingService.VERSION_30}} s in the 3.0 branch.
{quote}

And your analysis is correct, too. Thanks for catching and fixing it.;;;","24/Jul/17 13:52;jjirsa;[~jay.zhuang] - could you rebuild 3.11 on circleci? test failed (exceeded memory limit), I'm fairly confident it's benign, but we should have a green test before committing (I'm confident it's benign because the test actually passed on 3.11 before the messaging fix, but we should prove there's not a regression there).
;;;","24/Jul/17 17:47;jay.zhuang;Sure, rebuilt the CI, and it's passed: https://circleci.com/gh/cooldoger/cassandra/43;;;","24/Jul/17 18:03;jjirsa;lgtm will commit when I have time.
;;;","27/Jul/17 00:00;jjirsa;Committed to 3.0 as {{f919cf4a478cdbcb7864e8b47814a40bfcb343a7}} and merged to 3.11 and trunk. Thanks [~jay.zhuang], good find!
;;;","09/Apr/18 18:05;vinegh;[~jay.zhuang]: I am not sure if this the same issue. I appiled commit you have here on 3.11.1 and we still see the following issue on a 14 node cluster

Any ideas on what might be wrong?

ERROR [HintsDispatcher:1] 2018-04-06 16:26:44,423 CassandraDaemon.java:228 - Exception in thread Thread[HintsDispatcher:1,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Digest mismatch exception
at org.apache.cassandra.hints.HintsReader$BuffersIterator.computeNext(HintsReader.java:298) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsReader$BuffersIterator.computeNext(HintsReader.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.sendHints(HintsDispatcher.java:169) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.sendHintsAndAwait(HintsDispatcher.java:128) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:113) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:278) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:260) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:238) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:217) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_141]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_141]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_141]
at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_141]
Caused by: java.io.IOException: Digest mismatch exception

To overcome this we do a truncatehints and everything works after that. Issue happens once a while we have seen it twice so far on 14 node cluster and 7 node cluster

 ;;;","09/Apr/18 23:54;jay.zhuang;Hi [~vinegh], this should be a different issue, as [{{HintsDispatcher.java:128}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L128] sends hints with {{buffer}}s, this patch is only to fix the digest mismatch for [{{HintsDispatcher.java:129}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L129], which sends hints one by one.

Do you see the digest mismatch issue on one node or multiple nodes? Do you have schema change during that? Maybe you should file a separate ticket for that.;;;","10/Apr/18 06:35;vinegh;[~jay.zhuang]: Yes we see it on multiple hosts. Yes, schema is being propagated to other nodes. We create schema on one node and bring up rest of nodes so it should take a while to propagate. I will file a separate ticket too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect [2.1 <— 3.0] serialization of counter cells with pre-2.1 local shards,CASSANDRA-13691,13087066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,14/Jul/17 02:04,16/Apr/19 09:30,14/Jul/23 05:56,01/Aug/17 15:00,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,counters,upgrade,,,"We stopped generating local shards in C* 2.1, after CASSANDRA-6504 (Counters 2.0). But it’s still possible to have counter cell values
around, remaining from 2.0 times, on 2.1, 3.0, 3.11, and even trunk nodes, if they’ve never been overwritten.

In 2.1, we used two classes for two kinds of counter columns:
{{CounterCell}} class to store counters - internally as collections of {{CounterContext}} blobs, encoding collections of (host id, count, clock) tuples
{{CounterUpdateCell}} class to represent unapplied increments - essentially a single long value; this class was never written to commit log, memtables, or sstables, and was only used inside {{Mutation}} object graph - in memory, and marshalled over network in cases when counter write coordinator and counter write leader were different nodes
3.0 got rid of {{CounterCell}} and {{CounterUpdateCell}}, among other {{Cell}} classes. In order to represent these unapplied increments - equivalents of 2.1 {{CounterUpdateCell}} - in 3.0 we encode them as regular counter columns, with a ‘special’ {{CounterContext}} value. I.e. a counter context with a single local shard. We do that so that we can reuse local shard reconcile logic (summing up) to seamlessly support counters with same names collapsing to single increments in batches. See {{UpdateParameters.addCounter()}} method comments [here|https://github.com/apache/cassandra/blob/cassandra-3.0.14/src/java/org/apache/cassandra/cql3/UpdateParameters.java#L157-L171] for details. It also assumes that nothing else can generate a counter with local shards.

It works fine in pure 3.0 clusters, and in mixed 2.1/3.0 clusters, assuming that there are no counters with legacy local shards remaining from 2.0 era. It breaks down badly if there are.

{{LegacyLayout.serializeAsLegacyPartition()}} and consequently {{LegacyCell.isCounterUpdate()}} - classes responsible for serializing and deserialising in 2.1 format for compatibility - use the following logic to tell if a cell of {{COUNTER}} kind is a regular final counter or an unapplied increment:

{code}
private boolean isCounterUpdate()
{
    // See UpdateParameters.addCounter() for more details on this
    return isCounter() && CounterContext.instance().isLocal(value);
}
{code}

{{CounterContext.isLocal()}} method here looks at the first shard of the collection of tuples and returns true if it’s a local one.

This method would correctly identify a cell generated by {{UpdateParameters.addCounter()}} as a counter update and serialize it correctly as a 2.1 {{CounterUpdateCell}}. However, it would also incorrectly flag any regular counter cell that just so happens to have a local shard as the first tuple of the counter context as a counter update. If a 2.1 node as a coordinator of a read requests fetches such a value from a 3.0 node, during a rolling upgrade, instead of the expected {{CounterCell}} object it will receive a {{CounterUpdateCell}}, breaking all the things. In the best case scenario it will cause an assert in {{AbstractCell.reconcileCounter()}} to be raised.

To fix the problem we must find an unambiguous way, without false positives or false negatives, to represent and identify unapplied counter updates on 3.0 side. ",,aleksey,jjirsa,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 14:59:28 UTC 2017,,,,,,,,,,"0|i3hidj:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"15/Jul/17 00:06;aleksey;Ended up using a special clock id for counter update contexts, so that we can change for that clock id instead of merely looking at the shard type, to unambiguously tell regular counter values from counter updates in 3.0.

Branches with the fix: [3.0|https://github.com/iamaleksey/cassandra/tree/13691-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/13691-3.11], [4.0|https://github.com/iamaleksey/cassandra/tree/13691-4.0].

The commits include a small unit test to confirm that regular counter contexts with old local shards in first spot are no longer recognized as counter updates. Unfortunately it's a lot more painful given our existing framework to create upgrade tests that would span the whole range from 2.0 to 3.0, due to differences in supported driver protocol version.

Steps for manual reproduction and fix verification:

1. generate some 2.0 counter columns with local shards

{code}
ccm create test -n 3 -s -v 2.0.17

ccm node1 cqlsh
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
cqlsh> CREATE TABLE test.test (id int PRIMARY KEY, v1 counter, v2 counter, v3 counter);

python
>>> from cassandra.cluster import Cluster
>>> cluster = Cluster(['127.0.0.1', '127.0.0.2', '127.0.0.3'])
>>> session = cluster.connect()
>>> query = ""UPDATE test.test SET v1 = v1 + 1, v2 = v2 + 1, v3 = v3 + 1 where id = ?""
>>> prepared = session.prepare(query)
>>> for i in range(0, 1000):
...     session.execute(prepared, [i])
...

ccm flush
{code}

2. upgrade cluster to 2.1

{code}
ccm stop
ccm setdir -v 2.1.17
ccm start

ccm node1 nodetool upgradesstables
ccm node2 nodetool upgradesstables
ccm node3 nodetool upgradesstables
{code}

3. upgrade node3 to 3.0

{code}
ccm node3 stop
ccm node3 setdir -v 3.0.14
ccm node3 start
{code}

4. with a 2.1 coordinator, try to read the table with CL.ALL

{code}
ccm node1 cqlsh
cqlsh> CONSISTENCY ALL;
cqlsh> SELECT COUNT(*) FROM test.test;
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.AssertionError: Wrong class type: class org.apache.cassandra.db.BufferCounterUpdateCell"">
{code}

5. upgrade node3 to patched 3.0

{code}
ccm node3 stop
ccm node3 setdir --install-dir=/Users/aleksey/Code/cassandra
ccm node3 start
{code}

6. with a 2.1 coordinator, try again to read the table with CL.ALL

{code}
ccm node1 cqlsh
cqlsh> CONSISTENCY ALL;
cqlsh> SELECT COUNT(*) FROM test.test;

 count
-------
  1000

(1 rows)
{code}

I'll try to automate it as an upgrade test, too, but for now the manual steps and the unit test will have to do.;;;","26/Jul/17 16:36;aleksey;Dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13691] - requires latest ccm with [issue 463|https://github.com/pcmanus/ccm/issues/463] addressed.;;;","01/Aug/17 14:20;slebresne;Had a look, and this lgtm. Don't mean to stole Sam's thunder, but +1 as far as I'm concerned.;;;","01/Aug/17 14:59;aleksey;Thanks.

Committed to 3.0 as [ba71289778369e71d9abbdb93cb6b91ba67f9c85|https://github.com/apache/cassandra/commit/ba71289778369e71d9abbdb93cb6b91ba67f9c85] and merged into 3.11 and 4.0. dtest committed as [55c4ca8bd450b81da6eed5055981b629b55dea15|https://github.com/apache/cassandra-dtest/commit/55c4ca8bd450b81da6eed5055981b629b55dea15].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anticompaction race can leak sstables/txn,CASSANDRA-13688,13086717,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,12/Jul/17 21:52,15/May/20 08:03,14/Jul/23 05:56,11/Aug/17 17:31,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"At the top of {{CompactionManager#performAntiCompaction}}, the parent repair session is loaded, if the session can't be found, a RuntimeException is thrown. This can happen if a participant is evicted after the IR prepare message is received, but before the anticompaction starts. This exception is thrown outside of the try/finally block that guards the sstable and lifecycle transaction, causing them to leak, and preventing the sstables from ever being removed from View.compacting.",,aweisberg,bdeggleston,mbyrd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 11 17:31:19 UTC 2017,,,,,,,,,,"0|i3hg8n:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"12/Jul/17 21:58;bdeggleston;Branch below. This also fixes 2 edge cases I found while diagnosing this, adds some validation to CompactionTask, and tests them

1. If a promotion/demotion compaction fails, none of the sstables that have had their repairedAt/pendingRepair value changed prior to the failure were being moved to the correct strategy.
2. Removal of the compaction strategy for a given repair session doesn't check that the strategy is empty before removing it. If there are sstables still in the strategy, they will be left in compaction limbo until the node is bounced (or the compaction strategy is reloaded)
3. CompactionTask wasn't validating that repaired/unrepaired/pending repair sstables weren't being compacted together.

[trunk|https://github.com/bdeggleston/cassandra/tree/13688]
[utests|https://circleci.com/gh/bdeggleston/cassandra/71];;;","13/Jul/17 16:49;aweisberg;So there is the issue here https://github.com/apache/cassandra/compare/trunk...bdeggleston:13688?expand=1#diff-d4e3b82e9bebfd2cb466b4a30af07fa4R610 we talked about where it's using the wrong result value to decide whether to clean up.

Maybe run the dtests, but otherwise it looks good.;;;","13/Jul/17 19:26;bdeggleston;Pushed up fix for that [here|https://github.com/bdeggleston/cassandra/commit/994f2afe3f42811b1ebf64e3f6de9c14a3b7a28d].

Just for posterity, the problem being fixed here is that {{submitIfRunning}} will return a cancelled future if the executor is shutdown. However, we weren't checking the returned future, we were checking the submitted task, which shouldn't ever be cancelled.

[new utests| https://circleci.com/gh/bdeggleston/cassandra/73]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/125/];;;","17/Jul/17 21:00;aweisberg;First dtest run timed out. dtests ran here https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/127/
Does that materialized view test look like anything to you? I don't recall seeing it fail recently. Don't see it failing in the last 30 builds on trunk. I kicked off the dtests again.;;;","20/Jul/17 17:08;aweisberg;Will we ever get a clean run of this? https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/142/;;;","11/Aug/17 17:31;bdeggleston;Finally got a good dtest run. Committed as {{e9cc805db1133982c022657f8cab86cd24b3686f}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation typo,CASSANDRA-13686,13086466,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mshuler,andrewwuan,andrewwuan,12/Jul/17 04:50,15/May/20 08:04,14/Jul/23 05:56,05/Mar/18 14:56,3.11.3,4.0,4.0-alpha1,,,,Legacy/Documentation and Website,,,,,0,docuentation,,,,"Fix documentation typo under {quote}doc/html/cql/definitions.html#constants{quote}
and
{quote}doc/html/cql/ddl.html#the-clustering-columns{quote}",,andrewwuan,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/17 04:50;andrewwuan;fix.patch;https://issues.apache.org/jira/secure/attachment/12876759/fix.patch",,,,,,,,,,,,,,,,,,,1.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Mar 05 14:56:31 UTC 2018,,,,,,,,,,"0|i3heov:",9223372036854775807,3.10,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"05/Mar/18 14:56;mshuler;[d6982cd|https://github.com/apache/cassandra/commit/d6982cd221ae6482cbe1cb796fe73d61160a89e0] committed to cassandra-3.11 branch and merged to trunk. Thank you for the patch [~andrewwuan].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anticompaction can cause noisy log messages,CASSANDRA-13684,13086131,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,11/Jul/17 02:49,15/May/20 08:02,14/Jul/23 05:56,11/Jul/17 18:16,4.0,4.0-alpha1,,,,,,,,,,0,,,,,Anticompaction can cause unnecessarily noisy log messages,,bdeggleston,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 18:16:39 UTC 2017,,,,,,,,,,"0|i3hcn3:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"11/Jul/17 02:52;jjirsa;Trivial logging change [here|https://github.com/jeffjirsa/cassandra/tree/cassandra-13684] ;;;","11/Jul/17 02:53;jjirsa;[~krummas] or [~bdeggleston] up for review?
;;;","11/Jul/17 17:19;bdeggleston;+1;;;","11/Jul/17 18:16;jjirsa;Committed as {{ebd0aaefe54d8a1349a54d904831e1d9e5e812bf}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump should not use tool initialization,CASSANDRA-13683,13085989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cnlwsu,cnlwsu,cnlwsu,10/Jul/17 15:41,15/May/20 08:04,14/Jul/23 05:56,08/Aug/17 06:25,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"The tool initialization is not necessary for the sstabledump utility and by using it we possibly introduce a unnecessary requirement of being on the same C* instance (since we may introduce changes that require it, UDT changes can lead to dangerous assumptions) that has the schema. This also does things like introduce possibly devastating issues from having the same commitlog/sstable locations.

A good example is updating the sstable activity table, and having that flushed to commitlogs as an application outside the running C* service. This may not be on same user (ie root) which than when a postmemtable flusher after restart attempts to delete it will throw an exception and potentially kill that thread pool. One that happens commitlogs will cease to get recycled and will just burn disk space until we run out.",,cnlwsu,githubbot,jjirsa,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 06:25:09 UTC 2017,,,,,,,,,,"0|i3hbrr:",9223372036854775807,,,,,,,jjordan,,jjordan,,,Normal,,,,,,,,,,,,,,,,,,,"10/Jul/17 15:47;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/130

    Switch to client init for sstabledump for CASSANDRA-13683

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13683

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/130.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #130
    
----
commit b763eb2ac60bf35a3676c248e40c02a5737d094a
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-07-10T15:46:34Z

    Switch to client init for sstabledump for CASSANDRA-13683

----
;;;","28/Jul/17 21:47;jjordan;+1.  Looks good to me, we have definitely seen issues with tools doing things they probably shouldn't have by initializing too much.;;;","08/Aug/17 06:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/130
;;;","08/Aug/17 06:25;jjirsa;Thanks to both of you. Committed as {{9e3483f844d9db6fe2a6210550622fc2cd8aef72}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASIIndex and Clustering Key interaction,CASSANDRA-13674,13084968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,justinhwang,justinhwang,05/Jul/17 21:37,16/Apr/19 09:30,14/Jul/23 05:56,05/Jul/17 23:38,,,,,,,Feature/SASI,,,,,0,,,,,"Not sure if this is the right place to ask, but it has been a couple days and I haven't been able to figure this out.

The current setup of my table is as such:

{code}
CREATE TABLE test.user_codes (
    user_uuid text,
    code text,
    description text
    PRIMARY KEY (user_uuid, code)
);
CREATE CUSTOM INDEX user_codes_code_idx ON test.user_codes
(code) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS =
{'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 
'case_sensitive': 'false', 'mode': 'CONTAINS', 'analyzed': 'true'};

CREATE CUSTOM INDEX user_codes_description_idx ON test.user_codes
(description) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS =
{'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 
'case_sensitive': 'false', 'mode': 'CONTAINS', 'analyzed': 'true'};
{code}

I can successfully make the following call: 
{code}
SELECT * FROM user_codes WHERE user_uuid='xxxx' and description like 'Test%';
{code}
However, I can't make a similar call unless I allow filtering:
{code}
SELECT * FROM user_codes WHERE user_uuid='xxxx' and code like 'Test%';
{code}
I believe this is because the field `code` is a clustering key, but cannot figure out the proper way to set up the table such that the second call also works.",,justinhwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-07-05 21:37:14.0,,,,,,,,,,"0|i3h5hz:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair coordinator sometimes doesn't send commit messages,CASSANDRA-13673,13084912,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 18:04,15/May/20 08:04,14/Jul/23 05:56,06/Jul/17 17:33,4.0,4.0-alpha1,,,,,,,,,,0,,,,,,,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:33:43 UTC 2017,,,,,,,,,,"0|i3h55j:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"05/Jul/17 20:24;bdeggleston;If the repair executor was shutdown before the commit message is sent, none of the replicas will receive commit messages, so none of IR sessions will complete. This is because the repair complete callback shuts down the executor in question. For some reason I made the message sending happen in another thread, but since MessagingService.send* just puts stuff on a queue (or worst case starts a few messaging threads), this seems like overkill. Making the sendMessage stuff happen synchronously fixes the issue.

[trunk|https://github.com/bdeggleston/cassandra/tree/13673]
[utests|https://circleci.com/gh/bdeggleston/cassandra/63];;;","06/Jul/17 07:21;marcuse;+1;;;","06/Jul/17 17:33;bdeggleston;committed as {{3234c0704a4fef08dedc4ff78f4ded3b9226fe80}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental repair prepare phase can cause nodetool to hang in some failure scenarios,CASSANDRA-13672,13084910,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 18:00,15/May/20 08:06,14/Jul/23 05:56,06/Jul/17 17:38,4.0,4.0-alpha1,,,,,,,,,,0,,,,,Also doesn't log anything helpful,,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:38:28 UTC 2017,,,,,,,,,,"0|i3h553:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"05/Jul/17 20:33;bdeggleston;This patch improves error logging and handling of prepare phase failures. It also sets exceptions on coordinator sessions whenever a session fails, so nodetool doesn't hang.

[trunk|https://github.com/bdeggleston/cassandra/tree/13672]
[utests|https://circleci.com/gh/bdeggleston/cassandra/64];;;","06/Jul/17 07:24;marcuse;+1;;;","06/Jul/17 17:38;bdeggleston;committed as {{af37489092ca90bca336538adad02fb5ba859945}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodes compute their own gcBefore times for validation compactions,CASSANDRA-13671,13084909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 17:54,27/Apr/21 00:08,14/Jul/23 05:56,06/Jul/17 17:45,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"{{doValidationCompaction}} computes {{gcBefore}} based on the time the method is called. If different nodes start validation on different seconds, tombstones might not be purged consistently, leading to over streaming.",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:45:29 UTC 2017,,,,,,,,,,"0|i00hc3:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"05/Jul/17 18:21;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13671]
[utests|https://circleci.com/gh/bdeggleston/cassandra/62]

Patch gets nowInSec on repair coordinator side and transmits it to other nodes in validation request. Purging is then done on all replicas against a common nowInSec value.;;;","06/Jul/17 09:51;marcuse;+1;;;","06/Jul/17 17:45;bdeggleston;committed as {{9fdec0a82851f5c35cd21d02e8c4da8fc685edb2}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when starting cassandra: Unable to make UUID from 'aa' (SASI index),CASSANDRA-13669,13084807,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jasonstack,loucash,loucash,05/Jul/17 11:43,15/May/20 07:59,14/Jul/23 05:56,25/Jun/18 11:14,3.11.3,4.0,4.0-alpha1,,,,Feature/SASI,,,,,0,sasi,,,,"Recently I experienced a problem that prevents me to restart cassandra.
I narrowed it down to SASI Index when added on uuid field.



Steps to reproduce:
1. start cassandra (./bin/cassandra -f)
2. create keyspace, table, index and add data:

{noformat}
CREATE KEYSPACE testkeyspace
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} 
           AND durable_writes = true;

use testkeyspace ;

CREATE TABLE testtable (
   col1 uuid,
   col2 uuid,
   ts timeuuid,
   col3 uuid,
   PRIMARY KEY((col1, col2), ts) ) with clustering order by (ts desc);

CREATE CUSTOM INDEX col3_testtable_idx ON testtable(col3)
USING 'org.apache.cassandra.index.sasi.SASIIndex'
WITH OPTIONS = {'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer', 'mode': 'PREFIX'};

INSERT INTO testtable(col1, col2, ts, col3)
VALUES(898e0014-6161-11e7-b9b7-238ea83bd70b,
               898e0014-6161-11e7-b9b7-238ea83bd70b,
               now(), 898e0014-6161-11e7-b9b7-238ea83bd70b);
{noformat}

3. restart cassandra

It crashes with an error (sorry it's huge):
{noformat}
DEBUG 09:09:20 Writing Memtable-testtable@1005362073(0.075KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
ERROR 09:09:20 Exception in thread Thread[PerDiskMemtableFlushWriter_0:1,5,main]
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
Exception (java.lang.RuntimeException) encountered during startup: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
ERROR 09:09:20 Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730) [apache-cassandra-3.9.jar:3.9]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 6 common frames omitted
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 5 common frames omitted
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	... 3 common frames omitted
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168)
at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188)
at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 6 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 5 more
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
... 3 more
{noformat}

When I do ""nodetool flush"" I also get:
{noformat}
$  ./bin/nodetool flush
objc[35941]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin/java (0x1052a34c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10536b4e0). One of the two will be used. Which one is undefined.
error: Unable to make UUID from 'aa'
-- StackTrace --
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
{noformat}

Any ideas how to solve it?
I can keep col3 as text, I figured it out, but I already have bunch of data on production and I basically can't do anything with any of nodes, because I won't be able to start them again.

Thanks,
Lukasz","Tested on:
* macOS Sierra 10.12.5
* Ubuntu 14.04.5 LTS",adelapena,jasonstack,loucash,mbyrd,mshuler,mychal,n.v.harikrishna,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 11:14:05 UTC 2018,,,,,,,,,,"0|i3h4in:",9223372036854775807,3.11.0,3.11.1,4.0,,,,adelapena,,adelapena,,,Critical,,3.9,,,,,,,,,,,,,,,,,"05/Jul/17 15:12;loucash;Is there any other recovery other than deleting commit log? When commit log is deleted I loose (almost) all the data from a table.;;;","05/Jul/17 17:58;loucash;One way to avoid this error is to delete faulty index before stopping a node.;;;","19/Sep/17 16:37;mshuler;Reproduced on 3.11.0 release sha, cassandra-3.11 branch HEAD (commit 594f1c1d), and trunk branch HEAD (commit eb76692).;;;","14/May/18 13:35;jasonstack;The problem is that we cannot use Analyzer on UUID type..;;;","15/May/18 03:21;jasonstack;|branch|utest|
|[trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13669-trunk]|[utest|https://circleci.com/gh/jasonstack/cassandra/736?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|
|[3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13669-3.11]|[utest|https://circleci.com/gh/jasonstack/cassandra/735?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|

Changes:
 # Validate target type for SASI analyzer.
 # SASI Analyzer only supports Text/Ascii/Varchar

[~adelapena] do you mind having a look?;;;","15/May/18 12:13;adelapena;bq. [~adelapena] do you mind having a look?

Sure, I'll take a look ASAP.;;;","16/May/18 13:50;adelapena;Overall the patch looks good to me. Just a few nits:
 * I have let some minor typo fixes [here|https://github.com/adelapena/cassandra/commit/338b4abea0cd87a11102814936867ff04463d16b].
 * I think that the error message might show the CQL type, instead of the internal one. Done [here|https://github.com/adelapena/cassandra/commit/8b78595dd2eb9c5a75182ad34c93e698bd1f5acb].
* Not a big deal, but during the review I have modified the test to cover all the analyzers and data types, [here|https://github.com/adelapena/cassandra/commit/f5330feeeac1ef87a8dba2d627f241a44bdf7373]. Please take a look and merge it if you find it useful.;;;","17/May/18 07:46;jasonstack;Thanks for the review, I have included your branch. Resubmitted CI.;;;","18/May/18 00:59;jasonstack;unit tests passed, dtest failed to run due to recent changes on dtest.;;;","25/Jun/18 11:14;adelapena;Committed to 3.11 as [ea62d8862c311e3d9b64d622bea0a68d3825aa7d|https://github.com/apache/cassandra/commit/ea62d8862c311e3d9b64d622bea0a68d3825aa7d] and merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index query on partition key columns might not return partitions with only static data,CASSANDRA-13666,13084583,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bereng,blerer,blerer,04/Jul/17 15:07,14/Jul/20 21:50,14/Jul/23 05:56,20/May/20 09:35,3.0.21,3.11.7,4.0,4.0-beta1,,,Feature/2i Index,,,,,0,,,,,"The problem can be reproduced with the following test in {{3.0}}:
{code}
   @Test
    public void testIndexOnPartitionKeyWithPartitionWithoutRows() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk1 int, pk2 int, c int, s int static, v int, PRIMARY KEY((pk1, pk2), c))"");
        createIndex(""CREATE INDEX ON %s (pk2)"");

        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 2, 9, 2);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 3, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 4, 1, 1, 9, 1);
        flush();

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(3, 1, 1, 9, 1),
                   row(4, 1, 1, 9, 1));

        execute(""DELETE FROM %s WHERE pk1 = ? AND pk2 = ? AND c = ?"", 3, 1, 1);

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(3, 1, null, 9, null),  // This row will not be returned
                   row(4, 1, 1, 9, 1));
    }
{code}

The problem seems to be that the index entries for the static data are inserted with an empty clustering key. When the first {{SELECT}} is executed those entries are removed by {{CompositesSearcher::filterStaleEntries}} which consider that those entries are stales. When the second {{SELECT}} is executed the index ignore the (3, 1) partition as there is not entry for it anymore.",,bereng,blerer,jasonstack,samt,sbtourist,,,,,,,,,,,,,,,,,,,,,,"bereng opened a new pull request #559:
URL: https://github.com/apache/cassandra/pull/559


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Apr/20 12:33;githubbot;600","bereng commented on pull request #559:
URL: https://github.com/apache/cassandra/pull/559#issuecomment-618830473


   [CI run](https://circleci.com/workflow-run/bf02e9ad-a1e4-4700-bfba-daed55fcfc3c) passes utests which are the main ones to worry about. Failing dtests are due to connection issues, I tested some locally and they pass


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/20 06:35;githubbot;600","bereng commented on a change in pull request #559:
URL: https://github.com/apache/cassandra/pull/559#discussion_r414331879



##########
File path: src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
##########
@@ -231,16 +231,27 @@ private IndexEntry findEntry(Clustering clustering)
                 while (entriesIdx < entries.size())
                 {
                     IndexEntry entry = entries.get(entriesIdx++);
+                    Clustering indexedEntryClustering = entry.indexedEntryClustering;
                     // The entries are in clustering order. So that the requested entry should be the
                     // next entry, the one at 'entriesIdx'. However, we can have stale entries, entries
                     // that have no corresponding row in the base table typically because of a range
                     // tombstone or partition level deletion. Delete such stale entries.
-                    int cmp = comparator.compare(entry.indexedEntryClustering, clustering);
+                    int cmp = comparator.compare(indexedEntryClustering, clustering);
                     assert cmp <= 0; // this would means entries are not in clustering order, which shouldn't happen
                     if (cmp == 0)
                         return entry;
                     else
-                        staleEntries.add(entry);
+                    {
+                        boolean isStaticClustering = dataIter.metadata().hasStaticColumns();

Review comment:
       Might read a bit awkward, but given I am in a hot path I wanted to incur the cost to test for nulls only _if_ needed




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/20 06:39;githubbot;600","blerer commented on a change in pull request #559:
URL: https://github.com/apache/cassandra/pull/559#discussion_r426489932



##########
File path: src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
##########
@@ -231,16 +231,27 @@ private IndexEntry findEntry(Clustering clustering)
                 while (entriesIdx < entries.size())
                 {
                     IndexEntry entry = entries.get(entriesIdx++);
+                    Clustering indexedEntryClustering = entry.indexedEntryClustering;
                     // The entries are in clustering order. So that the requested entry should be the
                     // next entry, the one at 'entriesIdx'. However, we can have stale entries, entries
                     // that have no corresponding row in the base table typically because of a range
                     // tombstone or partition level deletion. Delete such stale entries.
-                    int cmp = comparator.compare(entry.indexedEntryClustering, clustering);
+                    int cmp = comparator.compare(indexedEntryClustering, clustering);
                     assert cmp <= 0; // this would means entries are not in clustering order, which shouldn't happen
                     if (cmp == 0)
                         return entry;
                     else
-                        staleEntries.add(entry);
+                    {
+                        boolean isStaticClustering = dataIter.metadata().hasStaticColumns();

Review comment:
       It is effectively a bit hard to read. I would have done something like:
   boolean hasStaticColumns = dataIter.metadata().hasStaticColumns();
   ```
   
   			// COMPACT COMPOSITE tables support null values in there clustering key but
                           // those tables do not support static columns. By consequence if a table
                           // has some static columns and all its clustering key elements are null
                           // it means that the partition exists and contains only static data 
                           if (!hasStaticColumns() || !containsOnlyNullValues(indexedEntryClustering) 
                                staleEntries.add(entry);
   ```
   What do you think?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/May/20 09:26;githubbot;600","blerer commented on a change in pull request #559:
URL: https://github.com/apache/cassandra/pull/559#discussion_r426489932



##########
File path: src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
##########
@@ -231,16 +231,27 @@ private IndexEntry findEntry(Clustering clustering)
                 while (entriesIdx < entries.size())
                 {
                     IndexEntry entry = entries.get(entriesIdx++);
+                    Clustering indexedEntryClustering = entry.indexedEntryClustering;
                     // The entries are in clustering order. So that the requested entry should be the
                     // next entry, the one at 'entriesIdx'. However, we can have stale entries, entries
                     // that have no corresponding row in the base table typically because of a range
                     // tombstone or partition level deletion. Delete such stale entries.
-                    int cmp = comparator.compare(entry.indexedEntryClustering, clustering);
+                    int cmp = comparator.compare(indexedEntryClustering, clustering);
                     assert cmp <= 0; // this would means entries are not in clustering order, which shouldn't happen
                     if (cmp == 0)
                         return entry;
                     else
-                        staleEntries.add(entry);
+                    {
+                        boolean isStaticClustering = dataIter.metadata().hasStaticColumns();

Review comment:
       It is effectively a bit hard to read. I would have done something like:
   
   ```
   boolean hasStaticColumns = dataIter.metadata().hasStaticColumns();
   
    // COMPACT COMPOSITE tables support null values in there clustering key but
    // those tables do not support static columns. By consequence if a table
    // has some static columns and all its clustering key elements are null
    // it means that the partition exists and contains only static data 
   if (!hasStaticColumns() || !containsOnlyNullValues(indexedEntryClustering) 
       staleEntries.add(entry);
   ```
   What do you think?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/May/20 09:27;githubbot;600","bereng commented on a change in pull request #559:
URL: https://github.com/apache/cassandra/pull/559#discussion_r427101980



##########
File path: src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
##########
@@ -231,16 +231,27 @@ private IndexEntry findEntry(Clustering clustering)
                 while (entriesIdx < entries.size())
                 {
                     IndexEntry entry = entries.get(entriesIdx++);
+                    Clustering indexedEntryClustering = entry.indexedEntryClustering;
                     // The entries are in clustering order. So that the requested entry should be the
                     // next entry, the one at 'entriesIdx'. However, we can have stale entries, entries
                     // that have no corresponding row in the base table typically because of a range
                     // tombstone or partition level deletion. Delete such stale entries.
-                    int cmp = comparator.compare(entry.indexedEntryClustering, clustering);
+                    int cmp = comparator.compare(indexedEntryClustering, clustering);
                     assert cmp <= 0; // this would means entries are not in clustering order, which shouldn't happen
                     if (cmp == 0)
                         return entry;
                     else
-                        staleEntries.add(entry);
+                    {
+                        boolean isStaticClustering = dataIter.metadata().hasStaticColumns();

Review comment:
       @blerer if you give me thumbs up I'll rebase, squash, forward merge and run CI before handing it over to you for committing.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/May/20 07:56;githubbot;600","bereng commented on a change in pull request #559:
URL: https://github.com/apache/cassandra/pull/559#discussion_r427101980



##########
File path: src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
##########
@@ -231,16 +231,27 @@ private IndexEntry findEntry(Clustering clustering)
                 while (entriesIdx < entries.size())
                 {
                     IndexEntry entry = entries.get(entriesIdx++);
+                    Clustering indexedEntryClustering = entry.indexedEntryClustering;
                     // The entries are in clustering order. So that the requested entry should be the
                     // next entry, the one at 'entriesIdx'. However, we can have stale entries, entries
                     // that have no corresponding row in the base table typically because of a range
                     // tombstone or partition level deletion. Delete such stale entries.
-                    int cmp = comparator.compare(entry.indexedEntryClustering, clustering);
+                    int cmp = comparator.compare(indexedEntryClustering, clustering);
                     assert cmp <= 0; // this would means entries are not in clustering order, which shouldn't happen
                     if (cmp == 0)
                         return entry;
                     else
-                        staleEntries.add(entry);
+                    {
+                        boolean isStaticClustering = dataIter.metadata().hasStaticColumns();

Review comment:
       @blerer if you give me thumbs up on the latest commit I just did I'll rebase, squash, forward merge and run CI before handing it over to you for committing.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/May/20 07:57;githubbot;600","bereng commented on pull request #559:
URL: https://github.com/apache/cassandra/pull/559#issuecomment-630841680


   @blerer squahsed, rebased & forward merged. Thx!


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/May/20 14:05;githubbot;600","bereng closed pull request #559:
URL: https://github.com/apache/cassandra/pull/559


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/May/20 15:36;githubbot;600",,0,5400,,,0,5400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bereng,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 09:35:32 UTC 2020,,,,,,,,,,"0|i3h353:",9223372036854775807,,,,,,,blerer,,blerer,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/86e1590042116b35a63a705676ecdffd5dfcde6c,,,,,,,,,Attched to PRs,,,,,"09/Apr/20 15:16;blerer;[~beobal] I reassigned the ticket to [~Bereng]. I hope you do not mind. 
My guess was that you were busy with some other tickets.;;;","09/Apr/20 15:18;samt;Not at all, thanks [~blerer] & [~Bereng] for taking it!;;;","15/Apr/20 14:54;bereng;Tested and 2.2 is not affected.;;;","24/Apr/20 09:37;bereng;CI runs in PRs:
- [cassandra 3.0|https://github.com/apache/cassandra/pull/559]
- [cassandra 3.11|https://github.com/apache/cassandra/pull/564]
- [cassandra trunk|https://github.com/apache/cassandra/pull/565];;;","20/May/20 09:35;blerer;Committed into 3.0 at 86e1590042116b35a63a705676ecdffd5dfcde6c and merged into 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeFetchMapCalculator should not try to optimise 'trivial' ranges,CASSANDRA-13664,13084534,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,04/Jul/17 12:28,15/May/20 08:00,14/Jul/23 05:56,14/Aug/17 12:35,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"RangeFetchMapCalculator (CASSANDRA-4650) tries to make the number of streams out of each node as even as possible.

In a typical multi-dc ring the nodes in the dcs are setup using token + 1, creating many tiny ranges. If we only try to optimise over the number of streams, it is likely that the amount of data streamed out of each node is unbalanced.

We should ignore those trivial ranges and only optimise the big ones, then share the tiny ones over the nodes.",,aweisberg,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/17 12:23;marcuse;Screen Shot 2017-08-14 at 14.22.23.png;https://issues.apache.org/jira/secure/attachment/12881726/Screen+Shot+2017-08-14+at+14.22.23.png",,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 14 12:35:34 UTC 2017,,,,,,,,,,"0|i3h2u7:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"04/Jul/17 12:30;marcuse;https://github.com/krummas/cassandra/commits/marcuse/opt_large

simply filters out ranges considered trivial (less than 1000 tokens for RP and M3P, no trivial ranges for BOP etc), then re-adds them once the optimisation is done.;;;","05/Jul/17 22:13;aweisberg;I still need to look at this in more detail but isn't the issue here that the streams aren't weighted and the decision made by total weight? RFMC is still pretty new to me so I need to look at exactly what is being done there.;;;","06/Jul/17 06:47;marcuse;bq. isn't the issue here that the streams aren't weighted
Yes, that would be a nicer solution. It wasn't obvious to me how to do maximum bipartite matching with weighted edges though so I went with the easy solution (I guess having an edge for each token would be a way, but that would be quite silly). Also have to say I didn't spend very much time trying to figure it out, so if you have an idea, please let me know.;;;","06/Jul/17 16:43;aweisberg;Changes makes sense to me. Can you run the dtests just to sanity check?;;;","19/Jul/17 02:50;marcuse;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/139/;;;","11/Aug/17 01:09;jjirsa;This is marked ready to commit - is it good to go? That dtest run has already expired.
;;;","11/Aug/17 12:26;marcuse;rerunning tests
https://circleci.com/gh/krummas/cassandra/68
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/176

I'll try to grab screen shots once they are finished;;;","14/Aug/17 12:24;marcuse;[^Screen Shot 2017-08-14 at 14.22.23.png] - looks like only flaky failures;;;","14/Aug/17 12:35;marcuse;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly timed kill -9 can put incremental repair sessions in an illegal state,CASSANDRA-13660,13084357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 22:07,15/May/20 07:59,14/Jul/23 05:56,06/Jul/17 17:20,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"If a node is killed after it has sent a finalize promise message to an incremental repair coordinator, but before that section of commit log has been synced to disk, it can startup with the incremental repair session in a previous state, leading the following exception:

{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition PREPARED -> FINALIZED
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:201) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.IllegalArgumentException: Invalid state transition PREPARED -> FINALIZED
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145) ~[guava-18.0.jar:?]
	at org.apache.cassandra.repair.consistent.LocalSessions.setStateAndSave(LocalSessions.java:452) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.repair.consistent.LocalSessions.handleStatusResponse(LocalSessions.java:679) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:188) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	... 7 more
{code}",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:20:19 UTC 2017,,,,,,,,,,"0|i3h1r3:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"03/Jul/17 22:18;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13660]
[utests|https://circleci.com/gh/bdeggleston/cassandra/61]

This prevents a scenario where sessions can revert to an earlier state on startup. Here, as part of the finalize propose sequence, we flush the repairs table after setting the local state to FINALIZE_PROMISED, but before responding to the coordinator. If we fail after, we'll be in a state we can get to FINALIZED to. If we fail before the sync, we won't have responded to the coordinator, so the session will end up failing anyway.;;;","04/Jul/17 06:42;marcuse;+1;;;","06/Jul/17 17:20;bdeggleston;committed as {{7df240e74f0bda9a15eff3c9de02eb0cd8771b20}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRepairManager race can cause NPE during validation,CASSANDRA-13659,13084355,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 21:59,15/May/20 08:03,14/Jul/23 05:56,06/Jul/17 00:07,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"{{getScanners}} assumes that a compaction strategy exists for the given repair session, which may not always be the case",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 00:07:21 UTC 2017,,,,,,,,,,"0|i3h1qn:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"03/Jul/17 22:02;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13659]
[utests|https://circleci.com/gh/bdeggleston/cassandra/60]

patch calls {{getOrCreate}} instead of {{get}};;;","04/Jul/17 06:40;marcuse;+1;;;","06/Jul/17 00:07;bdeggleston;committed as {{6a7fad6011dcc586344334c95aa9601477b9c5a3}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair failure recovery throwing IllegalArgumentException,CASSANDRA-13658,13084348,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 21:29,15/May/20 08:00,14/Jul/23 05:56,06/Jul/17 00:04,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FINALIZED
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:201)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FINALIZED
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)
	at org.apache.cassandra.repair.consistent.LocalSessions.setStateAndSave(LocalSessions.java:452)
	at org.apache.cassandra.repair.consistent.LocalSessions.handleStatusResponse(LocalSessions.java:679)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:188)
{code}
",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 00:04:53 UTC 2017,,,,,,,,,,"0|i3h1p3:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"03/Jul/17 21:52;bdeggleston;https://github.com/bdeggleston/cassandra/tree/13658

Added a check for noop state transitions;;;","04/Jul/17 06:40;marcuse;+1;;;","06/Jul/17 00:00;bdeggleston;utest run: https://circleci.com/gh/bdeggleston/cassandra/59;;;","06/Jul/17 00:04;bdeggleston;committed as {{5ccbebaf85b61673bb8c34b1f435d730183587ee}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range deletes in a CAS batch are ignored,CASSANDRA-13655,13084146,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jjirsa,jjirsa,jjirsa,03/Jul/17 06:40,15/May/20 08:04,14/Jul/23 05:56,11/Sep/17 16:39,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Lightweight Transactions,Legacy/CQL,,,,0,LWT,,,,Range deletes in a CAS batch are ignored ,,jay.zhuang,jjirsa,slebresne,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 16:39:06 UTC 2017,,,,,,,,,,"0|i3h0g7:",9223372036854775807,3.0.14,,,,,,slebresne,,slebresne,,,Critical,,,,,,,,,,,,,,,,,,,"05/Jul/17 22:17;jjirsa;|| Branch || Unit Tests || Dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/113/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/114/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/115/] |

[~slebresne] any chance you're interested in reviewing? 
;;;","06/Jul/17 15:04;slebresne;I can indeed have a look, but it won't be before next week, so if there is any other taker in the meantime, I won't get mad.;;;","12/Jul/17 03:02;jay.zhuang;[~jjirsa] It looks like a serious bug. Can I review? (and someone else could double review it);;;","12/Jul/17 04:24;jjirsa;[~jay.zhuang] - more eyes are never a bad thing!
;;;","12/Jul/17 05:47;jay.zhuang;Tried the patch locally, looks good to me, a few minor comments:
1. Would it be better to combine {{SliceUpdate}} and {{RowUpdate}}?
2. How about having a function for these 3 checks (like {{ModificationStatement.hasSlices()}} or a better name): [BatchStatement.java:420 | https://github.com/jeffjirsa/cassandra/commit/b9a6be6f5fc867718907d1abae124137d4f1cb45#diff-bee3b2111122530d9e0c5190e6773f62R420] and here: [ModificationStatement.java:629| https://github.com/jeffjirsa/cassandra/blob/cassandra-3.0-13655/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java#L629];;;","22/Jul/17 01:04;jjirsa;For #1, it should be an easy change, but I'll check to see if it somehow complicates things. I sort of like it the way it is, in case they eventually diverge, but perhaps that's planning for a future that may never happen.

I'm not thrilled about #2, to be honest. I'd prefer to leave them as distinct checks. 
;;;","18/Aug/17 13:22;slebresne;Sorry for the delay getting to this. It's definitively an oversight that range deletions are ignored for CAS. The patch lgtm, with just the following minor remarks:
* I'd rename {{CQL3CasRequest.SliceUpdate}} (and related {{addSliceUpdate()}} method) to {{RangeDeletion}} or something of the like. {{SliceUpdate}} is unnecessarily imprecise for what it is (I personally wouldn't merge it to {{RowUpdate}} however; having some private {{AbstractUpdate}} to combine the 3 common fields is an option but I'm honestly not sure it's worth the trouble).
* I agree with [~jay.zhuang]'s point #2 above, about moving the checks to a {{ModificationStatement.hasSlices()}} method. This is, after all, the conditions that are necessary to check before calling {{ModificationStatement.createSlices()}} so it would make it more explicit and we won't forget to change one of the 2 places if things evolve.
* It doesn't seem {{ModificationStatement.toSlices}} needs to be made package protected, it can stay private.
* There is a misplaced bracket [here|https://github.com/jeffjirsa/cassandra/commit/c2e3941352c1da31ba28f657974918ae46e81c97#diff-bee3b2111122530d9e0c5190e6773f62R462].;;;","29/Aug/17 20:33;jjirsa;Thank you both, sorry for the delay. Have pushed up a commit to each branch to address your collective comments. CircleCI is queued up, dtests are running. 

|| Branch || Unit Tests || Dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/226/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/227/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/228/] |
;;;","30/Aug/17 07:01;slebresne;If CI's happy, +1 from me, thanks.;;;","30/Aug/17 18:09;jjirsa;Unit tests are happy. DTests have the typical flakey tests + some failures that look environmental in 3.0 and 3.11, but trunk didnt run at all. The cqlsh copy dtests failed on both 3.0 and 3.11, and they both show a single failure in the last dozen runs, so that seems odd and I'll check that on the next run.

URLs will be:

Edit: rebuilding again ( no tests ran )
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/276/testReport/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/277/testReport/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/278/testReport/


;;;","11/Sep/17 16:39;jjirsa;Thank you both for your review (I've included both [~jay.zhuang] and [~slebresne] in the commit line, appreciate 2 sets of eyes).

Committed to 3.0 as {{433f24cb04dbcf74029a918ee73155f78d5f8111}} and merged up.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create meaningful toString() methods,CASSANDRA-13653,13084097,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djanand,jjirsa,jjirsa,03/Jul/17 01:51,15/May/20 08:05,14/Jul/23 05:56,05/Apr/18 12:21,4.0,4.0-alpha1,,,,,,,,,,0,lhf,low-hanging-fruit,,,"True low-hanging fruit, good for a first-time contributor:

There are a lot of classes without meaningful {{toString()}} implementations. Some of these would be very nice to have for investigating bug reports.

Some good places to start: 

- CQL3 statements (UpdateStatement, DeleteStatement, etc), QueryOptions, and Restrictions

Some packages not to worry about: 

- Deep internals that don't already have them (org.apache.cassandra.db.rows/partitions/etc)
",,agacha,dbrosius,djanand,djoshi,jasobrown,jeromatron,jjirsa,varuna,vinaykumarcse,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djanand,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 05 12:21:42 UTC 2018,,,,,,,,,,"0|i3h05b:",9223372036854775807,,,,,,,djoshi,,djoshi,,,Low,,,,,,,,,,,,,,,,,,,"03/Jul/17 04:40;dbrosius;imo, 

    @Override
    public String toString() {
        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);
    }

can be added as a snippet in your favorite IDE, and you can blow thru lots of classes, quickly. And does a pretty good job.;;;","09/Jul/17 06:05;agacha;Hi I am new to open source contribution.I am following this link as guide to contributing to Cassandra https://wiki.apache.org/cassandra/HowToContribute .I want to work on this issue but I can find assign button to assign this to myself. Do I need to request any access for this?;;;","09/Jul/17 06:13;varuna;[~agacha] You don't need to request just assign it to yourself and provide a patch. 
* Reviewers will review the patch
* Committers will commit it ;;;","25/Sep/17 05:19;djanand;Hi I am new to open source contribution as well. Though the ticket is already assigned I tired to take a stab at this. [13653-3.11|https://github.com/djanand/cassandra/tree/13653-3.11] [~jjirsa]/[~varuna]/[~dbrosius] - Could you please review ? Please let me know if there other classes which require toString() impl. I can make the required changes. Are there any other branches where I need to propagate this change ?;;;","25/Mar/18 04:54;djoshi;[~djanand] thank you for the patch. I can review it. I took a quick look at the patch and you have created it against 3.11. Typically you should start off with trunk unless it's a bug fix to an existing version.;;;","25/Mar/18 22:05;djanand;[~djoshi3] - Thank you. I have made the changes against trunk now ([13653-trunk|https://github.com/djanand/cassandra/tree/13653-trunk]). toString() changes have been made in the following packages cql3.statements , cql3.restrictions, cql3, cql3.conditions. Could you please review the same ? ;;;","26/Mar/18 22:55;zznate;[~djanand] Took a quick peek - looks like you have a format issue as the diffs for each class are essentially the whole file. Can you double check you have adhered to the formatting guidelines? [http://cassandra.apache.org/doc/latest/development/code_style.html]

Appreciate the effort, regardless. ;;;","27/Mar/18 04:51;djoshi;Hi [~djanand], other than [~zznate]'s feedback. I did a quick spot check and noticed that you're using Apache commons {{ToStringBuilder.reflectionToString}}. It uses reflection and is not the best when it comes to performance. I would err on the side of caution and use {{StringBuilder}} or concat Strings (the compiler should convert them to the builder equivalent) or at the very least use the builder version of {{ToStringBuilder}}.

See: https://antoniogoncalves.org/2015/06/30/who-cares-about-tostring-performance/;;;","27/Mar/18 05:15;jjirsa;[~djoshi3] -  [~dbrosius] specifically recommended using reflectionToString - most of the toString methods here won't be in a hot path, and really the only point of this is to make debugging slightly easier. If any of these ARE called in any sort of meaningfully hot path, they shouldn't use reflection, but we should try to verify that before proceeding.

 

 

 ;;;","27/Mar/18 05:23;djanand;[~djoshi3] - Thanks for the feedback. I went through the links provided and my reasoning is based on these two points. One, this will be used for debugging purposes and there are not many places where we would log/print the entire class. Second, if there are additions/deletions to the class variables developers need not worry about the toString() implementation. Having said that please help me understand if there will be plenty of logging in cql3.statements package in which case this might effect performance as you mentioned.
ps:sorry for the inconvenience on the patch. Will address Nate's feedback 
;;;","31/Mar/18 22:59;djanand;[~djoshi3] / [~jjirsa] / [~dbrosius] - I have incorporated the feedback with the latest trunk. Could you please take a look at it . [13653-trunk|https://github.com/djanand/cassandra/commit/4366955769dcda39fba6e4bf427045f3da9ac457]. I am open to any suggestions to make changes. Feel free to comment on the commits/per file for any specific file related issues. Please suggest any benchmarks which can be run if you think otherwise. ;;;","03/Apr/18 05:15;djoshi;Hi [~djanand] Your IDE seems to have inserted wildcard imports. For example see {{QueryOptions}}. Other than that, I think LGTM.;;;","05/Apr/18 04:39;djanand;Hi [~djoshi3] - Thank you. Removed them and squashed the [commit |https://github.com/djanand/cassandra/commit/f9d6982c9d54d07938aaec84d54c4e4b536b0ed9];;;","05/Apr/18 12:21;jasobrown;Thanks for addressing [~djoshi3]'s last comment. LGTM, as well. committed as sha {{946aaa7b06f2ccd697e31dcc15c29468da523311}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in AbstractCommitLogSegmentManager,CASSANDRA-13652,13083994,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,Fuud,Fuud,01/Jul/17 12:52,15/May/20 08:06,14/Jul/23 05:56,17/Jul/17 20:56,3.11.1,4.0,4.0-alpha1,,,,Legacy/Core,,,,,0,,,,,"AbstractCommitLogManager uses LockSupport.(un)park incorreclty. It invokes unpark without checking if manager thread was parked in approriate place. 
For example, logging frameworks uses queues and queues uses ReadWriteLock's that uses LockSupport. Therefore AbstractCommitLogManager.wakeManager can wake thread inside Lock and manager thread will sleep forever at park() method (because unpark permit was already consumed inside lock).

For examle stack traces:
{code}
""MigrationStage:1"" id=412 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.awaitAvailableSegment(AbstractCommitLogSegmentManager.java:263)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.advanceAllocatingFrom(AbstractCommitLogSegmentManager.java:237)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.forceRecycleAll(AbstractCommitLogSegmentManager.java:279)
    at org.apache.cassandra.db.commitlog.CommitLog.forceRecycleAllSegments(CommitLog.java:210)
    at org.apache.cassandra.config.Schema.dropView(Schema.java:708)
    at org.apache.cassandra.schema.SchemaKeyspace.lambda$updateKeyspace$23(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$382/1123232162.accept(Unknown Source)
    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)
    at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
    at org.apache.cassandra.schema.SchemaKeyspace.updateKeyspace(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1332)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282)
      - locked java.lang.Class@cc38904
    at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper.run(DebuggableThreadPoolExecutor.java:322)
    at com.ringcentral.concurrent.executors.MonitoredRunnable.run(MonitoredRunnable.java:36)
    at MON_R_MigrationStage.run(NamedRunnableFactory.java:67)
    at com.ringcentral.concurrent.executors.MonitoredThreadPoolExecutor$MdcAwareRunnable.run(MonitoredThreadPoolExecutor.java:114)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)

""COMMIT-LOG-ALLOCATOR:1"" id=80 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:128)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)
{code}

Solution is to use Semaphore instead of low-level LockSupport.",,aweisberg,bdeggleston,blambov,dikanggu,Fuud,githubbot,jasonstack,jay.zhuang,jjirsa,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 04:03:39 UTC 2017,,,,,,,,,,"0|i3gzif:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"01/Jul/17 12:58;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/127

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

    Change incorrect usage of LockSupport.park/unpark to Semaphore.
    Old implementation can deadlock because permit from unpark invocation can be consumed by park inside logging framework and manager thread will be parked forever.
    
    https://issues.apache.org/jira/browse/CASSANDRA-13652

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra commitlog_deadlock

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/127.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #127
    
----
commit 175be27297e9933906a9261cd8d0af3a772bff24
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-07-01T12:53:22Z

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

----
;;;","07/Jul/17 05:44;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/129

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

    PR with same result as #127.
    Instead of small fix in #127 this PR contains refactoring to make AbstractCommitLogSegmentManager code more clear. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra commitlog_deadlock_v2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/129.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #129
    
----
commit e1a695874dc24e532ae21ef627e852bf999a75f3
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-07-07T05:37:22Z

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

----
;;;","10/Jul/17 16:33;jjirsa;Anyone familiar with this code ( git history suggests maybe [~aweisberg] or [~blambov] ) interested in reviewing?



;;;","10/Jul/17 17:42;aweisberg;I thought that LockSupport was kind of low level when I first saw that, but I didn't want to bike shed. TIL. It was using a semaphore before, and I don't remember why we switched. I approve of the change I just want to make sure [~blambov] didn't have a specific reason for making the switch. ;;;","11/Jul/17 08:36;Fuud;Just to keep all things together, copy from mailing list
http://www.mail-archive.com/dev@cassandra.apache.org/msg11313.html

-------------------------------------------------------------

Hello,

I found possible deadlock in AbstractCommitLogSegmentManager. The root cause is incorrect use of LockSupport.park/unpark pair. Unpark should be invoked only if caller is sure that thread was parked in appropriate place. Otherwice permission given by calling unpark can be consumed by other structures (for example - inside ReadWriteLock).

Jira: https://issues.apache.org/jira/browse/CASSANDRA-13652

I suggest simplest solution: change LockSupport to Semaphore.
PR: https://github.com/apache/cassandra/pull/127

Also I suggest another solution with SynchronousQueue-like structure to move available segment from Manager Thread to consumers. With theese changes code became more clear and 
straightforward.

PR https://github.com/apache/cassandra/pull/129

We can not use j.u.c.SynchronousQueue because we need to support shutdown and there is only way to terminate SynchronousQueue.put is to call Thread.interrupt(). But C* uses nio and it does not expect ClosedByInterruptException during IO operations. Thus we can not interrupt Manager Thread. 
I implemented o.a.c.u.c.Transferer that supports shutdown and restart (needed for tests).
https://github.com/Fuud/cassandra/blob/e1a695874dc24e532ae21ef627e852bf999a75f3/src/java/org/apache/cassandra/utils/concurrent/Transferer.java

Also I modified o.a.c.d.c.SimpleCachedBufferPool to support waiting for free space.

Please feel free to ask any questions.

Thank you.

Feodor Bobin
fuudtorrentsru@gmail.com;;;","11/Jul/17 18:29;blambov;The {{park}} call at [line 130|https://github.com/apache/cassandra/pull/127/files#diff-85e13493c70723764c539dd222455979L130] is indeed suspect, as it does not check there's no action to perform before parking. I would solve the problem by dropping that call, which would make the usage of park/unpark conform to the specifications.;;;","11/Jul/17 18:53;aweisberg;Ah you are right it's the lack of the condition check that causes the problem. I think LockSupport park/unpark is fine it's just a thread specific semaphore bounded to a single permit.

It's technically ok if other usages of park are woken because spurious wakeups are part of the specification so other usages should handle it.;;;","11/Jul/17 19:02;aweisberg;Although TBH thinking on it why tempt fate with LockSupport.unpark without checking the thread is actually blocked on what we think it is? Let's go with the semaphore and drop the wait at line 130.;;;","12/Jul/17 08:00;Fuud;[~aweisberg]
>>It's technically ok if other usages of park are woken because spurious wakeups are part of the specification so other usages should handle it.

Yes. But such other useages will eat permit and Manager Thread will be blocked in our code.;;;","12/Jul/17 09:29;blambov;bq. Yes. But such other useages will eat permit and Manager Thread will be blocked in our code.

This is only relevant if the eating happens before checking we need to park and actually calling {{LockSupport.park()}}. This is indeed possible for the line 130 call, which is why it should be removed and the control allowed to continue to the checked park on line 146.;;;","12/Jul/17 09:34;aweisberg;Most usages of LockSupport don't introduce (as many) spurious unparks because they check to see of the thread is blocked on the specific condition not just any condition. I want to use a Semaphore so we don't introduce spurious unparks and trigger bugs in other usages of LockSupport.park. I don't see a huge advantage to working with LockSupport directly.

Yes either way blocking without checking the condition at line 130 has to go.;;;","12/Jul/17 11:42;blambov;The reason to prefer {{park}} for me is understandability of the code. This is a loop that does some work and pauses when there's no need to do any, a perfect candidate for park/unpark.

{{Semaphore}}, although applicable, implies something else. Our {{WaitQueue}} is a better alternative for this kind of application.;;;","12/Jul/17 19:12;aweisberg;WaitQueue seems even more obtuse? It's just a condition variable we are looking for so why not ReentrantLock and Condition or synchronized and wait/notify?

I mean I am fine with it. People working on Cassandra need to learn how WaitQueue works at some point. It just seems like a performance optimization to avoid locking.;;;","13/Jul/17 18:39;aweisberg;OK, how about this?

[3.11 code|https://github.com/apache/cassandra/compare/trunk...aweisberg:13652-3.11?expand=1], [utests|https://circleci.com/gh/aweisberg/cassandra/335], [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/124/]

[~Fuud] are you ok with me rewriting your patch this way?;;;","14/Jul/17 08:48;Fuud;Yes. Seems good.;;;","17/Jul/17 08:17;blambov;This looks good to me.;;;","17/Jul/17 20:56;aweisberg;Committed as [eb717f154bd24453273d7175006fdef75e5724c2|https://github.com/apache/cassandra/commit/eb717f154bd24453273d7175006fdef75e5724c2]. Thanks.;;;","30/Aug/17 05:42;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/127
  
    Hi @Fuud - eb717f154bd24453273d7175006fdef75e5724c2 has been merged and CASSANDRA-13652 is closed. Is this PR still needed? If not, can you close it (the committers are unable to close it on your behalf)? 
;;;","31/Aug/17 04:04;githubbot;Github user Fuud commented on the issue:

    https://github.com/apache/cassandra/pull/127
  
    @jeffjirsa Sorry, my fault. 
;;;","31/Aug/17 04:04;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/127
;;;","11/Sep/17 04:03;githubbot;Github user Fuud commented on the issue:

    https://github.com/apache/cassandra/pull/129
  
    eb717f1 has been merged and CASSANDRA-13652 is closed. 
;;;","11/Sep/17 04:03;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/129
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Large amount of CPU used by epoll_wait(.., .., .., 0)",CASSANDRA-13651,13083792,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,burmanm,iksaif,iksaif,30/Jun/17 14:03,15/May/20 08:06,14/Jul/23 05:56,24/Aug/18 15:12,4.0,4.0-alpha1,,,,,,,,,,0,pull-request-available,,,,"I was trying to profile Cassandra under my workload and I kept seeing this backtrace:
{code}
epollEventLoopGroup-2-3 State: RUNNABLE CPU usage on sample: 240ms
io.netty.channel.epoll.Native.epollWait0(int, long, int, int) Native.java (native)
io.netty.channel.epoll.Native.epollWait(int, EpollEventArray, int) Native.java:111
io.netty.channel.epoll.EpollEventLoop.epollWait(boolean) EpollEventLoop.java:230
io.netty.channel.epoll.EpollEventLoop.run() EpollEventLoop.java:254
io.netty.util.concurrent.SingleThreadEventExecutor$5.run() SingleThreadEventExecutor.java:858
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run() DefaultThreadFactory.java:138
java.lang.Thread.run() Thread.java:745
{code}

At fist I though that the profiler might not be able to profile native code properly, but I wen't further and I realized that most of the CPU was used by {{epoll_wait()}} calls with a timeout of zero.

Here is the output of perf on this system, which confirms that most of the overhead was with timeout == 0.

{code}
Samples: 11M of event 'syscalls:sys_enter_epoll_wait', Event count (approx.): 11594448
Overhead  Trace output                                                                                                                                                                                           ◆
  90.06%  epfd: 0x00000047, events: 0x7f5588c0c000, maxevents: 0x00002000, timeout: 0x00000000                                                                                                                   ▒
   5.77%  epfd: 0x000000b5, events: 0x7fca419ef000, maxevents: 0x00001000, timeout: 0x00000000                                                                                                                   ▒
   1.98%  epfd: 0x000000b5, events: 0x7fca419ef000, maxevents: 0x00001000, timeout: 0x000003e8                                                                                                                   ▒
   0.04%  epfd: 0x00000003, events: 0x2f6af77b9c00, maxevents: 0x00000020, timeout: 0x00000000                                                                                                                   ▒
   0.04%  epfd: 0x0000002b, events: 0x121ebf63ac00, maxevents: 0x00000040, timeout: 0x00000000                                                                                                                   ▒
   0.03%  epfd: 0x00000026, events: 0x7f51f80019c0, maxevents: 0x00000020, timeout: 0x00000000                                                                                                                   ▒
   0.02%  epfd: 0x00000003, events: 0x7fe4d80019d0, maxevents: 0x00000020, timeout: 0x00000000
{code}

Running this time with perf record -ag for call traces:
{code}
# Children      Self       sys       usr  Trace output                                                                        
# ........  ........  ........  ........  ....................................................................................
#
     8.61%     8.61%     0.00%     8.61%  epfd: 0x000000a7, events: 0x7fca452d6000, maxevents: 0x00001000, timeout: 0x00000000
            |
            ---0x1000200af313
               |          
                --8.61%--0x7fca6117bdac
                          0x7fca60459804
                          epoll_wait

     2.98%     2.98%     0.00%     2.98%  epfd: 0x000000a7, events: 0x7fca452d6000, maxevents: 0x00001000, timeout: 0x000003e8
            |
            ---0x1000200af313
               0x7fca6117b830
               0x7fca60459804
               epoll_wait
{code}

That looks like a lot of CPU used to wait for nothing. I'm not sure if pref reports a per-CPU percentage or a per-system percentage, but that would be still be 10% of the total CPU usage of Cassandra at the minimum.

I went further and found the code of all that: We schedule a lot of {{Message::Flusher}} with a deadline of 10 usec (5 per messages I think) but netty+epoll only support timeouts above the milliseconds and will convert everything bellow to 0.

I added some traces to netty (4.1):
{code}
diff --git a/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java b/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
index 909088fde..8734bbfd4 100644
--- a/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
+++ b/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
@@ -208,10 +208,15 @@ final class EpollEventLoop extends SingleThreadEventLoop {
         long currentTimeNanos = System.nanoTime();
         long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos);
         for (;;) {
-            long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L;
+            long timeoutNanos = selectDeadLineNanos - currentTimeNanos + 500000L;
+            long timeoutMillis =  timeoutNanos / 1000000L;
+            System.out.printf(""timeoutNanos: %d, timeoutMillis: %d | deadline: %d - now: %d | hastask: %d\n"",
+                    timeoutNanos, timeoutMillis,
+                    selectDeadLineNanos, currentTimeNanos, hasTasks() ? 1 : 0);
             if (timeoutMillis <= 0) {
                 if (selectCnt == 0) {
                     int ready = Native.epollWait(epollFd.intValue(), events, 0);
+                    System.out.printf(""ready: %d\n"", ready);
                     if (ready > 0) {
                         return ready;
                     }
{code}

And this gives :

{code}
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782341816510 - now: 2001781341816510 | hastask: 0
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782342087239 - now: 2001781342087239 | hastask: 0
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782342166947 - now: 2001781342166947 | hastask: 0
timeoutNanos: 508459, timeoutMillis: 0 | deadline: 2001781342297987 - now: 2001781342289528 | hastask: 0
ready: 0
timeoutNanos: 508475, timeoutMillis: 0 | deadline: 2001781342357719 - now: 2001781342349244 | hastask: 0
ready: 0
timeoutNanos: 509327, timeoutMillis: 0 | deadline: 2001781342394822 - now: 2001781342385495 | hastask: 0
ready: 0
timeoutNanos: 509339, timeoutMillis: 0 | deadline: 2001781342430192 - now: 2001781342420853 | hastask: 0
ready: 0
timeoutNanos: 509510, timeoutMillis: 0 | deadline: 2001781342461588 - now: 2001781342452078 | hastask: 0
ready: 0
timeoutNanos: 509493, timeoutMillis: 0 | deadline: 2001781342495044 - now: 2001781342485551 | hastask: 0
ready: 0
{code}

The nanosecond timeout all come from {{eventLoop.schedule(this, 10000, TimeUnit.NANOSECONDS);}} in {{Message::Flusher}}.

Knowing that, I'm not sure what would be best to do, and I have a hard time understanding Message::Flusher, but to me it looks like trying to schedule less tasks would probably help and I didn't think anything obvious that could be done with netty.

Changing {{if (++runsWithNoWork > 5)}} to 2 seems to help a little bit, but that isn't really significant.",,andrew.tolbert,benedict,burmanm,githubbot,iksaif,jasobrown,jeromatron,jjirsa,norman,philipthompson,rha,weideng,,,,,,,,,,,,,,,"iksaif commented on pull request #151: [CASSANDRA-13651]: Reduce epoll/timerfd CPU usage
URL: https://github.com/apache/cassandra/pull/151
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 08:53;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 16:52;iksaif;cpu-usage.png;https://issues.apache.org/jira/secure/attachment/12879868/cpu-usage.png",,,,,,,,,,,,,,,,,,,1.0,burmanm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 15:12:24 UTC 2018,,,,,,,,,,"0|i3gy9r:",9223372036854775807,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"30/Jun/17 18:33;iksaif;Things to check or try (for me):
* io.netty.eventLoopThreads
* Check if we could use the same eventloop instead of starting two
* Create a custom SelectStrategy that skips looking at fds if there is a scheduled task happening in a few microseconds
* Try to understand why Message::Flusher currently works this way;;;","30/Jun/17 20:00;jasobrown;/cc [~norman];;;","01/Jul/17 04:54;iksaif;Also check:
* https://github.com/netty/netty/issues/1759
* https://gist.github.com/jadbaz/47d98da0ead2e71659f343b14ef05de6
* Benchmark batching vs. stupid writeAndFlush()
* It's unclear why sending the response is done in the flusher right now
* https://github.com/spotify/netty-batch-flusher;;;","03/Jul/17 09:20;iksaif;I cooked a patch to use spotify's netty-batch-flusher instead of the current flusher. Here are some results:

{code}
normal:

Results:
Op rate                   :    4,220 op/s  [insert: 4,220 op/s]
Partition rate            :    4,220 pk/s  [insert: 4,220 pk/s]
Row rate                  :   41,851 row/s [insert: 41,851 row/s]
Latency mean              :    0.2 ms [insert: 0.2 ms]
Latency median            :    0.2 ms [insert: 0.2 ms]
Latency 95th percentile   :    0.2 ms [insert: 0.2 ms]
Latency 99th percentile   :    0.3 ms [insert: 0.3 ms]
Latency 99.9th percentile :    0.4 ms [insert: 0.4 ms]
Latency max               :   65.5 ms [insert: 65.5 ms]
Total partitions          :    100,000 [insert: 100,000]
Total errors              :          0 [insert: 0]
Total GC count            : 6
Total GC memory           : 3.473 GiB
Total GC time             :    0.4 seconds
Avg GC time               :   60.0 ms
StdDev GC time            :    5.1 ms
Total operation time      : 00:00:23
{code}

{code}
batched:

Results:
Op rate                   :    4,344 op/s  [insert: 4,344 op/s]
Partition rate            :    4,344 pk/s  [insert: 4,344 pk/s]
Row rate                  :   43,121 row/s [insert: 43,121 row/s]
Latency mean              :    0.2 ms [insert: 0.2 ms]
Latency median            :    0.2 ms [insert: 0.2 ms]
Latency 95th percentile   :    0.2 ms [insert: 0.2 ms]
Latency 99th percentile   :    0.3 ms [insert: 0.3 ms]
Latency 99.9th percentile :    0.4 ms [insert: 0.4 ms]
Latency max               :   63.4 ms [insert: 63.4 ms]
Total partitions          :    100,000 [insert: 100,000]
Total errors              :          0 [insert: 0]
Total GC count            : 6
Total GC memory           : 3.467 GiB
Total GC time             :    0.4 seconds
Avg GC time               :   60.0 ms
StdDev GC time            :    3.3 ms
Total operation time      : 00:00:23
{code}

So slightly more QPS, but more interestingly, the epoll thread now uses ~4 times less CPU. I'll try to do a full scale benchmark on a bigger workload with 3 nodes tomorrow.

Patch at https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk;;;","04/Jul/17 11:28;iksaif;I ran tests on a 3 node cluster, I can confirm that not using scheduled tasks and using a simpler batcher removes all the {{epoll_wait(..., 0)}} calls. This reduces the CPU used by epoll threads.
I need to take more time do check how efficient the batching still is, and compare the context switchs with and without it.;;;","25/Jul/17 12:35;iksaif;Latest patch (https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk)  tested with an actual workload, I attached the screenshot.
Looks like we can get ~2% of CPU back with -Dcassandra.netty_flush_delay_nanoseconds=0 and some more with -Dio.netty.eventLoopThreads=6. This doesn't not seem to affect latency

!Screenshot (5).png|thumbnail!;;;","01/Aug/17 16:52;iksaif;!cpu-usage.png!

Almost ~8% of CPU saving after updating all three nodes.;;;","02/Aug/17 05:18;jjirsa;That's really interesting. [~norman] - if you get a few minutes to glance, always appreciate your thoughts here. 

Also, [~iksaif] - can you share a bit more detail on your test? Is it just stress? Have you tested with many connected clients? Any immediate reason you suspect that CPU went down (significantly?), but throughput remains unchanged (do you see the next bottleneck, the op rate on that stress run looks pretty low, even for a single machine cluster?);;;","02/Aug/17 05:32;norman;Sorry for the late response.. didn't see the mention :(

So yes netty uses `epoll_wait` which only supports milli-seconds resulution so everything smaller then this will just cause `epoll_wait(...)` be called with a `0` and so a non-blocking check of ready fds. What we could do in our native transport implementation is that we make use of `timerfd` [1] to schedule timeouts but again this would only work for the case of using the native epoll transport and not when you use the nio transport (which works on all OS). So I think what you really want to do is have timeouts of >= 1ms. 

Comments and ideas welcome :)
;;;","02/Aug/17 06:50;iksaif;[~jjirsa]: The cassandra-stress report is for https://github.com/criteo/biggraphite/blob/master/tools/stress/biggraphite.yaml. The bottleneck here was the lack of parallelization on the client I guess.
The screenshot is the actual workload of BigGraphite: 3 nodes, 100 connected clients.


[~norman]: Both timeouts of 1ms or no timeouts would achieve the same thing I guess. I tested with no timeout as it implied less changes to the actual logic (see https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk). Currently (I believe that) the message itself it written after the delay, increasing the timeout will increase the latency of every operations. In my test I simply disable the timeout and schedule the flush task as soon as I can, this doesn't reduce the opportunities for batching that much if you keep the number of epoll threads low.;;;","02/Aug/17 13:47;jjirsa;[~iksaif] - this is not a review (and I think there are better people around to review this than myself, like either Norman, Jason, Jake or Ariel), but I'm unable to get a clean test run with your patch on trunk with either netty 4.0.44 or 4.1.13. You mention in the first post that you added some traces to 4.1 - can you confirm which version of netty you tested with? Can you also confirm whether or not {{ant test -Dtest.name=NativeTransportServiceTest}} passes cleanly for you?;;;","02/Aug/17 14:39;norman;Also for the record Scott Mitchell (another core netty dev) just created a PR in netty to support micro seconds timeouts when using the native epoll transport:

https://github.com/netty/netty/pull/7042

That said I still need to review it in detail and its not merged yet.;;;","02/Aug/17 14:52;iksaif;https://github.com/iksaif/cassandra/commit/c05f2eef6abc8066b69e50dc5025f17e17871f0c should fix the test.

I'm running with 4.0.44. The production test is using 3.11 as a base but I'm able to start trunk on my dev machine.
;;;","03/Aug/17 06:57;iksaif;Using timerfd is something that I looked at, but I though that it would be easier to just change the code in Cassandra for now. I'll be out in the next three weeks but I'll definitivement try a patched version of netty when I'm back.;;;","18/Aug/17 18:39;norman;[~iksaif] FYI we will merge the change to use timerfd today and so it will be part of the next Netty release. That said I think what you suggested (changing the Cassandra code) may make more sense in general.;;;","28/Aug/17 07:16;iksaif;Great ! I'm good with either bumping netty to this version or merging my patch, [~jjirsa] what do you think ?;;;","14/Sep/17 12:52;iksaif;Ping, anything against https://github.com/iksaif/cassandra/commits/cassandra-13651-trunk ? If not I'll send a proper pull request.;;;","14/Sep/17 22:50;jasobrown;[~iksaif] I just took a read through your patch.  Here's my thoughts:

- The refactoring of {{Dispatcher#FlushItem}} to {{DelayedFlusher}} is unnecessary. There's nothing functionally incorrect with the existing implementation. (I'll posit that the nested class within a nested class is ... non-standard.)
- {{NativeTransportService#defaultWorkerGroup}} is confusing. Why not just use the existing {{NativeTransportService#workerGroup}}?
- {{NativeTransportService#getDefaultWorkerGroup}} isn't thread safe, and so you could end up with mutiple {{EventLoopGroup}}s group created - although this maybe not a problem in practice.
- Scott's netty patch landed in 4.1.15. Let's update trunk for that.
- Minor nit: I'd prefer to rename {{netty_flush_delay_nanoseconds}} to something like {{native_transport_flush_delay_nanoseconds}} to make it explicit that this property will only affect the netty use in the native transport and not the internode messaging or streaming.

Honestly, I think the patch can be reduced to simply adding the {{FLUSH_DELAY}} logic to [{{Message.Dispatcher.Flusher#run}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Message.java#L488], and updating netty. Or possibly just update netty and call it a day. [~iksaif] let me know if you can run a test with just the updated netty library.;;;","15/Sep/17 06:25;iksaif;Ok, here is the plan (for myself):
* Separate the ""use only one worker group"" patch. It's useful because it creates less threads, but isn't really directly related to this.
* Update netty to 4.1.15 on our setup (without -Dnetty_flush_delay_nanoseconds) and see the effects
* Set -Dnetty_flush_delay_nanoseconds and see the effects. Depending on the results, propose a simpler version of the patch.
;;;","15/Sep/17 18:06;jasobrown;[~iksaif] This all sounds great! A quick note: if you are testing on something lower than trunk, I think you can use netty 4.0.51 as that will also have Scott's patch (and will be generally more compatible with everything pre-CASSANDRA-8457);;;","18/Sep/17 13:09;githubbot;GitHub user iksaif opened a pull request:

    https://github.com/apache/cassandra/pull/151

    [CASSANDRA-13651]: Reduce epoll/timerfd CPU usage

    - Bump Netty to 4.1.15
    - Add a setting to schedule flushes immediatly  

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/iksaif/cassandra cassandra-13651-trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/151.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #151
    
----
commit 969dcee971242ff08a2ba1baf9e3139935e85ee8
Author: Corentin Chary <c.chary@criteo.com>
Date:   2017-09-18T12:12:46Z

    Bump netty to 4.1.15
    
    This is to take advantages of the improvements from https://github.com/netty/netty/pull/7042

commit 594ef9eee5ad853eaec4234f9a11bc12e030ae6c
Author: Corentin Chary <c.chary@criteo.com>
Date:   2017-09-18T12:26:35Z

    CASSANDRA-13651: Reduce CPU used by epoll_wait() / timerfd_create()
    
    By setting -Dcassandra.native_transport_flush_delay_nanoseconds=0 one can
    schedule the flush() immediatly which will be more efficient when
    using epoll() on Linux by reducing the number of calls to epoll_wait().
    
    This is simmilarly more efficient on version of netty that use timerfd
    to get timeouts with microsecond resolution when calling epoll().
    
    On those platforms this can save up to 10% of CPU.

----
;;;","18/Sep/17 13:11;iksaif;Results: the calls to timerfd end up costing almost as much as what epoll_wait() before. It is still more efficient to execute instead of schedule.

I added a patch to bump netty to 4.1.15, and a way simpler version of my previous patch that allows one to configure the task delay.;;;","18/Sep/17 21:45;jasobrown;Cool. Thanks for the patch. The code itself is trivial (adding a configuration property), however I liked what you had before: 

{code}
if (FLUSH_DELAY > 0)
    eventLoop.schedule(this, FLUSH_DELAY, TimeUnit.NANOSECONDS);
else
    eventLoop.execute(this);
{code}

That way if the {{FLUSH_DELAY}} is 0, skip creating a scheduled task in netty and just submit it for execution. wdyt?

I'd like to run this in my test environment to verify if there's any change on my side. It may take a few days due to other on-going work.;;;","19/Sep/17 08:18;iksaif;Thanks for double-checking that, I pushed the wrong version. This should now be fixed.;;;","23/Nov/17 08:47;iksaif;ping ?;;;","09/May/18 11:37;burmanm;I've done a follow up patch to this also, available here:

[https://github.com/burmanm/cassandra/commit/eb8af9a47b00a836fbb4e155aa64024250470ba7]

I could not find a use-case where (at least using epoll) the timer would be beneficial. It's going to eat CPU all the time in any case, and I verified this by creating a small looping C program that did the same thing using epoll & timerfd directly (around ~4% CPU usage from the C program and ~6% from the Netty version when both are only rescheduling the job). So, I did the following to Flusher:
 * Removed automatic rescheduling by the Flusher
 * Instead, after every write we schedule the flush during the next eventloop invocation. If multiple writes are successful before that, they're flushed together

In my tests, this both improves the throughput slightly, but more importantly, it also decreases latency. On Azure D8s_V3 instances, the latency is decreased by 0.3ms with mediocre and more punishing loads. Obviously, the profiler shows that we spent more time in the writevAddresses(), but then EpollWait0 is reduced to next to nothing during stress run.

At this point I would suggest removing the batching completely and then return to this issue once we can sanely batch the work of encode also (and make that pipeline then flush at the end). 

 ;;;","21/Aug/18 15:51;benedict;So, I assume we're now defaulting to epoll in most cases, and this behaviour comes from a period where this wasn't the default (and was probably poorly justified at the time - AFAICR we used to benchmark with only a single connection, where this behaviour would be more beneficial).

It's a shame we no longer have any standard benchmarking tools for the project, but it seems we have multiple data points demonstrating a win (or no loss), and the code is simpler after the patch.

So, I'm +1 on the patch.  I will get a circleci run going shortly.;;;","21/Aug/18 15:54;norman;I am no committer but the netty project lead so from the point of view of netty usage I am also +1 on this.;;;","21/Aug/18 15:58;jjirsa;There are two patches here, [~iksaif]'s patch and [~burmanm]'s follow-patch, which did you each +1?
;;;","21/Aug/18 16:06;benedict;The latest, i.e. [~burmanm]'s.

It simplifies the code, and as I say, I'm not sure the original patch was as well justified as we thought - our benchmarking methodology was flawed at the time (using a single connection).

I suppose arguably there's value in maintaining the current behaviour for those users with a single connection and without epoll, but since epoll is now the default it's probably better to improve code clarity.

I'm open to dispute, of course, in which case we can revisit Corentin's patch (or try to reproduce the old benchmarks and see what we might be losing in modern C* in the worst case).  In this case, I would probably prefer to have a LegacyFlusher and a Flusher - the latter the cleaned code contributed by [~burmanm], the former the old unadulterated code, and to select between them based on the config property (with a default being determined by epoll usage status);;;","24/Aug/18 08:39;burmanm;I added a commit https://github.com/burmanm/cassandra/commit/681ab256e0af50827169d88931fadf89b3cd3770 to tree https://github.com/burmanm/cassandra/tree/netty_flusher which allows selecting either the new behavior (default) or the old one. I'm open to better naming and description ;) I've made the new behavior enabled for default now, or do you still wish to have the old behavior as default for non-epoll cases?;;;","24/Aug/18 08:50;benedict;Looks good.  The user-visible property should probably be adjacent to the native_transport settings, though, and prefixed by the same name; perhaps be named something like {{native_transport_flush_in_batches_legacy}};;;","24/Aug/18 10:21;burmanm;Sounds a lot better name. Changed to that, rebased to current trunk: https://github.com/burmanm/cassandra/commit/30e4dda2d3c3e6124fe118d61da798d216ad20b9;;;","24/Aug/18 15:12;benedict;Committed to [4.0|https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec] with some small edits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql_tests:SlowQueryTester.local_query_test and cql_tests:SlowQueryTester.remote_query_test failed on trunk,CASSANDRA-13650,13083734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasonstack,jasonstack,jasonstack,30/Jun/17 10:12,15/May/20 08:01,14/Jul/23 05:56,06/Jul/17 13:34,4.0,4.0-alpha1,,,,,Legacy/Distributed Metadata,,,,,0,,,,,"cql_tests.py:SlowQueryTester.local_query_test failed on trunk
cql_tests.py:SlowQueryTester.remote_query_test failed on trunk
SHA: fe3cfe3d7df296f022c50c9c0d22f91a0fc0a217


It's due to the dtest unable to find {{'SELECT \* FROM ks.test1'}} pattern from log.
but in the log, following info is showed: 
{{MonitoringTask.java:173 - 1 operations were slow in the last 10 msecs: <SELECT val FROM ks.test1 LIMIT 5000>, time 102 msec - slow timeout 10 msec}}

ColumnFilter.toString() should return {{*}}, but return normal column {{val}} instead ",,ifesdjeen,jasonstack,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 13:32:31 UTC 2017,,,,,,,,,,"0|i3gxwv:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Low,,,,,,,,,,,,,,,,,,,"01/Jul/17 10:08;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13650] | [junit|https://circleci.com/gh/jasonstack/cassandra/90]  | {{bootstrap_test.py:TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}} failed on trunk..| 

1. refactor column-filter to allow {{queried = null}} to represent {{wildcard (*)}} queries. (back to what the java comment described.);;;","01/Jul/17 12:16;jasonstack;[~ifesdjeen] could you review it? ;;;","06/Jul/17 13:32;ifesdjeen;Thank you for the patch! Committed with a minor change: took the {{queried == null}} and put it to the ternary operator instead of the outer {{if}} (the way it used to be in pre-13004 code).

Committed to trunk with [9359e1e977361774daf27e80112774210e55baa4|https://github.com/apache/cassandra/commit/9359e1e977361774daf27e80112774210e55baa4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade metrics to 3.1.5,CASSANDRA-13648,13083632,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,29/Jun/17 23:34,15/May/20 08:05,14/Jul/23 05:56,27/Jul/17 23:52,4.0,4.0-alpha1,,,,,Dependencies,,,,,0,,,,,"GH PR #123 indicates that metrics 3.1.5 will fix a reconnect bug:

https://github.com/apache/cassandra/pull/123
",,githubbot,jay.zhuang,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13361,,,,CASSANDRA-12924,,,,,,,,,,,,,,,,,,,,,"20/Jul/17 08:08;spod;metrics-core-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878110/metrics-core-3.1.5.jar.asc","20/Jul/17 08:08;spod;metrics-jvm-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878111/metrics-jvm-3.1.5.jar.asc","20/Jul/17 08:08;spod;metrics-logback-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878112/metrics-logback-3.1.5.jar.asc",,,,,,,,,,,,,,,,,3.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/cassandra/pull/123,https://github.com/apache/cassandra/pull/123,,,,,,,,,,9223372036854775807,,,Thu Jul 27 23:52:26 UTC 2017,,,,,,,,,,"0|i3gxa7:",9223372036854775807,,,,,,,spod,,spod,,,Normal,,,,,,,,,,,,,,,,,,,"29/Jun/17 23:36;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/123
  
    Created upstream issue https://issues.apache.org/jira/browse/CASSANDRA-13648
;;;","29/Jun/17 23:45;jjirsa;Patch on the attached GH PR

Tests queued but not yet complete: 
Circle CI: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13648 
DTests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/109/;;;","18/Jul/17 10:55;spod;Patch is missing the actual jar files in lib. The build.xml dependencies are just for creating the project pom, if I remember correctly.
;;;","18/Jul/17 17:29;jjirsa;Thanks [~spodxx@gmail.com] - just rebased and force pushed with new metrics-core, metrics-jvm, and metrics-logback jars (3.1.5)


;;;","18/Jul/17 22:18;jjirsa;New dtest run started @ https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/134/
CircleCI run green @ https://circleci.com/gh/jeffjirsa/cassandra/250
;;;","20/Jul/17 08:01;spod;Looks like dtest needs a rebuild. 

I also think you missed to move the corresponding lib/license files.

Did you omit the CHANGES.txt update intentionally? 
;;;","22/Jul/17 00:19;jjirsa;CHANGES was intentionally omitted because it always conflicts. The licenses weren't intentional. Changed that, kicked off new run here.

Also changed you to reviewer since you did most of the real work. 

Dtest @ https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/146/
;;;","24/Jul/17 14:57;spod;Patch looks good.;;;","27/Jul/17 23:52;jjirsa;Committed as {{1e7c4b9c0584b5f63d121a5c37e0fc1e352e6108}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bind parameters of collection types are not properly validated,CASSANDRA-13646,13083466,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,29/Jun/17 13:53,15/May/20 08:04,14/Jul/23 05:56,07/Jul/17 16:18,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,CQL/Interpreter,,,,,0,,,,,"It looks like C* is not validating properly the bind parameters for collection types. If an element of the collection is invalid the value will not be rejected and might cause an Exception later on.
The problem can be reproduced with the following test:
{code}
    @Test
    public void testInvalidQueries() throws Throwable
    {
        createTable(""CREATE TABLE %s (k int PRIMARY KEY, s frozen<set<tuple<int, text, double>>>)"");
        execute(""INSERT INTO %s (k, s) VALUES (0, ?)"", set(tuple(1,""1"",1.0,1), tuple(2,""2"",2.0,2)));
    }
{code}

The invalid Tuple will cause an ""IndexOutOfBoundsException: Index: 3, Size: 3""",,adelapena,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 11:51:37 UTC 2017,,,,,,,,,,"0|i3gw9r:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,2.2.0,,,,,,,,,,,,,,,,,"03/Jul/17 11:21;blerer;When collections are validated the collection elements are validated using the {{Serializer::validate}} method. As Tuples and UDFs use a {{BytesSerializer}} the validate method does not check if the bytes are valid for the expected Tuple or UDT.
The patches add some specific serializers for {{Tuples}} or {{UDTs}} that override the {{Serializer::validate}} method.
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...blerer:13646-2.2]||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:13646-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13646-3.11]||[trunk|https://github.com/apache/cassandra/compare/cassandra-trunk...blerer:trunk]||
I ran the patches on our internal CI and the failing tests seems unrelated to the patches.
;;;","03/Jul/17 15:06;blerer;[~adelapena] Could you review?;;;","03/Jul/17 15:29;adelapena;[~blerer], sure. Hopefully I'll have it ready in a few days.;;;","05/Jul/17 15:40;adelapena;Excellent patch, +1;;;","07/Jul/17 11:51;blerer;Thanks for the review :-).

Committed into 2.2 at b77e11cfd51ddb0f3ac07530399abe999df0573e and merged into 3.0, 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
converting expired ttl cells to tombstones causing unnecessary digest mismatches,CASSANDRA-13643,13082943,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,27/Jun/17 21:02,15/May/20 08:03,14/Jul/23 05:56,14/Jul/17 17:55,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,,0,,,,,"In [{{AbstractCell#purge}}|https://github.com/apache/cassandra/blob/26e025804c6777a0d124dbc257747cba85b18f37/src/java/org/apache/cassandra/db/rows/AbstractCell.java#L77]  , we convert expired ttl'd cells to tombstones, and set the the local deletion time to the cell's expiration time, less the ttl time. Depending on the timing of the purge, this can cause purge to generate tombstones that are otherwise purgeable. If compaction for a row with ttls isn't at the same state between replicas, this will then cause digest mismatches between logically identical rows, leading to unnecessary repair streaming and read repairs.",,bdeggleston,christianmovi,jjirsa,marcuse,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13561,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 17:39:59 UTC 2017,,,,,,,,,,"0|i3gt1r:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Low,,,,,,,,,,,,,,,,,,,"27/Jun/17 21:11;bdeggleston;Patch here: https://github.com/bdeggleston/cassandra/tree/13643

Could you take a look at this [~slebresne]? I've just called purge on the created tombstone, which I wouldn't think would cause any problems.;;;","27/Jun/17 23:02;bdeggleston;unit test run here: https://circleci.com/gh/bdeggleston/cassandra/55;;;","03/Jul/17 13:34;slebresne;Sound reasonable, +1 (though would be nice to have a dtest run as well to be sure). ;;;","03/Jul/17 14:34;slebresne;As an aside, running from memory but I'm reasonably confident this is a regression from 3.0 in that we use to do the transformation of 'expired cell' to 'equivalent tombstone' in the deserialization code, with the purge code running afterwards, so that if the generated tombstone is directly purgeable, it would be purged right away. So I suppose the argument can be made that this is a bug and hence putting it in 3.0/3.11. I don't have a huge preference either way.;;;","05/Jul/17 20:37;bdeggleston;Sound good, I'll apply to them as well. Thanks.

[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/112/];;;","14/Jul/17 17:55;bdeggleston;Committed as {{a033f51651e1a990adca795f92d683999c474151}};;;","01/Aug/17 17:39;jjirsa;Missing a fixver btw.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly evict pstmts from prepared statements cache,CASSANDRA-13641,13082875,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,27/Jun/17 16:40,16/Apr/19 09:30,14/Jul/23 05:56,28/Jun/17 19:20,3.11.1,,,,,,,,,,,0,,,,,Prepared statements that are evicted from the prepared statements cache are not removed from the underlying table {{system.prepared_statements}}. This can lead to issues during startup.,,blerer,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 28 19:20:55 UTC 2017,,,,,,,,,,"0|i3gsmn:",9223372036854775807,,,,,,,blerer,,blerer,,,Low,,3.10,,,,,,,,,,,,,,,,,"27/Jun/17 17:46;snazy;||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13641-evict-pstmt-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13641-evict-pstmt-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-trunk-dtest/lastSuccessfulBuild/]
;;;","28/Jun/17 07:35;blerer;Thanks for the patches, they look good to me.;;;","28/Jun/17 19:20;snazy;Thanks for the quick review!

Committed as [9562b9b69e08b84ec1e8e431a846548fa8a83b44|https://github.com/apache/cassandra/commit/9562b9b69e08b84ec1e8e431a846548fa8a83b44] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH error when using 'login' to switch users,CASSANDRA-13640,13082866,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,27/Jun/17 16:08,15/May/20 08:00,14/Jul/23 05:56,24/Aug/17 16:09,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/CQL,,,,,0,,,,,"Using {{PasswordAuthenticator}} and {{CassandraAuthorizer}}:

{code}
bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.14-SNAPSHOT | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> create role super with superuser = true and password = 'p' and login = true;
cassandra@cqlsh> login super;
Password:
super@cqlsh> list roles;

'Row' object has no attribute 'values'
{code}

When we initialize the Shell, we configure certain settings on the session object such as
{code}
self.session.default_timeout = request_timeout
self.session.row_factory = ordered_dict_factory
self.session.default_consistency_level = cassandra.ConsistencyLevel.ONE
{code}
However, once we perform a LOGIN cmd, which calls do_login(..), we create a new cluster/session object but actually never set those settings on the new session.

It isn't failing on 3.x. 

As a workaround, it is possible to logout and log back in and things work correctly.",,adelapena,jasonstack,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 16:09:15 UTC 2017,,,,,,,,,,"0|i3gskn:",9223372036854775807,3.0.14,,,,,,jasonstack,,jasonstack,,,Low,,,,,,,,,,,,,,,,,,,"27/Jun/17 18:22;adelapena;First draft of the patch [here|https://github.com/adelapena/cassandra/commit/95207a99402b15e32d75ad8e339e9f14d91a1b6e].;;;","29/Jun/17 08:33;adelapena;[Here|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13640-3.0] is the patch copying the session attributes into the new session created on {{do_login}}. 

A simple dtest reproducing the problem can be found [here|https://github.com/riptano/cassandra-dtest/compare/master...adelapena:CASSANDRA-13640].

Although this bug only affects to 3.0, the root cause is present in all the other branches (2.1, 2.2, 3.x and trunk). It doesn't affect to other versions because printing is done in a slightly different way, but relying on this would be IMHO quite risky. Also, the usage of {{LOGIN}} statement could lead to end up using a different default timeout or consistency level than the initially specified. So, I think that the patch should be applied to all branches. What do you think?;;;","29/Jun/17 10:44;adelapena;I ran the patch on our internal CI. There are not failures for the unit tests and the failing dtests are not related to the change.;;;","21/Aug/17 14:35;jasonstack;The fix looks good. 3.11/trunk are also broken for the same reason, need to fix as well.;;;","23/Aug/17 15:23;adelapena;Right. There are the patches for 3.0, 3.11, trunk and dtest:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13640-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13640-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13640-trunk]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13640]||

Thanks for the review.;;;","24/Aug/17 11:20;jasonstack;Thansk for the patch. +1;;;","24/Aug/17 16:09;adelapena;Committed as [9497191f5bab126c4d83ccbe023554fd6ea95257|https://github.com/apache/cassandra/commit/9497191f5bab126c4d83ccbe023554fd6ea95257].

Dtest committed as [da6ad8317e18ebaa5e8b428df79d1da086a19dd9|https://github.com/apache/cassandra-dtest/commit/da6ad8317e18ebaa5e8b428df79d1da086a19dd9].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index queries are rejected on COMPACT tables,CASSANDRA-13627,13081418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,21/Jun/17 13:04,15/May/20 08:02,14/Jul/23 05:56,26/Jun/17 11:34,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/CQL,,,,,0,,,,,"Since {{3.0}}, {{compact}} tables are using under the hood {{static}} columns. Due to that {{SELECT}} queries using secondary indexes get rejected with the following error:
{{Queries using 2ndary indexes don't support selecting only static columns}}.

This problem can be reproduced using the following unit test:
{code}    @Test
    public void testIndicesOnCompactTable() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk int PRIMARY KEY, v int) WITH COMPACT STORAGE"");
        createIndex(""CREATE INDEX ON %s(v)"");

        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 1, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 2, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 3, 3);

        assertRows(execute(""SELECT pk, v FROM %s WHERE v = 1""),
                   row(1, 1),
                   row(2, 1));

    }{code}",,adelapena,blerer,maedhroz,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 26 11:34:10 UTC 2017,,,,,,,,,,"0|i3gjnr:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"23/Jun/17 08:44;blerer;I pushed some patches for [3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13627-3.0] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13627-trunk]. I ran the test on our internal CI for {{3.0}}, {{3.11}} and {{trunk}}. There are no unit test failures and the DTests failure are unrelated to the patches.

The patches make make sure that {{Selection::containsStaticColumns}} and {{SelectStatement::selectOnlyStaticColumns}} (in trunk) return the correct value for static compact tables.    ;;;","23/Jun/17 16:54;adelapena;The patch looks good to me, +1.

Just a couple of trivial comments that you could perfectly ignore or fix during commit:
 * It seems that [here|https://github.com/blerer/cassandra/blob/26e7ca7b1b0b0c493508cc6565e16d185323763d/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java#L1306-L1308] there are double spaces which are probably accidental. 
 * The insert statements in the newly created {{SecondaryIndexTest#testIndicesOnCompactTable}} use an uppercase ""%S"" format specifier, which will produce the uppercased name of the table. This is not a problem at all because the unquoted table name is case insensitive, but I'm mentioning it just in case the choice of the uppercased format specifier were accidental.;;;","26/Jun/17 11:33;blerer;Thanks for the review. I fixed the mentioned problems.;;;","26/Jun/17 11:34;blerer;Committed into 3.0 at 57c590f6f71907dda6f3d88a16883b5dbcf259ee and merged into 3.11 and trunk. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check hashed password matches expected bcrypt hash format before checking,CASSANDRA-13626,13081266,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,20/Jun/17 23:47,15/May/20 08:04,14/Jul/23 05:56,31/Aug/17 05:06,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Authorization,,,,,0,,,,,"We use {{Bcrypt.checkpw}} in the auth subsystem, but do a reasonably poor job of guaranteeing that the hashed password we send to it is really a hashed password, and {{checkpw}} does an even worse job of failing nicely. We should at least sanity check the hash complies with the expected format prior to validating.
",,jeromatron,jjirsa,samt,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 05:06:22 UTC 2017,,,,,,,,,,"0|i3gipz:",9223372036854775807,,,,,,,samt,,samt,,,Low,,,,,,,,,,,,,,,,,,,"29/Aug/17 22:32;jjirsa;|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/233/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/234/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/235/] |
;;;","30/Aug/17 09:53;samt;bq.checkpw does an even worse job of failing nicely

In what way(s)? Checking with a few strings that would be caught by this validation, {{checkpw}} seems to behave as expected. 
This validation would help us detect that we have stored invalid hash values, so that could be useful in diagnosing unexpected auth failures & debugging their causes. That will obviously require logging when the validation fails before throwing the {{AuthenticationException}}, so we should separate it from the actual {{checkPw}} call.  

On the actual validation, the 22 character component is actually the salt, not the cost - the bcrypt format is {{$<id>$<cost>$<salt><digest>}}. Cost, salt and digest are all fixed length (2, 22 & 31 chars repectively), whereas id may be 1 or 2 chars, though we have only ever used a version of jbcrypt that generates the 2 char variant. So we could simplify that check to {{length == 60}}. If {{checkpw}} *is* correctly returning false when the stored hash is invalid though, we only really need to do the validation on failure, in which case we could a more thorough check than simply looking at the length, if that's worthwhile.   

EDIT: I know it's a pretty trivial change, but it would be nice to add some tests to go with it.;;;","30/Aug/17 17:40;jjirsa;Sam and I talked about this a bit offline, a few notes for those following along:
- The length is 60 now, but may be 59 with other bcrypt variants, and may be some other length in the future. The two components of the length that were chosen before (salt+digest) were expected to be fixed length, which is true now but may not be true later). ID is definitely variable length now, though. 
- We don't want a real regex, for a few reasons (combination of futureproofing and the risk of introducing auth weaknesses) - we should let bcrypt handle the hash, we don't need to get involved.
- We can be less invasive here and make it slightly more testable by breaking the hash check into its own static function 

Force pushed branches that simplifies things a bit, makes it less arbitrary.

|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/251/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/252/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/253/] |
;;;","30/Aug/17 19:28;samt;+1 LGTM.

I pushed some *tiny* additions to {{PasswordAuthenticatorTest}} [here|https://github.com/beobal/cassandra/commit/fba4d7fcb3630b4ba67833f9e451cf03aa11aa62]. If you think they're overkill, I won't argue too hard.;;;","31/Aug/17 05:06;jjirsa;Nice. Dtest environment looks pretty messy today, some of the slaves are acting up. I've read through some of the console logs and even on the aborted runs, there's nothing auth related, so I'm proceeding (since it's a fairly trivial patch). Added those tests and committed as {{5e7f60f6bf5da386076faa08cefb3970a6ba5cc0}}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove unused cassandra.yaml setting, max_value_size_in_mb, from 2.2.9",CASSANDRA-13625,13081259,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,20/Jun/17 23:25,16/Apr/19 09:30,14/Jul/23 05:56,22/Jun/17 00:05,2.2.11,,,,,,,,,,,0,lhf,,,,"{{max_value_size_in_mb}} is currently in the 2.2.9 cassandra.yaml, but does not make reference of the config in any place within its codebase:

https://github.com/apache/cassandra/blob/cassandra-2.2.9/conf/cassandra.yaml#L888-L891

CASSANDRA-9530, which introduced {{max_value_size_in_mb}}, has it's Fix Version/s marked as 3.0.7, 3.7, and 3.8.

Let's remove the {{max_value_size_in_mb}} from the cassandra.yaml from {{>= 2.2.9, < 3}}.

{NOFORMAT}
~/repos/cassandra[(HEAD detached at cassandra-2.2.9)] (joaquin)$ grep -r max_value_size_in_mb .
conf/cassandra.yaml:# max_value_size_in_mb: 256
{NOFORMAT}",,j.casares,jeromatron,jjirsa,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,j.casares,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 00:05:13 UTC 2017,,,,,,,,,,"0|i3giof:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"20/Jun/17 23:27;j.casares;Here's a PR for the single-file fix: https://github.com/apache/cassandra/pull/124.;;;","21/Jun/17 09:14;rha;Hi, I don't understand why do you want to remove this setting, it's still used in trunk:

{code}
$ grep -IRn getMaxValueSize
test/unit/org/apache/cassandra/db/compaction/BlacklistingCompactionsTest.java:86:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
test/unit/org/apache/cassandra/io/sstable/SSTableWriterTestBase.java:84:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
test/unit/org/apache/cassandra/io/sstable/SSTableCorruptionDetectionTest.java:91:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
src/java/org/apache/cassandra/db/ClusteringPrefix.java:377:                                : (isEmpty(header, offset) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : types.get(offset).readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/db/ClusteringPrefix.java:531:                          : (Serializer.isEmpty(nextHeader, i) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : serializationHeader.clusteringTypes().get(i).readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/db/rows/Cell.java:246:                    value = header.getType(column).readValue(in, DatabaseDescriptor.getMaxValueSize());
src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java:1056:            DecoratedKey key = metadata.partitioner.decorateKey(metadata.partitionKeyType.readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1054:    public static int getMaxValueSize()
{code}

{{getMaxValueSize()}} occurrences: https://github.com/apache/cassandra/search?q=getMaxValueSize;;;","21/Jun/17 16:09;j.casares;[~rha], I'm not for removing the setting from trunk, just from the 2.2.x cassandra.yaml where the setting has never been implemented.

If you were to switch to the cassandra-2.2.9 branch and run your grep script again, it should come back with no references.;;;","21/Jun/17 16:16;rha;Ok! I understood {{>= 2.2.9}} and not {{>= 2.2.9, < 3}};;;","21/Jun/17 16:24;j.casares;Sorry for that confusion, I've updated the main description. Thanks for pointing that out! :);;;","22/Jun/17 00:05;jjirsa;Committed in {{082af0a9ba6b5dde26055fcb9ddd2085e4240381}} , merged {{-s ours}} through 3.0/3.11/trunk, so it's only in 2.2.11.

Thanks!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better config validation/documentation,CASSANDRA-13622,13081030,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasonstack,KurtG,KurtG,20/Jun/17 07:31,15/May/20 08:01,14/Jul/23 05:56,12/Sep/17 14:35,3.0.15,3.11.1,4.0,4.0-alpha1,,,Local/Config,,,,,0,lhf,,,,"There are a number of properties in the yaml that are ""in_mb"", however resolve to bytes when calculated in {{DatabaseDescriptor.java}}, but are stored in int's. This means that their maximum values are 2047, as any higher when converted to bytes overflows the int.

Where possible/reasonable we should convert these to be long's, and stored as long's. If there is no reason for the value to ever be >2047 we should at least document that as the max value, or better yet make it error if set higher than that. Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user. That is, causing it to break but not in the way the user expected it to :)

Following are functions that currently could be at risk of the above:

{code:java|title=DatabaseDescriptor.java}
getThriftFramedTransportSize()
getMaxValueSize()
getCompactionLargePartitionWarningThreshold()
getCommitLogSegmentSize()
getNativeTransportMaxFrameSize()
# These are in KB so max value of 2096128
getBatchSizeWarnThreshold()
getColumnIndexSize()
getColumnIndexCacheSize()
getMaxMutationSize()
{code}

Note we may not actually need to fix all of these, and there may be more. This was just from a rough scan over the code.",,adelapena,jasonstack,jeromatron,KurtG,pauloricardomg,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 14:49:46 UTC 2017,,,,,,,,,,"0|i3gh9j:",9223372036854775807,,,,,,,KurtG,,KurtG,,,Low,,,,,,,,,,,,,,,,,,,"14/Jul/17 09:04;jasonstack;| [trunk| https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622] | [unit|https://circleci.com/gh/jasonstack/cassandra/153] | dtest: except for 1 known error in bootstrap_test |

handled NPE when empty entry in storage_directories and handled overflow when converting to int32 KB from MB;;;","18/Jul/17 08:00;KurtG;Not sure how keen I am on bounding commitlog_segment_size to 2GB. Obviously that's a ridiculous size for a segment, but I have had to increase it to 1.2GB in the past to allow a write through. Granted it was a side effect from MV's (I think it was streaming related), however the ability to do so helped. Obviously that's not great justification but I think it would be better to give it a much higher limit (store it as a long) and simply advise users (in docs+yaml) that they really shouldn't need to set it above the default. 

Same goes for max_value_size. I don't see any compelling reason we should stop people experimenting with that if they want to, as it's really just a safety net that the user configures. ;;;","25/Jul/17 02:50;jasonstack;That's java ByteBuffer restriction on 2GB.. ;;;","25/Jul/17 03:44;KurtG;Ah yeah didn't think about that. Well, guess that's as good as it will get then.;;;","08/Aug/17 06:23;jasonstack;[~KurtG] could you review it?;;;","14/Aug/17 00:25;KurtG;LGTM. Probably worth also applying the changes to 3.0/3.11. ;;;","15/Aug/17 08:46;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-trunk] |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-3.11] |
| [3.0| https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-3.0] |

Thanks for reviewing;;;","23/Aug/17 15:47;jasonstack;[~KurtG] could you have a look at the final patch before commiting? thanks;;;","24/Aug/17 00:44;KurtG;Sorry missed the email for your last comment. CircleCI appears to have failed on [trunk|https://circleci.com/gh/jasonstack/cassandra/424#tests/containers/0]. Might be related as it is testing DatabaseDescriptor. On that note probably wouldn't hurt to have some simple tests for these cases.

Also the merge into trunk has an extra line in {{CHANGES.txt}} before the change. The {{3.0.15}} should be removed.

Might want to rebase as well (sorry for my slowness :/)

Also not a committer so will prod one to commit/review once tests are written/are working.
Thanks.;;;","24/Aug/17 07:53;jasonstack;Thanks, rebased and build passed.;;;","12/Sep/17 14:34;adelapena;Committed as [8fc9275d3020fa0c80ed1852726be0a5a63e487c|https://github.com/apache/cassandra/commit/8fc9275d3020fa0c80ed1852726be0a5a63e487c].;;;","12/Sep/17 14:49;jasonstack;[~KurtG]  [~adelapena] thank you ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't skip corrupt sstables on startup,CASSANDRA-13620,13080802,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,19/Jun/17 12:02,15/May/20 08:02,14/Jul/23 05:56,28/Aug/17 13:43,3.0.15,3.11.1,4.0,4.0-alpha1,,,Local/SSTable,,,,,0,,,,,"If we get an IOException when opening an sstable on startup, we just [skip|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L563-L567] it and continue starting

we should use the DiskFailurePolicy and never explicitly catch an IOException here",,aweisberg,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7377,,,,,,,,,,,,,,,,,,,"24/Aug/17 07:04;marcuse;13620-3.0.png;https://issues.apache.org/jira/secure/attachment/12883498/13620-3.0.png","24/Aug/17 07:04;marcuse;13620-3.11.png;https://issues.apache.org/jira/secure/attachment/12883497/13620-3.11.png","24/Aug/17 07:04;marcuse;13620-trunk.png;https://issues.apache.org/jira/secure/attachment/12883499/13620-trunk.png",,,,,,,,,,,,,,,,,3.0,marcuse,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 13:43:22 UTC 2017,,,,,,,,,,"0|i3gfuv:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"19/Jun/17 13:40;marcuse;Patch to catch IOExceptions and rethrow them as CorruptSSTableExceptions - if we get an IOException when opening an sstable, it should be considered corrupt

https://github.com/krummas/cassandra/commits/marcuse/13620
https://github.com/krummas/cassandra/commits/marcuse/13620-3.11
https://github.com/krummas/cassandra/commits/marcuse/13620-trunk

The trunk version removes the {{throws IOException}} from all open* methods, I kept it in the earlier versions as I guess it can be considered a public API;;;","05/Jul/17 21:30;aweisberg;Can you run the dtests on this just to make sure the change in logging and signatures doesn't break any tests?;;;","06/Jul/17 06:52;marcuse;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/116/
Only running for 3.0 so far, if they look good I'll trigger for the other branches;;;","24/Aug/17 07:05;marcuse;3.0:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/210/testReport/
[^13620-3.0.png]

3.11:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/208/testReport/
[^13620-3.11.png]

trunk:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/207/testReport/
[^13620-trunk.png]

trying to figure out if any of the failures are caused by this patch;;;","24/Aug/17 19:08;aweisberg;I recognize all of them except ttl_is_respected_on_repair and test_13691.

13691 failed on the 3.0 branch. ttl_is_respected_on_repair failed maybe once in the last 30 builds on the 3.0 branch, but not on 3.11 or trunk.

The issue looks like the environment?


{noformat}
Error Message

[Errno 20] Not a directory: '/tmp/dtest-Bx7qYJ/test/node2/bin/cassandra'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Bx7qYJ
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140182908452560) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 320, in run
    self.setUp()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/cassandra-dtest/ttl_test.py"", line 349, in setUp
    self.cluster.populate(2).start()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/venv/local/lib/python2.7/site-packages/ccmlib/cluster.py"", line 393, in start
    p = node.start(update_pid=False, jvm_args=jvm_args, profile_options=profile_options, verbose=verbose, quiet_start=quiet_start, allow_root=allow_root)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 617, in start
    os.chmod(launch_bin, os.stat(launch_bin).st_mode | stat.S_IEXEC)
""[Errno 20] Not a directory: '/tmp/dtest-Bx7qYJ/test/node2/bin/cassandra'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Bx7qYJ\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140182908452560) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------""
{noformat}
;;;","25/Aug/17 06:29;marcuse;yeah, the suspicious tests pass locally multiple times;;;","28/Aug/17 13:43;marcuse;Committed as {{dfbe3fabd266493e69}}, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.nio.BufferOverflowException: null while flushing hints,CASSANDRA-13619,13080780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,milansm,milansm,19/Jun/17 10:32,15/May/20 08:05,14/Jul/23 05:56,18/Sep/17 07:01,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Coordination,Legacy/Core,,,,0,,,,,"I'm seeing the following exception running Cassandra 3.0.11 on 21 node cluster in two AWS regions when half of the nodes in one region go down, and the load is high on the rest of the nodes:

{code}
WARN  [SharedPool-Worker-10] 2017-06-14 12:57:15,017 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-10,5,main]: {}
java.lang.RuntimeException: java.nio.BufferOverflowException
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2549) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0-zing_17.03.1.0]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.11.jar:3.0.11]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0-zing_17.03.1.0]
Caused by: java.nio.BufferOverflowException: null
        at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:195) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:258) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength(ByteBufferUtil.java:296) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Columns$Serializer.serialize(Columns.java:405) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:120) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:625) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:305) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.Hint$Serializer.serialize(Hint.java:141) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:251) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:230) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBufferPool.write(HintsBufferPool.java:61) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsService.write(HintsService.java:154) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$11.runMayThrow(StorageProxy.java:2627) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2545) ~[apache-cassandra-3.0.11.jar:3.0.11]
        ... 5 common frames omitted
{code}

Relevant configurations from cassandra.yaml:

{code}
-cassandra_hinted_handoff_throttle_in_kb: 1024
 cassandra_max_hints_delivery_threads: 4
-cassandra_hints_flush_period_in_ms: 10000
-cassandra_max_hints_file_size_in_mb: 512
{code}

When I reduce -cassandra_hints_flush_period_in_ms: 10000 to 5000, the number of exceptions lowers significantly, but they are still present.",,aleksey,crichards,jasobrown,jasonstack,jay.zhuang,jjirsa,KurtG,marcuse,milansm,mshuler,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13339,,,,,,CASSANDRA-13339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,Correctness -> Consistency,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 18 07:01:14 UTC 2017,,,,,,,,,,"0|i3gfq7:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"07/Sep/17 12:50;marcuse;[~milansm] could you post your schema and the modification statements you are running?;;;","08/Sep/17 06:50;milansm;[~krummas] here it is:

The schema:
{code:java}
CREATE TABLE IF NOT EXISTS keyspace.ids (
    id uuid,
    prov int,
    eid text,
    PRIMARY KEY (id, prov)
) WITH CLUSTERING ORDER BY (proc ASC)
    AND bloom_filter_fp_chance = 0.1
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = 'External mappings'
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy', 'sstable_size_in_mb': '200'}
    AND compression = {'enabled': 'false'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 2592000
    AND gc_grace_seconds = 86400
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The mutation statements:
{code:java}
UPDATE keyspace.ids USING TTL ? SET eid = ? where id = ? AND prov = ?
UPDATE keyspace.ids SET eid = ? where id = ? AND prov = ?
{code};;;","08/Sep/17 15:21;marcuse;This reproduces pretty quickly with the following:
{code}
$ ccm create -n2 hints
$ ccm start
$ ccm node2 stop
$ tools/bin/cassandra-stress user profile=tools/cqlstress-singlepart.yaml ops\(insert=1\) -rate threads=65
{code}
where cqlstress-singlepart.yaml is [here|https://github.com/krummas/cassandra/blob/marcuse/stress_hints/tools/cqlstress-singlepart.yaml] - it just keeps updating a single partition

cc [~jasobrown] since this is most likely the same bug as CASSANDRA-13339 ;;;","08/Sep/17 16:26;jasobrown;Yup, this definitely looks related. Thanks for the repro steps, it'll be a *huge* help!;;;","08/Sep/17 19:09;marcuse;[~milansm] which java version and operating system are you running?;;;","08/Sep/17 20:05;jasobrown;Crap - i couldn't get [~krummas]'s script to repro for me, and I tried on three different systems (Linux VM on mac laptop, macOS on laptop, macOS desktop). Still, Marcus and I have some avenues for investigation we're gonna dig into.;;;","11/Sep/17 14:46;aleksey;On MacOS, with Oracle's ""1.8.0_144"", cannot reproduce either.;;;","11/Sep/17 15:19;marcuse;Does not repro with Oracles 1.8.0_144 for me either;;;","12/Sep/17 10:42;marcuse;In {{PartitionUpdate}}, {{isBuilt}} is non-volatile, and it is set once the {{Holder}} ref has been updated. {{build()}} is synchronized but {{maybeBuild()}} where we check {{isBuilt}} is not. That means this is basically double checked locking, but since {{isBuilt}} is not volatile, the assignments in {{build()}} can be reordered, making {{isBuilt}} true before {{holder}} is assigned.

It stops reproducing if I set {{isBuilt = this.holder != null}} instead of {{isBuilt = true}} to make sure that {{holder}} is set before {{isBuilt}} but making {{isBuilt}} volatile should be the correct solution.;;;","14/Sep/17 12:10;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13619-volatile
https://github.com/krummas/cassandra/commits/marcuse/13619-volatile-3.11
https://github.com/krummas/cassandra/commits/marcuse/13619-volatile-trunk

just running dtests for 3.0 to not waste build resources (this trivial patch really shouldn't cause any regressions....)
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/310/

Created CASSANDRA-13867 for the more complete fix.;;;","15/Sep/17 10:08;aleksey;+1;;;","18/Sep/17 07:01;marcuse;committed as {{bb3b332c4225660927}}, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Import cassandra-dtest project into it's own repository ,CASSANDRA-13613,13080276,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,zznate,zznate,16/Jun/17 00:29,16/Apr/19 09:30,14/Jul/23 05:56,27/Aug/17 19:13,,,,,,,Test/dtest/python,,,,,0,,,,,"Given the completion of CASSANDRA-13584, we can now import {{cassandra-dtest}} into ASF infrastructure. This is a top level issue for tracking the bits and pieces of this task. ",,jeromatron,mshuler,muru,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 13 15:02:56 UTC 2017,,,,,,,,,,"0|i3gcmf:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"13/Jul/17 15:02;mshuler;I've updated the Jenkins Job DSL seed to pull cassandra-dtest from the ASF git mirror.
https://git1-us-west.apache.org/repos/asf?p=cassandra-builds.git;a=commitdiff;h=b0f3195e4e85b1f7dddb63823a88216cd3a024a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change repair midpoint logging from  CASSANDRA-13052,CASSANDRA-13603,13079879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,14/Jun/17 20:59,15/May/20 08:06,14/Jul/23 05:56,05/Sep/17 19:02,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Streaming and Messaging,,,,,0,,,,,"In CASSANDRA-13052 , we changed the way we handle repairs on small ranges to make them more sane in general, but {{MerkleTree.differenceHelper}} now erroneously logs at error.",,bdeggleston,jay.zhuang,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 19:02:14 UTC 2017,,,,,,,,,,"0|i3ga6f:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"14/Jun/17 21:15;jjirsa;|| Branch || Testall || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/88/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/89/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/90/] |


For the reviewer: I really have mixed feelings about {{MerkleTree.differenceHelper}} in general. In particular, it presumes that there are differences in the trees, which is always true in the existing codepath, but may not be true later if someone misuses it in the future. It seems [fairly straightforward|https://github.com/jeffjirsa/cassandra/commit/cee5a2d75e8e867dc20f3d94f0fe5df360210d46] to make it properly handle the case where the trees are consistent for a given range, though I'm not sure it's necessary.

Thoughts [~spodxx@gmail.com] / [~bdeggleston] ? 
;;;","19/Jun/17 11:37;spod;I agree that it's easy to misuse MerkleTree.differenceHelper(). It should only really be called from MerkleTree.difference() and should have a  {{@VisibleForTesting}} annotation to indicate that it's internal use only. As for the suggested changes around midpoint result handling, I'm not really convinced that we should use this value to check if we should stop tree traversal. Although we should be at a leaf nodes already when getting an invalid midpoint value, we wouldn't be able to tell otherwise without any kind of error message, blindly following this assumption. This may cover other potential errors.

We already check {{node instanceof Leaf}} before recursively calling differenceHelper() with a sub-range. I'd suggest to do the same in difference(). It just doesn't make sense to call differenceHelper() with two leaf nodes, doesn't it?;;;","20/Jun/17 21:56;jjirsa;{quote}
We already check node instanceof Leaf before recursively calling differenceHelper() with a sub-range. I'd suggest to do the same in difference(). It just doesn't make sense to call differenceHelper() with two leaf nodes, doesn't it?
{quote}

I think you're right - pushed a new commit to each branch that checks to see if either node is {{instanceof Leaf}} and avoids that tree traversal. Also marked {{MerkleTree.differenceHelper}} as {{@VisibleForTesting}}

I think this makes the midpoint check below irrelevant, but may protect us from another bug later? Considering switching it to an assert rather than the block that exists now?

{code}
        if (midpoint.equals(active.left) || midpoint.equals(active.right))
{code} 



;;;","21/Jun/17 10:12;spod;We should continue to check the midpoint return value. Even if it's just for troubleshooting similar problems in the future. 

Throwing an AssertionError instead of just logging an error message is probably not a good idea in 2.2. See my comment from before the patch:

{quote}Unfortunately we can't throw here to abort the validation process, as the code is executed in it's own thread with the caller waiting for a condition to be signaled after completion and without an option to indicate an error (2.x only).
{quote}

But 3.0+ should be able to deal with failed SyncTasks, as those are now handled as futures. SyncTask.run is currently missing any exception handling, but that could be changed.;;;","29/Aug/17 23:39;jjirsa;Would love to revisit this.

[~spodxx@gmail.com] / [~bdeggleston] , pushed rebased versions, and here's shortcut links to combined diffs for the three branches: 

https://github.com/apache/cassandra/compare/cassandra-3.0...jeffjirsa:cassandra-3.0-13603?ws=1
https://github.com/apache/cassandra/compare/cassandra-3.11...jeffjirsa:cassandra-3.11-13603?ws=1 
https://github.com/apache/cassandra/compare/trunk...jeffjirsa:cassandra-13603?ws=1

Either of you see anything else you'd like to change that I didn't catch already? Either of you up to mark yourselves as formal reviewer? ;;;","30/Aug/17 18:43;bdeggleston;I think the current state of {{MerkleTree.differenceHelper}} is sort of the wrong approach for what it's trying to do in general. The current implementation has weird edge cases like this, and is too complex and difficult to follow. Also, it shares some of it's responsibilities with {{difference}}. 

Really, we're just trying to recursively compare these trees and record the largest contiguous out of sync ranges. The only complication here is that a given invocation of the method doesn't know if it's adjacent range is also out of sync, so it has to defer to the caller to do something if it's range is {{FULLY_INCONSISTENT}}. I put together an alternate implementation [here|https://github.com/bdeggleston/cassandra/tree/differencer-cleanup].

Having said all that, those changes are out of scope for this ticket and 3.0, so I'm +1 on the changes as proposed, and will open another ticket to refactor differece/differenceHelper.;;;","30/Aug/17 18:51;bdeggleston;opened CASSANDRA-13830;;;","05/Sep/17 19:02;jjirsa;Thanks. Committed as {{bc5c2316c5acfc1ee0ef101a3ebf00d03c89e283}} to 3.0 and merged 3.11 and trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement short read protection on partition boundaries,CASSANDRA-13595,13079423,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,adelapena,adelapena,13/Jun/17 09:20,07/Mar/23 11:52,14/Jul/23 05:56,30/Sep/17 11:07,3.0.15,3.11.1,,,,,Legacy/Coordination,,,,,0,Correctness,,,,"It seems that short read protection doesn't work when the short read is done at the end of a partition in a range query. The final assertion of this dtest fails:
{code}
def short_read_partitions_delete_test(self):
        cluster = self.cluster
        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
        cluster.set_batch_commitlog(enabled=True)
        cluster.populate(2).start(wait_other_notice=True)
        node1, node2 = self.cluster.nodelist()

        session = self.patient_cql_connection(node1)
        create_ks(session, 'ks', 2)
        session.execute(""CREATE TABLE t (k int, c int, PRIMARY KEY(k, c)) WITH read_repair_chance = 0.0"")

        # we write 1 and 2 in a partition: all nodes get it.
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (1, 1)"", consistency_level=ConsistencyLevel.ALL))
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (2, 1)"", consistency_level=ConsistencyLevel.ALL))

        # we delete partition 1: only node 1 gets it.
        node2.flush()
        node2.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 1""))
        node2.start(wait_other_notice=True)

        # we delete partition 2: only node 2 gets it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node2, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 2""))
        node1.start(wait_other_notice=True)

        # read from both nodes
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ALL)
        assert_none(session, ""SELECT * FROM t LIMIT 1"")
{code}
However, the dtest passes if we remove the {{LIMIT 1}}.

Short read protection [uses a {{SinglePartitionReadCommand}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/DataResolver.java#L484], maybe it should use a {{PartitionRangeReadCommand}} instead?",,adelapena,aleksey,jasonstack,jjirsa,mshuler,samt,sbtourist,varung,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 30 11:07:37 UTC 2017,,,,,,,,,,"0|i3g7d3:",9223372036854775807,2.1.x,2.2.x,3.0.x,3.11.x,5.0,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"12/Sep/17 09:22;jasonstack;The cause is no short-read-protection generated for node2 with key=2, because no UnfilteredRowIterator with key=2 for node2...

{code}
For initial read:
Node1 returns:  
           PartitionIterator {
                UnfilteredIterator( k=1@tombstone, back by short-read-protection)  
                UnfilteredIterator( k=2, back by short-read-protection)  
          }
Node2 returns: 
           PartitionIterator {
                UnfilteredIterator( k=1, back by short-read-protection) 
          }
{code}

;;;","13/Sep/17 10:30;jasonstack;It seems that current ShortReadProtection only supports the same partition.

The idea is to extend current ShortReadProtection function with across-partition support. If {{current response (UnfilteredPartitionIterator)}} reached the end and its last key is ""behind"" (token smaller than) other responses' current key, we will do a {{PartitionRange retry}} from the last key to make sure no response is falling short of unfetched partitions. ;;;","13/Sep/17 11:16;aleksey;bq. The idea is to extend current ShortReadProtection function with across-partition support.

Correct. Short read protection hasn't been implemented properly for range reads, which causes correctness issues in particular with paging.

I'm currently addressing the few outstanding issues with single partition short reads on 3.0 and above (CASSANDRA-13794, CASSANDRA-12872). This would be an extension of that work, I guess - or at least there is strong overlap. Feel free to give it a go though - however it may or may not have to be altered afterwards to harmonise both implementations.;;;","13/Sep/17 15:34;jasonstack;[~iamaleksey] Thanks for the heads up.

| Source |
| [3.0|https://github.com/jasonstack/cassandra/commits/13595-3.0] |
| [dtest|https://github.com/riptano/cassandra-dtest/commits/13595] |

Here is draft of making {{ShortReadProtection}} extend {{MorePartitions}}.

Changes:
   1. If there is no more rows ahead, no short-read-protection(both partition and row) is triggered
   2. Short-read-partition-protection only fetches range from last-partition-key to max token of current command range
   3. Extend UnfilteredPartitionItr with short-read-partition-protection first, then apply short-read-partition

If you have better solution or integration, feel free to mark it as duplicated. ;;;","13/Sep/17 16:11;jjirsa;Quick note that the dtest repo has moved to [apache/cassandra-dtest|https://github.com/apache/cassandra-dtest] - so we should move [e706952e|https://github.com/riptano/cassandra-dtest/commit/e706952e8d3bb18af1f96cb0cfc283e53260513e] over to the new repo;;;","13/Sep/17 16:27;aleksey;[~jasonstack] Yours is not a duplicate, I'd just recommend holding it off a bit until the fixes for CASSANDRA-13794 and CASSANDRA-12872 get in. The code you've submitted doesn't seem very far off of what it should be, either. I will probably review/collaborate after I'm done with the other two tickets.

But this is 3.0. Have you looked into 2.1? I'm not sure how I feel about committing it to 2.1. On one hand, it's a huge correctness issues. Does it count as critical?;;;","14/Sep/17 01:23;jasonstack;[~jjirsa] thanks, I will move to new repo.

[~iamaleksey] I haven't looked at 2.2 yet, just started off with 3.0, will do that after you finalized 13794, 12872. 

bq. Does it count as critical?

As a user, yes..;;;","20/Sep/17 17:23;aleksey;[~jasonstack] My current thinking is that as bad as this is, fixing in 3.0+ is fine. You need to upgrade to get the fix though.

Can you rebase based on the latest 3.0, though - with CASSANDRA-13794 committed? Then we can take it from there.;;;","22/Sep/17 16:23;aleksey;Ok first cut, but there are some major issues with it.

1. You can't rely on the class of the command to determine if it's a single partition or a range read command. We do use range read commands for indexed queries even when the partition key is set. See CASSANDRA-11617 and CASSANDRA-11872 for some more context.

2. You cannot use {{command.limits().forShortReadRetry()}} method for the new limits here. It wasn't written with ranged reads in mind, and does among other things throw away the per partition limit - not what you want to happen.

3. The command created doesn't get passed original {{command.isForThrift()}} and hardcodes it as {{false}}. This was an issue with row-level SRP as well, but I fixed it yesterday. Should be using {{PartitionRangeReadCommand.withUpdatedLimitsAndDataRange()}} instead. As a bonus, it preserves the correct {{indexMetadata}} so you don't have to do an extra lookup.

4. I don't think that new range calculation is correct, and accounts for collisions of multiple partitions keys mapping to tokens.

5. {{shouldDoPartitionShortReadProtection()}} can be written a lot simpler, and some is redundant. {{if (mergedResultCounter.counted() >= command.limits().count())}} can't ever be true (but also is equivalent to {{if (mergedResultCounter.isDone())}}). The only meaningful thing you can do here is
{code}
            // if the returned partition doesn't have enough partitions/rows to satisfy even the original limit, don't ask for more
            if (!singleResultCounter.isDone())
                return null;
{code}
, to be honest.

6. I'm not a huge fan of the way {{expectedRows}} is shared between two protections, and am not sure it's correct.

7. The metric for SRP requests isn't being incremented.

I have a version of this that fixes most of these. Needs some more work and manual testing, and eventually approval. It's a bit urgent for me, so do you mind if I take it over from this point? Thanks.;;;","23/Sep/17 08:16;jasonstack;Thanks for the feedback. Feel free to take over.;;;","26/Sep/17 10:23;aleksey;[3.0 branch|https://github.com/iamaleksey/cassandra/commits/13595-3.0], [a new dtest|https://github.com/iamaleksey/cassandra-dtest/commits/13595], [CircleCI run|https://circleci.com/gh/iamaleksey/cassandra/41], [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/335/testReport/].

utests have he usual circle issues with {{RemoveTest}} and {{ViewComplexTest}}, dtests have mostly the git clone issue again with some of the tests. Everything relevant seems to be passing.;;;","29/Sep/17 14:44;samt;The patch looks good to me, there's just one thing I'm not entirely clear about. In CASSANDRA-13794 we added an (arbitrary) lower bound of 8 to limit of the single partition read in SRRP. This removes that, but it I'm not quite sure why.
;;;","29/Sep/17 17:37;aleksey;[~beobal] Sorry, it was indeed a little unclear. {{forShortReadRetry()}} was modified to retain the per partition limit, so asking for more rows then the limit no longer worked, and made the code a bit confusing.

It also just happens that that arbitrary minimum was bothering me anyway. We should probably not go beyond the limit set by the user - unless we know for sure that it is safe. But until we are smarter about this, and start taking row sizes into account, I feel uneasy about enforcing arbitrary minimums.

Pushed another commit on top that addresses two issues:
1. An NPE when trying to perform SRP on an empty iterator ({{lastPartitionKey}} would be null)
2. Lack of a reliable stop condition causing looping

With the change all relevant dtests are passing locally.;;;","29/Sep/17 18:14;samt;Thanks, that explanation is helpful.
Looks good to me, +1 assuming CI looks rosy.;;;","29/Sep/17 19:21;aleksey;Thanks. Squashed the last two commits together, will commit once CI is happy.;;;","30/Sep/17 11:07;aleksey;Committed to 3.0 as [68a67469f8b25534d086b29b8fe0fa4ec3f9d1ec|https://github.com/apache/cassandra/commit/68a67469f8b25534d086b29b8fe0fa4ec3f9d1ec] and merged with 3.11 and trunk. [Utests|https://circleci.com/gh/iamaleksey/cassandra/49] only had the usual {{ViewComplexTest}} and {{CommitLogSegmentManagerTest}} failures. [Dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/lastCompletedBuild/testReport/] was bad as usual but with no failures related to the read path.

Dtest committed as [b76a06672ca418a2a7e90278886252deccdc9edd|https://github.com/apache/cassandra-dtest/commit/b76a06672ca418a2a7e90278886252deccdc9edd].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use an ExecutorService for repair commands instead of new Thread(..).start(),CASSANDRA-13594,13079386,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,13/Jun/17 06:23,15/May/20 08:01,14/Jul/23 05:56,14/Aug/17 12:16,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Currently when starting a new repair, we create a new Thread and start it immediately

It would be nice to be able to 1) limit the number of threads and 2) reject starting new repair commands if we are already running too many.",,aweisberg,jeromatron,jkni,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/17 12:32;marcuse;13594.png;https://issues.apache.org/jira/secure/attachment/12881450/13594.png",,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 15 17:07:45 UTC 2017,,,,,,,,,,"0|i3g74v:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"13/Jun/17 06:23;marcuse;https://github.com/krummas/cassandra/commits/marcuse/limit_repair_command_threads

introduces 2 new config variables:
* {{repair_command_pool_size}} - how many repair commands can we run at the same time
* {{repair_command_pool_full_strategy}} what do we do when the pool is full ({{queue}} or {{reject}});;;","05/Jul/17 22:05;aweisberg;I think it's pretty unlikely there is a test dependency on repair command concurrency, but but maybe run the dtests just to be safe?

The code itself looks good.;;;","06/Jul/17 06:34;marcuse;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/119/;;;","06/Jul/17 18:01;aweisberg;Maybe a legit failure? 
{{thread_count_repair_test (repair_tests.repair_test.TestRepair) ... Build timed out (after 20 minutes). Marking the build as aborted.}};;;","07/Jul/17 09:05;marcuse;Yes, it is

Since we are reusing threads in the ExecutorService we need to clear out the tracing state between runs, I have pushed a commit that does that and restarted the dtests.;;;","20/Jul/17 17:23;aweisberg;The dtest you started has almost certainly been aged out. https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/145/

I think we need to set up archiving of information from Apache Jenkins since they are throwing away everything after a mere 10 builds.;;;","08/Aug/17 12:02;marcuse;Still trying to get dtests to run successfully, they seem quite broken right now;;;","08/Aug/17 15:07;aweisberg;I've noticed that short_read test seems to time out quite a bit. I'll run it locally a bit and see if it reproduces. I think I did before with no luck. This issue might only appear in Apache Jenkins.;;;","11/Aug/17 12:34;marcuse;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/170/ ([^13594.png]);;;","11/Aug/17 17:59;aweisberg;Unrelated but I did manage to reproduce short_read_test failing after letting it run a lot of times.;;;","14/Aug/17 12:16;marcuse;Committed, thanks
;;;","15/Aug/17 16:27;jkni;This causes a test failure in {{DatabaseDescriptorRefTest}} because of the new Config class - I've pushed a fix [here|https://github.com/jkni/cassandra/commit/ec3e7a84e5bae4b6968ee39a39f331fe0f5dd036].;;;","15/Aug/17 16:33;marcuse;+1, sorry about that, again...;;;","15/Aug/17 17:07;jkni;No problem - fix committed as {{256a74faa31fcf25bdae753c563fa2c69f7f355c}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null Pointer exception at SELECT JSON statement,CASSANDRA-13592,13079207,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,panibus,panibus,12/Jun/17 13:33,15/May/20 08:06,14/Jul/23 05:56,07/Jul/17 16:17,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,Legacy/CQL,,,,,0,beginner,,,,"A Nulll pointer exception appears when the command

{code}
SELECT JSON * FROM examples.basic;

---MORE---
<Error from server: code=0000 [Server error] message=""java.lang.NullPointerException"">

Examples.basic has the following description (DESC examples.basic;):
CREATE TABLE examples.basic (
    key frozen<tuple<uuid, int>> PRIMARY KEY,
    wert text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The error appears after the ---MORE--- line.

The field ""wert"" has a JSON formatted string.",Debian Linux,blerer,jasonstack,mildebrandt,panibus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/17 13:33;panibus;system.log;https://issues.apache.org/jira/secure/attachment/12872697/system.log",,,,,,,,,,,,,,,,,,,1.0,jasonstack,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 03 16:51:45 UTC 2017,,,,,,,,,,"0|i3g613:",9223372036854775807,3.10,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"27/Jun/17 07:39;jasonstack;In Json path, the bytebuffer is being consumed with position = capacity. So in page-state, no partition key bytes are written.  

Using bf.duplicate() would fix this issue.

{code}
    public static List<ByteBuffer> rowToJson(List<ByteBuffer> row, ProtocolVersion protocolVersion, ResultSet.ResultMetadata metadata)
    {
        StringBuilder sb = new StringBuilder(""{"");
        for (int i = 0; i < metadata.names.size(); i++)
        {
            if (i > 0)
                sb.append("", "");

            ColumnSpecification spec = metadata.names.get(i);
            String columnName = spec.name.toString();
            if (!columnName.equals(columnName.toLowerCase(Locale.US)))
                columnName = ""\"""" + columnName + ""\"""";

            ByteBuffer buffer = row.get(i);
            sb.append('""');
            sb.append(Json.quoteAsJsonString(columnName));
            sb.append(""\"": "");
            if (buffer == null)
                sb.append(""null"");
            else
                // use duplicate() to avoid buffer being consumed
                sb.append(spec.type.toJSONString(buffer.duplicate(), protocolVersion));
        }
        sb.append(""}"");
        return Collections.singletonList(UTF8Type.instance.getSerializer().serialize(sb.toString()));
    }
{code};;;","28/Jun/17 04:03;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [https://circleci.com/gh/jasonstack/cassandra/65] | | 

let the user of 'type.toJSONString()' to handle BB position changes.;;;","28/Jun/17 16:16;blerer;Thanks for the patch.

The {{pagingOnToJsonQuery()}} test is passing even without the change to {{Selection::rowToJson}}. After looking into it seems that the problem is in fact in {{TupleType::toJSONString}}. The method is actually moving the buffer position while it should not. Which is why simple types like {{int}} or {{text}} are not affected by the problem.
I guess that we might have similar problems with the {{list}}, {{set}}, {{map}} or UDT types. So, it will be nice to add some extra tests for those type as well.

I would put the tests in {{JsonTest}} instead of in {{PagingQueryTest}}.

The {{json}} support has been introduced in 2.2 so we will also need some patches for 2.2, 3.0, 3.11. ;;;","29/Jun/17 05:01;jasonstack;Thanks for reviewing.. I will update the 2.0/3.0/3.11 branch when trunk CI finishes.

""list , set, map or UDT types"" are not allowed in key, ideally it shouldn't cause issues. I think only key buffer will be reused eg. paging-state, subsequent rows deserialization. 
I have included them in the test.

One more issue is in ToJsonFct.

{code}
    public ByteBuffer execute(ProtocolVersion protocolVersion, List<ByteBuffer> parameters) throws InvalidRequestException
    {
        assert parameters.size() == 1 : ""Expected 1 argument for toJson(), but got "" + parameters.size();
        ByteBuffer parameter = parameters.get(0);
        if (parameter == null)
            return ByteBufferUtil.bytes(""null"");
        // same..
        return ByteBufferUtil.bytes(argTypes.get(0).toJSONString(parameter.duplicate(), protocolVersion));
    }
{code};;;","29/Jun/17 08:26;blerer;{quote}""list , set, map or UDT types"" are not allowed in key{quote}
They are, as long as they are {{frozen}}.

Sorry, it seems that I was not clear. Fixing the problem in {{rowToJson}} or in the {{ToJsonFct}} is in my opinion wrong. Most of the types will not change the buffer position in the {{toJSONString}} method and I will argue that it is the correct behavior. The  {{toJSONString}} methods should not change the buffer position.
If you fix it at a higher level, like in the {{rowToJson}} method or in the {{ToJsonFct}} class, you let a potential bug in the code that can hit us later if we add some new code that use the {{toJSONString}} method.
Moreover, your changes create in most of the cases some unnecessary objects.

The proper way to fix that problem is to fix the {{toJSONString}} methods that changes the buffer position which seems to be the one of: {{TupleType}}, {{MapType}}, {{ListType}} and {{SetType}}.   ;;;","29/Jun/17 14:28;jasonstack;Thanks.. Yes, it's better solve it within {{type.toJSONString}}

I will add test the specification of {{type.toJSONString}}.  all should not change buffer position.

Got a question about {{DurationType.toJSONString()}}, imo, it should return string value with double quote {{""-2h9m""}}, like {{TimeType.toJSONString()}}.   but it only returns string value {{-2h9m}} .  Is it expected?  it would be different from user's json input. 

One more thing is about: EmptyType. it should directly {{return ""\""\"""";}} instead of using parent method which causes null-pointer exception.  EmptyType seems not used for json-path, but good to keep it safe.;;;","30/Jun/17 06:18;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [junit|https://circleci.com/gh/jasonstack/cassandra/84]  | {{cql_tests.py:SlowQueryTester.local_query_test}} failed on trunk
{{cql_tests.py:SlowQueryTester.remote_query_test}} failed on trunk
{{bootstrap_test.TestBootstrap.simultaneous_bootstrap_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13506]
| 
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.11] |  [junit|https://circleci.com/gh/jasonstack/cassandra/82] | {{topology_test.TestTopology.size_estimates_multidc_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13229]
{{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13250] | 
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.0] |  [junit|https://circleci.com/gh/jasonstack/cassandra/83] | {{auth_test.TestAuth.system_auth_ks_is_alterable_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13113]
{{offline_tools_test.TestOfflineTools.sstableofflinerelevel_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-12617]
{{repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13515]| 
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-2.2] |  [junit|https://circleci.com/gh/jasonstack/cassandra/85] | passed | 

1. in {{listType, mapType, setType, TupleType}}.toJSONString(), keep buffer position the same.
2. change {{DurationType}}.toJSONString() to {{return ""\"""" +.... +""\"""";}} (with double-quote) to be consistent with user json input
3. change {{EmptyType}}.toJSONString() to directly {{return ""\""\"""";}}, otherwise parent method throws NPE.;;;","30/Jun/17 10:18;jasonstack;I have created [ticket|https://issues.apache.org/jira/browse/CASSANDRA-13650] for {{cql_tests.py:SlowQueryTester.local_query_test}} & {{cql_tests.py:SlowQueryTester.remote_query_test}};;;","03/Jul/17 13:36;blerer;The patch looks mostly good. I just have the following nits:
* The unit test: {{JsonTest::testPagingWithJsonQuery}} is hard to read. It seems to me that the loop does not bring much and makes the code harder to understand. I would remove it, even if it results in a longer method.
* In the same method you should check the output and not only the number of rows. The returned values could be invalid even if the number of returned rows is the good one.
* I would replace {{/** Converts a value to a JSON string. buffer position not changed */}} by something like:
{code}
/** 
 * Converts the specified value into its JSON representation. 
 * <p>The buffer position will stay the same.</p>
* @param buffer the value to convert
* @param protocolVersion the protocol version to use for the conversion
* @return  a JSON string representing the specified value
 */
{code}
;;;","04/Jul/17 03:06;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [junit|https://circleci.com/gh/jasonstack/cassandra/102]  | {{cql_tests.py:SlowQueryTester.local_query_test}} {{cql_tests.py:SlowQueryTester.remote_query_test}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13592]
{{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13576]
| 
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.11] |  [junit|https://circleci.com/gh/jasonstack/cassandra/99] | {{topology_test.TestTopology.size_estimates_multidc_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13229]
{{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13250]| 
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.0] |  [junit|https://circleci.com/gh/jasonstack/cassandra/100] | {{offline_tools_test.TestOfflineTools.sstableofflinerelevel_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-12617] 
{{cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_with_single_cor}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13244] | 
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-2.2] |  [junit|https://circleci.com/gh/jasonstack/cassandra/97] | passed | 


Updated java doc and split the test cases. ;;;","07/Jul/17 16:15;blerer;Thanks for the changes. 
The paging tests were not using paging. I fixed that before committing.;;;","07/Jul/17 16:17;blerer;Committed into 2.2 at cb6fad3efcd7cd3dc87d02ca7e8e97eb277a66ab and merged into 3.0, 3.11 and trunk.;;;","02/Nov/17 22:50;mildebrandt;I'm getting almost exactly the same stacktrace using Cassandra 3.11.1:
{noformat}
java.lang.NullPointerException: null
        at org.apache.cassandra.dht.Murmur3Partitioner.getHash(Murmur3Partitioner.java:230) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.dht.Murmur3Partitioner.decorateKey(Murmur3Partitioner.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.config.CFMetaData.decorateKey(CFMetaData.java:627) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.service.pager.PartitionRangeQueryPager.<init>(PartitionRangeQueryPager.java:44) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.PartitionRangeReadCommand.getPager(PartitionRangeReadCommand.java:268) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.getPager(SelectStatement.java:475) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:288) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:530) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:507) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:146) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{noformat}

It can be recreated with this project:
https://github.com/eyeofthefrog/CASSANDRA-13592

I think it's the same root cause, but let me know if I should open another issue. ;;;","03/Nov/17 08:42;blerer;It is a different issue. Could you open another ticket?
;;;","03/Nov/17 16:51;mildebrandt;Sure, https://issues.apache.org/jira/browse/CASSANDRA-13991
Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock during CommitLog replay when Cassandra restarts,CASSANDRA-13587,13078797,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,09/Jun/17 23:05,16/Apr/19 09:30,14/Jul/23 05:56,30/Aug/17 13:38,3.0.15,,,,,,Legacy/Core,,,,,0,,,,,"Possible deadlock found when Cassandra is replaying commit log and at the same time Mutation gets triggered by SSTableReader(SystemKeyspace.persistSSTableReadMeter). As a result Cassandra restart hangs forever

Please find details of stack trace here:

*Frame#1* This thread is trying to apply {{persistSSTableReadMeter}} mutation and as a result it has called {{writeOrder.start()}} in {{Keyspace.java:533}}
but there are no Commitlog Segments available because {{createReserveSegments (CommitLogSegmentManager.java)}} is not yet {{true}} 

Hence this thread is blocked on {{createReserveSegments}} to become {{true}}, please note this thread has already started {{writeOrder}}

{quote}
""pool-11-thread-1"" #251 prio=5 os_prio=0 tid=0x00007fe128478400 nid=0x1b274 waiting on condition [0x00007fe1389a0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.advanceAllocatingFrom(CommitLogSegmentManager.java:277)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.allocate(CommitLogSegmentManager.java:196)
        at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:260)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:540)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
        at org.apache.cassandra.db.SystemKeyspace.persistSSTableReadMeter(SystemKeyspace.java:1181)
        at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1.run(SSTableReader.java:2202)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Frame#2* This thread is trying to recover commit logs and as a result it tries to flush Memtable by calling following code:
{{futures.add(Keyspace.open(SystemKeyspace.NAME).getColumnFamilyStore(SystemKeyspace.BATCHES).forceFlush());}}
As a result Frame#3 (below) gets created

{quote}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe1c64ec400 nid=0x1af29 waiting on condition [0x00007fe1c94a1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
parking to wait for  <0x00000006370da0c0> (a com.google.common.util.concurrent.ListenableFutureTask)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
        at java.util.concurrent.FutureTask.get(FutureTask.java:191)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:383)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:207)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:182)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:295)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{quote}

*Frame#3* This thread is waiting at {{writeBarrier.await();}} in {{ColumnFamilyStore.java:1027}} 
but {{writeBarrier}} is locked by thread in Frame#1, and Frame#1 thread is waiting for more CommitlogSegements to be available. 
Frame#1 thread will not get new segment because variable {{createReserveSegments(CommitLogSegmentManager.java)}} is not yet {{true}}. 
This variable gets set to {{true}} after successful execution of Frame#2.

Here we can see Frame#3 and Frame#1 are in deadlock state and Cassandra restart hangs forever.
 
{quote}
""MemtableFlushWriter:5"" #433 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0400 nid=0x1dea8 waiting on condition [0x00007e753c2ca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:419)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1027)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)


""MemtablePostFlush:3"" #432 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0000 nid=0x1dea7 waiting on condition [0x00007e753c30b000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
 parking to wait for  <0x00000006370d9cd0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:941)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:924)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Reproducible steps*: Reproducing this problem is tricky as it involves multiple conditions to happen at the same time and is timing bases, so I have done some small code change to reproduce this:
1. Create a Keyspace and table
2. Inject data until there are few SSTables generated and CommitLog available
3. Kill Cassandra process
4. Use the custom code (in the attached file ""Reproduce_CASSANDRA-13587.txt"") on top of 3.0.14 branch 
5. Build Cassandra jar and use this custom jar
6. Restart Cassandra
    Here you will see Cassandra is hanging forever
7. Now apply this fix on top of ""Reproduce_CASSANDRA-13587.txt"", and repeat step-6
    Here you should see Cassandra is starting normally

*Solution*: I am proposing that we should enable variable {{createReserveSegments(CommitLogSegmentManager.java)}} before recovering any CommitLogs in CommitLog.java file
so this will not block Frame#1 from acquiring new segment as a result Frame#1 will finish and then Frame#2 will also finish.
Please note, this variable {{createReserveSegments}} has been removed from the trunk branch as part of (https://issues.apache.org/jira/browse/CASSANDRA-10202), also in the trunk branch CommitLog segments gets created when needed. So as per my understanding enabling this variable before CommitLog recovery should not create any other side effect, please let me know your comments.
",,chovatia.jaydeep@gmail.com,jasobrown,jay.zhuang,jjirsa,jlida,sbtourist,szhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/17 23:12;chovatia.jaydeep@gmail.com;13587-3.0.txt;https://issues.apache.org/jira/secure/attachment/12872375/13587-3.0.txt","12/Jun/17 23:21;chovatia.jaydeep@gmail.com;Reproduce_CASSANDRA-13587.txt;https://issues.apache.org/jira/secure/attachment/12872780/Reproduce_CASSANDRA-13587.txt",,,,,,,,,,,,,,,,,,2.0,chovatia.jaydeep@gmail.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 13:38:55 UTC 2017,,,,,,,,,,"0|i3g4cv:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"09/Jun/17 23:12;chovatia.jaydeep@gmail.com;Please find patch attached.;;;","09/Jun/17 23:15;chovatia.jaydeep@gmail.com;Please find GitHub fix here: https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:CASSANDRA-13587;;;","12/Jun/17 23:21;chovatia.jaydeep@gmail.com;Reproducible steps patch;;;","29/Aug/17 23:04;jasobrown;[~chovatia.jaydeep@gmail.com] Thanks for the patch. I've created a branch on my repo and pushed to circleci (for unit tests) and apache jenkins (for dtests)

||3.0||
|[branch|https://github.com/jasobrown/cassandra/tree/13587-3.0]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/232/console]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13587-3.0]|

As {{CommitLogSegmentManager}} is reworked on the 3.11 and trunk branches, it makes a fix on 3.0 less critical. I agree your patch is straight-forward (and easy to review), I'm going to need to ask one or two other folks who are more familiar with the CL path to take a look as well.;;;","29/Aug/17 23:37;jay.zhuang;We deployed the patch to our prod clusters locally for about 2 months now. So far it looks good.;;;","29/Aug/17 23:45;jasobrown;well, damn, [~jay.zhuang] you should have said so 2 months ago :P. Srsly, though, that is good to hear. If the tests pass, I'll go ahead and commit.;;;","30/Aug/17 13:38;jasobrown;I reread the 3.0 CommitLog code this morning, and determined this change is actually pretty safe. We don't recycle the replayed commit log files (just delete them as of 2.2), so the {{createReserveSegments}} is pretty useless anyway.

+1 and committed as sha {{d03c046acbcfe3e3a9d8fafa628030cc3fc40f34}} to 3.0 only.

Thanks for the patch [~chovatia.jaydeep@gmail.com], and for the nice analysis!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in IR cleanup when columnfamily has no sstables,CASSANDRA-13585,13078494,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,09/Jun/17 02:15,15/May/20 08:04,14/Jul/23 05:56,09/Jun/17 10:55,4.0,4.0-alpha1,,,,,,,,,,0,incremental_repair,,,,"On {{PendingAntiCompaction::abort()}} , we try to release refs to sstables and a lifecycle transaction that can be null if there are no sstables in the table.
",,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 10:55:41 UTC 2017,,,,,,,,,,"0|i3g2hj:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"09/Jun/17 02:24;jjirsa;|| [trunk branch|https://github.com/jeffjirsa/cassandra/tree/cassandra-13585] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13585] |[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/81/] ||;;;","09/Jun/17 02:25;jjirsa;Either [~yukim] , [~krummas] or [~bdeggleston] up for quick review?
;;;","09/Jun/17 08:27;marcuse;+1;;;","09/Jun/17 10:55;jjirsa;Thanks Marcus, committed as {{ff29d609979aae2c584ec6e6cd370da56945dc18}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in rebuild_test.TestRebuild.disallow_rebuild_from_nonreplica_test,CASSANDRA-13583,13078100,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,michael.hamm,michael.hamm,07/Jun/17 20:06,15/May/20 08:05,14/Jul/23 05:56,17/Jul/17 16:59,4.0,4.0-alpha1,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/524/testReport/rebuild_test/TestRebuild/disallow_rebuild_from_nonreplica_test

{noformat}
Error Message

ToolError not raised
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: Python driver version in use: 3.10
dtest: DEBUG: cluster ccm directory: /tmp/dtest-0tUjhX
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 DC1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 DC1> discovered
--------------------- >> end captured logging << ---------------------
{noformat}


{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrappedtestrebuild
    f(obj)
  File ""/home/automaton/cassandra-dtest/rebuild_test.py"", line 357, in disallow_rebuild_from_nonreplica_test
    node1.nodetool('rebuild -ks ks1 -ts (%s,%s] -s %s' % (node3_token, node1_token, node3_address))
  File ""/usr/lib/python2.7/unittest/case.py"", line 116, in __exit__
    ""{0} not raised"".format(exc_name))
{noformat}",,aweisberg,jkni,marcuse,michael.hamm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/17 20:06;michael.hamm;node1.log;https://issues.apache.org/jira/secure/attachment/12871912/node1.log","07/Jun/17 20:06;michael.hamm;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12871914/node1_debug.log","07/Jun/17 20:06;michael.hamm;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12871909/node1_gc.log","07/Jun/17 20:06;michael.hamm;node2.log;https://issues.apache.org/jira/secure/attachment/12871911/node2.log","07/Jun/17 20:06;michael.hamm;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12871913/node2_debug.log","07/Jun/17 20:06;michael.hamm;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12871907/node2_gc.log","07/Jun/17 20:06;michael.hamm;node3.log;https://issues.apache.org/jira/secure/attachment/12871910/node3.log","07/Jun/17 20:06;michael.hamm;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12871915/node3_debug.log","07/Jun/17 20:06;michael.hamm;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12871908/node3_gc.log",,,,,,,,,,,9.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 17 16:59:23 UTC 2017,,,,,,,,,,"0|i3g01z:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"19/Jun/17 18:40;jkni;This is consistently failing when run without vnodes after the commit of [CASSANDRA-4650].;;;","20/Jun/17 15:29;marcuse;https://github.com/krummas/cassandra/commits/marcuse/13583

problem was that we ignored the sourceFound variable if localhost was in the map, but we need to still check that since localhost is filtered away when doing rebuild;;;","05/Jul/17 21:46;aweisberg;So I don't quite have the context to understand this. For bootstrap we filter out localhost using an ISourceFilter for obvious reasons. But for rebuild we don't want to generate a source not found error if the localhost is the only source, but neither do we want to stream from it? I don't understand why we don't need a source to stream from for rebuild.;;;","05/Jul/17 21:57;aweisberg;OK, I think I had it backwards. In the test case there is no source including localhost so we want the error and we previously didn't get it. But we also don't want to stream from localhost ever even if it counts as a source for some non-rebuild and non-bootstrap purpose.

Do I have it right?;;;","06/Jul/17 06:33;marcuse;bq. Do I have it right?
yes

running dtests here: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/118/;;;","06/Jul/17 17:53;aweisberg;The dtest failures look unrelated so  1.;;;","17/Jul/17 16:59;marcuse;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test,CASSANDRA-13576,13077785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,michael.hamm,michael.hamm,06/Jun/17 20:18,15/May/20 08:02,14/Jul/23 05:56,17/Aug/17 14:41,4.0,4.0-alpha1,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/445/testReport/bootstrap_test/TestBootstrap/consistent_range_movement_false_with_rf1_should_succeed_test

{noformat}
Error Message
31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:
INFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....
See system.log for remainder
{noformat}

{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 236, in consistent_range_movement_false_with_rf1_should_succeed_test
    self._bootstrap_test_with_replica_down(False, rf=1)
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 278, in _bootstrap_test_with_replica_down
    jvm_args=[""-Dcassandra.consistent.rangemovement={}"".format(consistent_range_movement)])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 696, in start
    self.wait_for_binary_interface(from_mark=self.mark)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 514, in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 471, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:\nINFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....\n
{noformat}

{noformat}
-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-PKphwD\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.protocol: WARNING: Server warning: When increasing replication factor you need to run a full (-full) repair to distribute the data.\ncassandra.connection: WARNING: Heartbeat failed for connection (139927174110160) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 32.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 64.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 128.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------""
{noformat}",,ifesdjeen,jasonstack,jkni,marcuse,michael.hamm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13654,,,,,,CASSANDRA-4650,,,,,,,,,,,,,"17/Aug/17 13:02;marcuse;Screen Shot 2017-08-17 at 14.46.00.png;https://issues.apache.org/jira/secure/attachment/12882355/Screen+Shot+2017-08-17+at+14.46.00.png","06/Jun/17 20:18;michael.hamm;node1.log;https://issues.apache.org/jira/secure/attachment/12871676/node1.log","06/Jun/17 20:18;michael.hamm;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12871677/node1_debug.log","06/Jun/17 20:18;michael.hamm;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12871678/node1_gc.log","06/Jun/17 20:18;michael.hamm;node2.log;https://issues.apache.org/jira/secure/attachment/12871679/node2.log","06/Jun/17 20:18;michael.hamm;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12871680/node2_debug.log","06/Jun/17 20:18;michael.hamm;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12871681/node2_gc.log","06/Jun/17 20:18;michael.hamm;node3.log;https://issues.apache.org/jira/secure/attachment/12871682/node3.log","06/Jun/17 20:18;michael.hamm;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12871683/node3_debug.log","06/Jun/17 20:18;michael.hamm;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12871684/node3_gc.log",,,,,,,,,,10.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 14:41:50 UTC 2017,,,,,,,,,,"0|i3fy3z:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"19/Jun/17 17:28;jkni;It looks like this has consistently been failing, with or without vnodes, since [CASSANDRA-4650] was committed.;;;","04/Jul/17 05:25;jasonstack;{quote}
useStrictConsistency = cassandra.consistent.rangemovement(default true) && !replacing;

useStrictSource = useStrictConsistency && tokens != null && tokenMetadata.getSizeOfAllEndpoints() != strategy.getReplicationFactor(); 

if (useStrictSource == true)
  -> getRangeFetchMap() // it handles the case of {{useStrictConsistency=false}} and rf=1 added in [11848|https://issues.apache.org/jira/browse/CASSANDRA-11848?focusedCommentId=15299052&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15299052]
else 
  -> getOtimizedRangeFetchMap()  // added in 4650, it doesn't handle the case of {{useStrictConsistency=false}} and rf=1.  so test failed.  added in [4650|https://issues.apache.org/jira/browse/CASSANDRA-4650]
{quote}

one simple fix would be change {{getOtimizedRangeFetchMap}} to handle the special case when {{rf=1 and useStrictConsistency=false}}..   

my understanding on [4650|https://issues.apache.org/jira/browse/CASSANDRA-4650] is that it values throughput over consistency. ( the optimization is applied when {{cassandra.consistent.rangemovement=false}} or {{node count equals to RF(rare cases)}})

[~jkni] what do you think?;;;","07/Jul/17 15:09;jasonstack;:D  LGTM;;;","14/Aug/17 16:14;marcuse;not sure what happened here [~ifesdjeen] why did you remove your branch+comment?

anyway, [here|https://github.com/krummas/cassandra/tree/marcuse/13576] is a patch to not optimise if rf == 1

dtest run:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/182;;;","15/Aug/17 11:24;ifesdjeen;[~krummas] I can't recall the details anymore, iirc someone mentioned it should've been fixed in the scope of some bigger issue, so I retracted my changes, although it seems that it was never committed.

I've pushed the changes [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13576-trunk] again, and I can say they're more or less identical to yours, I've just extracted {{AbstractReplicationStrategy}} to the separate variable to avoid looking it up several times.;;;","15/Aug/17 16:54;marcuse;I'm fine just not optimising rf = 1

As my patch has spent more than 15h in queue on builds.apache.org, lets commit that one once it is finished? Could you review [~ifesdjeen]?;;;","16/Aug/17 08:33;ifesdjeen;+1, LGTM;;;","17/Aug/17 14:09;marcuse;dtests failures look flaky (and the ones that could be related fail on trunk as well): [^Screen Shot 2017-08-17 at 14.46.00.png]

waiting for a clean circleci build before committing: https://circleci.com/gh/krummas/cassandra/72;;;","17/Aug/17 14:41;marcuse;circle ci looks ok - {{testFixedSize - org.apache.cassandra.db.commitlog.CommitLogStressTest}} failed, but that is unrelated

commited as 4f5bf0b67d2e0a93595cc8061018b20aa2309566;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mx4j default listening configuration comment is not correct,CASSANDRA-13574,13077346,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,05/Jun/17 21:06,27/May/22 19:24,14/Jul/23 05:56,29/Jun/21 21:02,3.0.25,3.11.11,4.0.1,4.1,4.1-alpha1,,Legacy/Tools,,,,,0,jmx_interface,,,,"{noformat}
By default mx4j listens on 0.0.0.0:8081.
{noformat}
https://github.com/apache/cassandra/blob/cassandra-2.2/conf/cassandra-env.sh#L302

It's actually set to Cassandra broadcast_address and it will never be {{0.0.0.0}}:
https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/utils/Mx4jTool.java#L79",,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 29 21:02:26 UTC 2021,,,,,,,,,,"0|i3fw6n:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"05/Jun/17 21:25;jay.zhuang;Please review the patch: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13574-3.0?expand=1;;;","29/Jun/21 21:02;brandon.williams;I was unable to look at your branch but since it was just comments I fixed them [here|https://github.com/apache/cassandra/commit/2d8b304e75805304bde5aab7a9d4d78085079e14].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnMetadata.cellValueType() doesn't return correct type for non-frozen collection,CASSANDRA-13573,13077216,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,ostefano,ostefano,05/Jun/17 10:38,29/Jun/21 17:32,14/Jul/23 05:56,08/Aug/17 14:58,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Materialized Views,Legacy/Core,Legacy/CQL,Legacy/Tools,,0,,,,,"Schema and data""
{noformat}
CREATE TABLE ks.cf (
    hash blob,
    report_id timeuuid,
    subject_ids frozen<set<int>>,
    PRIMARY KEY (hash, report_id)
) WITH CLUSTERING ORDER BY (report_id DESC);

INSERT INTO ks.cf (hash, report_id, subject_ids) VALUES (0x1213, now(), {1,2,4,5});
{noformat}

sstabledump output is:

{noformat}
sstabledump mc-1-big-Data.db 
[
  {
    ""partition"" : {
      ""key"" : [ ""1213"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 16,
        ""clustering"" : [ ""ec01eed0-49d9-11e7-b39a-97a96f529c02"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-06-05T10:29:57.434856Z"" },
        ""cells"" : [
          { ""name"" : ""subject_ids"", ""value"" : """" }
        ]
      }
    ]
  }
]
{noformat}

While the values are really there:

{noformat}
cqlsh:ks> select * from cf ;

 hash   | report_id                            | subject_ids
--------+--------------------------------------+-------------
 0x1213 | 02bafff0-49d9-11e7-b39a-97a96f529c02 |   {1, 2, 4}
{noformat}
",,adelapena,cnlwsu,jasonstack,ostefano,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14199,,CASSANDRA-12594,CASSANDRA-12628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 15:03:45 UTC 2017,,,,,,,,,,"0|i3fvdr:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,3.0.9,,,,,,,,,,,,,,,,,"06/Jul/17 08:19;jasonstack;There are some issues in sstabledump

1. as you reported,  frozen collections

2. non-frozen UDT


{quote}
Exception in thread ""main"" java.lang.ClassCastException: org.apache.cassandra.db.marshal.UserType cannot be cast to org.apache.cassandra.db.marshal.CollectionType
	at org.apache.cassandra.tools.JsonTransformer.serializeCell(JsonTransformer.java:413)
	at org.apache.cassandra.tools.JsonTransformer.serializeColumnData(JsonTransformer.java:396)
	at org.apache.cassandra.tools.JsonTransformer.serializeRow(JsonTransformer.java:276)
	at org.apache.cassandra.tools.JsonTransformer.serializePartition(JsonTransformer.java:210)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at org.apache.cassandra.tools.JsonTransformer.toJson(JsonTransformer.java:100)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:236)
{quote};;;","07/Jul/17 08:30;jasonstack;I am thinking to change all {{type.getString()}} in {{JsonTransformer}} to {{type.toJSONString}} when [13592|https://issues.apache.org/jira/browse/CASSANDRA-13592] merged.

Current {{collectionType.getString()}} only generates entire data as byte string. Imo,{{getString}} is not designed for generate json readable values.;;;","07/Jul/17 14:10;cnlwsu;The json transformer uses jackson's JsonGenerator so you do not need things designed to be in json output. Switching to {{toJSONString}} would end up double escaped as the jackson generator will try to remove the json encoding from the JSONString output.

If I remember correctly pretty much all UDTs were not supported initially because the information to deserialize the UDT requires the system schema tables to be read, which may not be available unless running on the same system with exact configuration and requires client initialization (which is dangerous and we dont want to do). If we have adequate info to deserialize the UDTs from the stats metadata it probably just needs a different check to handle it.;;;","09/Jul/17 11:23;jasonstack;I think we could use {{`writeRawValue(String)`}}  to avoid double escape.  Now that [13592|https://issues.apache.org/jira/browse/CASSANDRA-13592] is merged, we should have correct json representations for all types.. better than using {{type.toString}} which, imo, serves a different purposes.

bq.  all UDTs were not supported initially because the information to deserialize the UDT requires the system schema tables to be read (which is dangerous and we dont want to do)

If there is dependency to local schema tables, I agree not to support UDT and print raw-bytes instead.

But AFAIK after 8099, there should not be a dependency on schema tables while deserializing UDT.;;;","09/Jul/17 15:36;jasonstack;Some issues are related to {{ColumnMetadata.cellValueType()}} which currently :  {{a}}. if CollectionType, returns value's type; {{b}} otherwise,  its own type.

It doesn't handle properly:  {{1}}. frozen collection type,  {{2}}. non-frozen udt which requires cellPath to retrieve value's type..  

There are 3 kind of usage for {{ColumnMetadata.cellValueType()}}:

1. to check if column is counter type, it's safe

2. used in MV to check if cell value (base's non key column in view's primary key) is changed. 
    in the existing implementation, it will get {{cellValueType}} of a {{frozen collection}} to decode {{frozen collection bytes}}
    it can easily result in runtime error. eg. base has non-key column {{frozen<list<tuple<text,text>>}}  as view's primary key. so  {{tuple<text, text> type}} is used to decode {{frozen<list>}}
 
 {{non-frozen-udt}} cannot be used as view's primary key. the issue here is only with {{frozen-collection}}.
 
3. in {{sasi}}.  haven't check how it is affected.  will check it later;;;","10/Jul/17 02:59;jasonstack;first draft of the patch. if it looks good, I will prepare fixes for 2.2/3.0/3.11 as well
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573] | [unit|https://circleci.com/gh/jasonstack/cassandra/130] | [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13573] |

unit test: passed.
dtest: {{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} & {{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}} both are broken for some time

changes:
1. use {{type.toJSONString()}} with {{json.writeRawValue()}} instead of {{type.getString()}} to generate readable content 
2. {{column.cellValueType}} now :  {{a}}. if non-frozen collection, return value type, {{b}}. otherwise, return column type.
;;;","11/Jul/17 02:12;jasonstack;About the impact on SASI,  now sasi doesn't support {{complex}} type (aka, type with cellPath, non-frozen collection or udt).  

By fixing {{column.cellValueTytpe}}, the {{columnIndex.isLiteral()}} is now properly returning {{false}} if indexed column is {{frozen-collection}}.;;;","01/Aug/17 10:02;adelapena;The patch looks good to me, excellent job.

The assertion at {{ColumnMetadata#cellValueType()}} to check that the precondition mentioned in the comment is satisfied makes sense to me.

There are some minor nits about code style:
* There are some missed space-after-comma at {{ViewTest#testFrozenCollectionsWithComplicatedInnerType()}}. Also, it would be nice for the sake of uniformity to align the create table and create view statement as they are in similar tests in the same file.
* It would be good to add a {{@jira_ticket CASSANDRA-13573}} tag in the dtest docstring.
* This is just an idea, but I think that the dtest {{CREATE TABLE}} and insertion could be more readable using a cell per line, something like:
{code}
session.execute('CREATE TABLE ks.cf ('
                'key int PRIMARY KEY,'
                'list_f frozen<list<int>>,'
                'set_f frozen<set<int>>, '
                'map_f frozen<map<int,int>>,'
                'tuple_f frozen<tuple<int,int>>, '
                'user_type_f frozen<user_type>, '
                'list_v list<int>,'
                'set_v set<int>,'
                'map_v map<int,int>,'
                'tuple_v tuple<int,int>,'
                'user_type_v simple_type)')
...
session.execute(statement, [1,
                            [1, 2, 3],  # list_f
                            {1, 2, 3},  # set_f
                            {1: 1, 2: 2, 3: 3},  # map_f
                            (9, 9),  # map_f
                            FrozenUserType('SG', 100100, {'321', '123'}),  # user_type_f
                            [1, 2],  # list_v
                            {1, 2},  # set_v
                            {1: 1, 2: 2},  # map_v
                            (8, 8),  # tuple_v
                            NonFrozenUserType('SG', 100100)  # user_type_v
                            ])
{code}
What do you think?;;;","01/Aug/17 13:42;cnlwsu;Can we make sure to test this with CASSANDRA-13683 ? The sstabledump tool was changed to be tool initiated for last few versions which can lead to UDTs mistakenly accessed from the C* system tables which are not available always. We do not want to add it as a requirement to have to run sstabledump on a C* node.;;;","02/Aug/17 05:13;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-trunk] | [unit|https://circleci.com/gh/jasonstack/cassandra/356 ] |irrelevant:
materialized_views_test.TestMatyerializedViews.view_tombstones_test 
bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test
cql_tests.cqlsh_tests.TestCqlsh.test_describe
 |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-3.11] | [unit|https://circleci.com/gh/jasonstack/cassandra/360] | passed |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-3.0] | [unit|https://circleci.com/gh/jasonstack/cassandra/350]  | authe_test.TestAuth.system_auth_ks_is_alterable_test irrelevant | 
| [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13573] |

Addressed comments in dtest and src. 

I tested with ""clientInitialization()"" with data/schema folder removed. UDT works.   ;;;","03/Aug/17 10:08;adelapena;The changes look good to me. It seems that the CI tests that are finished are ok; it can be committed if the remaining tests pass.

One tiny detail that I forgot and can be fixed during commit, the comment ""Test that sstabledump against non-frozen udt"" in the dtest should be ""Test sstabledump against non-frozen udt"", without the ""that"".

Thanks!;;;","04/Aug/17 19:22;cnlwsu;lgtm +1;;;","08/Aug/17 14:57;adelapena;Committed to 3.0 as [3960260472fcd4e0243f62cc813992f1365197c6|https://github.com/apache/cassandra/commit/3960260472fcd4e0243f62cc813992f1365197c6] and merged into 3.11 and trunk.

Dtest committed to master as [959208749d70e5808aec144e87b73e90d56a7f91|https://github.com/apache/cassandra-dtest/commit/959208749d70e5808aec144e87b73e90d56a7f91];;;","08/Aug/17 15:03;jasonstack;Thank you both for your review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool listsnapshots output is missing a newline, if there are no snapshots",CASSANDRA-13568,13076432,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,arnd,arnd,arnd,01/Jun/17 08:34,15/May/20 08:05,14/Jul/23 05:56,22/Jun/17 02:19,3.0.15,3.11.1,4.0,4.0-alpha1,,,Tool/nodetool,,,,,0,,,,,"When there are no snapshots, the nodetool listsnaphots command output is missing a newline, which gives a somewhat bad user experience:

{code}
root@cassandra2:~# nodetool listsnapshots
Snapshot Details: 
There are no snapshotsroot@cassandra2:~# 
{code}

I",,arnd,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/17 08:37;arnd;0001-CASSANDRA-13568-add-newline-to-output-if-there-are-n.patch;https://issues.apache.org/jira/secure/attachment/12870758/0001-CASSANDRA-13568-add-newline-to-output-if-there-are-n.patch",,,,,,,,,,,,,,,,,,,1.0,arnd,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 02:19:20 UTC 2017,,,,,,,,,,"0|i3fqjr:",9223372036854775807,3.10,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"01/Jun/17 08:37;arnd;From fe868e2da9b79977d7819bbeb92f69264abef803 Mon Sep 17 00:00:00 2001
From: Arnd Hannemann <arnd@arndnet.de>
Date: Thu, 1 Jun 2017 10:34:52 +0200
Subject: [PATCH] CASSANDRA-13568: add newline to output if there are no
 snaphots

Before this patch, the nodetool listsnaphots command output is missing a newline,
if there are no snaphots.
---
 src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java b/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
index 1b3065bf11..4c22906609 100644
--- a/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
+++ b/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
@@ -42,7 +42,7 @@ public class ListSnapshots extends NodeToolCmd
             final Map<String,TabularData> snapshotDetails = probe.getSnapshotDetails();
             if (snapshotDetails.isEmpty())
             {
-                System.out.printf(""There are no snapshots"");
+                System.out.println(""There are no snapshots"");
                 return;
             }

-- 
2.11.0
;;;","22/Jun/17 01:57;jjirsa;lgtm, will commit as soon as we bump all the versions for the recent releases.
;;;","22/Jun/17 02:19;jjirsa;Actually [~mshuler] gave me the go-ahead to commit now, so it's committed to 3.0 as {{96bd3d53637d95ce268e7d4521a62bb400bc161b}} and merged up.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in topology_test.TestTopology.size_estimates_multidc_test,CASSANDRA-13567,13076316,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,mshuler,mshuler,31/May/17 21:20,16/Apr/19 09:30,14/Jul/23 05:56,22/Jun/17 15:38,,,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/367/testReport/topology_test/TestTopology/size_estimates_multidc_test

{noformat}
Error Message

Expected [['-3736333188524231709', '-2688160409776496397'], ['-6639341390736545756', '-3736333188524231709'], ['-9223372036854775808', '-6639341390736545756'], ['8473270337963525440', '8673615181726552074'], ['8673615181726552074', '-9223372036854775808']] from SELECT range_start, range_end FROM system.size_estimates WHERE keyspace_name = 'ks2', but got [[u'-3736333188524231709', u'-2688160409776496397'], [u'-9223372036854775808', u'-6639341390736545756'], [u'8673615181726552074', u'-9223372036854775808']]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-yNH4mu
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Creating cluster
dtest: DEBUG: Setting tokens
dtest: DEBUG: Starting cluster
dtest: DEBUG: Nodetool ring output 
Datacenter: dc1
==========
Address    Rack        Status State   Load            Owns                Token                                       
                                                                          8473270337963525440                         
127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -6639341390736545756                        
127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -2688160409776496397                        
127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              -2506475074448728501                        
127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              8473270337963525440                         

Datacenter: dc2
==========
Address    Rack        Status State   Load            Owns                Token                                       
                                                                          8673615181726552074                         
127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              -3736333188524231709                        
127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              8673615181726552074                         

  Warning: ""nodetool ring"" is used to output all the tokens of a node.
  To view status related info of a node use ""nodetool status"" instead.


  
dtest: DEBUG: Creating keyspaces
cassandra.policies: INFO: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
dtest: DEBUG: Refreshing size estimates
dtest: DEBUG: Checking node1_1 size_estimates primary ranges
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/topology_test.py"", line 107, in size_estimates_multidc_test
    ['8673615181726552074', '-9223372036854775808']])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 170, in assert_all
    assert list_res == expected, ""Expected {} from {}, but got {}"".format(expected, query, list_res)
'Expected [[\'-3736333188524231709\', \'-2688160409776496397\'], [\'-6639341390736545756\', \'-3736333188524231709\'], [\'-9223372036854775808\', \'-6639341390736545756\'], [\'8473270337963525440\', \'8673615181726552074\'], [\'8673615181726552074\', \'-9223372036854775808\']] from SELECT range_start, range_end FROM system.size_estimates WHERE keyspace_name = \'ks2\', but got [[u\'-3736333188524231709\', u\'-2688160409776496397\'], [u\'-9223372036854775808\', u\'-6639341390736545756\'], [u\'8673615181726552074\', u\'-9223372036854775808\']]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-yNH4mu\ndtest: DEBUG: Done setting configuration options:\n{   \'num_tokens\': None,\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ndtest: DEBUG: Creating cluster\ndtest: DEBUG: Setting tokens\ndtest: DEBUG: Starting cluster\ndtest: DEBUG: Nodetool ring output \nDatacenter: dc1\n==========\nAddress    Rack        Status State   Load            Owns                Token                                       \n                                                                          8473270337963525440                         \n127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -6639341390736545756                        \n127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -2688160409776496397                        \n127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              -2506475074448728501                        \n127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              8473270337963525440                         \n\nDatacenter: dc2\n==========\nAddress    Rack        Status State   Load            Owns                Token                                       \n                                                                          8673615181726552074                         \n127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              -3736333188524231709                        \n127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              8673615181726552074                         \n\n  Warning: ""nodetool ring"" is used to output all the tokens of a node.\n  To view status related info of a node use ""nodetool status"" instead.\n\n\n  \ndtest: DEBUG: Creating keyspaces\ncassandra.policies: INFO: Using datacenter \'dc1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\ndtest: DEBUG: Refreshing size estimates\ndtest: DEBUG: Checking node1_1 size_estimates primary ranges\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\n--------------------- >> end captured logging << ---------------------'
{noformat}",,jkni,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/17 21:27;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12870653/node1.log","31/May/17 21:27;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12870652/node1.log","31/May/17 21:27;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12870654/node1_debug.log","31/May/17 21:27;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12870655/node1_gc.log","31/May/17 21:27;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12870656/node2.log","31/May/17 21:27;mshuler;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12870657/node2_debug.log","31/May/17 21:27;mshuler;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12870658/node2_gc.log","31/May/17 21:27;mshuler;node3.log;https://issues.apache.org/jira/secure/attachment/12870659/node3.log","31/May/17 21:27;mshuler;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12870660/node3_debug.log","31/May/17 21:27;mshuler;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12870661/node3_gc.log",,,,,,,,,,10.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 15:38:17 UTC 2017,,,,,,,,,,"0|i3fptz:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"22/Jun/17 15:38;jkni;This test was added after [CASSANDRA-9639], which only went into 3.0.11+. It was version-gated in dtest commit [3cf276e966f253a49df91293a1a0b46620192c59|https://github.com/riptano/cassandra-dtest/commit/3cf276e966f253a49df91293a1a0b46620192c59].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra removenode makes Gossiper Thread hang forever,CASSANDRA-13562,13075922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,30/May/17 18:37,16/Apr/19 09:30,14/Jul/23 05:56,12/Jul/17 05:45,3.0.14,,,,,,Legacy/Core,,,,,0,,,,,"We have seen nodes in Cassandra (3.0.11) ring gets into split-brain somehow. We don't know exact reproducible steps but here is our observation:

Let's assume we have 5 node cluster n1,n2,n3,n4,n5. In this bug when do nodetool status on each node then each one has different view of DN node

e.g.
n1 sees n3 as DN and other nodes are UN
n3 sees n4 as DN and other nodes are UN
n4 sees n5 as DN and other nodes are UN and so on...

One thing we have observed is once n/w link is broken and restored then sometimes nodes go into this split-brain mode but we still don't have exact reproducible steps.

Please let us know if I am missing anything specific here.",,chovatia.jaydeep@gmail.com,jasonstack,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13308,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 05:43:27 UTC 2017,,,,,,,,,,"0|i3fnef:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"05/Jul/17 00:31;jasonstack;you may consider checking `phi_convict_threshold` in cassandra.yaml and turning on the logging for phi value in gossip. then you will get a better idea why c* node thinks another as down node.;;;","12/Jul/17 05:43;chovatia.jaydeep@gmail.com;I analyzed stack trace when Cassandra goes into split-brain mode and found that Gossiper thread is stuck at following stack location forever for HintsDispatchExecutor.java to complete, and HintsDispatchExecutor.java executor thread is blocked in delivering hints to the node being removed. They are going in dead-lock state and thats the reason behind this split brain. 

{quote}
""GossipStage:1"" #310
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ab000720> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
	at org.apache.cassandra.hints.HintsDispatchExecutor.completeDispatchBlockingly(HintsDispatchExecutor.java:112)
	at org.apache.cassandra.hints.HintsService.excise(HintsService.java:323)
	at org.apache.cassandra.service.StorageService.excise(StorageService.java:2265)
	at org.apache.cassandra.service.StorageService.excise(StorageService.java:2278)
	at org.apache.cassandra.service.StorageService.handleStateRemoving(StorageService.java:2234)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1690)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:2474)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1060)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:76)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
{quote}


Here are the reproducible steps:
1. Created Cassandra 3.0.13 cluster with few nodes (say 5 nodes)
2. Set {{hinted_handoff_throttle_in_kb}} to 1 (so that hint propagation will take time, we must hit removenode while hints are in-preogress to reproduce this issue)
3. Start a load on this cluster specifically write traffic
4. Purposefully shutdown one node and let hints build 
5. Restart node momentarily and make sure all nodes are in UN state, wait for 30 seconds to 1 min. so that {{HintsDispatchExecutor.java}} starts dispatching hints to the node
6. Kill Cassandra on that node again
7. Try removing that down node using {{nodetool removenode force}} or {{nodetool assassinate}}, at this point check {{nodetool status}} on each node and you will see they are in split-brain mode due to Gossip thread is stuck. At this point the only way to come out of this situation is to reboot Cassandra.

Fix for this problem is to do {{future.cancel}}, upon further investigation I found that it has already fixed as part of CASSANDRA-13308. I have tried reproducing this with 3.0.14 and it is no longer reproduced in 3.0.14.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema version id mismatch while upgrading to 3.0.13,CASSANDRA-13559,13075498,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,29/May/17 00:59,16/Apr/19 09:30,14/Jul/23 05:56,31/May/17 16:19,3.0.14,3.11.0,,,,,Legacy/Core,,,,,0,,,,,"As the order of SchemaKeyspace is changed ([6991556 | https://github.com/apache/cassandra/commit/6991556e431a51575744248a4c484270c4f918c9], CASSANDRA-12213), the result of function [{{calculateSchemaDigest}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L311] is also changed for the same schema. Which causes schema mismatch while upgrading 3.0.x -> 3.0.13.
It could cause cassandra fail to start because Unknown CF exception. And streaming will fail:
{noformat}
ERROR [main] 2017-05-26 18:58:57,572 CassandraDaemon.java:709 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 83c8eae0-3a65-11e7-9a27-e17fd11571e3
{noformat}
{noformat}
WARN  [MessagingService-Incoming-/IP] 2017-05-26 19:27:11,523 IncomingTcpConnection.java:101 - UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 922b7940-3a65-11e7-adf3-a3ff55d9bcf1. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
{noformat}

Restart the new node will cause:
{noformat}
Exception (java.lang.NoSuchFieldError) encountered during startup: ALL
java.lang.NoSuchFieldError: ALL
        at org.apache.cassandra.service.ClientState.<clinit>(ClientState.java:67)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<init>(QueryProcessor.java:155)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<clinit>(QueryProcessor.java:149)
        at org.apache.cassandra.cql3.QueryProcessor.internalQueryState(QueryProcessor.java:163)
        at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:286)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:294)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:900)
        at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:354)
        at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:110)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:179)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{noformat}

I would suggest to have the older list back for digest calculation and release 3.0.14.",,aleksey,jasonstack,jay.zhuang,jeromatron,jjirsa,mbyrd,ostefano,rha,sbtourist,scubadrew,snazy,stefania,tvdw,verma7,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 18:22:44 UTC 2018,,,,,,,,,,"0|i3fks7:",9223372036854775807,3.0.13,,,,,,stefania,,stefania,,,Critical,,,,,,,,,,,,,,,,,,,"29/May/17 01:35;jay.zhuang;Here is the patch: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13559-3.0?expand=1

To remove the older list, maybe we should bump the message version.
;;;","31/May/17 03:41;stefania;I agree this is a problem, thanks for reporting it. Since we need both the new and old orders, I prefer to only change the order for flushing and restore the old order for everything else. I am going to prepare a patch shortly.;;;","31/May/17 04:07;jjirsa;Any thoughts on an upgrade path from 3.0.13 - 3.0.14 Stefania?
;;;","31/May/17 07:22;stefania;Here are the patches for 3.0 and 3.11:

[patch for 3.0|https://github.com/stef1927/cassandra/tree/13559-3.0]
[patch for 3.11|https://github.com/stef1927/cassandra/tree/13559-3.11]

Since we don't need it for 4.0, I decided to keep the initial patch suggested by [~jay.zhuang], I merely added some comments and the entry into CHANGES.txt. I'm running the tests for 3.0 on our internal CI hosts. I will post a comment when the results are available.

Regarding the upgrade from 3.0.13 to 3.0.14, I don't think there is much we can do without a bump in the messaging version. I would tend to think we should release 3.0.14 with the patch asap. I also wonder why we did not notice this at all in our [upgrade tests|http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/lastCompletedBuild/testReport/].

For 3.11 we definitely need the patch since we haven't released it yet; whilst for 4.0 we don't need it because the messaging version has been bumped and hence the schema version will not be compared.

Regarding the {{NoSuchFieldError}} in the ticket description, it doesn't make much sense to me and I cannot reproduce it. The node starts without problems, perhaps a build issue? ;;;","31/May/17 09:41;stefania;The test results look good, no failures for the unit tests and 5 known failures for the dtests.;;;","31/May/17 10:08;snazy;+1, assuming tests look good.;;;","31/May/17 10:41;ostefano;[~Stefania], could you please elaborate a bit further on the .13-.14 upgrade path?
I am running .13 in production and I am quite concerned what to expect during the next round of upgrades.;;;","31/May/17 13:06;aleksey;+1 from me as well. Sorry for not spotting the issue in the original ticket, or even steering the implementation towards this issue.;;;","31/May/17 13:27;jjirsa;Can someone explain the upgrade path from 3.0.13 to 3.0.14 please?;;;","31/May/17 13:35;aleksey;You face the storm; no way around it it seems.

Pragmatically however there will be a lot more people upgrading from versions that aren't 3.0.13 then from the single recently released 3.0.13, so it makes sense to optimise for the majority of the users.;;;","31/May/17 13:40;jjirsa;There are three stack traces in the ticket that aren't just migration storm, and 2 of them are fail-to-startup errors. How does changing the schema version cause any of those?

I realize I can go run through minor version uploads and try to hit this myself, but I don't intuitively ""get"" why those stacks happen with ""just"" changing the schema version, but it seems likely they'll happen for anyone going 3.0.13 -> 3.0.14.
 

;;;","31/May/17 13:44;aleksey;bq. How does changing the schema version cause any of those?

It can't and it doesn't. Those should be separate JIRA tickets, if valid at all.

This patch is only addressing the difference in digest calculation, which is objectively an issue and has been reproduced.;;;","31/May/17 14:33;stefania;bq. could you please elaborate a bit further on the .13-.14 upgrade path?

When an upgraded node joins the ring, all non-upgraded nodes will pull the schema from it, after 1 minute. They shouldn't pull any longer from this node unless a real schema change happens. Since the schema is identical, a schema pull results in all schema mutations being applied and the schema tables being flushed, nothing else since the delta would be nil. When another node is upgraded and joins, the process repeats itself. 

Furthermore, the upgraded nodes will see the schema of the non-upgraded nodes as different and pull from them. Eventually the process converges, when all nodes are upgraded the schema versions will be the same. 

Things visible to the operator should be the schema migrations in the logs. Other noticeable things will be the schema version logged, which will be different after the upgrade, and different schema versions appearing in {{nodetool describecluster|describering}}. Also, in the system local table. This means that if any problems are likely to occur, they will mostly likely be client side, i.e. clients will not see a schema agreement until the upgrade is completed. This might cause issues in some applications. 

To limit the flow of schema pulls. the schema should not be changed during the upgrade, if at all possible. I'll add a section to NEWS.txt. 

That's all I could find from code inspection. [~spodxx@gmail.com] rightly pointed out in the dev mailing list that the only urgent thing is to pull 3.0.13. We can take our time before releasing 3.0.14, at a minimum we should write an upgrade test.

The startup problem is not related to the digest mismatch: the {{ALL}} field exists, so I don't understand it at all. I also struggle to explain the unknown cf exceptions, they are probably unrelated but the upgrade test should shed some light.;;;","31/May/17 14:43;jjirsa;If this is really just a single version change, this is a minor issue that doesn't justify pulling the package - CASSANDRA-13441 is present in .13 and earlier 3.0 versions will cause far more schema changes than this patch will for anyone upgrading from 2.1/2.2. This is worth fixing, but I don't see why it's worth pulling binaries.;;;","31/May/17 14:49;aleksey;bq. CASSANDRA-13441 is present in .13 and earlier 3.0 versions will cause far more schema changes than this patch will for anyone upgrading from 2.1/2.2

Yeah, but for people upgrading from 3.0.x that aren't 3.0.13 the fix would still be nice, and there are more of those than people on 3.0.13 - I would assume.

Not advocating for pulling binaries. Neither is a an urgent 3.0.14 release necessary imo. But the fix should go in. ;;;","31/May/17 15:22;spod;There are two different upgrade issues here. Let's not mix those up.


*3.0.x -> 3.0.13*

Based on the description by [~jay.zhuang] for this ticket, any node on  3.0.x that will be upgraded to the latest 3.0.13 bug fix release, will fail to start. This is the actual point why pulling the .13 release from the download page is probably a good idea, so people won't upgrade their cluster just to find themselves in a situation unable to bring any node up again.

*3.0.13 -> 3.0.14 (unrelease)*

Some users may already run 3.0.13 (e.g. by upgrading from 2.x or fresh installs). Maybe even on large production clusters. If we change the schema digest calculation in .14 again, we need to make sure that the users on .13 will be able to upgrade to .14 as well without seeing major issues, e.g. with schema migration storms. ;;;","31/May/17 15:43;ostefano;bq. Some users may already run 3.0.13 (e.g. by upgrading from 2.x or fresh installs).

I am not quite sure the issue as described by Jay Zhuang was meant to be deterministic.
I have upgraded from 3.0.12 to 3.0.13 without issues but the schema change storm.;;;","31/May/17 15:48;spod;Regarding the {{NoSuchFieldError}} startup error seems to be caused by some dirty class files. I can always reproduce this locally by doing something like this:

{noformat}
git checkout cassandra-3.0.12
ant clean jar
ccm create 3.0-3n --install-dir=/home/spod/git/cassandra-3.0 -n 3
ccm start
ccm node3 stop
git checkout cassandra-3.0.13
ant jar # NO CLEAN
ccm node3 start
{noformat}

If I do ""ant clean jar"" instead, the node is starting up fine. 

[~jay.zhuang], are you sure you've installed the vanilla Apacha tarbar cleanly on your side?;;;","31/May/17 15:57;aleksey;bq. There are two different upgrade issues here. Let's not mix those up.

There is only one confirmed issue here. It's a shame the JIRA is mixed up. We accidentally changed the way the digest is calculated, slightly, in 3.0.13. The issue is increased migration traffic. Not a big deal, but we should roll it back, so that the majority (lagging behind latest, always) is not affected.

bq.  If we change the schema digest calculation in .14 again, we need to make sure that the users on .13 will be able to upgrade to .14 as well without seeing major issues, e.g. with schema migration storms.

Well, we can't, so you have to choose one:
1. prevent extra migration traffic for those migrating from [3.0.0, 3.0.12] to 3.0.14+
2. prevent extra migration traffic for those migrating from 3.0.13 to 3.0.14+

With 3.0.13 being super fresh, with relatively few people on it (who already went through some extra migrations and survived anyway), I would say (1) is more important than (2).;;;","31/May/17 16:03;spod;That makes it much clearer, thanks for the wrap up! ;;;","31/May/17 16:17;jay.zhuang;Thanks [~Stefania] for the quick fix.

For a normal upgrade, I don't see the ""fail to start"" issue. 

For call stack:
{noformat}
ERROR [main] 2017-05-26 18:58:57,572 CassandraDaemon.java:709 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 83c8eae0-3a65-11e7-9a27-e17fd11571e3
{noformat}
It happens when the schema is changed while upgrading. For our case, a few nodes are not upgraded, so we have a cluster with mixed versions (3.0.x and 3.0.13) plus we add a table during that time which causes this issue.

For
{noformat}
Exception (java.lang.NoSuchFieldError) encountered during startup: ALL
java.lang.NoSuchFieldError: ALL
{noformat}
I don't think it's caused by this issue.

Schema version id mismatch also happens when upgrading from 2.2.x -> 3.0.x and maybe other major/minor version upgrade. I think it not worth pulling 3.0.13 binaries.;;;","31/May/17 16:18;aleksey;Committed to 3.0 as [f96a5dc5840c7d1fea99aa450543af8e889c161e|https://github.com/apache/cassandra/commit/f96a5dc5840c7d1fea99aa450543af8e889c161e] and merged into 3.11, trunk omitted. Thanks.;;;","31/May/17 16:20;aleksey;[~jay.zhuang] Feel free to continue in a new JIRA ticket to handle your remaining issues.;;;","01/Jun/17 03:22;stefania;Thank you for committing the ticket [~iamaleksey] and to everybody for the clarifications.

I've created an upgrade test [here|https://github.com/riptano/cassandra-dtest/compare/master...stef1927:13559]. To run it export {{JAVA8_HOME}} and then choose one of the following test combinations:

{code}
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_12_To_released_3_0_13.test_schema_agreement
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_13_To_indev_3_0_x.test_schema_agreement
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_12_To_indev_3_0_x.test_schema_agreement
{code}

The test verifies that the schemas match before and during upgrade, when creating a table before and during upgrade. 3.0.12 -> current will succeed, whilst 3.0.12 -> 3.0.13 and 3.0.13 -> current will fail. In the logs we can see the schema pulls but no other issues (the test checks for a schema match before creating the table during the upgrade). 

I'll leave the test with the custom versions available for a few days, if people won't to play with it, e.g. by testing with a larger cluster or by adding more functionality during the upgrade. Before creating a pull request, we need to uncomment  {{@since('3.0.14', max_version='3.99')}} and remove the lines in [{{OVERRIDE_MANIFEST}}|https://github.com/riptano/cassandra-dtest/compare/master...stef1927:13559#diff-7de72be82b9e05ef3451358461197944R121].
;;;","02/Jun/17 11:03;spod;I've now created CASSANDRA-13569, which should help making any schema pulls created by mismatching schemas less stormy. Should be a good idea to get this in along with this patch in 3.0.14.;;;","02/Jun/17 11:40;spod;Looks like hints will also not get dispatched in cases of mismatching schema IDs (see HintsDispatchTrigger). This should also be put in the yet to be updated NEWS.txt.;;;","02/Jun/17 11:44;aleksey;bq. Looks like hints will also not get dispatched in cases of mismatching schema IDs (see HintsDispatchTrigger). This should also be put in the yet to be updated NEWS.txt.

Only until the pair of nodes converges on the same schema - eventually. Just like with regular convergence. Not sure it's NEWSworthy.;;;","05/Jun/17 01:31;stefania;{{NEWS.txt}} updated (only for schema migrations and only in 3.0.x), see commit [6b36d9|https://github.com/apache/cassandra/commit/6b36d9f0506351f03555efaa3a0784d097913adf].

Pull request for upgrade test created [here|https://github.com/riptano/cassandra-dtest/pull/1477].;;;","22/May/18 18:22;scubadrew;Hello everyone. I am seeing this error when restarting a 3.0.12 node.

ERROR [main] 2018-05-22 17:53:09,355 CassandraDaemon.java:710 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 10c30740-1284-11e8-862f-5f9049872b18
 at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:206) ~[apache-cassandra-3.0.12.jar:3.0.12]

I realize this thread is mostly concerned with upgrades. We are not trying to upgrade. Simply trying to stop and then restart the service on a single node in a long running cluster.

Any advice how we can recover this node?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial format error in StorageProxy,CASSANDRA-13551,13074640,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,24/May/17 20:36,15/May/20 08:02,14/Jul/23 05:56,20/Jun/17 17:06,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Maybe I should just ninja it: 

{code}
diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java
index ea082d5..1ab8dd6 100644
--- a/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -1319,7 +1319,7 @@ public class StorageProxy implements StorageProxyMBean
                 }
                 catch (Exception ex)
                 {
-                    logger.error(""Failed to apply mutation locally : {}"", ex);
+                    logger.error(""Failed to apply mutation locally"", ex);
                 }
             }
@@ -1345,7 +1345,7 @@ public class StorageProxy implements StorageProxyMBean
                 catch (Exception ex)
                 {
                     if (!(ex instanceof WriteTimeoutException))
-                        logger.error(""Failed to apply mutation locally : {}"", ex);
+                        logger.error(""Failed to apply mutation locally"", ex);
                     handler.onFailure(FBUtilities.getBroadcastAddress());
                 }
             }

{code}",,aweisberg,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 17:06:29 UTC 2017,,,,,,,,,,"0|i3ffhj:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"24/May/17 20:39;aweisberg;+1;;;","20/Jun/17 17:06;jjirsa;Thanks Ariel for the review and the nudge.

Committed to trunk as {{f21202e83f308ea22cd430499da60aebbfa8ffbc}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cqlsh throws and error when querying a duration data type,CASSANDRA-13549,13074352,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,akhilm,akhilm,akhilm,23/May/17 23:00,15/May/20 08:05,14/Jul/23 05:56,29/May/17 15:46,3.11.0,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"h3. Overview

Querying duration related data from the cqlsh prompt results in an error.

Consider the following create table and insert statement.
{code:title=Table and insert statement with duration data type|borderStyle=solid}
CREATE TABLE duration_test (
  primary_key text,
  col20 duration,
  PRIMARY KEY (primary_key)
);
INSERT INTO duration_test (primary_key, col20) VALUES ('primary_key_example', 1y5mo89h4m48s);
{code}

On executing a select query on col20 in cqlsh I get an error ""Failed to format value '""\x00\xfe\x02GS\xfc\xa5\xc0\x00' : 'ascii' codec can't decode byte 0xfe in position 2: ordinal not in range(128)""
{code:title=Duration Query|borderStyle=solid}
Select  col20 from duration_test;
{code}

h3. Investigation

On investigating this further I found that the current python Cassandra driver used found in lib/cassandra-driver-internal-only-3.7.0.post0-2481531.zip does not seem to support duration data type. This was added in Jan this year https://github.com/datastax/python-driver/pull/689.

So I downloaded the latest driver release https://github.com/datastax/python-driver/releases/tag/3.9.0. I embedded the latest driver into cassandra-driver-internal-only-3.7.0.post0-2481531.zip. This fixed the driver related issue but there was still a formatting issue. 

I then went on to modify the format_value_duration methos in the pylib/cqlshlib/formatting.py. Diff posted below

{code}
 @formatter_for('Duration')
 def format_value_duration(val, colormap, **_):
-    buf = six.iterbytes(val)
-    months = decode_vint(buf)
-    days = decode_vint(buf)
-    nanoseconds = decode_vint(buf)
-    return format_python_formatted_type(duration_as_str(months, days, nanoseconds), colormap, 'duration')
+    return format_python_formatted_type(duration_as_str(val.months, val.days, val.nanoseconds), colormap, 'duration')
{code}

This resulted in fixing the issue and duration types are now correctly displayed.

Happy to fix the issue if I can get some guidance on:
# If this is a valid issue. Tried searching JIRA but did not find anything reported. 
# If my assumptions are correct i.e. this is actually a bug
# how to package the new driver into the source code. 







",Cassandra 3.10 dev environment running on a MacOS Sierra,aholmber,akhilm,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,akhilm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 29 15:46:08 UTC 2017,,,,,,,,,,"0|i3fdpj:",9223372036854775807,3.10,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"24/May/17 07:47;blerer;I need to have a look into it because, it should have worked with the {{3.10}} internal driver even if it was not supporting natively durations. ;;;","24/May/17 13:39;blerer;I am not fully sure of what changed at the driver level since CASSANDRA-11873 was committed.

With the current internal driver ({{cassandra-driver-internal-only-3.7.0.post0-2481531.zip}}) the CQL type being returned is a custom type.
Which makes sense as it is the type returned by Cassandra for {{Duration}} when the protocol version is {{4}}.
Due to that the proper formatter is not found correctly and the {{ascii}} one is used.
With the latest driver. Even if C* return a custom type the driver map it to a {{duration}} and return a {{Duration}} object that cannot be processed by the current formatter.

The proper fix is in my opinion to provide 2 formatters in the 3.11 version one for the custom type and one for the duration type.

I have pushed a patch for it [here|https://github.com/apache/cassandra/compare/trunk...blerer:13549-3.11].

[~aholmber] does my approach make sense to you?


;;;","24/May/17 17:57;aholmber;Historically we would only have one formatter because it is assumed that the combination of bundled driver, and protocol version in use is fixed for a given version of cqlsh. I could see making the case for two if you don't want to upgrade now, but want to be defensive about changes in driver version, or possibly to maintain parity in `cqlshlib.formatting` across branches. I don't know of anything that would preclude updating the bundled driver at this point.

I'm still a little perplexed by what changed if there was no driver upgrade, but I haven't gone looking.

To answer the earlier question:
bq. how to package the new driver into the source code.
See https://wiki.apache.org/cassandra/HowToContribute#Bundled_Drivers;;;","24/May/17 20:11;blerer;bq. I'm still a little perplexed by what changed if there was no driver upgrade, but I haven't gone looking.

We updated the driver since  CASSANDRA-11873 but I am not convinced that the problem was coming from there. I suspect that it was caused by some change I did but I could not find which change was the guilty one :-(

My concern was that the {{duration}} is not required for the protocol {{V4}} but only for {{V5}}. Nevertheless, updating the driver is what make the most sense.

 [~akhilm] The fix that you suggested is the good one :-) If you want to finish the work that you started feel free to asign the ticket to yourself and put me as reviewer.;;;","25/May/17 10:53;akhilm;[~blerer] Thanks for the feedback. Will complete the fix. I do not have permissions to assign the ticket to myself. Can you please assign the ticket to me or give me permission to assign the ticket to myself. Thanks

;;;","26/May/17 01:18;akhilm;[~blerer] I just noticed that version 3.10.0 of the python driver was released yesterday (https://github.com/datastax/python-driver/releases/tag/3.10.0). Tested my changes against 3.10.0 and everything worked as expected.

Do you want me to stick with the 3.9.0 driver or switch to the 3.10.0 driver as it is the latest?

Thanks for your input. 

;;;","26/May/17 02:23;akhilm;I have committed the changes to the following two branches.

[13549-trunk|https://github.com/amehra/cassandra/tree/13549-trunk]
[13549-3.11|https://github.com/amehra/cassandra/tree/13549-3.11]

Both branches have 3.10.0 driver in them and the required changes in formatting.py. If you want me to drop it back to 3.9.0 python driver please let me know. 

Is there any way I can run the dtests on these two branches on http://cassci.datastax.com/ ?

Thanks ;;;","29/May/17 08:23;blerer;Thanks for the patches :-)

bq. If you want me to drop it back to 3.9.0 python driver please let me know. 

According to the driver [CHANGELOGS|https://github.com/datastax/python-driver/blob/master/CHANGELOG.rst] the support for the {{Duration}} type has been added to the {{3.10}} version so we should use that one. It also not make a lot of sense to build a pre-release of {{3.11}}.

bq. Is there any way I can run the dtests on these two branches on http://cassci.datastax.com/ ?

{{cassci}} will soon be shutdown so we should not run build there anymore. I will run the tests for the 2 branches on our internal CI.

  ;;;","29/May/17 15:20;blerer;I ran the test for 3.11 and trunk. There are few failling CQLSH tests but they were failing before the patch so I am +1 for the patch.

Thanks for the work.;;;","29/May/17 15:46;blerer;Committed into 3.11 at 5a860a70f28b7756e0073d2d5d239b5e748a0b73 and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in CompactionExecutor leading to tmplink files not being removed,CASSANDRA-13545,13073913,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,derokhin,derokhin,22/May/17 15:54,16/Apr/19 09:30,14/Jul/23 05:56,24/Jan/19 18:44,,,,,,,Local/Compaction,,,,,0,,,,,"We are facing an issue where compactions fail on a few nodes with the following message
{code}
ERROR [CompactionExecutor:1248] 2017-05-22 15:32:55,390 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:1248,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.sstable.IndexSummary.<init>(IndexSummary.java:86) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.build(IndexSummaryBuilder.java:235) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:316) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:170) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:115) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.append(DefaultCompactionWriter.java:64) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:184) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:256) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}
Also, the number of tmplink files in /var/lib/cassandra/data/<keyspace name>/blocks/tmplink* is growing constantly until node runs out of space. Restarting cassandra removes all tmplink files, but the issue still continues.
We are using Cassandra 2.2.5 on Debian 8 with Oracle Java 8
{code}
root@cassandra-p10:/var/lib/cassandra/data/mugenstorage/blocks-33167ef0447a11e68f3e5b42fc45b62f# dpkg -l | grep -E ""java|cassandra""
ii  cassandra                      2.2.5                        all          distributed storage system for structured data
ii  cassandra-tools                2.2.5                        all          distributed storage system for structured data
ii  java-common                    0.52                         all          Base of all Java packages
ii  javascript-common              11                           all          Base support for JavaScript library packages
ii  oracle-java8-installer         8u121-1~webupd8~0            all          Oracle Java(TM) Development Kit (JDK) 8
ii  oracle-java8-set-default       8u121-1~webupd8~0            all          Set Oracle JDK 8 as default Java
{code}",,derokhin,jay.zhuang,jjirsa,mbyrd,pauloricardomg,szhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 05 17:17:22 UTC 2017,,,,,,,,,,"0|i3fazz:",9223372036854775807,,,,,,,,,,,,Critical,,2.2.5,,,,,,,,,,,,,,,,,"05/Jun/17 17:17;derokhin;One of our engineers has been able to find at least one issue which leads to this condition. His findings are below.
---

With a consistent reproduction outside of the production cluster, I downloaded the cassandra source code, setup a remote debugger (eclipse) and connected it to the cassandra process running on my node.
 
At this point I was able to setup breakpoints and examine a live system, starting at the last frame in the traceback (org.apache.cassandra.io.sstable.IndexSummary.<init>(IndexSummary.java:86)). Stepping through the code duing a live compaction, I was able to determine that the issue is indeed a bug in Cassandra that occurs when it is trying to run a compaction job with a very large number of partitions.
 
The SafeMemoryWriter class is used to build the index summary for the new sstable.
{code:java}
public class SafeMemoryWriter extends DataOutputBuffer
{
    private SafeMemory memory;
 
    @SuppressWarnings(""resource"")
    public SafeMemoryWriter(long initialCapacity)
    {
        this(new SafeMemory(initialCapacity));
    }
 
    private SafeMemoryWriter(SafeMemory memory)
    {
        super(tailBuffer(memory).order(ByteOrder.BIG_ENDIAN));
        this.memory = memory;
    }
 
    public SafeMemory currentBuffer()
    {
        return memory;
    }
 
    @Override
    protected void reallocate(long count)
    {
        long newCapacity = calculateNewSize(count);
        if (newCapacity != capacity())
        {
            long position = length();
            ByteOrder order = buffer.order();
 
            SafeMemory oldBuffer = memory;
            memory = this.memory.copy(newCapacity);
            buffer = tailBuffer(memory);
 
            int newPosition = (int) (position - tailOffset(memory));
            buffer.position(newPosition);
            buffer.order(order);
 
            oldBuffer.free();
        }
    }
 
    public void setCapacity(long newCapacity)
    {
        reallocate(newCapacity);
    }
 
    public void close()
    {
        memory.close();
    }
 
    public Throwable close(Throwable accumulate)
    {
        return memory.close(accumulate);
    }
 
    public long length()
    {
        return tailOffset(memory) +  buffer.position();
    }
 
    public long capacity()
    {
        return memory.size();
    }
 
    @Override
    public SafeMemoryWriter order(ByteOrder order)
    {
        super.order(order);
        return this;
    }
 
    @Override
    public long validateReallocation(long newSize)
    {
        return newSize;
    }
 
    private static long tailOffset(Memory memory)
    {
        return Math.max(0, memory.size - Integer.MAX_VALUE);
    }
 
    private static ByteBuffer tailBuffer(Memory memory)
    {
        return memory.asByteBuffer(tailOffset(memory), (int) Math.min(memory.size, Integer.MAX_VALUE));
    }
}
{code}
The appears like it is intended to work with buffers larger than Integer.MAX_VALUE, however if the initial size of the buffer is larger than that the initial value of length() will be incorrect (it won’t be zero) and writing via the DataOutputBuffer will write in the wrong location (it won’t start at offset 0).
 
 
{code:java}
    public IndexSummaryBuilder(long expectedKeys, int minIndexInterval, int samplingLevel)
    {
        this.samplingLevel = samplingLevel;
        this.startPoints = Downsampling.getStartPoints(BASE_SAMPLING_LEVEL, samplingLevel);
 
        long maxExpectedEntries = expectedKeys / minIndexInterval;
        if (maxExpectedEntries > Integer.MAX_VALUE)
        {
            // that's a _lot_ of keys, and a very low min index interval
            int effectiveMinInterval = (int) Math.ceil((double) Integer.MAX_VALUE / expectedKeys);
            maxExpectedEntries = expectedKeys / effectiveMinInterval;
            assert maxExpectedEntries <= Integer.MAX_VALUE : maxExpectedEntries;
            logger.warn(""min_index_interval of {} is too low for {} expected keys; using interval of {} instead"",
                        minIndexInterval, expectedKeys, effectiveMinInterval);
            this.minIndexInterval = effectiveMinInterval;
        }
        else
        {
            this.minIndexInterval = minIndexInterval;
        }
 
        // for initializing data structures, adjust our estimates based on the sampling level
        maxExpectedEntries = Math.max(1, (maxExpectedEntries * samplingLevel) / BASE_SAMPLING_LEVEL);
        offsets = new SafeMemoryWriter(4 * maxExpectedEntries).order(ByteOrder.nativeOrder());
        entries = new SafeMemoryWriter(40 * maxExpectedEntries).order(ByteOrder.nativeOrder());
 
        // the summary will always contain the first index entry (downsampling will never remove it)
        nextSamplePosition = 0;
        indexIntervalMatches++;
    }
{code}
The bug occurs when the entries table in the index summary for the new sstable is larger than Integer.MAX_VALUE bytes (2 GiB). This happens when expectedKeys > Integer.MAX_VALUE / 40 * minIndexInterval . Our partitions for the blocks table have a mean size of 179 bytes, so we would expect to see issues on this table for compactions over about 1.12 TiB.
 
The default value of minIndexInterval is 128, however it is adjustable per table and can be used to avoid this condition. It should be set to a power of 2. I ran this cql on my test node:
{code:sql}
ALTER TABLE tablename.blocks WITH min_index_interval = 512 ;
{code}
Since this change, I haven’t seen the assertion. The compaction has proceeded much farther than before, but it has not completed yet since it is so large.
{noformat}
$ nodetool compactionstats -H
pending tasks: 1
                                     id   compaction type       keyspace    table   completed     total    unit   progress
   9965f4b0-4749-11e7-b21c-91cb0a91f895        Compaction   tablename   blocks   629.51 GB   1.34 TB   bytes     45.71%
Active compaction remaining time :        n/a
{noformat}
I would expect that making this change would fix the issue for all future compactions on all nodes.
 
The index summary is used to reduce disk io to the sstable index. A larger index interval would result in a less efficient index summary and more io to the sstable index. However the min is just the minimum value, the actual value is controlled automatically by Cassandra. On p10, it is 2048 for the larger blocks sstables, so I would not expect a performance impact.

Compaction failed with new error
{code}
ERROR [CompactionExecutor:6] 2017-06-04 10:15:26,115 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.AssertionError: Illegal bounds [-2147483648..-2147483640); size: 3355443200
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:339) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:104) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:224) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedSegmentedFile.createMappedSegments(CompressedSegmentedFile.java:80) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.<init>(CompressedPoolingSegmentedFile.java:38) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:101) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:188) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:179) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:345) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:333) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:297) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:345) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:169) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:79) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:169) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:179) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:89) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:196) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:256) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool scrub/cleanup/upgradesstables exit code,CASSANDRA-13542,13073772,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,22/May/17 06:28,15/May/20 08:03,14/Jul/23 05:56,02/Jun/17 13:53,3.0.14,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,We exit nodetool with success if we fail marking sstables as compacting,,jjirsa,marcuse,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 13:53:55 UTC 2017,,,,,,,,,,"0|i3fa4n:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"22/May/17 06:36;marcuse;https://github.com/krummas/cassandra/tree/marcuse/exitcode
https://github.com/krummas/cassandra/tree/marcuse/exitcode-3.11
https://github.com/krummas/cassandra/tree/marcuse/exitcode-trunk;;;","22/May/17 16:20;jjirsa;+1
;;;","02/Jun/17 13:53;marcuse;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The keyspace repairTime metric is not updated,CASSANDRA-13539,13073170,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,18/May/17 15:10,15/May/20 08:06,14/Jul/23 05:56,19/May/17 21:09,4.0,4.0-alpha1,,,,,Legacy/Observability,,,,,0,,,,,repairTime metric at keyspace metric isnt updated when repairs complete so its always zeros.,,bdeggleston,cnlwsu,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 19 21:09:48 UTC 2017,,,,,,,,,,"0|i3f6ev:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"18/May/17 15:28;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/113

    Update repairTime for keyspaces on completion (CASSANDRA-13539)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13539

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/113.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #113
    
----
commit 2132d7b66f05cd9c6e5b4488a5025754456c82cc
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-05-18T15:27:51Z

    Update repairTime for keyspaces on completion (CASSANDRA-13539)

----
;;;","18/May/17 22:14;bdeggleston;+1, will commit after [utests|https://circleci.com/gh/bdeggleston/cassandra/52] finish;;;","19/May/17 21:09;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/113
;;;","19/May/17 21:09;bdeggleston;Committed to trunk as {{e1f2300a1ae7dab1660c16fc38bcb852fdcd44ef}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnIdentifier object size wrong when tables are not flushed,CASSANDRA-13533,13072508,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,16/May/17 16:07,15/May/20 08:05,14/Jul/23 05:56,26/May/17 19:24,3.0.14,3.11.0,4.0,4.0-alpha1,,,Legacy/Core,,,,,0,,,,,"It turns out that the object size of {{ColumnIdentifier}} is wrong when *cassandra.test.flush_local_schema_changes: false*. This looks like stuff is being wrongly reused when no flush is happening.

We only noticed this because we were using the prepared stmt cache and noticed that prepared statements would account for *1-6mb* when *cassandra.test.flush_local_schema_changes: false*. With *cassandra.test.flush_local_schema_changes: true* (which is the default) those would be around *5000 bytes*.

Attached is a test that reproduces the problem and also a fix.

Also after talking to [~jkni] / [~blerer] we shouldn't probably take {{ColumnDefinition}} into account when measuring object sizes with {{MemoryMeter}}
",,eduard.tudenhoefner,jjirsa,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/17 23:46;eduard.tudenhoefner;columnidentifier.png;https://issues.apache.org/jira/secure/attachment/12868425/columnidentifier.png",,,,,,,,,,,,,,,,,,,1.0,eduard.tudenhoefner,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 19:24:34 UTC 2017,,,,,,,,,,"0|i3f2d3:",9223372036854775807,3.0.13,,,,,,jkni,,jkni,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"16/May/17 16:19;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/CASSANDRA-13533-30
Tests: https://circleci.com/gh/nastra/cassandra/tree/CASSANDRA-13533-30;;;","16/May/17 22:48;jjirsa;{quote}
This looks like stuff is being wrongly reused when no flush is happening
{quote}

Obviously that flag is only for unit tests, but can you describe the symptom that led you to finding/fixing this?  



;;;","16/May/17 23:59;eduard.tudenhoefner;[~jjirsa] we were using that flag in some of our integration tests where we found out that, _very rarely_, simple things (usually taking < 1 second) would take a long amount of time (> 30 seconds). 

We boiled it then down to the point where we saw that this time was 99% spent in the prepared stmt cache for adding/evicting elements. And in our case, eviction was happening because a single prepared stmt would account *1mb - 6mb* (prep stmt cache size was *8mb* for us). After looking closer why prep stmts would be so large, we eventually saw that the binary representation of {{ColumnIdentifier#text}} was be wrong and not what we would have expected (see attached screenshot).

We also only detected all of this because on *cassandra-3.0* we don't use *.omitSharedBufferOverhead()* with {{MemoryMeter}} object measurements ( [QueryProcessor.java#L67|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L67]). 
On *cassandra-3.11* things got refactored over time and the prep stmt cache was using {{MemoryMeter}} stuff from {{ObjectSizes}}, where *.omitSharedBufferOverhead()* is being applied and the issue wouldn't happen ([ObjectSizes.java#L35|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/utils/ObjectSizes.java#L35]).

;;;","24/May/17 20:49;jkni;The patch looks good. I merged this forward through 3.11 and trunk - testall and dtests look good for all branches relative to upstream.

I'm not sure about including the test as written; it passes on 3.11 and trunk even before the fix because of the default of offheap memtables introduced in [CASSANDRA-9472]. It seems to me that we might as well reduce this test to just checking that interning a ColumnIdentifier uses a minimal bytebuffer and add it to {{ColumnIdentifierTest}}.;;;","24/May/17 21:52;jkni;As a concrete example, something like the following would suffice to show the interning issue on all active branches.

{code}
    @Test
    public void testInterningUsesMinimalByteBuffer()
    {
        byte[] bytes = new byte[2];
        bytes[0] = 0x63;
        ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);
        byteBuffer.limit(1);

        ColumnIdentifier c1 = ColumnIdentifier.getInterned(byteBuffer, UTF8Type.instance);

        Assert.assertEquals(2, byteBuffer.capacity());
        Assert.assertEquals(1, c1.bytes.capacity());
    }
{code}

What do you think, [~eduard.tudenhoefner]?;;;","24/May/17 23:02;eduard.tudenhoefner;that works for me [~jkni]. The test was only there to show that stuff was broken and anything we can do to shorten the circumstances for reproduction is a +1 from my side.;;;","26/May/17 19:24;jkni;+1 - thanks for the patch. I ran tests for all relevant branches; no unit tests failed on 3.0/3.11/trunk, and dtests looked the same as the present state of those branches.

Committed to 3.0 as {{8ffdd26cbee33c5dc1205c0f7292628e1a2c69e3}} and merged forward.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump reports incorrect usage for argument order,CASSANDRA-13532,13072099,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,varuna,ian.ilsley@datastax.com,ian.ilsley@datastax.com,15/May/17 18:08,15/May/20 08:01,14/Jul/23 05:56,11/Aug/17 01:16,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/Tools,,,,,0,lhf,,,,"sstabledump usage reports 

{{usage: sstabledump <options> <sstable file path>}}

However the actual usage is 

{{sstabledump  <sstable file path> <options>}}",,ian.ilsley@datastax.com,jasonstack,jjirsa,micksear,varuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/17 06:30;varuna;sstabledump#printUsage.patch;https://issues.apache.org/jira/secure/attachment/12874398/sstabledump%23printUsage.patch",,,,,,,,,,,,,,,,,,,1.0,varuna,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 01 11:32:32 UTC 2017,,,,,,,,,,"0|i3ezuf:",9223372036854775807,3.0.4,,,,,,jasonstack,,jasonstack,,,Low,,,,,,,,,,,,,,,,,,,"25/Jun/17 06:30;varuna;I'm submitting a patch to fix the `printUsage` function of `SSTableExport.java`. ;;;","25/Jun/17 06:31;varuna;PFA the patch file;;;","26/Jun/17 14:33;jasonstack;LGTM;;;","11/Aug/17 01:16;jjirsa;Thanks all! Committed as {{fab384560311ec1f3043fbf6137093ea129afa68}}
;;;","01/Sep/17 11:32;micksear;I just came across this bug myself.  I think, though, that it would be preferable to fix the parser so it consumes a single value per argument.  e.g. :

{code:java}
sstabledump -k mykey1 -k mykey2 mysstable
{code}

Don't you think?  I'd have thought this would be more consistent with the way arguments are normally used on the command line.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more (incremental) repair metrics,CASSANDRA-13531,13071898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,15/May/17 07:33,15/May/20 08:04,14/Jul/23 05:56,15/May/17 14:45,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Patch adds the following metrics;
* time spent anticompacting data before participating in a consistent repair
* time spent creating merkle trees
* time spent syncing data in a repair
* approximate number of bytes read while creating merkle trees
* number of partitions read creating merkle trees
* number of bytes read while doing anticompaction
* number of bytes where the whole sstable was contained in a repairing range so that we only mutated the repair status
* ratio of how much we anticompact vs how much we could mutate the repair status
* total time spent as a repair coordinator
* total time spent preparing for repair

https://github.com/krummas/cassandra/commits/marcuse/metrics-trunk",,cnlwsu,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 14:45:36 UTC 2017,,,,,,,,,,"0|i3eylr:",9223372036854775807,,,,,,,cnlwsu,,cnlwsu,,,Normal,,,,,,,,,,,,,,,,,,,"15/May/17 07:34;marcuse;could you review this [~cnlwsu] ?;;;","15/May/17 13:47;cnlwsu;+1 lgtm;;;","15/May/17 14:45;marcuse;thanks, committed

also added some docs about the new metrics to doc/source/operating/metrics.rst;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool describeclusters shows different snitch info as to what is configured.,CASSANDRA-13528,13071508,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Lerh Low,superpaul,superpaul,12/May/17 11:17,15/May/20 08:05,14/Jul/23 05:56,18/Jan/18 12:37,3.11.2,4.0,4.0-alpha1,,,,Tool/nodetool,,,,,0,lhf,,,,"I couldn't find any similar issue as this one so I'm creating one.
I noticed that doing nodetool describecluster shows a different Snitch Information as to what is being set in the configuration file.

My setup is hosted in AWS and I am using Ec2Snitch.

cassandra@cassandra3$ nodetool describecluster
Cluster Information:
	Name: testv3
	Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		fc6e8656-ee7a-341b-9782-b569d1fd1a51: [10.0.3.61,10.0.3.62,10.0.3.63]

I checked via MX4J and it shows the same, I haven't verified tho using a different Snitch and I am using 2.2.6 above and 3.0.X ",,jasobrown,jay.zhuang,jjirsa,KurtG,Lerh Low,rha,superpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13572,,,,,,,,,,,,,,,,,,,,,"17/Jan/18 22:47;Lerh Low;13528-trunk.txt;https://issues.apache.org/jira/secure/attachment/12906500/13528-trunk.txt","12/May/17 11:17;superpaul;Screen Shot 2017-05-12 at 14.15.04.png;https://issues.apache.org/jira/secure/attachment/12867759/Screen+Shot+2017-05-12+at+14.15.04.png",,,,,,,,,,,,,,,,,,2.0,Lerh Low,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 12:37:18 UTC 2018,,,,,,,,,,"0|i3ew73:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,2.2.6,,,,,,,,,,,,,,,,,"14/May/17 13:30;rha;Actually {{DynamicEndpointSnitch}} is a wrapper, so it wraps {{EC2Snitch}} here. When DES is disabled, {{nodetool describecluster}} gives you what you expect.  I agree it's not very user friendly nor very useful.

Something like this would be more helpful:
{code}
Snitch: <Snitch name here e.g. org.apache.cassandra.locator.GossipingPropertyFileSnitch>
DynamicEndpointSnitch: (enabled|disabled)
{code}


;;;","18/May/17 22:45;Lerh Low;I like that approach, sounds reasonable. I've attached some patches, one for 2.1 and one for 2.2, 3.0, 3.X, trunk (it applies cleanly). Any thoughts? 

{code}
Cluster Information:
        Name: Test Cluster
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: enabled
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                9cf2fd0e-ab63-348e-b9af-6db0a3da5a29: [127.0.0.1]
{code}

{code}
Cluster Information:
        Name: Test Cluster
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: disabled
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                9cf2fd0e-ab63-348e-b9af-6db0a3da5a29: [127.0.0.1]
{code};;;","19/May/17 05:35;superpaul;I like the solution! It provides proper information of the snitch and eliminates the confusion.
It would be nice also to have this patch on the forthcoming releases.

Thanks!;;;","16/Jun/17 00:26;jay.zhuang;Not sure if changing the behave of the existing JMX interface is a good idea. I would suggest to use {{[DynamicEndpointSnitchMBean.getSubsnitchClassName()|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java#L362]}}, which will give you the same information.;;;","16/Jun/17 00:30;Lerh Low;Hell. My bad, I didn't know that existed. I'll look into it, if that already exists then definitely we should use it. Thanks for your suggestion. ;;;","05/Dec/17 05:36;Lerh Low;Hello, sorry for taking so long on this, it's ancient by now. I've reuploaded a new patch without changing the MBean interfaces, though the UI is slightly different now (less code changes required): 

{code}
Cluster Information:
	Name: Test Cluster
	Snitch: org.apache.cassandra.locator.SimpleSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]
{code}

{code}
Cluster Information:
	Name: Test Cluster
	Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
	SubSnitch: org.apache.cassandra.locator.SimpleSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]
{code}

Applies cleanly to 2.1/2.2/3.0.X/3.X, and tested as well. ;;;","12/Jan/18 00:12;Lerh Low;Just wondering if anyone would like to review the patch? :);;;","12/Jan/18 15:35;jjirsa;Marking patch-available so reviewers see it.
;;;","12/Jan/18 19:49;jasobrown;I can review this.;;;","12/Jan/18 20:07;jasobrown;This patch works and code is fine, but I kinda like the version that printed like this:

{quote}
Cluster Information:
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: enabled
{quote}

This is better because operators can choose to configure the {{endpoint_snitch}} in the yaml, and to output that as a ""Subsnitch"" from {{nodetool describecluster}} is little confusing as it's not what they configured. Calling it Subsnitch is the correct wrt the src code, it's not operator-friendly.

Which branches should we apply this change to? I'm thinking 3.11 and trunk. Any strong argument for 3.0?;;;","17/Jan/18 22:46;Lerh Low;Thanks for reviewing Jason :)

Agreed. For some reason back then I was thinking the code would be less elegant to support that, but now revisiting it it's still relatively straightforward and doesn't need too many hacks. 

So it's now back to: 
{code:java}
Cluster Information:
Name: Test Cluster
Snitch: org.apache.cassandra.locator.SimpleSnitch
DynamicEndPointSnitch: enabled
Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
Schema versions:
59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]{code}

And 
{code:java}
Cluster Information:
Name: Test Cluster
Snitch: org.apache.cassandra.locator.SimpleSnitch
DynamicEndPointSnitch: disabled
Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
Schema versions:
59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]{code}

Just finished testing this on trunk and 3.11 (patch applies cleanly). Here's the Github link: [https://github.com/juiceblender/cassandra/commit/e93c219f86469357e01595f3adcea81efd0dec84] if that floats your boat more, otherwise the attached patch has also been updated. 

I believe it actually applies alright all the way down to 2.1 (so we can just apply it to 3.0 if we do wish). But it's up to you, just let me know. ;;;","18/Jan/18 12:37;jasobrown;committed as sha {{fc3357a00e2b6e56d399f07c5b81a82780c1e143}}.

 

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool cleanup on KS with no replicas should remove old data, not silently complete",CASSANDRA-13526,13071316,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jjirsa,jjirsa,11/May/17 20:11,15/May/20 08:01,14/Jul/23 05:56,07/Dec/17 05:51,3.0.16,3.11.2,4.0,4.0-alpha1,,,Local/Compaction,,,,,0,usability,,,,"From the user list:

https://lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167@%3Cuser.cassandra.apache.org%3E

If you have a multi-dc cluster, but some keyspaces not replicated to a given DC, you'll be unable to run cleanup on those keyspaces in that DC, because [the cleanup code will see no ranges and exit early|https://github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L427-L441]",,aleksey,githubbot,jaid,jasonstack,jjirsa,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10047,,,,,,CASSANDRA-10991,,,,,,,,,,,,,CASSANDRA-9652,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 13:40:17 UTC 2017,,,,,,,,,,"0|i3ev0f:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"11/May/17 22:34;jaid;The issue I am seeing on C* cluster with the below setup

Cassandra version : 2.1.16
Datacenters: 4 DC
RF: NetworkTopologyStrategy with 3 RF in each DC
Keyspaces: 50 keyspaces, few replicating to one DC and few replicating to multiple DC

;;;","06/Jul/17 04:45;jasonstack;| branch | unit | [dtest|https://github.com/jasonstack/cassandra-dtest/commits/CASSANDRA-13526] |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526] |  [pass|https://circleci.com/gh/jasonstack/cassandra/182] | bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test known |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.11]|  [pass|https://circleci.com/gh/jasonstack/cassandra/186] | pass |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.0]|  [pass|https://circleci.com/gh/jasonstack/cassandra/181] | offline_tools_test.TestOfflineTools.sstableofflinerelevel_test  auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-2.2]|  [pass|https://circleci.com/gh/jasonstack/cassandra/185] |  ttl_test.TestTTL.collection_list_ttl_test |

unit test all passed, some irrelevant dtests failed.

when no local range && node has joined token ring,  clean up will remove all base local sstables.  ;;;","12/Jul/17 02:27;jasonstack;[~jjirsa] could you review ? thanks..;;;","16/Jul/17 06:59;jjirsa;Thanks [~jasonstack] .  I glanced at it and it looked reasonable, though I'll do a more thorough review next week.

Since it's a bug fix (and a pretty serious one at that), it seems like we should have patches for at least 3.0 and 3.11 , and perhaps even 2.1 and 2.2. Are you able to port your fix to 3.0 and 3.11? 

We should also add a unit test to make sure we prevent this sort of regression again in the future.



;;;","16/Jul/17 07:06;jasonstack;[~jjirsa] thanks for reviewing. {{trunk}} was draft for review. I will prepare for older branches and more tests.;;;","16/Jul/17 07:24;jjirsa;[~jasonstack] if you give me a few days I'll do a real review, and you can backport after that if it's easier
;;;","16/Jul/17 07:42;jasonstack;sure. it's not urgent.;;;","19/Jul/17 04:59;jjirsa;Patch looks good to me, dtest looks good as well, with two comments:

1) New dtest repo is https://github.com/apache/cassandra-dtest

2) You should remove dc1 from the replication strategy [here|https://github.com/riptano/cassandra-dtest/commit/15bf712988fb50ae29994da246dec186beff69bd#diff-9d7bd37d410a5598b9700b71476845ebR159] to be very explicit about what we expect to happen.

Would you backport to 3.0 and 3.11 ? 
;;;","19/Jul/17 14:54;jasonstack;thanks for reviewing, I will back port to 3.0/3.11 this week.  I was stuck in other issues..;;;","20/Jul/17 03:24;githubbot;GitHub user jasonstack opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/1

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…

    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jasonstack/cassandra-dtest-1 CASSANDRA-13526

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/1.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1
    
----
commit 3c8877c0fa3eb998ed2ee9945ebb8d43687e65fa
Author: Zhao Yang <zhaoyangsingapore@gmail.com>
Date:   2017-07-20T03:18:18Z

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete

----
;;;","20/Jul/17 05:41;githubbot;Github user jasonstack closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/1
;;;","24/Jul/17 06:32;jasonstack;[~jjirsa] sorry for the delay, I updated the dtest result for 2.2/3.0/3.11/trunk, some irrelevant dtests failed. I skipped 2.1 since this is not critical.;;;","28/Jul/17 16:37;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/1
  
    (You could leave the PR open and I'll close it on merge with CASSANDRA-13526 )

;;;","29/Jul/17 03:11;githubbot;Github user jasonstack commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/1
  
    thanks..
;;;","29/Jul/17 03:11;githubbot;GitHub user jasonstack reopened a pull request:

    https://github.com/apache/cassandra-dtest/pull/1

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…

    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jasonstack/cassandra-dtest CASSANDRA-13526

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/1.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1
    
----
commit ccb6e81451f3d9ca0d192c508beaeef0959e56fc
Author: Zhao Yang <zhaoyangsingapore@gmail.com>
Date:   2017-07-20T03:18:18Z

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete

----
;;;","20/Oct/17 23:41;jjirsa;Hi [~jasonstack] Really appreciate your patience in the time it's taken me to back to this. I hope to review it this weekend.

[~krummas] / [~iamaleksey] - what do you folks think about versions here? 2.2 or 3.0? 

;;;","04/Dec/17 21:35;aleksey;I'd probably go with 3.0+ only, but 2.2 is acceptable too.;;;","04/Dec/17 23:03;jjirsa;I've rebased your patch and I'm re-running CI, just because it took me so very long to review this patch.

Generally the patches look fine, but I don't understand why you're running this method twice [here|https://github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8#diff-ba6d3d8e296151fc283ef11ac4594e62R211] (and in the very similar helper below it)?

I'm inclined to remove one of those calls. Other than that, marking as ready-to-commit, and I'll merge when CI finishes.
;;;","06/Dec/17 10:06;jasonstack;[~jjirsa] it's a mistake in 3.11 PR.. thanks for the fix.;;;","07/Dec/17 05:51;jjirsa;Thank you so much for the patch and your patience. Committed to 3.0 as {{090f418831be4e4dace861fda380ee4ec27cec35}} and merged up, fixing the 3.11 test on the way.

;;;","07/Dec/17 06:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/1
;;;","07/Dec/17 13:40;jasonstack;Thanks for reviewing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReverseIndexedReader may drop rows during 2.1 to 3.0 upgrade,CASSANDRA-13525,13071133,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,samt,samt,11/May/17 11:27,16/Apr/19 09:30,14/Jul/23 05:56,24/May/17 18:15,3.0.14,3.11.0,,,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"During an upgrade from 2.1 (or 2.2) to 3.0 (or 3.x) queries which perform reverse iteration may silently drop rows from their results. This can happen before sstableupgrade is run and when the sstables are indexed.
",,jeromatron,jjirsa,jjordan,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 18:15:09 UTC 2017,,,,,,,,,,"0|i3etvr:",9223372036854775807,3.0.13,3.10,,,,,,,,,,Critical,,,,,,,,,,,,,,,,,,,"11/May/17 11:52;samt;
The problem occurs when a row spans an block in the row index. When deciding whether to continue reading from disk or move to the next index block, {{UnfilteredDeserializer.OldFormatDeserializer}} accounts for the fact that {{hasNext()}} has bumped the file pointer past the end of the last consumed {{Unfiltered}}. But, it doesn't account for reading the legacy atoms from disk to feed the unfiltered iterator. The legacy atom iterator actually reads the first atom of the {{Unfiltered}} *after* the next one, so that needs to be included when calculating the {{lastConsumedPosition}}, which in turn determines whether the current index block has been exhausted. This only affects the reverse iterator as it's mitigation for rows which cross index block boundaries is more complex than the forward iterator. 

For 3.0, I've pushed [a branch|https://github.com/beobal/cassandra/tree/13525-3.0] with a fix and unit test. Also, it adds a unit test for CASSANDRA-13236 (there's a [dtest PR|https://github.com/riptano/cassandra-dtest/pull/1469] open already), this issue was discovered during investigation into that one but I struggled to figure out a decent unit test for it at the time. 

The 3.11 branch should be pretty much identical, but merging 3.0 -> 3.11 is broken at the moment, (looks like [263740daa4|https://github.com/apache/cassandra/commit/263740daa4c8162a157aa6fbb97793f158d142d1] needs to be merged with {{-s ours}}, but I'll double check). When that's fixed I'll push a 3.11 branch.
;;;","11/May/17 14:27;samt;Fixed the 3.0 -> 3.11 merge issue, so both branches are up now, I'll update with CI results shortly.
Trunk obviously isn't affected through removing support for pre-3.0 sstables.

||branch||circle-ci||
|[13525-3.0|https://github.com/beobal/cassandra/tree/13525-3.0]|
|[13525-3.11|https://github.com/beobal/cassandra/tree/13525-3.11]|

;;;","16/May/17 03:21;jjirsa;Code looks good to me.

Looks like I found your circleCI run for [3.0|https://circleci.com/gh/beobal/cassandra/10#tests/containers/2] :

{code}
Your build ran 4768 tests with 1 failure
testAccessAndSchema-compression - org.apache.cassandra.cql3.ViewSchemaTest
{code}

For [3.11|https://circleci.com/gh/beobal/cassandra/11#tests/containers/2]:

{code}
Your build ran 5959 tests with 2 failures
testNoArgs-compression - org.apache.cassandra.tools.CompactionStressTest
morejava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.cassandra.stress.CompactionStress
testWriteAndCompact-compression - org.apache.cassandra.tools.CompactionStressTest
morejava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.cassandra.stress.CompactionStress
{code}

Neither of those seem related to this patch, so I'm good with that.

I've kicked off build #49 for 3.0 ( https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/ ) and #50 for 3.11 , though it'll be quite some time before it's complete (maybe many days).



;;;","17/May/17 17:38;jjirsa;The dtests are awful - [3.0 has 971 failures|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/49/] and [3.11 has 667 failures|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/50/]

Most look related to ip/port bind failures, but they started with this patch, which is odd. 

;;;","17/May/17 17:43;samt;There are similar bind related results for trunk though, which obv. doesn't have this patch:

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-dtest/142/#showFailuresLink;;;","17/May/17 19:45;jjirsa;Re-queued as #55 and #56
;;;","24/May/17 17:40;jjirsa;lgtm, commit away!;;;","24/May/17 18:15;samt;Thanks, committed to 3.0 in {{62092e45c8bbb75ac9f680188b3746913602507b}} and merged to 3.11, and then to trunk with {{-s ours}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra core connector - Guava incompatibility: Detected incompatible version of Guava in the classpath. You need 16.0.1 or higher,CASSANDRA-13524,13071092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,michael.hornung@salt-solutions.de,michael.hornung@salt-solutions.de,11/May/17 08:43,26/Jun/20 07:26,14/Jul/23 05:56,11/May/17 09:43,,,,,,,,,,,,0,,,,,"Hallo,

with my application I have a AKKA-http microservice which want’s to acess a cassandra database table from scala.
Therefore I included this dependency in SBT:
         ""com.datastax.cassandra"" % ""cassandra-driver-core""       % ""3.2.0""
In my scalafile I have this coding:
--------------------------------------------------------------------------
….
import com.datastax.driver.core.Cluster
import com.google.common.util.concurrent._
….
    val cassandraHost    = ""localhost""
    val keyStore         = ""data4service""
    //setup cassandra
    val cluster = {
                    Cluster.builder()
                      .addContactPoint(cassandraHost)
                      .withCredentials(""user"", ""password"")
                      .build()
                  }
    //connect to cassandra keystore
    val session = cluster.connect(keyStore)
    val product = ""123e4567-e89b-12d3-a456-426655440003""
    val select = s""SELECT quantity FROM stock WHERE product = $product""
    val result = session.execute(select)   
….
--------------------------------------------------------------------------

During build guava Version 19.0 is downloaded automatically

Unfortunately if I run my application I get this Error during runtime: 
--------------------------------------------------------------------------
com.datastax.driver.core.exceptions.DriverInternalError: Detected incompatible version of Guava in the classpath. You need 16.0.1 or higher.
        at com.datastax.driver.core.GuavaCompatibility.selectImplementation(GuavaCompatibility.java:138)
        at com.datastax.driver.core.GuavaCompatibility.<clinit>(GuavaCompatibility.java:52)
--------------------------------------------------------------------------

that is not logical bevause Guava 19.0 is on the system. Can you help me?

Kind regards,
Michael
",,michael.hornung@salt-solutions.de,stumeikle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 08:43;michael.hornung@salt-solutions.de;build.sbt;https://issues.apache.org/jira/secure/attachment/12867522/build.sbt","11/May/17 08:43;michael.hornung@salt-solutions.de;error_log.txt;https://issues.apache.org/jira/secure/attachment/12867521/error_log.txt",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 07:26:10 UTC 2020,,,,,,,,,,"0|i3etmn:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"26/Jun/20 07:26;stumeikle;If you set an issue to resolved could you add a comment about why so others can follow please?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in snitch.rst,CASSANDRA-13520,13070922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,donchy,donchy,donchy,10/May/17 19:10,15/May/20 08:07,14/Jul/23 05:56,15/May/17 16:51,4.0,4.0-alpha1,,,,,Legacy/Documentation and Website,,,,,0,,,,,A patch to fix the typo: https://github.com/dongqixue/cassandra/commit/c70496884967e166859abce597e737e4f77b1ddc,,bdeggleston,donchy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,donchy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 16:51:59 UTC 2017,,,,,,,,,,"0|i3eskv:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"15/May/17 16:51;bdeggleston;Committed as {{b87f79798ca244006906c41e31d35a84112b54be}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in paxos_tests.TestPaxos.contention_test_many_threads,CASSANDRA-13517,13070552,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aholmber,aweisberg,aweisberg,09/May/17 19:13,26/Mar/21 20:28,14/Jul/23 05:56,26/Mar/21 20:28,4.0,4.0-rc1,,,,,Test/dtest/python,,,,,0,dtest,test-failure,test-failure-fresh,,"Error Message
AssertionError: value=278, errors=22, retries=2888 assert (278 == (300 * 1))

{noformat}

> assert (value == threads * iterations) and (errors == 0), ""value={}, errors={}, retries={}"".format(value, errors, retries) 
E AssertionError: value=278, errors=22, retries=2888 E assert (278 == (300 * 1)) 
paxos_test.py:195: AssertionError

  {noformat}

 ",,aholmber,aweisberg,blerer,e.dimitrova,jasobrown,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12987,,,,,,,,,,,,,,,,,,,"09/May/17 19:13;aweisberg;test_failure.txt;https://issues.apache.org/jira/secure/attachment/12867165/test_failure.txt",,,,,,,,,,,,,,,,,,,1.0,aholmber,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 20:27:55 UTC 2021,,,,,,,,,,"0|i3eqan:",9223372036854775807,,,,,,,,,blerer,e.dimitrova,,Normal,,4.0-beta4,,,https://github.com/apache/cassandra-dtest/commit/af190977b64176fdcbca296cb49089ce8cb57266,,,,,,,,,this is a small test change,,,,,"24/Oct/18 00:30;jasobrown;Closing for now as it doesn't seem to be a problem of late.;;;","25/Mar/21 18:32;aholmber;This test appears to still be flaky.;;;","26/Mar/21 00:35;aholmber;The test fails this way intermittently, but consistently on a resource-constrained VM. Failures are characterized by driver heartbeat timeouts which [exits the worker|https://github.com/apache/cassandra-dtest/blob/49f46fce94c8f25f32e9b778ded8b14c30ad851e/paxos_test.py#L145-L149] and does not retry. I think the server and cluster are just being overwhelmed. This never fails on a well-provisioned machine.

The proposed change creates a client connection with ample timeouts and heartbeats disabled. I'm also reducing the concurrency from one arbitrary number to another slightly smaller arbitrary number to make it a bit more appropriate in the envelope of a single-host three-node test cluster.

[test patch|https://github.com/aholmberg/cassandra-dtest/pull/5]
 [ci|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-13517] (started, not reviewed);;;","26/Mar/21 00:43;brandon.williams;[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/509/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/509/pipeline]
;;;","26/Mar/21 01:22;e.dimitrova;Can we loop like 100 times the updated test on this resource-constrained VM? 

(not a review, that I can do tomorrow :) );;;","26/Mar/21 12:31;aholmber;I did loop it about 50 times before submitting the patch. Before it would easily fail one in five times in the VM. It failed zero in 50. I started the loop again.;;;","26/Mar/21 12:38;aholmber;-The test failed in CI.-

[~brandon.williams], I think the run you linked did not run against my dtest branch.;;;","26/Mar/21 12:53;brandon.williams;Whoops, you're right.  Updated.;;;","26/Mar/21 15:32;aholmber;[~e.dimitrova] my local spin count is >200 with zero failures. Previous fail rate in this VM was 1/5.;;;","26/Mar/21 17:22;blerer;+1;;;","26/Mar/21 17:37;e.dimitrova;Thanks [~aholmber], I am also +1, the CI is still running so in order to be politically correct I will commit the patch when CI run is over later today :) ;;;","26/Mar/21 20:27;e.dimitrova;CI looks fine; 1 unrelated failure.

Committed [here|https://github.com/apache/cassandra-dtest/commit/af190977b64176fdcbca296cb49089ce8cb57266]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test ,CASSANDRA-13515,13070511,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,aweisberg,aweisberg,09/May/17 17:39,16/Apr/19 09:30,14/Jul/23 05:56,26/Nov/18 23:08,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,test-failure-fresh,,"{noformat}
 Failed 7 times in the last 24 runs. Flakiness: 47%, Stability: 70%
Error Message

errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout('Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message=""Operation timed out - received only 0 responses."" info={\'received_responses\': 0, \'required_responses\': 1, \'consistency\': \'ONE\'}',)}, last_host=127.0.0.2
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-uYGL17
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
dtest: DEBUG: insert data
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
dtest: DEBUG: bringing down node 3
dtest: DEBUG: inserting additional data into node 1 and 2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
dtest: DEBUG: restarting and repairing node 3
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
dtest: DEBUG: stopping node 2
dtest: DEBUG: inserting data in nodes 1 and 3
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
dtest: DEBUG: start and repair node 2
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695947871376) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695354796304) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695383359824) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140693197814160) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695354906960) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.connection: WARNING: Heartbeat failed for connection (140695947873424) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695366436368) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.connection: WARNING: Heartbeat failed for connection (140695354794256) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695925137168) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695367411408) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695363867088) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
dtest: DEBUG: replace node and check data integrity
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140694303505168) to 127.0.0.3
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.protocol: WARNING: Server warning: Aggregation query used without partition key
dtest: DEBUG: Retrying read after timeout. Attempt #0
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 409, in multiple_repair_test
    assert_one(session, ""SELECT COUNT(*) FROM ks.cf LIMIT 200"", [149])
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2018, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 3822, in result
    raise self._final_exception
'errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout(\'Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\\\' responses] message=""Operation timed out - received only 0 responses."" info={\\\'received_responses\\\': 0, \\\'required_responses\\\': 1, \\\'consistency\\\': \\\'ONE\\\'}\',)}, last_host=127.0.0.2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-uYGL17\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'memtable_allocation_type\': \'offheap_objects\',\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.policies: INFO: Using datacenter \'datacenter1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ndtest: DEBUG: insert data\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ndtest: DEBUG: bringing down node 3\ndtest: DEBUG: inserting additional data into node 1 and 2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ndtest: DEBUG: restarting and repairing node 3\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ndtest: DEBUG: stopping node 2\ndtest: DEBUG: inserting data in nodes 1 and 3\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ndtest: DEBUG: start and repair node 2\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695947871376) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354796304) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695383359824) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140693197814160) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354906960) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.connection: WARNING: Heartbeat failed for connection (140695947873424) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695366436368) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354794256) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695925137168) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695367411408) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695363867088) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ndtest: DEBUG: replace node and check data integrity\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140694303505168) to 127.0.0.3\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.protocol: WARNING: Server warning: Aggregation query used without partition key\ndtest: DEBUG: Retrying read after timeout. Attempt #0\n--------------------- >> end captured logging << ---------------------'
{noformat}",,aweisberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-05-09 17:39:39.0,,,,,,,,,,"0|i3eq1j:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI full-text search queries using standard analyzer do not work in multi-node environments,CASSANDRA-13512,13070394,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,09/May/17 12:00,16/Apr/19 09:30,14/Jul/23 05:56,12/Jul/17 10:06,,,,,,,Feature/SASI,,,,,0,,,,,"SASI full-text search queries using standard analyzer do not work in multi-node environments. Standard Analyzer will rewind the buffer and search term will be empty for any node other than coordinator, so will return no results.",,adelapena,ifesdjeen,jasonstack,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 10:06:07 UTC 2017,,,,,,,,,,"0|i3epcf:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,,,,,,,,,,,,,,,,,,"09/May/17 12:20;ifesdjeen;|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:13512-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-3.11-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-3.11-dtest/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13512-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-trunk-dtest/]|;;;","23/Jun/17 09:24;adelapena;The patch looks good to me, with some nits about the [dtest|https://github.com/ifesdjeen/cassandra-dtest/tree/13512-master]:
* There should be a {{@since('3.11')}} annotation in [{{multinode_full_text_search}}|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1444].
* I seems there is a typo [here|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1442], ""dests"" by ""dtests"".
* This is only a suggestion, I think it would be nice to break [this line|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1466] to satisfy the 120 char width. ;;;","12/Jul/17 10:06;ifesdjeen;Thank you for the review & sorry for taking so long to get back. I've made the suggested changes to [dtest|https://github.com/riptano/cassandra-dtest/pull/1492]

Committed to 3.11 with [ab640b2123826fd67d31860a9f0ca8a4224e3845|https://github.com/apache/cassandra/commit/ab640b2123826fd67d31860a9f0ca8a4224e3845] and merged up to [trunk|https://github.com/apache/cassandra/commit/86964da69d36bdf3afd65ed77ecccd9dff24757b].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in bootstrap_test.TestBootstrap.simultaneous_bootstrap_test,CASSANDRA-13506,13070185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,aweisberg,aweisberg,08/May/17 21:50,16/Apr/19 09:30,14/Jul/23 05:56,26/Nov/18 23:08,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,test-failure-fresh,,"{noformat}
Failed 11 times in the last 30 runs. Flakiness: 62%, Stability: 63%
Error Message

errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout('Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message=""Operation timed out - received only 0 responses."" info={\'received_responses\': 0, \'required_responses\': 1, \'consistency\': \'ONE\'}',)}, last_host=127.0.0.2
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-VsuThg
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 datacenter1> discovered
cassandra.protocol: WARNING: Server warning: Aggregation query used without partition key
dtest: DEBUG: Retrying read after timeout. Attempt #0
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/bootstrap_test.py"", line 659, in simultaneous_bootstrap_test
    assert_one(session, ""SELECT count(*) from keyspace1.standard1"", [500000], cl=ConsistencyLevel.ONE)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py"", line 2018, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py"", line 3822, in result
    raise self._final_exception
'errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout(\'Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\\\' responses] message=""Operation timed out - received only 0 responses."" info={\\\'received_responses\\\': 0, \\\'required_responses\\\': 1, \\\'consistency\\\': \\\'ONE\\\'}\',)}, last_host=127.0.0.2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-VsuThg\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 datacenter1> discovered\ncassandra.protocol: WARNING: Server warning: Aggregation query used without partition key\ndtest: DEBUG: Retrying read after timeout. Attempt #0\n--------------------- >> end captured logging << ---------------------'
{noformat}",,aweisberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-05-08 21:50:40.0,,,,,,,,,,"0|i3eo1z:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in snapshot_test.TestSnapshot.test_snapshot_and_restore_dropping_a_column,CASSANDRA-13483,13067693,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,mshuler,mshuler,28/Apr/17 13:45,16/Apr/19 09:30,14/Jul/23 05:56,03/May/17 10:36,,,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/488/testReport/snapshot_test/TestSnapshot/test_snapshot_and_restore_dropping_a_column

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['refresh', 'ks', 'cf']] exited with non-zero status; exit status: 1; 
stdout: nodetool: Unknown keyspace/cf pair (ks.cf)
See 'nodetool help' or 'nodetool help <command>'.
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 145, in test_snapshot_and_restore_dropping_a_column
    node1.nodetool('refresh ks cf')
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 789, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 2002, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",,jasonstack,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13495,,,,,,,,,,,,,,,,,,,"28/Apr/17 13:45;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12865549/node1.log","28/Apr/17 13:45;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12865550/node1_debug.log","28/Apr/17 13:45;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12865548/node1_gc.log",,,,,,,,,,,,,,,,,3.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 00:06:44 UTC 2017,,,,,,,,,,"0|i3e8of:",9223372036854775807,2.2.x,,,,,,philipthompson,,philipthompson,,,Normal,,,,,,,,,,,,,,,,,,,"30/Apr/17 16:30;jasonstack;This test is added in [CASSANDRA-13276|https://issues.apache.org/jira/browse/CASSANDRA-13276] in 3.11 depending on schema.cql feature added in [CASSANDRA-7190|https://issues.apache.org/jira/browse/CASSANDRA-7190] in 3.10.

'@since('3.11)' should fix the failed test. 

Here is the [dtest|https://github.com/riptano/cassandra-dtest/pull/1466]

[~ifesdjeen] could you review it? thanks;;;","02/May/17 00:06;jasonstack;dtest merged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on non-existing row read when row cache is enabled,CASSANDRA-13482,13067657,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,28/Apr/17 10:52,15/May/20 08:04,14/Jul/23 05:56,12/Jul/17 13:55,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,,1,,,,,"The problem is reproducible on 3.0 with:

{code}
-# row_cache_class_name: org.apache.cassandra.cache.OHCProvider
+row_cache_class_name: org.apache.cassandra.cache.OHCProvider

-row_cache_size_in_mb: 0
+row_cache_size_in_mb: 100
{code}

Table setup:

{code}
CREATE TABLE cache_tables (pk int, v1 int, v2 int, v3 int, primary key (pk, v1)) WITH CACHING = { 'keys': 'ALL', 'rows_per_partition': '1' } ;
{code}

No data is required, only a head query (or any pk/ck query but with full partitions cached). 

{code}
select * from cross_page_queries where pk = 10000 ;
{code}

{code}
java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:193) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:461) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:395) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1794) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2472) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}",,batenev,ifesdjeen,jasonstack,jay.zhuang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 13:55:44 UTC 2017,,,,,,,,,,"0|i3e8gf:",9223372036854775807,3.10,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"18/May/17 16:00;batenev;I have same error on cassandra 3.10:

query fail with row_cache on, and fine with cache off.
consistency level doesn't matter - one/quorum/all -> same result.
switch cache off and when on and/or restart cassandra doesn't fix problem.


java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:210) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:474) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:374) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:407) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:48) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.10.jar:3.10]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]


---

cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 2, 'consistency': 'QUORUM'}

cassandra@cqlsh:sb> ALTER TABLE  session WITH caching = {'keys': 'ALL'};
cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;

 agent_id | session_id | all_isps | all_locations | all_logins | last_ip | last_login | last_page | last_time | legacy_id
----------+------------+----------+---------------+------------+---------+------------+-----------+-----------+-----------

(0 rows)

cassandra@cqlsh:sb> ALTER TABLE  session WITH compaction = {'class': 'LeveledCompactionStrategy'} and caching = {'keys': 'ALL','rows_per_partition': 1};
cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 2, 'consistency': 'QUORUM'}

---

if i do select with another agent_id - all work fine! only one key fail...;;;","09/Jun/17 09:23;ifesdjeen;I've composed a patch to mitigate the problem. 

In order to fix it, we have to allow for concatenating the iterators with different amounts of columns, although make sure that cases with wrapped iterators, limits, stopping and empty iterators all have some predictable behaviour. 

While working on this problem I have also discovered the slight inconsistency in the way concatenation and {{MoreRows}} is working right now: {{DataLimits}} filter [here|https://github.com/apache/cassandra/blob/a87b15d1d6c42f4247c84b460ed39899d8813a6f/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L423] will call {{stopInPartition}}, which would effectively ""stop"" the original {{iter}} iterator, and make it return {{hasNext() => false}}, even though only one row was consumed. 

Right now, it works only because internally {{concat}} would take {{input}} from the {{BaseIterator}} and discard this {{isStopped}} in [tryGetMoreContent|https://github.com/apache/cassandra/blob/81f6c784ce967fadb6ed7f58de1328e713eaf53c/src/java/org/apache/cassandra/db/transform/BaseIterator.java#L124]. In the context of this patch this would mean that we'd get only cached results and avoid reading mem/sstable. 

In other words, current behaviour can be described as:

{code}
    iter1 = /* some iterator yielding: 1, 2, 3 */;
    iter2 = /* some iterator yielding: 3, 4, 5, 6, 7, 8, 9 */;
    concatenated = concat(iter1, DataLimits.cqlLimits(3).filter(iter2));
{code}

would result in {{concatenated}} yielding 1 through 9, which is incorrect.

The patch implements the following changes:

  * {{concat}} [now allows|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-57d0dfa95504bfd17d30539b3b338c0cL205] different amount of columns from iterators, but returned columns will be a union of the two iterators
  * {{BaseIterator}} [would now|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-d14a4b314544d3720010343e330e7e3cR125] take the {{stop}} from the child iterator, which means that the iterator was stopped, even if it might have contents, it will not yield additional data. Unfortunately, since the iterator itself had a stopping transformation, and it's own {{stop}} reference is ""leaked"" during the creation of the transformation, we have to save (and check) both the original stop and the child one. Here, the naming might be a bit off.
  * {{cacheIterator}} [is now|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-2e17efa5977a71330df6651d3bec0d12R424] using a custom wrapped iterator that will not call {{stop}} on the wrapping iterator, but previous problem with {{Unfiltered}} is now fixed

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:13482-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.0-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.0-dtest/]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:13482-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.11-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.11-dtest/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-trunk-dtest/]|;;;","12/Jul/17 07:39;slebresne;lgtm, +1.;;;","12/Jul/17 13:55;ifesdjeen;Thank you for the review,

Committed to 3.0 with [7251c9559805d83423ca5ddbe4f955ce668c3d9a|https://github.com/apache/cassandra/commit/7251c9559805d83423ca5ddbe4f955ce668c3d9a] and merged up to [3.11|https://github.com/apache/cassandra/commit/29db2511621e420b8d64c867a16e317589397d36] and [trunk|https://github.com/apache/cassandra/commit/f48a319ac884ef8d6eb54db3176ea2acf627bb89].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair can hang forever if we lose the notification for the repair completing/failing,CASSANDRA-13480,13067566,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mbyrd,mbyrd,mbyrd,28/Apr/17 01:19,07/Mar/23 11:52,14/Jul/23 05:56,29/Jun/17 19:17,4.0,4.0-alpha1,,,,,Tool/nodetool,,,,,0,repair,,,,"When a Jmx lost notification occurs, sometimes the lost notification in question is the notification which let's RepairRunner know that the repair is finished (ProgressEventType.COMPLETE or even ERROR for that matter).
This results in nodetool process running the repair hanging forever. 

I have a test which reproduces the issue here:
https://github.com/Jollyplum/cassandra-dtest/tree/repair_hang_test

To fix this, If on receiving a notification that notifications have been lost (JMXConnectionNotification.NOTIFS_LOST), we instead query a new endpoint via Jmx to receive all the relevant notifications we're interested in, we can replay those we missed and avoid this scenario.

It's possible also that the JMXConnectionNotification.NOTIFS_LOST itself might be lost and so for good measure I have made RepairRunner poll periodically to see if there were any notifications that had been sent but we didn't receive (scoped just to the particular tag for the given repair).

Users who don't use nodetool but go via jmx directly, can still use this new endpoint and implement similar behaviour in their clients as desired.
I'm also expiring the notifications which have been kept on the server side.
Please let me know if you've any questions or can think of a different approach, I also tried setting:
 JVM_OPTS=""$JVM_OPTS -Djmx.remote.x.notification.buffer.size=5000""
but this didn't fix the test. I suppose it might help under certain scenarios but in this test we don't even send that many notifications so I'm not surprised it doesn't fix it.
It seems like getting lost notifications is always a potential problem with jmx as far as I can tell.",,bdeggleston,cnlwsu,dikanggu,githubbot,jeromatron,jjirsa,leonhardt,mbyrd,mck,rustyrazorblade,tania.engel@quest.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8076,CASSANDRA-14453,,,,,,,,,,CASSANDRA-8076,,,,,,,,,,,,,,,,,,,,,0.0,mbyrd,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 21:23:11 UTC 2018,,,,,,,,,,"0|i3e7w7:",9223372036854775807,2.1.16,3.0.13,5.0,,,,cnlwsu,,cnlwsu,,,Low,,,,,,,,,,,,,,,,,,,"28/Apr/17 02:49;cnlwsu;how many notifications you see doesnt impact the notification buffer. JMX will create a buffer of notifications and cycle through them indexing new events as they are created. The JMX client will request events with the last index it has seen. Since the server does not store the state of the clients or know what they are listening for, ALL events regardless of listening state are appended into buffer. Even if nothing is listening to them all the storage notifications, the streaming notifications, the jvm hotspot notifications are being pushed onto that buffer. If your client takes too long between polling it will get lost notifications (and it will tell you how many it lost). 5000 still may not be nearly enough, but its gonna cost the heap dearly to make that value too large.

Nodetool actually used to just shut down on lost notifications, but in some clusters/workloads its almost impossible for a client to keep up. In CASSANDRA-7909 it was just starting to be logged. Querying a different endpoint wouldnt really help, only the repair coordinator has the events and it doesnt keep it around (and its cycled outta buffer). We could in theory pull expose a JMX operation that checks the repair_history table or current repair states to determine if the repair has been completed or errored out, and on lost notifications call it to make sure we did not miss a complete event.;;;","28/Apr/17 23:01;mbyrd;So the patch I have currently also caches the notifications for repairs for a limited time on the co-ordinator, it was initially targeting a release where we didn't yet have the repair history tables.
I suppose there is a concern that caching these notifications could under some circumstances cause unwanted extra heap usage. 
(Similarly to the notifications buffer, although at least here we're only caching a subset that we care more about)
So using the repair history tables instead and exposing this information by imx seems like a reasonable alternative.
There are perhaps a couple of kinks to work out, but I'll have a go at adapting the patch that I have to work in this way.
For one we only have the cmd id int sent back to the nodetool process (rather than the parent session id which the internal table is partition keyed off)
We could either keep track of the cmd id int -> parent session uuid in the co-ordinator, either in memory cached to expire or in another internal table,
or we could parse the uuid out of the notification sent for the start of the parent repair.
Parsing the message is a bit brittle though and not full proof in theory (we could miss that notification also).
Ideally I suppose running a repair could return and communicate on the basis of the parent session uuid rather than the int cmd id, but this is a pretty major overhaul and has all sorts of compatibility questions.;;;","19/Jun/17 23:01;mbyrd;||Trunk|||
|[branch|https://github.com/Jollyplum/cassandra/tree/13480]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/98/]|
|[testall|https://circleci.com/gh/Jollyplum/cassandra/14]|
;;;","19/Jun/17 23:03;githubbot;GitHub user Jollyplum opened a pull request:

    https://github.com/apache/cassandra/pull/122

    When lost notifications occur and periodically, check for the parent …

    …repair status and exit if we've completed/failed
    
    patch by Matt Byrd, reviewed by Chris Lohfink for CASSANDRA-13480

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Jollyplum/cassandra 13480

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/122.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #122
    
----
commit b4c0a5a65ef94b1013793adb088ca11f563ff14b
Author: Matt Byrd <matthew.l.byrd@gmail.com>
Date:   2017-05-27T00:03:17Z

    When lost notifications occur and periodically, check for the parent repair status and exit if we've completed/failed
    patch by Matt Byrd, reviewed by Chris Lohfink for CASSANDRA-13480

----
;;;","29/Jun/17 14:56;cnlwsu;+1;;;","29/Jun/17 19:17;jjirsa;Thanks all, committed into 4.0 as {{20d5ce8b9b587be2f0b7bc5765254e8dc6e0bd3b}}
;;;","12/Oct/17 22:16;githubbot;Github user Jollyplum closed the pull request at:

    https://github.com/apache/cassandra/pull/122
;;;","27/Feb/18 16:48;tania.engel@quest.com;[~mbyrd] : I have reason to believe I just hit this in 3.11.1, I at the very least ran into a repair which has never completed on an 11 node cluster. Is there a way to get this fix in 3.11?;;;","17/May/18 23:26;mck;I can see immediate benefit to users of Reaper with this, so would like to see it back ported to 3.11.x

[~jjirsa], what are your thoughts?

Regarding the patch:
 - the only public signature added are:
 -- the method {{List<String> StorageServiceMBean.getParentRepairStatus(int cmd)}}
 -- the enum {{ActiveRepairService.ParentRepairStatus}}
 -- the method {{ActiveRepairService.recordRepairStatus(…)}}
 - the addition of the 100k entry cache for ParentRepairStatus events

While it seems to be reasonably safe, if it is back ported I wonder if the {{getParentRepairStatus(..)}} method should be marked as {{@Beta}}?;;;","18/May/18 21:23;rustyrazorblade;Assuming it back ports cleanly I'm +1 on bringing it to 3.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set javac encoding to utf-8 in 2.1 and 2.2.,CASSANDRA-13466,13065958,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,21/Apr/17 18:06,16/Apr/19 09:30,14/Jul/23 05:56,21/Apr/17 20:46,2.1.18,2.2.10,,,,,Build,,,,,0,,,,,"There are non-ASCII characters in 2.1 and 2.2 source code, but javac is not configured to interpret source files in UTF-8",,aweisberg,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 21 20:46:03 UTC 2017,,,,,,,,,,"0|i3dxzb:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"21/Apr/17 20:04;aweisberg;||code|utests||
|[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...aweisberg:cassandra-13466-2.1?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13466-2%2E1]|
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:cassandra-13466-2.2?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13466-2%2E2]|;;;","21/Apr/17 20:10;jasobrown;+1;;;","21/Apr/17 20:46;aweisberg;Committed as [572fef8a06172bd925c478ec16540e8e28e84bd7|https://github.com/apache/cassandra/commit/572fef8a06172bd925c478ec16540e8e28e84bd7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to create Materialized view with a specific token range,CASSANDRA-13464,13065453,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,krishna.koneru,nkojima,nkojima,20/Apr/17 09:21,07/Mar/23 11:52,14/Jul/23 05:56,16/Jul/21 16:51,3.0.25,3.11.11,4.0.1,,,,Feature/Materialized Views,,,,,0,materializedviews,,,,"Failed to create Materialized view with a specific token range.

Example :

{code:java}
$ ccm create ""MaterializedView"" -v 3.0.13
$ ccm populate  -n 3
$ ccm start
$ ccm status
Cluster: 'MaterializedView'
---------------------------
node1: UP
node3: UP
node2: UP
$ccm node1 cqlsh
Connected to MaterializedView at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.13 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':3};
cqlsh> CREATE TABLE test.test ( id text PRIMARY KEY , value1 text , value2 text, value3 text);

$ccm node1 ring test 
Datacenter: datacenter1
==========
Address    Rack        Status State   Load            Owns                Token
                                                                          3074457345618258602
127.0.0.1  rack1       Up     Normal  64.86 KB        100.00%             -9223372036854775808
127.0.0.2  rack1       Up     Normal  86.49 KB        100.00%             -3074457345618258603
127.0.0.3  rack1       Up     Normal  89.04 KB        100.00%             3074457345618258602

$ ccm node1 cqlsh
cqlsh> INSERT INTO test.test (id, value1 , value2, value3 ) VALUES ('aaa', 'aaa', 'aaa' ,'aaa');
cqlsh> INSERT INTO test.test (id, value1 , value2, value3 ) VALUES ('bbb', 'bbb', 'bbb' ,'bbb');
cqlsh> SELECT token(id),id,value1 FROM test.test;

 system.token(id)     | id  | value1
----------------------+-----+--------
 -4737872923231490581 | aaa |    aaa
 -3071845237020185195 | bbb |    bbb

(2 rows)

cqlsh> CREATE MATERIALIZED VIEW test.test_view AS SELECT value1, id FROM test.test WHERE id IS NOT NULL AND value1 IS NOT NULL AND TOKEN(id) > -9223372036854775808 AND TOKEN(id) < -3074457345618258603 PRIMARY KEY(value1, id) WITH CLUSTERING ORDER BY (id ASC);
ServerError: java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
{code}

Stacktrace :
{code:java}
INFO  [MigrationStage:1] 2017-04-19 18:32:48,131 ColumnFamilyStore.java:389 - Initializing test.test
WARN  [SharedPool-Worker-1] 2017-04-19 18:44:07,263 FBUtilities.java:337 - Trigger directory doesn't exist, please create it and try again.
ERROR [SharedPool-Worker-1] 2017-04-19 18:46:10,072 QueryMessage.java:128 - Unexpected error during query
java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
	at org.apache.cassandra.db.view.View.relationsToWhereClause(View.java:275) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.CreateViewStatement.announceMigration(CreateViewStatement.java:219) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.13.jar:3.0.13]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.13.jar:3.0.13]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
ERROR [SharedPool-Worker-1] 2017-04-19 18:46:10,073 ErrorMessage.java:349 - Unexpected exception during request
java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
	at org.apache.cassandra.db.view.View.relationsToWhereClause(View.java:275) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.CreateViewStatement.announceMigration(CreateViewStatement.java:219) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.13.jar:3.0.13]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.13.jar:3.0.13]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
INFO  [IndexSummaryManager:1] 2017-04-19 19:20:43,246 IndexSummaryRedistribution.java:74 - Redistributing index summaries
{code}


I don't know if it is a bug.
I want to create materialized view with a specific token range.
",,blerer,brstgt,Damien Stevenson,jasonstack,krishna.koneru,KurtG,nkojima,pauloricardomg,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,krishna.koneru,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 16 16:51:27 UTC 2021,,,,,,,,,,"0|i3duv3:",9223372036854775807,3.0.13,3.11.x,5.0,,,,jasonstack,,blerer,brandon.williams,jasonstack,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/44604b7316fcbfd7d0d7425e75cd7ebe267e3247,,,,,,,,,,,,,,"23/Jun/17 07:05;krishna.koneru;EDIT : This patch not valid.new patch in later comment.

-I have triggered tests for the patch on CircleCI .-

||Patch||CircleCI||
|[-3.0- | https://github.com/apache/cassandra/compare/cassandra-3.0...krishna-koneru:CASSANDRA-13464-3.0] |[-circleci-3.0-|https://circleci.com/gh/krishna-koneru/cassandra/4]|
|[-3.11- | https://github.com/apache/cassandra/compare/cassandra-3.11...krishna-koneru:CASSANDRA-13464-3.11]| [-circleci-3.11-|https://circleci.com/gh/krishna-koneru/cassandra/6]|
|[-trunk- | https://github.com/apache/cassandra/compare/trunk...krishna-koneru:CASSANDRA-13464-trunk] |[-circleci-trunk-|https://circleci.com/gh/krishna-koneru/cassandra/5]|

;;;","26/Jun/17 02:26;zznate;[~krishna.koneru] This is interesting and we appreciate the effort, but I'm not sure it's something we want to support directly. What is your use case here? [~brstgt], other MV user folks, Thoughts?

Also, did you test this with repair or bootstrap? I think there are a lot of gotchas with those two use cases. ;;;","26/Jun/17 04:48;brstgt;I personally don't see a use case. The token range in the mv does not relate to a predictable or contiguous range of data in the base table. So I don't know why I would like to do sth like that. If you want to partition your mvs into more tables the you should rather think of a different partition key from my point of view;;;","26/Jun/17 05:07;krishna.koneru;Thanks [~zznate] and [~brstgt] . So the recommended fix would be to restrict CQL syntax to not use token functions in where clause and throw a proper error message (like its currently done if functions are used in select clause ) ?

{code}
cqlsh:test> CREATE MATERIALIZED VIEW mv2 as select token(a),b,c from t1 where a is not null and b is not null and c=1 primary key(a,b);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot use function when defining a materialized view""
{code}

;;;","27/Jun/17 03:04;zznate;bq. So the recommended fix would be to restrict CQL syntax to not use token functions in where clause and throw a proper error message (like its currently done if functions are used in select clause ) ?

Yeah, that's totally fair. ;;;","03/Jul/17 07:48;krishna.koneru;New patch that does not allow token() function to be used in WHERE clause of CREATE MV

||Patch||CircleCI||
|[3.0 | https://github.com/apache/cassandra/compare/cassandra-3.0...krishna-koneru:CASSANDRA-13464-3.0] |[circleci-3.0|https://circleci.com/gh/krishna-koneru/cassandra/18]|
|[3.11 | https://github.com/apache/cassandra/compare/cassandra-3.11...krishna-koneru:CASSANDRA-13464-3.11]| [circleci-3.11|https://circleci.com/gh/krishna-koneru/cassandra/16]|
|[trunk | https://github.com/apache/cassandra/compare/trunk...krishna-koneru:CASSANDRA-13464-trunk] |[circleci-trunk|https://circleci.com/gh/krishna-koneru/cassandra/17]|

;;;","22/Aug/17 05:47;jasonstack;Thanks for the patch. The fix looks good.

A few nits:

1. the assertion in View.java maybe not necessary.
2. In test, there is existing {{assertInvalidThrowMessage}} function. And good to put those tests cases into ViewSchemaTest.;;;","16/May/18 04:36;Damien Stevenson;I have taken the liberty to update [~krishna.koneru]'s patch and make changes based on the comments from [~jasonstack].

Note: I have removed the _testViewAlterBaseTable_ unit test as it no logger makes sense with the changes from CASSANDRA-11500. More specifically:
{noformat}
7. Disallow drop of columns on base tables with MVs because we cannot tell if the dropped column is keeping a view row alive (will be fixed on CASSANDRA-13826){noformat}
Please have another look and let me know what you think. Thanks.


||Patch||CircleCI||
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...damien-instaclustr:13464-mv-token-3.0]|[CirclCI-3.0|https://circleci.com/gh/damien-instaclustr/cassandra/90]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...damien-instaclustr:13464-mv-token-3.11]|[CirclCI-3.11|https://circleci.com/gh/damien-instaclustr/cassandra/88]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...damien-instaclustr:13464-mv-token-trunk]|[CirclCI-3.11|https://circleci.com/gh/damien-instaclustr/cassandra/85]|;;;","14/Jul/21 19:32;brandon.williams;This looks good to me. [~blerer] or [~jasonstack] how do this look to you?;;;","15/Jul/21 07:54;blerer;The patch looks good. I started rebasing it to trigger a CI run before commit. ;;;","15/Jul/21 08:50;jasonstack;looks good to me.

one nit on the javadoc: ""// Use of UDFs *in* where clause *is* not yet supported.token() is the only function allowed in where clause of select."";;;","16/Jul/21 16:48;blerer;I did some minor changes and removed the comment as I am not convinced that it is true. 
CI looks good.
||  Branch || CircleCI ||
| [3.0|https://github.com/blerer/cassandra/commit/45fdfbee7b15ddabb706fe5dd483a620492faaaf] |  [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/185/workflows/422e23bd-5a72-409b-853c-885bdc39bedc] |
| [3.11|https://github.com/blerer/cassandra/commit/d057923aae59e96fb5fb0dadab8045e04564a744] | [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/184/workflows/c8d24b17-8565-4f10-8305-c6e68fd659c4]|
|[4.0|https://github.com/blerer/cassandra/tree/CASSANDRA-13464-4.0] | [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/186/workflows/46f8bf94-d744-4724-a884-d9cc8d3624fd], [j11|https://app.circleci.com/pipelines/github/blerer/cassandra/186/workflows/066508f9-bdf0-40bb-bc83-7c1282cfd850] |;;;","16/Jul/21 16:51;blerer;Committed into 3.0 at 44604b7316fcbfd7d0d7425e75cd7ebe267e3247 and merged into 3.11, 4.0 and trunk ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5 protocol flags decoding broken,CASSANDRA-13443,13063713,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,13/Apr/17 07:06,15/May/20 08:02,14/Jul/23 05:56,13/Apr/17 09:24,3.11.0,4.0,4.0-alpha1,,,,,,,,,0,,,,,"Since native protocol version 5 we deserialize the flags in {{org.apache.cassandra.cql3.QueryOptions.Codec#decode}} as follows:
{code}
            EnumSet<Flag> flags = Flag.deserialize(version.isGreaterOrEqualTo(ProtocolVersion.V5)
                                                   ? (int)body.readUnsignedInt()
                                                   : (int)body.readByte());
{code}

This works until the highest bit (0x80) is not used. {{readByte}} must be changed to {{readUnsignedByte}}.",,snazy,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 09:24:28 UTC 2017,,,,,,,,,,"0|i3dkq7:",9223372036854775807,,,,,,,stefania,,stefania,,,Low,,,,,,,,,,,,,,,,,,,"13/Apr/17 07:09;snazy;||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13443-proto-flags-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13443-proto-flags-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-trunk-dtest/lastSuccessfulBuild/]
;;;","13/Apr/17 07:48;stefania;Nice catch, +1, provided tests pass.;;;","13/Apr/17 09:24;snazy;Thank you!

Committed as [0a438d59e65ee79bca7ffc44b8b958e62448e5c3|https://github.com/apache/cassandra/commit/0a438d59e65ee79bca7ffc44b8b958e62448e5c3] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Schema version changes for each upgraded node in a rolling upgrade, causing migration storms",CASSANDRA-13441,13063579,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,12/Apr/17 18:23,15/May/20 08:02,14/Jul/23 05:56,24/Apr/17 16:13,3.0.14,3.11.0,4.0,4.0-alpha1,,,Legacy/Distributed Metadata,,,,,0,,,,,"In versions < 3.0, during a rolling upgrade (say 2.0 -> 2.1), the first node to upgrade to 2.1 would add the new tables, setting the new 2.1 version ID, and subsequently upgraded hosts would settle on that version.

When a 3.0 node upgrades and writes its own new-in-3.0 system tables, it'll write the same tables that exist in the schema with brand new timestamps. As written, this will cause all nodes in the cluster to change schema (to the version with the newest timestamp). On a sufficiently large cluster with a non-trivial schema, this could cause (literally) millions of migration tasks to needlessly bounce across the cluster.

",,aleksey,jaid,jasonstack,jeromatron,jjirsa,juliuszaromskis,mck,mshuler,philipthompson,tania.engel@quest.com,xuzhongxing,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11983,,,,,,,,,,,CASSANDRA-13812,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 18 14:11:54 UTC 2017,,,,,,,,,,"0|i3djwf:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"12/Apr/17 22:09;mck;I suspect we'll see a number of people doing 2.1.x and 2.2.x upgrades to 3.11.x (especially the bigger clusters after a few patch releases on 3.11), long before we see many upgrading to 4.0.x.

Why not slate this for 3.11.x ?;;;","12/Apr/17 22:55;jjirsa;The risk is that 3.11 is wire-compatible with 3.0, which means a 3.0 -> 3.11 upgrade would be ugly with this patch (constantly bouncing schemas back and forth between the 3.0 and 3.11 versions).
;;;","14/Apr/17 17:22;jjirsa;Patch for trunk at: https://github.com/jeffjirsa/cassandra/tree/cassandra-13441
CircleCI running unit tests: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13441
Dtest will run from here: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/13/ (may be quite some time until dtests run, maybe available by Monday)

;;;","18/Apr/17 02:19;mck;{quote}Patch for trunk at: https://github.com/jeffjirsa/cassandra/tree/cassandra-13441{quote}

Did you push the branch [~jjirsa] ? (i can't see any such branch in your fork);;;","18/Apr/17 02:59;jjirsa;Aleksey mentioned he had some concerns with the approach in IRC and, more importantly, suggested a cleaner way to do it (rather than change the hashing, write new system tables with fixed time stamps). 

Using this method, we're able to actually push this to 3.0, 3.11, and trunk without causing the ping-pong effect the original patch caused, and it's much cleaner (doesn't go all the way down into the storage engine). 

New patches here:

|| Branch || Testall || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.0-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/16/] |
| [3.11|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.11-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/17/] |
| [trunk|https://github.com/jeffjirsa/cassandra/commits/cassandra-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/18/] |


(Unit and dtests are queued now but will likely take some time to complete);;;","18/Apr/17 14:30;aleksey;The patches look good to me. +1

P.S. Thought initially that you overlooked specifying a 0 timestamp for keyspace metadata itself (replication params and durable_writes), but apparently there, in {{MigrationManager.maybeAddKeyspace()}} we already do correctly set 0 timestamp. Makes me wonder how we forgot to do the same for tables back then.;;;","24/Apr/17 16:13;jjirsa;Thanks for the review Aleksey.

Committed as {{032a8cc1e9424c718a78e1463530d62e2e310d4a}} ;;;","11/May/17 15:12;juliuszaromskis;Hi, any workaround for this issue? I've hit this after upgrading from 3.0.9 to 3.0.13 and doing sstableupgrade. Noticed weird disk write patterns and started seeing migration tasks bouncing around. I've only managed to update first of the 3 nodes. Migration tasks have stopped after I've rebooted first node.

{noformat}
Cluster Information:
        Name: cloud.zaromskis.lt cluster
        Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                600b7268-d42a-3b72-8706-093b6c8cfaff: [10.240.0.6]
                77a40699-8e9e-35aa-834e-68c32e40a45a: [10.240.0.7, 10.240.0.8]
{noformat}

{noformat}
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack
UN  10.240.0.6  284.95 GB  256          63.4%             d0d83d9d-0dec-45cd-9ca9-93515fa131f3  rack1
UN  10.240.0.7  288.53 GB  256          64.1%             6d9709a0-0e10-46a1-9afa-d106b74ca9e0  rack1
UN  10.240.0.8  326.31 GB  256          72.5%             5c969700-8bd9-49a4-9772-1284439f8364  rack1
{noformat}

The migrations are in fact executed, as I can see that on the disk, new files are created every second in system keyspace. Why doesn't cluster settle on the same schema version then?

The schema version of first node would not propagate to other nodes. I'm afraid further upgrades might create new schema versions? I can't afford to lose any data. Any advise?

;;;","11/May/17 16:02;jjirsa;Hi [~juliuszaromskis] - if you're upgrading from 3.0.9 to 3.0.13, it's unlikely that this is your issue (this would mostly impact people going from 2.1 -> 3.0, or 2.2 -> 3.0. Unless you're very confident that the schema version on {{10.240.0.6}} is different and more desirable than that on the other two nodes, the most likely solution is to issue a {{nodetool resetlocalschema}} on 10.240.0.6, allowing it to re-pull its schema from .7 and .8.

;;;","11/May/17 16:35;juliuszaromskis;Here's a apiece of debug log, maybe this will help to identify if it's the same issue or not. 

{noformat}
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,896 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22120-big-Data.db (0.098KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8644066)
DEBUG [InternalResponseStage:589] 2017-05-11 16:04:05,941 MigrationManager.java:556 - Gossiping my schema version 77a40699-8e9e-35aa-834e-68c32e40a45a
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:05,944 ColumnFamilyStore.java:850 - Enqueuing flush of keyspaces: 2079 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:05,944 Memtable.java:368 - Writing Memtable-keyspaces@1326973542(0.582KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:05,951 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22200-big-Data.db (0.489KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8685297)
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:05,971 ColumnFamilyStore.java:850 - Enqueuing flush of tables: 65895 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,972 Memtable.java:368 - Writing Memtable-tables@512792876(20.714KiB serialized bytes, 31 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,980 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mc-22197-big-Data.db (13.019KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8700606)
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,005 ColumnFamilyStore.java:850 - Enqueuing flush of columns: 204664 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,006 Memtable.java:368 - Writing Memtable-columns@643217662(43.015KiB serialized bytes, 286 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,018 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22195-big-Data.db (20.975KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8707212)
DEBUG [CompactionExecutor:47263] 2017-05-11 16:04:06,055 CompactionTask.java:146 - Compacting (75968370-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22193-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22194-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22192-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22195-big-Data.db:level=0, ]
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in dropped_columns
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in triggers
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in views
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in types
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in functions
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in aggregates
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:850 - Enqueuing flush of indexes: 598 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,057 Memtable.java:368 - Writing Memtable-indexes@1653592334(0.127KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,068 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22121-big-Data.db (0.098KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8742156)
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,090 CompactionTask.java:146 - Compacting (759bdaa0-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22120-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22121-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22119-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22118-big-Data.db:level=0, ]
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,112 MigrationManager.java:556 - Gossiping my schema version 77a40699-8e9e-35aa-834e-68c32e40a45a
DEBUG [CompactionExecutor:47263] 2017-05-11 16:04:06,120 CompactionTask.java:218 - Compacted (75968370-3663-11e7-ab7b-d7e32ecfc62d) 4 sstables to [/mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22196-big,] to level=0.  29,348 bytes to 10,277 (~35% of original) in 64ms = 0.153139MB/s.  0 total partitions merged to 6.  Partition merge counts were {1:2, 4:4, }
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,136 CompactionTask.java:218 - Compacted (759bdaa0-3663-11e7-ab7b-d7e32ecfc62d) 4 sstables to [/mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22122-big,] to level=0.  392 bytes to 98 (~25% of original) in 45ms = 0.002077MB/s.  0 total partitions merged to 1.  Partition merge counts were {4:1, }
DEBUG [NonPeriodicTasks:1] 2017-05-11 16:04:06,782 MigrationManager.java:124 - submitting migration task for /10.240.0.6
DEBUG [NonPeriodicTasks:1] 2017-05-11 16:04:06,782 MigrationManager.java:124 - submitting migration task for /10.240.0.7
DEBUG [InternalResponseStage:586] 2017-05-11 16:04:06,792 ColumnFamilyStore.java:850 - Enqueuing flush of keyspaces: 2079 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,793 Memtable.java:368 - Writing Memtable-keyspaces@847538504(0.582KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,799 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22201-big-Data.db (0.489KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8939092)
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,816 CompactionTask.java:146 - Compacting (760aa200-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22201-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22198-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22199-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22200-big-Data.db:level=0, ]
DEBUG [InternalResponseStage:586] 2017-05-11 16:04:06,816 ColumnFamilyStore.java:850 - Enqueuing flush of tables: 65895 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,817 Memtable.java:368 - Writing Memtable-tables@1477384931(20.714KiB serialized bytes, 31 ops, 0%/0% of on/off-heap limit)
{noformat}

This has continued for several hours until 've decided to continue with the migration to 3.0.13, here's the result:

{noformat}
Cluster Information:
        Name: cloud.zaromskis.lt cluster
        Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                600b7268-d42a-3b72-8706-093b6c8cfaff: [10.240.0.6, 10.240.0.7, 10.240.0.8]
{noformat}

Looks like cluster is consistent now;;;","07/Jul/17 06:07;xuzhongxing;I still see this when a new node is joining the cluster. The cassandra version are all 3.11.0

ERROR [main] 2017-07-07 13:56:29,873 MigrationManager.java:172 - Migration task failed to complete
;;;","18/Sep/17 14:11;juliuszaromskis;This has indeed resolved an issue I was having. Using 3.0.14 now.

Note: if anyone else has problems adding new nodes or datacenter to the cluster, this resolves an issue with schema version mismatch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testall failure in org.apache.cassandra.index.internal.CassandraIndexTest.indexOnRegularColumn,CASSANDRA-13427,13062450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,07/Apr/17 14:48,16/Apr/19 09:30,14/Jul/23 05:56,20/Apr/17 09:34,,,,,,,,,,,,0,,,,,"Because of the name clash, there's a following failure happening (extremely infrequently, it's worth noting, seen it only once, no further traces / instances found):

{code}
Error setting schema for test (query was: CREATE INDEX v_index ON cql_test_keyspace.table_22(v))
{code}

Stacktrace:

{code}
java.lang.RuntimeException: Error setting schema for test (query was: CREATE INDEX v_index ON cql_test_keyspace.table_22(v))
{code}",,adelapena,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 09:33:27 UTC 2017,,,,,,,,,,"0|i3dcy7:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,,,,,,,,,,,,,,,,,,"07/Apr/17 15:23;ifesdjeen;Prepared a simple patch: 

|[3.0|https://github.com/apache/cassandra/compare/3.0...ifesdjeen:13427-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-3.0-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/3.11...ifesdjeen:13427-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-3.11-testall/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13427-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-trunk-testall/]|;;;","10/Apr/17 11:11;adelapena;I think that you have forgotten to increase the variable [{{CassandraIndexTest.indexCounter}}|https://github.com/ifesdjeen/cassandra/blob/13427-3.0/test/unit/org/apache/cassandra/index/internal/CassandraIndexTest.java#L529] used to generate the unique names for indexes. The tests don't fail because the generated index name also contains the table name, which contains an {{AtomicInteger}} sequence number with similar purpose. Alternatively, [CASSANDRA-13385|https://issues.apache.org/jira/browse/CASSANDRA-13385] could be useful to get the generated index names. A part from this, the patch looks good to me.;;;","11/Apr/17 07:32;ifesdjeen;[~adelapena] great find! Sorry I missed it. Fixed, rebased and re-triggered CI.;;;","11/Apr/17 11:48;adelapena;Thanks [~ifesdjeen], now it's perfect, +1;;;","20/Apr/17 09:33;ifesdjeen;Committed to 3.0 as [e5c2a1839f2cdf16771dcba726f862e61fda8d4f|https://github.com/apache/cassandra/commit/e5c2a1839f2cdf16771dcba726f862e61fda8d4f] and merged up to [3.11|https://github.com/apache/cassandra/commit/fc834186ff0faa5ff78512637badc59990e51173] and [trunk|https://github.com/apache/cassandra/commit/37f5005a15c9addc3b99c5a35cb72f9fc9c2c912],;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionStrategyManager should take write not read lock when handling remove notifications,CASSANDRA-13422,13062252,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,06/Apr/17 21:34,15/May/20 08:01,14/Jul/23 05:56,10/Apr/17 23:50,3.11.0,4.0,4.0-alpha1,,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"{{getNextBackgroundTask}} in various compaction strategies (definitely {{LCS}}) rely on checking the result of {{DataTracker.getCompacting()}} to avoid accessing data and metadata related to tables that have already head their resources released.

There is a race where this check is unreliable and will claim a table that has its resources already released is not compacting resulting in use after free.

[{{LeveledCompactionStrategy.findDroppableSSTable}}|https://github.com/apache/cassandra/blob/c794d2bed7ca1d10e13c4da08a3d45f5c755c1d8/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L504] for instance has this three part logical && condition where the first check is against the compacting set before calling {{worthDroppingTombstones}} which fails if the table has been released.

The order of events is basically that CompactionStrategyManager acquires the read lock in getNextBackgroundTask(), then proceeds eventually to findDroppableSSTable and acquires a set of SSTables from the manifest. While the manifest is thread safe it's not accessed atomically WRT to other operations. Once it has acquired the set of tables it acquires (not atomically) the set of compacting SSTables and iterates checking the former against the latter.

Meanwhile other compaction threads are marking tables obsolete or compacted and releasing their references. Doing this removes them from {{DataTracker}} and publishes a notification to the strategies, but this notification only requires the read lock. After the compaction thread has published the notifications it eventually marks the table as not compacting in {{DataTracker}} or removes it entirely.

The race is then that the compaction thread generating a new background task acquires the sstables from the manifest on the stack. Any table in that set that was compacting at that time must remain compacting so that it can be skipped. Another compaction thread finishes a compaction and is able to remove the table from the manifest and then remove it from the compacting set. The thread generating the background task then acquires the list of compacting tables which doesn't include the table it is supposed to skip.

The simple fix appears to be to require threads to acquire the write lock in order to publish notifications of tables being removed from compaction strategies. While holding the write lock it won't be possible for someone to see a view of tables in the manifest where tables that are compacting aren't compacting in the view.",,aweisberg,marcuse,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 23:50:52 UTC 2017,,,,,,,,,,"0|i3dbqf:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"06/Apr/17 22:02;aweisberg;||Code|utests|dtests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13422-3.11?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/10]|[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/6/#showFailuresLink]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13422-trunk?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/12]|[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/5/]|;;;","07/Apr/17 12:59;marcuse;+1;;;","10/Apr/17 23:50;aweisberg;Committed as [c97514243e8c58bdda0ebf75212a8a217f3d017e|https://github.com/apache/cassandra/commit/f6f50129d72b149a62f7e26e081e4d43097f9236];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WriteResponseHandlerTest is sensitive to test execution order,CASSANDRA-13421,13062240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,06/Apr/17 20:32,15/May/20 08:00,14/Jul/23 05:56,02/May/17 21:27,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,,,,,One of the tests checks statistics that aren't reset between tests. If the test doesn't run first then the expected values are incorrect.,,aweisberg,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 21:27:49 UTC 2017,,,,,,,,,,"0|i3dbnr:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"06/Apr/17 21:59;aweisberg;||Code|utests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13421-trunk?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/9]|;;;","14/Apr/17 20:37;jjirsa;lgtm, +1
;;;","02/May/17 21:27;aweisberg;Committed as [d4933a019d8a717029444887f5f8a72d61cedd95|https://github.com/apache/cassandra/commit/d4933a019d8a717029444887f5f8a72d61cedd95];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pending repair info was added in 4.0,CASSANDRA-13420,13062067,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,06/Apr/17 10:34,15/May/20 08:01,14/Jul/23 05:56,13/Apr/17 11:40,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Pending repair information was actually added in 4.0

https://github.com/krummas/cassandra/commits/marcuse/pendingrepairversion",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 11:40:42 UTC 2017,,,,,,,,,,"0|i3dalb:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"10/Apr/17 17:35;bdeggleston;+1;;;","13/Apr/17 11:40;marcuse;and committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal unicode character breaks compilation on Chinese env OS,CASSANDRA-13417,13061816,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,05/Apr/17 17:23,15/May/20 08:05,14/Jul/23 05:56,05/Apr/17 17:28,3.11.0,4.0,4.0-alpha1,,,,,,,,,0,,,,,"Creating JIRA for tracking GH issue https://github.com/apache/cassandra/pull/104

Fix is contained within a comment block, so skipping CI.

",,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/cassandra/pull/104,,,,,,,,,,9223372036854775807,,,Wed Apr 05 17:28:22 UTC 2017,,,,,,,,,,"0|i3d91j:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"05/Apr/17 17:28;jjirsa;Committed as {{bfdc1e0fdb3e4adad8d044203feaab8350dfdee8}}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple test failures with NPE in org.apache.cassandra.streaming.StreamSession,CASSANDRA-13416,13061785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,mshuler,mshuler,mshuler,05/Apr/17 15:52,16/Apr/19 09:30,14/Jul/23 05:56,05/Apr/17 15:59,,,,,,,,,,,,0,test-failure,,,,"example failures:

http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences_compression

one example stack trace:
{code}
Stacktrace

java.lang.NullPointerException
	at org.apache.cassandra.streaming.StreamSession.isKeepAliveSupported(StreamSession.java:244)
	at org.apache.cassandra.streaming.StreamSession.<init>(StreamSession.java:196)
	at org.apache.cassandra.streaming.StreamCoordinator$HostStreamingData.getOrCreateNextSession(StreamCoordinator.java:293)
	at org.apache.cassandra.streaming.StreamCoordinator.getOrCreateNextSession(StreamCoordinator.java:158)
	at org.apache.cassandra.streaming.StreamPlan.requestRanges(StreamPlan.java:94)
	at org.apache.cassandra.repair.LocalSyncTask.startSync(LocalSyncTask.java:85)
	at org.apache.cassandra.repair.SyncTask.run(SyncTask.java:75)
	at org.apache.cassandra.repair.LocalSyncTaskTest.testDifference(LocalSyncTaskTest.java:117)
{code}",,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/17 15:52;mshuler;jenkins-trunk_testall-1491_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12862097/jenkins-trunk_testall-1491_logs.tar.gz",,,,,,,,,,,,,,,,,,,1.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 15:59:29 UTC 2017,,,,,,,,,,"0|i3d8un:",9223372036854775807,,,,,,,,,,,,Critical,,,,,,,,,,,,,,,,,,,"05/Apr/17 15:59;mshuler;It looks like this was fixed in https://github.com/apache/cassandra/commit/633babf0f02fac56cad7bff03a4ff415feb38f39;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run more test targets on CircleCI,CASSANDRA-13413,13061719,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,05/Apr/17 12:33,15/May/20 08:03,14/Jul/23 05:56,07/Apr/17 06:15,2.1.18,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,CI,,,,,0,CI,,,,"Currently we only run {{ant test}} on circleci, we should use all the (free) containers we have and run more targets in parallel.",,aweisberg,colinkuo,eduard.tudenhoefner,jasobrown,jjirsa,marcuse,mshuler,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 18:54:50 UTC 2017,,,,,,,,,,"0|i3d8fz:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Normal,,,,,,,,,,,,,,,,,,,"05/Apr/17 12:36;marcuse;https://github.com/krummas/cassandra/commits/marcuse/testcircle

It runs {{ant eclipse-warnings; ant test}} if we only have one container configured, if we have 4 it also runs {{ant long-test}}, {{ant test-compression}} and {{ant stress-test}} on the different containers

https://circleci.com/gh/krummas/cassandra/5 (not finished when posting this, so might be broken);;;","05/Apr/17 12:47;jasobrown;this is awesome! +1.

Maybe add 'microbench' target somewhere? Not sure how long it takes to execute, though...;;;","05/Apr/17 13:03;marcuse;bq. Maybe add 'microbench' target somewhere?
would be nice, but not sure the place is in CI - we would need jmh to output some sort of junit xml test report, and fail builds based on the benchmark times I guess? And not sure how predictable the performance is in CircleCI if it actually gives us something (other than we make sure that the benchmarks can actually build/execute);;;","05/Apr/17 13:13;jasobrown;bq. not sure how predictable the performance is

Yeah, you are correct about that, and that's the best reason for not including it. I'm still +1 on the current patch on your branch;;;","05/Apr/17 13:16;marcuse;my test failed quite badly: https://circleci.com/gh/krummas/cassandra/5

rerunning;;;","05/Apr/17 15:59;jjirsa;Build #6 looks better, but it's somehow not gathering the junit result properly 

{quote}
Your build ran 2980 tests in junit with 0 failures
Slowest test: org.apache.cassandra.cql3.PagingQueryTest pagingOnRegularColumn (took 105.24 seconds).
{quote}

{quote}
    [junit] Testsuite: org.apache.cassandra.net.MessagingServiceTest-compression
    [junit] Testsuite: org.apache.cassandra.net.MessagingServiceTest-compression Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.835 sec
    [junit] 
    [junit] Testcase: testDroppedMessages(org.apache.cassandra.net.MessagingServiceTest)-compression:	FAILED
    [junit] expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
    [junit] junit.framework.AssertionFailedError: expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
    [junit] 	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:114)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.net.MessagingServiceTest FAILED
{quote}

Passed on 3 of the 4 containers/suites, though, so it's pretty close. {{test-compression}} puts results into {{build/test/output/compression/}} so you'll need to copy those into the results directory in the post step.;;;","05/Apr/17 17:06;mshuler;I was working on multiple runners and fixing a broken test in CASSANDRA-13414. It looks like we OOM the container with more than one runner, so since y'all are here, can you include this one-liner to fix o.a.c.utils.BitSetTest.compareBitSets?
{code}
diff --git a/circle.yml b/circle.yml
index f0df31265a..66e752f367 100644
--- a/circle.yml
+++ b/circle.yml
@@ -8,6 +8,7 @@ test:
     - sudo ifconfig lo:2 127.0.0.3 up
     - sudo ifconfig lo:3 127.0.0.4 up
     - sudo ifconfig lo:4 127.0.0.5 up
+    - sudo apt-get update; sudo apt-get install wamerican
     - ant build
 
   override:
{code}

Since you're removing the pre: section, I'm not sure how to include that in the parallel bits, yet. Thanks!;;;","05/Apr/17 17:08;marcuse;sure, will include!;;;","05/Apr/17 17:12;marcuse;running a new test with wamerican installed and it also collects the compression xmls, thanks guys: https://circleci.com/gh/krummas/cassandra/8;;;","06/Apr/17 17:14;marcuse;ok, finally got it working: https://circleci.com/gh/krummas/cassandra/16;;;","06/Apr/17 17:35;mshuler;{{ant test-cdc}} writes its result xml files to {{build/test/output/cdc}} and `ant test-all` writes to multiple for each dep target. Jenkins allows ant-style globbing, so we use {{**/TEST-*.xml}} everywhere to just grab everything. I'd bet something like a blanket {{find ./build/test/output -type f -name *.xml}} and cp to $CIRCLE_TEST_REPORTS/junit/ - sorry, I think that will overwrite same-named files, so maybe {{find ./build/test/output -type d}} for finding all the directories under there and then doing the recursive copy of each?

Sorry I don't have the bandwidth to test all the possibles, but just spitballing something to cover all the possible targets a little better than a single conditional.;;;","06/Apr/17 17:38;marcuse;My patch essentially runs test-all:
{{<target name=""test-all"" depends=""eclipse-warnings,test,long-test,test-compression,stress-test"" ...}}
and this patch runs those different targets in the 4 different containers.

Had no idea test-cdc existed, is that being run on cassci?;;;","06/Apr/17 17:42;mshuler;test-cdc is running on the ASF Jenkins branches that contain it, so cassandra-3.11 and trunk - https://builds.apache.org/view/A-D/view/Cassandra/;;;","06/Apr/17 17:48;marcuse;updated patch to copy all xml files in output directory, running now;;;","06/Apr/17 17:49;marcuse;and it wont overwrite the files, the commands are being run on each of the containers;;;","06/Apr/17 19:15;jjirsa;lgtm

;;;","06/Apr/17 19:29;mshuler;ship it! :);;;","06/Apr/17 21:55;aweisberg;These seems less useful for me in day to day testing where I would rather optimize for response time vs. daily test runs that are more thorough.

If I need to build several branches at once I would rather use my containers for that or better yet for running the same branch faster.

For me this would be a step backwards.;;;","07/Apr/17 04:47;marcuse;[~aweisberg] just leave the parallelism at 1 and it should works just like before;;;","07/Apr/17 06:15;marcuse;committed, thanks;;;","07/Apr/17 16:10;aweisberg;Where do I set the parallelism to 1? This is still going to take a 1.5 hour test run and turn it into 4 hours of testing that needs to run through my pool of 4 containers.;;;","07/Apr/17 16:11;aweisberg;Found out where you adjust parallelism https://circleci.com/gh/[username]/cassandra/edit#parallel-builds;;;","07/Apr/17 16:20;marcuse;bq. This is still going to take a 1.5 hour test run and turn it into 4 hours of testing
what do you mean?

with parallelism=1 circleci will only run 'case 0' here: https://github.com/apache/cassandra/blob/trunk/circle.yml#L10;;;","07/Apr/17 16:24;aweisberg;Ah great. I thought it was going to run them serially.;;;","17/Aug/17 18:54;eduard.tudenhoefner;It looks like the *stress-test* target only got introduced with CASSANDRA-11638 (3.10) and so *stress-test* isn't valid in all previous versions. [~krummas] I created CASSANDRA-13775, could you review it please?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update of column with TTL results in secondary index not returning row,CASSANDRA-13412,13061716,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,ebautistabar,ebautistabar,05/Apr/17 12:17,16/Apr/19 09:30,14/Jul/23 05:56,10/May/17 11:04,2.1.18,2.2.10,,,,,Feature/2i Index,,,,,0,,,,,"Cassandra versions: 2.2.3, 3.0.11
1 datacenter, keyspace has RF 3. Default consistency level.

Steps:
1. I create these table and index.
{code}
CREATE TABLE my_table (
    a text,
    b text,
    c text,
    d set<int>,
    e float,
    f text,
    g int,
    h double,
    j set<int>,
    k float,
    m set<text>,
    PRIMARY KEY (a, b, c)
) WITH read_repair_chance = 0.0
   AND dclocal_read_repair_chance = 0.1
   AND gc_grace_seconds = 864000
   AND bloom_filter_fp_chance = 0.01
   AND caching = { 'keys' : 'ALL', 'rows_per_partition' : 'NONE' }
   AND comment = ''
   AND compaction = { 'class' : 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' }
   AND compression = { 'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor' }
   AND default_time_to_live = 0
   AND speculative_retry = '99.0PERCENTILE'
   AND min_index_interval = 128
   AND max_index_interval = 2048;
CREATE INDEX my_index ON my_table (c);
{code}
2. I have 9951 INSERT statements in a file and I run the following command to execute them. The INSERT statements have no TTL and no consistency level is specified.
{code}
cqlsh <ip> <port> -u <user> -f <file>
{code}
3. I update a column filtering by the whole primary key, and setting a TTL. For example:
{code}
UPDATE my_table USING TTL 30 SET h = 10 WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
{code}
4. After the time specified in the TTL I run the following queries:
{code}
SELECT * FROM my_table WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
SELECT * FROM my_table WHERE c = 'test_c';
{code}
The first one returns the correct row with an empty h column (as it has expired). However, the second query (which uses the secondary index on column c) returns nothing.

I've done the query through my app which uses the Java driver v3.0.4 and reads with CL local_one, from the cql shell and from DBeaver 3.8.5. All display the same behaviour. The queries are performed minutes after the writes and the servers don't have a high load, so I think it's unlikely to be a consistency issue.

I've tried to reproduce the issue in ccm and cqlsh by creating a new keyspace and table, and inserting just 1 row, and the bug doesn't manifest. This leads me to think that it's an issue only present with not trivially small amounts of data, or maybe present only after Cassandra compacts or performs whatever maintenance it needs to do.",,adelapena,ebautistabar,ifesdjeen,julienlau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 11:02:39 UTC 2017,,,,,,,,,,"0|i3d8fb:",9223372036854775807,2.1.17,2.2.9,,,,,ifesdjeen,,ifesdjeen,,,Normal,,2.2.3,,,,,,,,,,,,,,,,,"12/Apr/17 12:33;adelapena;[~ebautistabar], I can reproduce the problem in 2.1 and 2.2 this way:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};

CREATE TABLE k.t (
  a text,
  b text,
  c text,
  d set<int>,
  e float,
  f text,
  g int,
  h double,
  j set<int>,
  k float,
  m set<text>,
  PRIMARY KEY (a, b, c)
);
CREATE INDEX ON k.t(c);

INSERT INTO k.t (a, b, c, h) VALUES ('test_a', 'test_b', 'test_c', 1);
UPDATE k.t USING TTL 10 SET h = 10 WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
-- Wait 10 seconds

SELECT * FROM k.t WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c'; -- 1 row
SELECT * FROM k.t WHERE c = 'test_c'; -- 0 rows
{code}
Or, in a simpler way:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

CREATE TABLE k.t (
    pk int,
    ck int,
    a int,
    b int,
    PRIMARY KEY (pk, ck)
);
CREATE INDEX ON k.t(ck);

INSERT INTO k.t (pk, ck, a, b) VALUES (1, 2, 3, 4);
UPDATE k.t USING TTL 10 SET b = 10 WHERE pk = 1 AND ck = 2;
-- Wait 10 seconds

SELECT * FROM k.t WHERE pk = 1 AND ck = 2; -- 1 row
SELECT * FROM k.t WHERE ck = 2; -- 0 rows
{code}
However, I can't reproduce the problem in 3.0.11. Could you please provide a sequence of insertions producing the failure in 3.0.11?;;;","13/Apr/17 08:39;adelapena;The problem is also affecting to indexes on partition key components:
{code}
CREATE TABLE k.t (
    pk1 int,
    pk2 int,
    a int,
    b int,
    PRIMARY KEY ((pk1, pk2))
);
CREATE INDEX ON k.t(pk1);

INSERT INTO k.t (pk1, pk2, a, b) VALUES (1, 2, 3, 4);
UPDATE k.t USING TTL 10 SET b = 10 WHERE pk1 = 1 AND pk2 = 2;
-- Wait 10 seconds

SELECT * FROM k.t WHERE pk1 = 1 AND pk2 = 2; -- 1 row
SELECT * FROM k.t WHERE pk1 = 1; -- 0 rows
{code}

Index entries inherit the TTL of the indexed cell, and this is right with regular columns. However, indexes of primary key columns index every column, meaning that their index entries will always get the TTL of the last updated row column. 

The proposed solution is to not generate expiring cells when the indexed column is part of the primary key. Here is the patch for 2.1 and 2.2:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13412-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13412-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-dtest/]|
;;;","17/Apr/17 09:00;ebautistabar;[~adelapena], thanks a lot for taking care of this. I'll try to reproduce it again on 3.0.11 and write back with more information.

When you reproduced it in 2.2.x, what did you use? CCM, a real cluster or both?;;;","17/Apr/17 11:12;adelapena;You are very welcome. At first, I've reproduced the problem in a three nodes ccm cluster. Then, since the cause of the problem seems to be local, I've reproduced it in a single instance and I've added the two unit tests that can be found in the suggested patch.;;;","20/Apr/17 08:20;ifesdjeen;+1, the patch looks good.

Minor remark: we might want to add a test for a regular column, too (e.g. that the new value isn't queryable) and possibly add same tests to 3.0+, as the behaviour is important (although it does work on the later branches).;;;","20/Apr/17 15:49;adelapena;Totally agree. I have added a new test for an index on a regular column. Here is the new patch, where the unaffected 3.0+ branches have only the unit tests:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13412-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13412-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-dtest/]|
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13412-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-3.0-testall/]| |
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13412-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-3.11-testall/]| |
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13412-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-trunk-testall/]| |;;;","26/Apr/17 13:57;ebautistabar;I've tried to identify a precise sequence of steps to reproduce this in 3.0.x but have had no luck so far. You can close this issue if you want. If I manage to reproduce it in 3.0.x consistently I will open another issue (or re-open this one, whatever you prefer).

Thanks!;;;","09/May/17 13:41;ifesdjeen;Sorry for the delay! Thank you for the patch,

+1 from my side, LGTM.;;;","10/May/17 11:02;adelapena;Committed as [b0db519b79701cecac92a7a2c93101cf17fb928d|https://github.com/apache/cassandra/commit/b0db519b79701cecac92a7a2c93101cf17fb928d], thanks for reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool upgradesstables/scrub/compact ignores system tables,CASSANDRA-13410,13061492,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,04/Apr/17 17:45,15/May/20 08:06,14/Jul/23 05:56,05/Apr/17 17:13,3.0.13,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,"CASSANDRA-11627 changed the behavior of nodetool commands that work across all keyspaces. Sometimes it's OK (not compacting system.peers when you call compact probably isn't going to anger anyone), but sometimes it's not (disableautocompaction, flush, upgradesstables, etc).

",,jeromatron,jjirsa,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11830,,,,,,,,,,,CASSANDRA-11627,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 17:13:32 UTC 2017,,,,,,,,,,"0|i3d71j:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,3.0.6,,,,,,,,,,,,,,,,,"04/Apr/17 19:34;jjirsa;|| branch || utests || dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13410-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13410-dtest/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-13410-dtest/] |

;;;","04/Apr/17 19:37;jjirsa;Reviewer: ignore the circle.yml commit, it's only there to enable me to run unit tests. The single line patch applies to all 3 versions cleanly.
;;;","05/Apr/17 10:32;marcuse;+1;;;","05/Apr/17 13:37;pauloricardomg;For the record this also affects 2.2 and was actually introduced by CASSANDRA-5483, as reported by CASSANDRA-11830, so I'm closing that as duplicate of this. But given 2.2 is nearly in critical-fixes-only mode, and would require a different patch I think it's fine to keep this 3.0+ only.;;;","05/Apr/17 17:13;jjirsa;Committed as {{6efb44b3ab5816cb7c2b007dba68b3224f899ac8}} to 3.0 and merged up. 

Agreeing with [~pauloricardomg] , skipping 2.2 as it's not critical.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure at RemoveTest.testBadHostId,CASSANDRA-13407,13061357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,04/Apr/17 07:27,16/Apr/19 09:30,14/Jul/23 05:56,20/Apr/17 09:54,,,,,,,,,,,,0,,,,,"Example trace:

{code}
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:881)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:876)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:2201)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1855)
	at org.apache.cassandra.Util.createInitialRing(Util.java:216)
	at org.apache.cassandra.service.RemoveTest.setup(RemoveTest.java:89)
{code} 

[failure example|https://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.service/RemoveTest/testBadHostId/]
[history|https://cassci.datastax.com/job/trunk_testall/lastCompletedBuild/testReport/org.apache.cassandra.service/RemoveTest/testBadHostId/history/]
",,ifesdjeen,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 09:53:45 UTC 2017,,,,,,,,,,"0|i3d67j:",9223372036854775807,,,,,,,jkni,,jkni,,,Normal,,,,,,,,,,,,,,,,,,,"06/Apr/17 13:21;ifesdjeen;It seems that the problem was in the fact that we are staring {{Gossiper}}:

|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13407-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-trunk-testall/]|

(utests were running ~25 times without this failure occuring, multiplexing a single test didn't help reproducing it with a patch either).;;;","06/Apr/17 17:01;jkni;For posterity, this is the race possible when the Gossiper is started, as far as I can tell.

In setup, we initialize a fake ring using Util.createInitialRing. This will intialize the nodes in an unsafe manner and then inject the token states. If a status check runs before the tokens state is set, the previously decommissioned node will look like a fat client, since it won't have tokens and will not have a DEAD_STATE. Since we aren't gossiping, we won't have heard from it in greater than fatClientTimeout, so we'll remove it. If this races with the ss.onChange in createInitialRing, we can remove the endpointstate while processing it, which will cause a NPE as above.

We also need to remove SchemaLoader.loadSchema() as you did in the patch - this is because it starts the Gossiper as well. This is fine; we don't appear to need it.

The patch looks good - the race exists in theory on 2.1/2.2, but it appears to only manifest on 3.0+. I don't think it is worth committing to 2.1 for that reason - let's do 2.2+ forward and run the test at least once on each branch before committing.

;;;","07/Apr/17 08:23;ifesdjeen;Looks like I was able to gather a bit more information on the issue. To confirm what you're saying. It is possible to reproduce locally by tweaking timeouts (particularly making the gossip interval shorter, to emulate the slow VM). 

{code}
INFO  [GossipTasks:1] 2017-04-03 23:05:53,433 Gossiper.java:810 - FatClient /127.0.0.4 has been silent for 1000ms, removing from gossip
DEBUG [GossipTasks:1] 2017-04-03 23:05:53,436 Gossiper.java:432 - removing endpoint /127.0.0.4
DEBUG [GossipTasks:1] 2017-04-03 23:05:53,436 Gossiper.java:407 - evicting /127.0.0.4 from gossip
{code}

After that we can get an NPE either in {{Gossiper#getHostId}} or {{StorageService#isStatus}}. 

The patch for 2.0 and 3.0 is slightly different, as if we do not initialise schema, we'll get the following error: 

{code}
    [junit] junit.framework.AssertionFailedError: []
    [junit]     at org.apache.cassandra.db.lifecycle.Tracker.getMemtableFor(Tracker.java:312)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1185)
    [junit]     at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:573)
    [junit]     at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
    [junit]     at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
    [junit]     at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
    [junit]     at org.apache.cassandra.db.SystemKeyspace.updatePeerInfo(SystemKeyspace.java:712)
    [junit]     at org.apache.cassandra.service.StorageService.updatePeerInfo(StorageService.java:1801)
    [junit]     at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:2014)
    [junit]     at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1669)
    [junit]     at org.apache.cassandra.Util.createInitialRing(Util.java:213)
    [junit]     at org.apache.cassandra.service.RemoveTest.setup(RemoveTest.java:77)
{code}

|[2.2|https://github.com/apache/cassandra/compare/2.2...ifesdjeen:13407-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-2.2-testall/]|
|[3.0|https://github.com/apache/cassandra/compare/3.0...ifesdjeen:13407-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-3.0-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/3.11...ifesdjeen:13407-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-3.11-testall/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13407-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-trunk-testall/]|;;;","10/Apr/17 17:27;jkni;Patches look good - good catch on 2.2/3.0 difference.

+1.;;;","20/Apr/17 09:53;ifesdjeen;Committed to 2.2 as [31590f5da10de8bbcf36d19617ced02b37be2a57|https://github.com/apache/cassandra/commit/31590f5da10de8bbcf36d19617ced02b37be2a57] and merged up to [3.0|https://github.com/apache/cassandra/commit/b063b38f33474cd0687167599a3a8cec7ed82631], [3.11|https://github.com/apache/cassandra/commit/638df6f971b806460113031524e14b21ae3e20f8] and [trunk|https://github.com/apache/cassandra/commit/90e50789680c8d386a57884a0d58ee62e5f4e73d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ViewBuilder can miss data due to sstable generation filter,CASSANDRA-13405,13061195,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,03/Apr/17 17:54,16/Apr/19 09:30,14/Jul/23 05:56,04/Apr/17 16:49,3.0.13,3.11.0,,,,,Feature/Materialized Views,Legacy/Local Write-Read Paths,,,,0,materializedviews,,,,"The view builder for one MV is restarted when other MVs are added on the same keyspace.  There is an issue if compactions are running between these restarts that can cause the view builder to skip data, since the builder tracks the max sstable generation to filter by when it starts back up.

I don't see a need for this generation tracking across restarts, it only needs to be tracked during a builders life (to avoid adding in newly compacted data).  

",,jeromatron,jjirsa,pauloricardomg,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 16:49:35 UTC 2017,,,,,,,,,,"0|i3d57j:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"03/Apr/17 18:24;tjake;Fix with test to repro.
Basically ignore the generation value...

I've also added some better debug logging to help operators and us see what's happening.

[3.0|https://github.com/tjake/cassandra/tree/13405-3.0]
[testall|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.0-testall/]
[dtest|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.0-dtest/]


[3.11|https://github.com/tjake/cassandra/tree/13405-3.11]
[testall|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.11-testall/]
[dtest|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.11-dtest/];;;","03/Apr/17 20:24;pauloricardomg;bq. The view builder for one MV is restarted when other MVs are added on the same keyspace.

Just curious, is there any reason for this? I don't see a reason for interrupting current view builds when a new view is added so perhaps we should improve this? Either here or in another ticket..

bq. I don't see a need for this generation tracking across restarts, it only needs to be tracked during a builders life (to avoid adding in newly compacted data). 

good catch, I think the original idea was to support resume after crash but this is not safe since compacted sstables can be garbage collected in-between view build restart - we could try to handle this by keeping the references between rebuild, but resuming by token range is probably good enough already and much simpler.

The patch looks good and new logs will be very helpful, commented a few minor nits directly on github code. +1 after nits are addressed and 3.11 dtest results look good. Thanks!;;;","04/Apr/17 13:59;tjake;Nits addressed running CI again;;;","04/Apr/17 16:49;tjake;Committed {{2f1ab4a4248ac24c890e195cd5714ca54510c19a}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArithmeticException: / by zero when index is created on table with clustering columns only,CASSANDRA-13400,13061084,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,03/Apr/17 10:24,16/Apr/19 09:30,14/Jul/23 05:56,04/Apr/17 13:04,3.0.13,,,,,,Legacy/Local Write-Read Paths,,,,,0,2i,secondary_index,,,"If we create an index over the clustering key of a table without regular columns:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE k.t (pk text, ck text, PRIMARY KEY (pk, ck));
INSERT INTO k.t (pk, ck) VALUES ( 'pk','ck');
CREATE INDEX idx ON k.t(ck);
{code}
Then the following error index creation erros is logged:
{code}
INFO  10:19:34 Submitting index build of idx for data in BigTableReader(path='/Users/adelapena/datastax/cassandra/data/data/k/t-ed3d6f90185611e7949f55d18a2e5858/mc-1-big-Data.db')
ERROR 10:19:34 Exception in thread Thread[SecondaryIndexManagement:2,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:402) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:723) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$5(CassandraIndex.java:693) ~[main/:na]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_112]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_112]
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_112]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:398) ~[main/:na]
	... 7 common frames omitted
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.index.SecondaryIndexManager.calculateIndexingPageSize(SecondaryIndexManager.java:629) ~[main/:na]
	at org.apache.cassandra.index.SecondaryIndexBuilder.build(SecondaryIndexBuilder.java:62) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1347) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	... 5 common frames omitted
{code}
Any further queries using the index will fail:
{code}
SELECT * FROM k.t where ck = 'ck';
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}",,adelapena,blerer,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13448,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 13:04:22 UTC 2017,,,,,,,,,,"0|i3d4iv:",9223372036854775807,3.0.11,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"03/Apr/17 11:09;adelapena;The problem is due to a typo at {{SecondaryIndexManager.calculateIndexingPageSize}}.

The bug only affects 3.0. It doesn't affect to 3.11 nor trunk because it was solved with [this direct commit|https://github.com/adelapena/cassandra/commit/eaced9a541d09d55973b6f88d720e16ac948a559].

This patch fixes the typo and adds a simple test:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13400-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13400-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13400-3.0-dtest/]|;;;","04/Apr/17 12:18;blerer;+1
Thanks for the patch.;;;","04/Apr/17 13:04;blerer;Committed into 3.0 at 828ca7cc925de90c3883e935c66f7beec6fa9113 and merged into 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDA fails without input rows,CASSANDRA-13399,13061049,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,PAF,PAF,03/Apr/17 07:34,16/Apr/19 09:30,14/Jul/23 05:56,02/May/17 23:18,3.11.0,,,,,,Legacy/CQL,,,,,0,,,,,"When creating the following user defined AGGREGATION and FUNCTION:

{code:title=init.cql|borderStyle=solid}
CREATE FUNCTION state_group_and_total(state map<uuid, int>, type uuid)
    RETURNS NULL ON NULL INPUT
    RETURNS map<uuid, int>
    LANGUAGE java AS '
        Integer count = (Integer) state.get(type);

        count = (count == null ? 1 : count + 1);
        state.put(type, count);

        return state;
    ';

CREATE OR REPLACE AGGREGATE group_and_total(uuid)
    SFUNC state_group_and_total
    STYPE map<uuid, int>
    INITCOND {};
{code}

And creating a statement like:

{code}
SELECT group_and_total(""id"") FROM mytable;
{code}

When mytable is empty, it throws the following null assertion

{code}
ERROR [Native-Transport-Requests-1] 2017-04-03 07:25:09,787 Message.java:623 - Unexpected exception during request; channel = [id: 0xd7d9159b, L:/172.19.0.2:9042 - R:/172.19.0.3:43444]
java.lang.AssertionError: null
        at org.apache.cassandra.cql3.functions.UDAggregate$2.compute(UDAggregate.java:189) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.AggregateFunctionSelector.getOutput(AggregateFunctionSelector.java:53) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:592) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:430) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:424) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.10.jar:3.10]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]

{code}

Even if my FUNCTION only returns state, it creates that assertion null.

Thank you in advance.",,blerer,PAF,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 23:18:26 UTC 2017,,,,,,,,,,"0|i3d4b3:",9223372036854775807,3.10,,,,,,blerer,,blerer,,,Low,,,,,,,,,,,,,,,,,,,"03/Apr/17 08:25;snazy;This is a regression in 3.x.

||cassandra-3.0|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...snazy:13399-uda-state-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.0-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.0-dtest/lastSuccessfulBuild/]
||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13399-uda-state-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13399-uda-state-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-trunk-dtest/lastSuccessfulBuild/]

Fix in 3.11 branch. The new utest is applied to 3.0, which does not show this behaviour.;;;","02/May/17 19:24;blerer;Thanks for the patch. It looks good to me. ;;;","02/May/17 23:18;snazy;Thanks!
Committed as [388c961e4bb8f0a414c6aa700c67cd76eaf01046|https://github.com/apache/cassandra/commit/388c961e4bb8f0a414c6aa700c67cd76eaf01046] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return value of CountDownLatch.await() not being checked in Repair,CASSANDRA-13397,13060816,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,szhou,szhou,szhou,31/Mar/17 22:01,15/May/20 08:00,14/Jul/23 05:56,21/Apr/17 00:36,3.0.14,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,"While looking into repair code, I realize that we should check return value of CountDownLatch.await(). Most of the places that we don't check the return value, nothing bad would happen due to other protection. However, ActiveRepairService#prepareForRepair should have the check. Code to reproduce:
{code}
    public static void testLatch() throws InterruptedException {
        CountDownLatch latch = new CountDownLatch(2);
        latch.countDown();

        new Thread(() -> {
            try {
                Thread.sleep(1200);
            } catch (InterruptedException e) {
                System.err.println(""interrupted"");
            }
            latch.countDown();
            System.out.println(""counted down"");
        }).start();


        latch.await(1, TimeUnit.SECONDS);
        if (latch.getCount() > 0) {
            System.err.println(""failed"");
        } else {
            System.out.println(""success"");
        }
    }
{code}",,colinkuo,githubbot,jay.zhuang,jeromatron,pauloricardomg,szhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/17 05:24;szhou;CASSANDRA-13397-v1.patch;https://issues.apache.org/jira/secure/attachment/12861577/CASSANDRA-13397-v1.patch",,,,,,,,,,,,,,,,,,,1.0,szhou,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 08 00:13:10 UTC 2017,,,,,,,,,,"0|i3d2vb:",9223372036854775807,3.0.10,,,,,,pauloricardomg,,pauloricardomg,,,Low,,,,,,,,,,,,,,,,,,,"01/Apr/17 05:24;szhou;The attached patch includes the fix and a minor improvement (bail out early if there is any unavailable neighbor). [~krummas] could you help review this patch?;;;","17/Apr/17 13:32;pauloricardomg;good catch! lgtm, will merge after CI looks good:

||3.0||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-13397]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-13397-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-13397-dtest/lastCompletedBuild/testReport/]|
;;;","19/Apr/17 13:26;pauloricardomg;Tests look good but there was a minor conflict when merging to trunk so I will submit a new CI round with the trunk patch:

||trunk||
|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13397]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-13397-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-13397-dtest/lastCompletedBuild/testReport/]|
;;;","21/Apr/17 00:35;pauloricardomg;Committed to 3.0 and merged up as {{f5b36f12df65a780a52851207c285db7a8b4122f}}. Thanks!;;;","21/Apr/17 01:49;szhou;Thank you [~pauloricardomg]!;;;","25/Apr/17 23:40;szhou;[~pauloricardomg], in case you haven't done so, are you going to merge the fix to trunk?;;;","25/Apr/17 23:43;pauloricardomg;It's already on [trunk|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/ActiveRepairService.java#L369]..;;;","08/May/17 00:12;githubbot;GitHub user grom358 opened a pull request:

    https://github.com/apache/cassandra/pull/109

    Backport CASSANDRA-13397

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/instaclustr/cassandra cameron_13397

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/109.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #109
    
----
commit bd49317cf8d406824e8be0b3a7c676a0a6bb95f9
Author: Alwyn Davis <alwyndav@gmail.com>
Date:   2016-09-30T00:44:22Z

    Test commit

commit 2f45b53ee590fbefd3d15382765466fe716675d0
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-06-23T08:58:14Z

    Fixed conflicts

commit c64c6a80a5a56f517625197fb30154f2ad99c808
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-08-03T14:34:27Z

    Release sstables of failed stream sessions only when outgoing transfers are finished
    
    Patch by Paulo Motta; reviewed by Yuki Morishita for CASSANDRA-11345

commit 7a7c219024128063fee1bac382b68b659f93ea66
Author: Paulo Motta <pauloricardomg@gmail.com>
Date:   2016-08-13T01:06:27Z

    Throw RuntimeException if starting transfer of already completed OutgoingFileMessage

commit 05e9f07723ef53eb4b31ea3543699d6260797e3f
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-06-13T13:29:08Z

    Avoid missing sstables when getting the canonical sstables
    
    Patch by marcuse; reviewed by Stefania Alborghetti for CASSANDRA-11996

commit f258bab2954be32c7637c1ba936a11f1d500d52e
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-25T14:35:33Z

    AssertionError with MVs on updating a row that isn't indexed due to a null value
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-12247

commit abdb8224a04a56a12a4b8ea6984d68f99234b2c8
Author: Alex Petrov <oleksandr.petrov@gmail.com>
Date:   2016-05-09T09:06:43Z

    Allow updating UDT nested in non-frozen map after ALTERing the UDT
    
    Patch by Alex Petrov; reviewed by jknighton for CASSANDRA-11604

commit 909dfa82a5576a4ff2274511009b569ce9e50cc9
Author: Edward Capriolo <edlinuxguru@gmail.com>
Date:   2016-06-10T15:45:57Z

    StorageService shutdown hook should use a volatile variable
    
    patch by Ed Capriolo; reviewed by Stefania Alborghetti for CASSANDRA-11984

commit 63c6e9b8efaf91f6782f674cf33a8db13dc40f57
Author: Sam Tunnicliffe <sam@beobal.com>
Date:   2016-06-24T10:47:25Z

    Ensure new CFS is initialized before adding to schema
    
    Patch by Sam Tunnicliffe; reviewed by Aleksey Yeschenko for
    CASSANDRA-12083

commit 97d9b149c1189b82f68216be8eeac5f67f92b711
Author: Alwyn <alwyn@instaclustr.com>
Date:   2016-10-06T04:24:00Z

    Fix for incorrect test case in CASSANDRA-11604

commit 34a71bc0ea1fd8f3378a7fa9a286010c09f44956
Author: Alwyn Davis <alwyndav@gmail.com>
Date:   2016-10-15T23:17:01Z

    Bumped version number

commit 9e922d358a6ce8175b6f303b69b3c84a229514db
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T06:57:52Z

    Update README.asc
    
    Changed README to be our FAQ and text. Includes link to actual readme for apache cassandra

commit 5a5d54d2583f6402c67e2ec6cc822e7ac99650cd
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:17:49Z

    Merge pull request #1 from benbromhead/patch-1
    
    Update README.asc

commit e36f435a5901a185c196ffb5a69bdebca5540444
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:21:43Z

    Update README.asc
    
    Reworded a few things, fixed some typos

commit ca1d94eec089694998d71869246d19516a1ef487
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:28:01Z

    Update README.asc
    
    added link to Instaclustr.com

commit 2d1c3f5ae2000899b74af91aaf883bf5690585c9
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T17:38:16Z

    Update README.asc

commit 488d07eaae3489aaf5468aeffde7859957292f3f
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T18:28:45Z

    Update README.asc
    
    a word

commit deb53f468fe27aeb65f9729937233ea8e954f123
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-09-29T20:05:12Z

    Merge branch 'cassandra-3.0' into cassandra-3.7-instaclustr
    Fix merkle tree depth calculation
    
    Patch by Paulo Motta; Reviewed by Yuki Morishita for CASSANDRA-12580

commit e09f1abd7261d3372081b72713fa7e57ccc9d3cd
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-10-20T14:47:36Z

    Fix unreleased resource sockets
    
    patch by Arunkumar M; reviewed by yukim for CASSANDRA-12330

commit 8405b187d5bbd7951fd7de409b9370bfe3f668cd
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-10-20T14:45:28Z

    Fix unreleased resource sockets
    
    patch by Arunkumar M; reviewed by yukim for CASSANDRA-12329

commit 53ebca6850b58f9f8ff8f3e03b50d2c604fdfc3a
Author: Jeff Jirsa <jeff.jirsa@crowdstrike.com>
Date:   2016-10-19T01:11:17Z

    Correct log message for statistics of offheap memtable flush
    
    Patch by Kurt Greaves; Reviewed by Jeff Jirsa for CASSANDRA-12776

commit ab0b88621071ee820699af742eeef12b3dd9e75c
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-10-21T07:03:31Z

    Don't skip sstables based on maxLocalDeletionTime
    
    Patch by Cameron Zemek; reviewed by marcuse for CASSANDRA-12765

commit 6e694e76a02fdd30ac73a46d07c55689ba5f93f5
Author: Jeff Jirsa <jeff.jirsa@crowdstrike.com>
Date:   2016-10-22T02:04:04Z

    Split consistent range movement flag correction
    
    Patch by Sankalp Kohli; Reviewed by Jeff Jirsa for CASSANDRA-12786

commit 86e6e4ba1ed414e4ace38c3e3c51a151e74c45bd
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-20T12:29:16Z

    NullPointerExpception when reading/compacting table
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-11988

commit 0576db93629790247fcb8cf281e413635b656b1f
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-29T10:36:40Z

    NullPointerException during compaction on table with static columns
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-12336

commit 030cc592e8cf1ddc4079fab4570407cb24b058f6
Author: Stefania Alborghetti <stefania.alborghetti@datastax.com>
Date:   2016-09-23T05:52:02Z

    Avoid sstable corrupt exception due to dropped static column
    
    Patch by Stefania Alborghetti; reviewed by Carl Yeksigian for CASSANDRA-12582

commit 2cd54679d0cb1f2e2ef7b8de7a16c13fd374461e
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-06-23T07:46:00Z

    Don't try to get sstables for non-repairing column families
    
    Patch by marcuse; reviewed by Paulo Motta for CASSANDRA-12077

commit d7fc4873eed70f8923e87f15af83900598061962
Author: kgreav <kurt201@hotmail.com>
Date:   2016-11-21T01:44:24Z

    Merge pull request #3 from instaclustr/kurt_2
    
    Backports

commit f866272e2e9ba2f39bd480c986e4c0711c689044
Author: Kurt <kurt@instaclustr.com>
Date:   2016-11-21T01:49:11Z

    Bump version to 3.7.2

commit b5587f41e711b07a9392309c179c4d5746db1405
Author: Kurt <kurt@instaclustr.com>
Date:   2016-11-23T00:35:54Z

    Updated list of backports

----
;;;","08/May/17 00:13;githubbot;Github user grom358 closed the pull request at:

    https://github.com/apache/cassandra/pull/109
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.10: ClassCastException in ThreadAwareSecurityManager,CASSANDRA-13396,13060689,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ehubert,appodictic,appodictic,31/Mar/17 14:08,15/May/20 08:05,14/Jul/23 05:56,26/Mar/18 11:27,3.11.3,4.0,4.0-alpha1,,,,,,,,,9,,,,,https://www.mail-archive.com/user@cassandra.apache.org/msg51603.html,,apassiou,appodictic,bdeggleston,bijanfahimi,blerer,claude,colinkuo,ehubert,Erik.Kringen,fedefernandez,githubbot,gus,jasobrown,jasonstack,jay.zhuang,jeromatron,jjirsa,jjordan,komueller,KurtG,philipthompson,slebresne,snazy,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13424,CASSANDRA-13961,,,,,,,,,,,,,,,,,,,,"20/Mar/18 12:57;ehubert;CASSANDRA-13396_ehubert_1.patch;https://issues.apache.org/jira/secure/attachment/12915316/CASSANDRA-13396_ehubert_1.patch","23/Mar/18 22:30;ehubert;CASSANDRA-13396_ehubert_2.patch;https://issues.apache.org/jira/secure/attachment/12916000/CASSANDRA-13396_ehubert_2.patch","24/Mar/18 17:31;ehubert;CASSANDRA-13396_ehubert_3.patch;https://issues.apache.org/jira/secure/attachment/12916069/CASSANDRA-13396_ehubert_3.patch",,,,,,,,,,,,,,,,,3.0,ehubert,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 26 15:35:11 UTC 2018,,,,,,,,,,"0|i3d233:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"31/Mar/17 14:19;appodictic;https://github.com/apache/cassandra/compare/trunk...edwardcapriolo:CASSANDRA-13396?expand=1;;;","31/Mar/17 14:34;snazy;I'm strongly -1 on this change.

This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which _cannot_ be caught by neither unit nor dtests because it's an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled ""by us""). IMO, supporting C* in such an environment will cause other issues. Technically, it's not a major bug - changed it to wish.;;;","31/Mar/17 14:50;appodictic;How come everyone in Cassandra's first reaction is to -1 everything? 

The entire model of apache is ""I have an itch to scratch"". This person WANTS to run Cassandra in a container it is an ""itch"". The immediate opposition position should not be ""BUT DON'T SCRATCH THAT ITCH"", because I say so.

;;;","31/Mar/17 14:51;apassiou;@[~snazy]: OK but what if the cassandra daemon is not embedded anywhere but is simply running with a classpath containing several slf4j bindings?
It will still crash, right?

@[~appodictic]: please don't over-react (and don't hijack my question), it's an open discussion ;-);;;","31/Mar/17 14:54;appodictic;LOL I just posted this tweet yesterday.

https://twitter.com/edwardcapriolo/status/847484593041100800

What comedy cassandra is. No one even bothers to say ""how can we work together?"" or ""how can we wrote the code to make all users happy"" They just instantly drop a -1 on things. lol;;;","31/Mar/17 14:56;appodictic;So funny that i litteraly wake up, go out of my way to fix an issue for someone, and even though everyone is Cassandra is too busy to reply to emails and help people they are Johnny on the spot to jump on Jira and -1 code.;;;","31/Mar/17 15:03;appodictic;""Open discussions"" in cassandra always start with the concept of ""its not my idea so -1"" which is the exact opposite of ""scratch an itch"". 

""We do not support embedding C* in a container""

Really? who says? where is it said? Who is ""we""?;;;","31/Mar/17 15:10;snazy;[~apassiou], right, it's true for any other slf4j binding. 
Reason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. That's why I'm against such a change. We cannot foresee the consequences, because we have not tested other bindings. Even further, the performance implications of using another logger implementation are not determined. Believe me, it's not blindly shooting something down - I had a hard time to fix this issue and do not like to see it happen again. BTW: It's late in the afternoon over here, so it's not a too quick reaction early in the morning.;;;","31/Mar/17 15:15;appodictic;{quote}
Reason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. 
{quote}

So because someone made bugs in the past, which are ""hard to figure out"" and you can not ""foresee the consequences"" . Is this back to the future part 4?

Please verify your claim of ""not supporting containers"" before finding other reasons to not like the idea of fixing an obvious problem.


;;;","31/Mar/17 15:21;appodictic;So strange:

No such statement about supporting containers seems to exist.
[edward@jackintosh cassandra]$ find . -type f | xargs grep containers
./src/java/org/apache/cassandra/db/ColumnFamilyStore.java:     * thread safety.  All we do is wipe the sstable containers clean, while leaving the actual
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:        private final List<TokenTreeBuilder> containers = new ArrayList<>();
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                containers.add(keys);
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            if (containers.size() > 0)
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                for (TokenTreeBuilder tokens : containers)
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            containers.clear();
./conf/jvm.options:# This helps prevent soft faults in containers and makes
Binary file ./build/classes/main/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder$MutableDataBlock.class matches
[edward@jackintosh cassandra]$ find . -type f | xargs grep Containers

Its almost as if people just make up things, and then when you corner them on their position being false they just pivot and make up a new reason not to like the idea.;;;","31/Mar/17 15:31;apassiou;Edward, I appreciate that you wanted to help but please stop hijacking my question, or at least try to be constructive...

I don't know what you or Robert have understood when I said ""container"" but in my case it's just an application (a plain Java main()) that instantiates a CassandraDaemon and sometimes other stuff. But one could also do it in a ""unit"" test (which is not really a unit bt more an automatic integration test).

@Robert, I can understand your concerns about not-tested behavior of other bindings, then shouldn't it be stated in the docs that other bindings are not supported, and a more explicit error thrown?
But I don't think the performance impact is a good argument because logback and slf4j are configurable by themselves with configuration files it can have a very strong impact on the performance (log patterns, where you log to) even if one uses logback.;;;","31/Mar/17 15:57;snazy;[~apassiou], C* is meant to run as a standalone application using the dependencies that are in the {{lib/}} folder. Any change to those dependencies and the way C* is started, is basically up to the person who changes the dependencies. We can of course talk about using a different logger implementation instead of logback and discuss the pros and cons. But that is IMO way beyond an {{instanceof}} check.

I'm generally concerned about stability and hidden performance issues and a change to a (logger implementation) library, which is nearly everywhere in the hot code path. Mean, we use logback now for a really long time - but we have no test nor production experience running something else. One example: one thing that may happen is some hidden contention in that logger library causing weird outliers - people would complain that C* is slow but don't realize it's in this case because of that change. That's one reason why we are so careful with library updates especially in minor versions. All I'm saying is, that getting _all_ the consequences of such a change is a lot of work.;;;","31/Mar/17 15:59;snazy;bq. just an application (a plain Java main()) that instantiates a CassandraDaemon

If that's just for testing, why not just use logback?;;;","31/Mar/17 16:25;appodictic;{quote}
people would complain that C* is slow but don't realize it's in this case because of that change. 
{quote}

First, its an obvious bug. The entire point of plug-gable logging implementations is so that you can replace them. 
 
Second, the only person being actually affected would be Anton, because effective no one else is changing logging implementations so no one else is hitting that block.

For Anton (and anyone else) they would have to manually change the files in the lib folder and the configuration. So nothing is 'hidden' to him. He/They make a change and they can report if there actually is a performance issue.  

Because they can ""scratch their itch"" of running Cassandra in a container they might find new problems or they might make new opportunities. For example, they may find that some other implementation is actually better or faster. 

If anyone was actually trying to convince me that this bug is intentional, (which is almost laughable). The proper practice would be:

{code}
if (!logger instanceof XYZ){
  throw new IllegalArgumentException(""we only support XYZ for reasons ABC"");
}
{code}

But instead we are attempting to pretend the opposite, that the bug is intentional and the correct thing to do is throw a ClassCastException. Which is a joke.
;;;","01/Apr/17 04:37;jjirsa;Seems pretty reasonable to me

Certainly logback isn't the only performant slf4j logger available.
;;;","01/Apr/17 04:56;jjirsa;Changes back to bug, because even if the belief is that other loggers shouldn't be encouraged, we surely can do better than throwing a cast exception

Given that log4j2 is likely faster than logback and has been suggested as far back as 2013 CASSANDRA-5883 it seems like artificially forcing logback is a position that would need to be more rigorously defended - I'm +1 on this change conceptually (but this is not a review).
;;;","01/Apr/17 13:52;appodictic;Rigorous defenses are in no short supply around here.

I'm sure someone next will argue that this was intended to 
{code}
    /**
     * The purpose of this class is
     */
{code}
because the purpose code is soooo self documenting it describes itself. Want to fix it? No -1 the comment is perfect and heavily tested!;;;","03/Apr/17 08:09;blerer;The problem that I see with this ticket is the following: ""Once we agree to allow people to use the loggers that they wish we somehow become responsible for the bugs that can show up"".
As [~snazy] point it up, some of those issues might be non trivial to figure out. Simply because when someone will open a bug he might not mention that he changed the logging library and we might end up wasting a lot of time to reproduce the problem.
Due to that, I tend to be in favor of Robert suggestion of not supporting it (years spend debugging crappy issues have made me somehow paranoiac).

Now, I think it also make sense to allow people to use another logging library as long as they know that we do not fully support it.
My proposal woud be to log a warning at startup saying that the logging library that they use is not supported.
;;;","03/Apr/17 08:28;spod;I'm leaning to go with the proposed patch with a warning message as suggested by Benjamin. But in any case, this needs to be documented. I'd propose that any patch would also have to add a page [here|http://cassandra.apache.org/doc/latest/configuration/index.html] describing if and how logging can be customized and with a list of all relevant config files.;;;","03/Apr/17 09:11;snazy;bq. add a page here describing if and how logging can be customized and with a list of all relevant config files
bq. it also make sense to allow people to use another logging library

Don't get me wrong, but documenting something that is not supported and has never seen CI does not sound good. It would give people just skimming that page the impression that other logging backends _are_ actually supported. Before we document that, we should have full CI for those backends in place - i.e. all utests and dtests for each backend - or _at least_ document something like ""the combination of C* version X.Y and \[logger backend version Z\] was CI-tested on X/Y/Z using this configuration"". If we allow people to use other logging backends, there must be a way to tell them ""version X of Y should be good, because it has been CI tested"".
Anyway, supporting another logging backend still sounds like a new feature to me.;;;","03/Apr/17 09:25;apassiou;All I was asking for is avoid crashing, and I am concious that I am not in a standard use-case.
I am perfectly fine with a big warning in the log saying that using something other than logback is at my own risk + the the doc that states that nothing but logback is officially supported which makes you not responsible for any bug. 

Of course, as Benjamin says, there is still the risk of you investigating an issue without knowing that a different logger is used, but if you are really paranoid you can also imaging people configuring their logback in a very inefficient or wrong way and create crappy issues while being perfectly ""legal"".;;;","03/Apr/17 09:28;spod;I did not suggest to that we should support different loggers, just because we document Cassandra logging:

bq. add a page here describing **if** and **how** logging can be customized and with a list of all relevant config files

We can as well state something like ""Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk."". If that is the consensus of this discussion.
;;;","03/Apr/17 09:40;snazy;bq. I did not suggest to that we should support different loggers

Ah, ok, got it wrong then. Adding a page about how to configure logback (and point to the logback docs) sounds good.
;;;","03/Apr/17 09:40;slebresne;bq. This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code)

So why isn't there *any* comment around the code the patch updates to explain why this exists in the first place and why it's so important that it's here?

;;;","03/Apr/17 10:35;snazy;Ninja'd comments for this code.;;;","03/Apr/17 13:56;appodictic;-1 on ninja fixes. ;;;","03/Apr/17 14:00;appodictic;{quote}
We can as well state something like ""Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk."". If that is the consensus of this discussion.
{quote}

-1 The point of this Jira is not to create some new policy to avoid committing things. Fake blockers like 'this might cause a bug in the future' are not a valid technical reason to reject something. If that is the case shutdown all development on everything. I appreciate the attempt to compromise, but this is not the right direction. ""Not supported"" puts this in a trigger like situation. It will create a policy that will stop active research. Anyone will be free to reject further development using the ""not supported"" argument regardless of how baseless it is. ;;;","03/Apr/17 14:59;blerer;We do not run any test with another logging library and I do not think that we plan to do it. Which, for me means that we do not officially support any other library. Warning the user about it seems normal to me. As a user I would prefer to know. ;;;","03/Apr/17 16:15;jjirsa;There are really three issues:

1) The existing comments were clearly inadequate, and that's been ninja'd into place. +1 on that. 

2) Throwing a ClassCastException is objectively wrong. The patch fixes that, and should be committed. 

3) As a side effect, the patch allows other loggers, almost all of which are untested. The assertion from [~snazy] is that doing so is dangerous, specifically citing past bugs where other loggers which may do IO and cause sandbox access problems. That's a valid concern, and worth a logged warning in my opinion.

Like [~spodxx@gmail.com] (and I think [~blerer]) suggest above, I think Ed's patch+warning makes sense to me.

If someone wants to ""officially"" support another logger in order to remove the warning, then I think the burden is on them to open a proper ticket and demonstrate that it's sufficiently tested.;;;","03/Apr/17 17:16;appodictic;1) ninja fix 
How does meritocracy work when we spend globs of time striking down patches, while simultaneously 'ninja fixing' stuff? Go make a patch and get it reviewed like everyone else. 

2) Agreed.

3) What a backwards argument. The ""critical past bugs"" sited in CASSANDRA-12535 where caused by the person attempting to drop the -1 on this patch. This directly translates to ""No one can edit the buggy code I introduced because THEY might make bugs.""  Consider throwing a GetOffMyLawn exception
https://github.com/apache/cassandra/commit/8f15eb1b717548816a9ee8314269d4d1e2ee7084


 ;;;","03/Apr/17 18:37;jjirsa;{quote}
How does meritocracy work when we spend globs of time striking down patches, while simultaneously 'ninja fixing' stuff? Go make a patch and get it reviewed like everyone else.
{quote}

The project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There's no reason to fight about adding comments after the fact. 

{quote}
This directly translates to ""No one can edit the buggy code I introduced because THEY might make bugs.""
{quote}

Logging a warning for users isn't the same as throwing an exception. It's not like we're talking about a system property here that requires explicit operator involvement to even run with another logger, it's logging a single warning message that bugs may happen and we haven't actively tested other configs. I don't think that's unreasonable, and it's not ""get off my lawn"". This isn't an unreasonable compromise - we don't crash, but we give operators a chance to know that they're running an untested config. ;;;","03/Apr/17 18:40;appodictic;A true cassandra special. A patch with a dubious ClassCastException and a half finished comment passed a review, and now the next person who touches the code needs to ""sufficiently test"" to earn the ""officially supported"" designation only granted to committers that make untrue statements like ""We do not support embedding C* in a container "" and ninja fix stuff.;;;","03/Apr/17 19:14;appodictic;{quote}
Logging a warning for users isn't the same as throwing an exception. It's not like we're talking about a system property here that requires explicit operator involvement to even run with another logger, it's logging a single warning message that bugs may happen and we haven't actively tested other configs. I don't think that's unreasonable, and it's not ""get off my lawn"". This isn't an unreasonable compromise - we don't crash, but we give operators a chance to know that they're running an untested config.
{quote}

The ""get off my lawn"" is related to this entire process. It had not even checked who added the code originally. I did not quite understand why it got a -1 so fast. -1s are ""rare"" and kill the proposal dead.

https://www.apache.org/foundation/voting.html
For code-modification votes, +1 votes are in favour of the proposal, but -1 votes are vetos and kill the proposal dead until all vetoers withdraw their -1 votes.

The original reasons given were ""This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which cannot be caught by neither unit nor dtests because it's an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled ""by us"")""

Lets break this down:
1) ""This change will cause weird and hard to catch follow-up issues""
Hard to quantify and the statement itself is a hypothesis. Can ""WILL CAUSE"" be proven? No.

2) which cannot be caught by neither unit nor dtests because it's an unsupported setup
Even though we are SURE issues that ""WILL HAPPEN"" they ""CANNOT BE CAUGHT"" . Amazing how that logic works.

3) We do not support embedding C* in a container
Untrue. How does one run the CDC daemon? Not a written rule anyway.

If adding a single if statement to block of code and getting 3 completely ludicrous objections from the person who happened to write said code is not ""get off my lawn"" then I don't know what is.

;;;","03/Apr/17 19:29;appodictic;{quote}
The project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There's no reason to fight about adding comments after the fact.
{quote}
No. It is important. The -1 er is using his technical insight as a justification for his -1. The incomplete comment shows how much time he really spent working on the given code. No tests, no argument checking, and a half done comment.;;;","03/Apr/17 20:28;jjirsa;Let's focus on the problems and solutions. 

There were missing and incomplete comments around a broken piece of code that fixed a very-hard-to-troubleshoot bug. We've fixed the missing and incomplete comments, we still need to fix the broken code, and we can do so without ignoring the past very-hard-to-troubleshoot-bug. 

We have a patch that fixes the ClassCastException, which should be reviewed. We have a (non-binding) -1 on that review. One of the thing that 3 committers (including myself) seem to have suggested is at least adding a warning. [~snazy] is that agreeable to you?


;;;","04/Apr/17 10:39;snazy;[~jjirsa], can live with that - i.e. logging an explicit warning using a new {{StartupCheck}} that also mentions that UDFs/UDAs might be broken, if a logger that's not logback is used.;;;","04/Apr/17 12:20;apassiou;Happy to see that everybody seem to converge to a reasonable solution ;-);;;","04/May/17 06:58;eugenefedoto;This is my first ticket. [~jjirsa] helped me get started. Since it hasn't been updated in a while, I've taken the liberty of adapting [~appodictic]'s patch to match the consensus described in this ticket. GitHub links below:

https://github.com/eugenefedoto/cassandra/tree/13396-3.0
https://github.com/eugenefedoto/cassandra/tree/13396-3.11
https://github.com/eugenefedoto/cassandra/tree/13396-trunk;;;","04/May/17 15:56;jjirsa;[~eugenefedoto] - two quick notes

1) In all three versions of your patch, we can't [throw|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-30a3dbf7d783cf329b5fb28a8b14332eR110] in {{ThreadAwareSecurityManager.java}} or we'll end up with the same problem we had before.

2) In your [StartupCheck|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-a5df240149285ae528cdd3c41aa59360R419] , the second log line (on L419) isn't necessary.

[~snazy] - Eugene is a new contributor, I've talked him through how to contribute offline via email, and probably shaped his approach (notably, the {{instanceof}} check from Ed's patch isn't sufficient, because if logback has been removed from the classpath, we'll throw a {{NoClassDefFoundError}} instead). Given that, do you want to review? ;;;","04/May/17 16:58;eugenefedoto;I made the [suggested changes by Jeff|https://issues.apache.org/jira/browse/CASSANDRA-13396?focusedCommentId=15996968&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15996968].;;;","29/Jun/17 22:37;gus;Looking forward to the resolution of this issue in any of the following ways:

1) Don't load this security manager and policies if UDF's are configured to be disabled 
2) Handle other possible loggers conditionally (log4j2 being my case)
3) Provide an option to run with insecure UDF's ( by not installing this security manager). Not everyone is exposing UDF's to folks they don't trust. In some use cases it might be a feature to be able to read system properties etc.

Glancing at the discussion it sounds like this is heading towards a ""break UDF's but continue"" strategy, which will also work for me since I don't need UDF's but seems likely to trip folks.

My exact itch is documented here: https://github.com/nsoft/jesterj/issues/89

If option 1 or 3 were available, that would greatly simplify my life, because this security manager installs policies in a class initializer and these policies assume a codePath with a url scheme of ""file"" but in my case the scheme is ""onejar""... which forced me into lots of gyrations to force an early load and then un-set your policies so that the rest of my code could have permissions.
;;;","29/Jun/17 22:43;gus;And yes I might be interested in providing a patch for 3 if folks seem in favor... 1 is probably beyond my knowledge of Cassandra, but a version of 3 dependent on a system property seems tractable.;;;","30/Jun/17 02:56;gus;After some IRC discussion, I've been encouraged to submit a patch. Here's an implementation of my #3 suggestion above: 

https://github.com/nsoft/cassandra/commit/382a44c238b6d4bd7e3d8cc7bbd6710b0a7c5274

Though Github's diff has done a fabulous job of obfuscating it, the patch is very simple all I did was add a constant, and two conditions that read the system property represented by the constant and prevent this security manager and its policies from getting installed via the install() method if the system property has been set. 

Circle CI here: https://circleci.com/gh/nsoft/cassandra/2 (still running as of this comment, but I expect it to pass) Ran tests locally and Installed a version with this patch in JesterJ and everything seems happy there.;;;","05/Jul/17 16:16;gus;FYI, Circle CI did pass. Any commentary or review would be appreciated. I won't be able to release without knowing what direction this issue is going.;;;","30/Aug/17 19:00;gus;Any one have a chance to look at my patch yet?;;;","04/Sep/17 12:39;ehubert;We faced the same underlying issue after upgrading from Cassandra 3.9 to 3.11.0 when using Cassandra embedded for integration testing using JUnit. 
As our application uses a different logging backend and we did not want to switch it and provide appropriate redundant configuration for logback, we excluded logback dependencies and only provided our implementation to also avoid any warnings about duplicate bindings. This setup worked fine with Cassandra 3.9, but fails with Cassandra >= 3.10; the server does not startup, because of the missing classes. So in this case any patch working with instanceof checks still attempting to load those classes without specific try/catch would obviously also fail. 

In addition to SMAwareReconfigureOnChangeFilter in org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager.install() using multiple logback internals (added with CASSANDRA-12535) I also found the change with CASSANDRA-12509 adding ch.qos.logback.core.hook.DelayingShutdownHook in StorageService#initServer problematic.
Would it be an alternative to handle all access to the underlying logging implementation via reflection? 
E.g. attempting to load logback classes and only if this does not fail, perform implementation specific actions via reflection (otherwise log a warning about missing logback presence, which can be ignored in integration test setups). We are mostly talking about one-time initialization, so the performance impact should be really negligible.
This solution would require users to properly exclude logback logging libs if they want to use other sf4j implementation bindings. Providing multiple logging implementations with sl4fj bindings anyway triggers a warning which should be handled.;;;","06/Sep/17 12:00;ostefano;I had the very same issue when using the {{EmbeddedCassandra}} from {{spark-cassandra-connector}}. Moved to {{logback}} fixes the issue. This was a bit annoying because I had to exclude all other sf4j implementations (there's no way afaik to force one implementation over another in case multiple ones are loaded). Anyway, anything but a ClassCastException is better option imho.;;;","17/Oct/17 12:50;fedefernandez;I have the same problem while attempting to use an embedded Cassandra instance for integration tests in Scala projects. Sadly, I found another strange behavior in SBT that avoids me removing {{log4j-slf4j-impl}} from the classpath (if someone is interested [here is the ticket|https://github.com/sbt/sbt/issues/3645]).

I'm stuck in 3.9 version, so I'm looking forward to a solution on this.;;;","14/Feb/18 15:44;claude;It is unclear to me whether or not the proposed patch has been/will be accepted into the code base.  What is the current thinking and if it is to be added when might that be?

I have hit this issue when trying to run Cassandra in a unit test situation where I am testing an implementation of an SPI and am unable to convert product to logback.;;;","14/Feb/18 17:49;jasobrown;OK, looks like this has drifted for long enough. I'm gonna look at the patches and review for commit.

UPDATE: I'll wait until CASSANDRA-14183 is committed before attacking this (as it touches logging, as well);;;","14/Feb/18 20:41;ehubert;[~jasobrown], maybe you can have a glance on [my comment|#comment-16152569] as well. I have not started working on a patch, because I did not receive any feedback on my suggestion on a conceptual level. I did not want to waste time working on an implementation without knowing of a chance of inclusion. Because we faced some issues in production which are either fixed in 3.11.1 or will be fixed in 3.11.2 we really would like to update, but this still blocks us.;;;","14/Feb/18 20:56;jasobrown;[~ehubert] 3.11.2 is up for vote & release this week, so this patch will not be included in that release. Do you have any test cases you can offer here? Please note that allowing other logging implementation seems to be used seems in scope, but will *not*, in any way, be supported. I'll need to reread through all the commentary again to get the full scope of what people are really asking for, but I'm at least willing to get this to completion.;;;","14/Feb/18 22:18;ehubert;[~jasobrown], fair enough. From what I read/understood the majority of users (if not all, definitely including us) facing this issue, wanted to use Cassandra as an embedded server (mostly for integration testing purposes) utilizing class org.apache.cassandra.service.CassandraDaemon - so no production deployment of a standalone server or cluster.

While running in the same JVM alongside an application using slf4j with any slf4j supported backend != logback after upgrading to Cassandra 3.10 or later you are in trouble and no longer able to start the Cassandra server, although logging/logging performance are none of your (primary) goals/concerns.

I doubt there are many users who want to configure a standalone single or multi-node Cassandra installation using a different logging backend to use this in production and have your support, but many users want to write automated integration/scenario tests of their own application interacting with Cassandra using an embedded Cassandra server in addition to plain unit tests using mocks without being forced to switch the logging backend chosen for their application for similar reasons you have. Therefore I see no conflict at all.

 

An implementation could even somehow enforce the differentiation between embedded and standalone usage similar to ""runManaged"" in CassandraDeamon to only allow/support other logging backends (skip special logging backend specific configuration) when CassandraDeamon is used/configured differently than done from main() used by a standalone server installation, if this is really a concern you want to see addressed.

Even in this case one should exchange nasty runtime exceptions or even JVM errors (ClassCastException or NoClassDefFoundError) with a dedicated error message:
""When using a Cassandra standalone installation the only supported logging backend is logback.""
For ClassCastException add something like ""slf4j is currently bound to a different logging framework. Please ensure your classpath only contains logback implementations!""
For NoClassDefFoundError add something like ""No logback implementation was found. Please ensure your classpath contains the bundled logback implementation!""
You can decide to abort the startup or have the same behavior as for the embedded case, but only providing a detailed error logging regarding the unsupported setup.

For embedded use cases one could advice programmers to activate the CassandraDaemon differently (e.g. some parameter) and here I would propose to simply not execute all logback specific configuration logic - e.g. try loading specific logback classes via reflection, so in this mode logback could be easily replaced by any slf4j logging backend which the application currently uses without further adjustments.

JUnit test cases might be a bit tricky, because I think they involve different classpath setups of the used test runner to simulate/trigger those type of issues.;;;","15/Feb/18 00:53;jasobrown;[~ehubert] Thank you. This use case summary was very helpful.;;;","15/Feb/18 07:23;claude;+1 for [~qb3rt] summary, it touches all the points I was going to make and then some.

 ;;;","19/Feb/18 23:02;gus;Though my embedded usage is not only for unit test, my choice of Cassandra relates more to the fact that you are Apache licensed, pure java and clustered rather than performance concerns. When there's a viable alternative I'll worry about whether or not you're faster... in the mean time, I'm very happy to be responsible for (or take credit for) any performance variation from plugging in my preferred logging framework (log4j2). Please don't use performance worries as an excuse to not fix this. Generally +1 on Eric's summary also. I don't mind doing something extra to enable pluggable logging so that you can default to your supported config, so long as it doesn't impact the command line invocation of my project (i.e. requiring -D or -agentlib, etc).;;;","20/Mar/18 13:17;ehubert;Hi [~jasobrown]! Today, I took some time to prepare a patch against the Cassandra 3.11 branch which basically:
 * bundles all logback implementation specific functionality in one class (required a bit of code reorganization)
 * extracted an interface to be able to a) minimize use of reflection and b) be able to provide alternative implementations (the patch itself only provides a no-op fallback implementation)
 * load and instantiate logging-implementation specific extension according to used slf4j binding via reflection (Cassandra code only works on new interface which has no java class dependencies to specific implementations)

So far there are no new (integration) tests which likely would also require some classpath /  ClassLoader magic.

I tested the change using ""a neutral"" application use case by utilizing [Cassandra Unit|https://github.com/jsevellec/cassandra-unit].

The ""test"" involved adjusting log4j config from Cassandra Unit test resources, changing the used cassandra-all version in parent pom, excluding logback deps from the pom and executing any of the tests.

With stock Cassandra 3.11.2 we see:
{code:java}
2018-03-20 10:51:43,753 [pool-2-thread-1] ERROR cassandra.service.CassandraDaemon - Exception encountered during startup
java.lang.NoClassDefFoundError: ch/qos/logback/classic/Logger
    at org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager.install(ThreadAwareSecurityManager.java:92)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:192)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:602)
    at org.cassandraunit.utils.EmbeddedCassandraServerHelper.lambda$startEmbeddedCassandra$1(EmbeddedCassandraServerHelper.java:144)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: ch.qos.logback.classic.Logger
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 7 more
{code}
Using the patch [^CASSANDRA-13396_ehubert_1.patch] I provided we have a successful server startup with a warning:
{code:java}
2018-03-20 13:47:32,688 [pool-2-thread-1] WARN  utils.logging.LoggingSupportFactory - You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j binding: org.slf4j.impl.Log4jLoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.
{code}

Please consider this as an initial patch suggestion to gather quick feedback on the approach! I'm willing to adjust things according to your requirements or are happy if you like to tweak it to your requirements.

Would be great to see this in Cassandra 3.11.3 if possible.;;;","23/Mar/18 09:25;ehubert;Anyone with some feedback on my submitted patch? I may have some time tomorrow to incorporate feedback/make adjustments etc, but next week might become rather busy.;;;","23/Mar/18 15:34;claude;Scanning through the solution it looks good to me.  However, I have not merged it or tried it.  I have no objections to applying it.;;;","23/Mar/18 21:51;jasobrown;[~ehubert] the attached 'patch' file completely fails to apply to cassandra-3.11. Can you either push up a git patch or diff file, or just push your branch up to your github repo and share the link?  Reading the code in the attachment now,,,,;;;","23/Mar/18 22:04;jasobrown;Looks like you are using {{StaticLoggerBinder}} to figure out which logging implementation to use. That seems [deprecated|https://www.slf4j.org/faq.html#changesInVersion18] as of slf4j v1.8 (soon to be released), in lieu of some JIgsaw modularization. I have absolutely zero desire to investigate that modularization, wrt to cassandra as a whole. What are the alternatives here? Remaining on the current version of slf4j is certainly an option

ftr, 3.11 uses slf4j 1.7.7;;;","23/Mar/18 22:44;ehubert;Hi Jason! Sorry about the patch file. I now quickly recreated it from command line using git diff after pulling all updates from the 3.11 branch and attached it as  [^CASSANDRA-13396_ehubert_2.patch].
Regarding your question about alternatives to determine the logging implementation slf4j bound to when using slf4j >= 1.8 I do not think this will be much related to Jigsaw modularization (although I may be wrong). Looks like they make use of the long existing ServiceLoader JDK implementation (AFAIK since JDK6). I happily offer to investigate this. Other alternatives to simply do something similar by attempting to load specific implementation classes via reflection is by far not as elegant if you ask me.
;;;","23/Mar/18 23:34;ehubert;I'm willing to test this out, but I think for slf4j >=1.8 we may need something like the following untested litte utility method to do the job of determining the binding
{code}
private static SLF4JServiceProvider determineSlf4jProvider() 
{
        ServiceLoader<SLF4JServiceProvider> serviceLoader = ServiceLoader.load(SLF4JServiceProvider.class);
        Iterator<SLF4JServiceProvider> serviceIterator = serviceLoader.iterator();
        if (serviceIterator.hasNext()) 
        {
            return serviceIterator.next();
        } else {
            return new NOPServiceProvider()
        }
}
{code}
or via reflection make org.slf4j.LoggerFactory#getProvider() accessible and call it
or provde an own class in package org.slf4j and wrap this static message call to this package-scoped method.

Maybe this is all not necessary and one can directly work with public org.slf4j.LoggerFactory#getILoggerFactory() using the bound provider to return a implementation-specific factory. As said, I'll happily have a closer look at this. I'm pretty sure something which can be addressed rather easily. The latter might even be somehow possible to use for a compatible implementation although returned implementation classes are very likely completely different for those slf4j versions.;;;","24/Mar/18 00:18;jasobrown;hmm, yeah those aren't great options. I think we can stick with the {{StaticLoggerBinder}} while we're still on slf4j < 1.8, to keep it simple. Maybe we can include a note for the future for when we do upgrade slf4j 1.8. wdyt?;;;","24/Mar/18 01:27;jasobrown;[~ehubert] I can successfully apply your patch now. I moved the logback jars out of the way, added slf4j-jdk14-1.7.7.jar into lib, and was able to bring up cassandra without problem with the patch. Got this message as output:

{noformat}
WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j binding: org.slf4j.impl.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.
{noformat} 

Thus the code went down the {{else}} block.

Review will continue ...;;;","24/Mar/18 17:42;ehubert;[~jasobrown] thanks for your feedback, which helped me to adjust the logging implementation detection from using an slf4j implementation depended mechanism ({{StaticLoggerBinder}}) instead just using what is available via its public API {{org.slf4j.LoggerFactory#getILoggerFactory()}} which is totally sufficient for this purpose (see updated patch [^CASSANDRA-13396_ehubert_3.patch]).

I now also tested some slf4j backend implemenations with different versions of slf4j:

+slf4j 1.7.12 - JUL:+
 WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: org.slf4j.*impl*.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.

+slf4j 1.7.12 - Logback 1.1.3 or 1.2.3:+
 No warning - successful Cassandra startup

+slf4j 1.8.0-beta2 - JUL:+
 WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: org.slf4j.*jul*.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.

+slf4j 1.8.0-beta2 - Logback 1.3.0-alpha4+

_logback depencendy upgrade to 1.3.0 required - slf4j upgrade to 1.8.x should only be done once logback 1.3.x release is availabe_
 No warning - successful Cassandra startup

I did not test whether the logback-implementation specific code (with my patch centralized in {{org.apache.cassandra.utils.logging.LogbackLoggingSupport}}) is still working properly after this library upgrade. I only tested implementation detection does not need to be adjusted when upgrading to the newer slf4j version.;;;","24/Mar/18 21:13;githubbot;GitHub user e-hubert opened a pull request:

    https://github.com/apache/cassandra/pull/210

    CASSANDRA-13396: 

    Centralize logback specific implementation and load/instantiate it via reflection

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/e-hubert/cassandra CASSANDRA-13396

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/210.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #210
    
----

----
;;;","26/Mar/18 11:27;jasobrown;In addition to pulling in the minor fix [~ehubert] added over the weekend, I made some minor editorial changes, and committed as sha {{bd0804065daaa01ba478c0ed97f7411f1180eef9}}. Thanks!;;;","26/Mar/18 15:35;githubbot;Github user e-hubert commented on the issue:

    https://github.com/apache/cassandra/pull/210
  
    Closing this pull request as thanks to @jasobrown those changes were pushed to 3.11 with some editorial changes.
;;;","26/Mar/18 15:35;githubbot;Github user e-hubert closed the pull request at:

    https://github.com/apache/cassandra/pull/210
;;;",,,,,,,,,,,,,,,,,,
Expired rows without regular column data can crash upgradesstables,CASSANDRA-13395,13060674,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,31/Mar/17 13:30,16/Apr/19 09:30,14/Jul/23 05:56,04/Apr/17 10:28,3.0.13,3.11.0,,,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"In {{2.x}} if an expired row is compacted its row marker will be converted into a {{DeletedCell}}. In {{3.0}}, when the row is read by {{LegacyLayout}} it will be converted in a row without {{PrimaryKeyLivenessInfo}}. If the row does not contains any data for the regular columns, or if the table simply has no regular columns it will then be considered as {{empty}}. Which will crash {{upgradesstables}} with the following error:
{code}
java.lang.AssertionError
        at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:207)
        at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
        at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
        at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
        at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
        at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:416)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:308)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$0(NamedThreadFactory.java:79)
        at java.lang.Thread.run(Thread.java:745)
{code}
This problem is cause",,blerer,bradfordcp,eyablon,jeromatron,jjirsa,jjordan,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 10:28:05 UTC 2017,,,,,,,,,,"0|i3d1zr:",9223372036854775807,3.0.12,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"31/Mar/17 13:56;blerer;I discussed offline with [~slebresne] and the internal iterators do not accept empty rows for performance reasons.
As we know that except for indexes the deleted cells are caused by the compaction of expired row marker we can avoid the empty row problem by treating those rows as the expired ones. The only information missing being the original TTL we can simply replace that one by a fake one.

I pushed an initial version of the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:13395-3.0].  ;;;","31/Mar/17 14:08;blerer;Waiting for the upgrade tests.
;;;","03/Apr/17 08:31;blerer;The CI results for the upgrade tests: |[3.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13395-upgrade-3.0-upgrade/]|[3.11|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13395-upgrade-3.11-upgrade/]|. 
I checked the failing tests and they are the same as the ones on 3.0 and 3.11 HEADs.;;;","03/Apr/17 08:43;slebresne;+1;;;","04/Apr/17 10:28;blerer;Committed into 3.0 at 462b9cf63bf986671f8a080ef1802f0c27e7c772 and merged into 3.X.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix weightedSize() for row-cache reported by JMX and NodeTool,CASSANDRA-13393,13060301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Fuud,Fuud,Fuud,30/Mar/17 08:52,15/May/20 08:05,14/Jul/23 05:56,09/Apr/17 09:02,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,Tool/nodetool,,,,,0,lhf,,,,"Row Cache size is reported in entries but should be reported in bytes (as KeyCache do).
It happens because incorrect OHCProvider.OHCacheAdapter.weightedSize method. Currently it returns cache size but should return ohCache.memUsed()",,alekiv,Fuud,githubbot,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Fuud,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 08:45:54 UTC 2017,,,,,,,,,,"0|i3czov:",9223372036854775807,,,,,,,snazy,,snazy,,,Low,,,,,,,,,,,,,,,,,,,"31/Mar/17 13:21;snazy;[~Fuud], mind to provide a patch?;;;","07/Apr/17 07:08;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/105

    Fix invalid value for rowCache size (in MB). CASSANDRA-13393

    Row Cache size is reported in entries but should be reported in bytes (as KeyCache do).
    It happens because incorrect OHCProvider.OHCacheAdapter.weightedSize method. Currently it returns cache size but should return ohCache.memUsed()

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra 13393

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/105.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #105
    
----
commit 0fa5ca057016c250a4888b8b6d982fcac569d6a5
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-04-07T07:07:08Z

    Fix invalid value for rowCache size (in MB). CASSANDRA-13393

----
;;;","07/Apr/17 07:08;Fuud;Patch:
https://github.com/apache/cassandra/pull/105;;;","09/Apr/17 09:02;snazy;Thanks for the patch!

Committed as [470f15be652ffb3c471161d6fb10c8893ff59e46|https://github.com/apache/cassandra/commit/470f15be652ffb3c471161d6fb10c8893ff59e46] to [cassandra-2.2|https://github.com/apache/cassandra/tree/cassandra-2.2] and merged up to trunk.
;;;","09/Apr/17 16:20;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/105
  
    This was committed as 470f15be652ffb3c471161d6fb10c8893ff59e46 - mind closing the PR? 

;;;","13/Apr/17 08:45;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/105
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE on upgrade to 3.0/3.X in case of IO errors,CASSANDRA-13389,13059899,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefania,stefania,stefania,29/Mar/17 01:53,16/Apr/19 09:30,14/Jul/23 05:56,30/Mar/17 01:16,3.0.13,3.11.0,,,,,Local/Startup and Shutdown,,,,,0,,,,,"There is a NPE on upgrade to 3.0/3.X if a data directory contains directories that generate IO errors, for example if the cassandra process does not have permission to read them.

Here is the exception:

{code}
ERROR [main] 2017-03-06 16:41:30,678  CassandraDaemon.java:710 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.io.util.FileUtils.delete(FileUtils.java:372) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.db.SystemKeyspace.migrateDataDirs(SystemKeyspace.java:1359) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:190) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
{code}

This is caused by {{File.listFiles()}}, which returns null in case of an IO error.",,ifesdjeen,jjirsa,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 01:16:05 UTC 2017,,,,,,,,,,"0|i3cx7j:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"29/Mar/17 02:41;stefania;The patch replaces {{File.listFiles()}} with directory streams. The 3.0 patch applies cleanly to 3.11. We don't require any patch for trunk since the upgrade code is no longer applicable for 4.0. CI pending.

||3.0||3.11||
|[patch|https://github.com/stef1927/cassandra/tree/13389-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13389-3.11]|
|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.11-testall/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.11-dtest/]|
;;;","29/Mar/17 11:38;ifesdjeen;LGTM, +1

Thank you for the patch!
For history purposes, I'll note that this patch isn't applicable to trunk as it's related to legacy table migration.;;;","30/Mar/17 01:16;stefania;Thanks for the review. Committed to 3.0 as 849f8cd6162c4850d64581a2c4a542c677e43e0a and merged into 3.11, then merged into trunk with {{-s ours}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy caching options can prevent 3.0 upgrade,CASSANDRA-13384,13059612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,28/Mar/17 04:20,16/Apr/19 09:30,14/Jul/23 05:56,29/Mar/17 00:32,3.0.13,3.11.0,,,,,Legacy/Distributed Metadata,,,,,0,,,,,"In 2.1, we wrote caching options as a JSONified map, but we tolerated raw strings [""ALL"", ""KEYS_ONLY"", ""ROWS_ONLY"", and ""NONE""|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/cache/CachingOptions.java#L42].

If a 2.1 node with any of these strings is upgraded to 3.0, the legacy schema migration will fail.",,aleksey,jeromatron,jjirsa,jjordan,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 29 00:32:37 UTC 2017,,,,,,,,,,"0|i3cvfz:",9223372036854775807,,,,,,,jjordan,,jjordan,,,Low,,3.0.0,,,,,,,,,,,,,,,,,"28/Mar/17 17:30;jjirsa;No patch for trunk, we don't expect 2.x -> 4.0 upgrades, they have to go through 3.0, which will handle the upgrade. 

|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13384]  | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13384-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13384-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13384]  | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13384-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13384-dtest/] |

;;;","28/Mar/17 17:38;jjordan;Change looks good to me, but maybe we should add an upgrade dtest which triggers the issue?;;;","28/Mar/17 18:31;jjirsa;How strong is your desire to see a dtest? I think it's a reasonable ask, but it's also a LOT of effort. 

The dtest upgrade logic that exists requires a consistent native proto version through the test. Since 2.0 and 3.0 don't have a common native proto, none of the existing rolling tests can be used - we'd have to write a new harness that shuts down the session and reconnects midway through the test, which is do-able, but a fair amount of effort for something that seems like it's testable with a unit test.

;;;","28/Mar/17 19:02;jjirsa;Note: 3.11 seems to have made {{testReport/junit/org.apache.cassandra.schema/LegacySchemaMigratorTest}} flakey

{code}
    [junit] Testcase: testMigrateLegacyCachingOptions(org.apache.cassandra.schema.LegacySchemaMigratorTest):	FAILED
    [junit] This assertion failure is probably due to accessing Schema.instance from client-mode tools - See CASSANDRA-8143.
    [junit] junit.framework.AssertionFailedError: This assertion failure is probably due to accessing Schema.instance from client-mode tools - See CASSANDRA-8143.
    [junit] 	at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:286)
    [junit] 	at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:65)
    [junit] 	at org.apache.cassandra.config.CFMetaData$Builder.build(CFMetaData.java:1328)
    [junit] 	at org.apache.cassandra.config.CFMetaData.compile(CFMetaData.java:427)
    [junit] 	at org.apache.cassandra.db.SystemKeyspace.compile(SystemKeyspace.java:435)
    [junit] 	at org.apache.cassandra.db.SystemKeyspace.<clinit>(SystemKeyspace.java:116)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigrator.<clinit>(LegacySchemaMigrator.java:65)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.testMigrateLegacyCachingOptions(LegacySchemaMigratorTest.java:106)
    [junit]
    [junit]
    [junit] Testcase: testMigrate(org.apache.cassandra.schema.LegacySchemaMigratorTest):	Caused an ERROR
    [junit] Could not initialize class org.apache.cassandra.db.SystemKeyspace
    [junit] java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.db.SystemKeyspace
    [junit] 	at org.apache.cassandra.config.Schema.<init>(Schema.java:70)
    [junit] 	at org.apache.cassandra.config.Schema.<clinit>(Schema.java:49)
    [junit] 	at org.apache.cassandra.cql3.functions.UDFunction.<init>(UDFunction.java:215)
    [junit] 	at org.apache.cassandra.cql3.functions.JavaBasedUDFunction.<init>(JavaBasedUDFunction.java:190)
    [junit] 	at org.apache.cassandra.cql3.functions.UDFunction.create(UDFunction.java:233)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.keyspaceWithUDFs(LegacySchemaMigratorTest.java:374)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.keyspacesToMigrate(LegacySchemaMigratorTest.java:293)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.testMigrate(LegacySchemaMigratorTest.java:72)
    [junit]
{code}

Investigating to see if it's a test problem or regression.
;;;","28/Mar/17 20:58;jjirsa;There's a second commit that more explicitly initializes the tests, which handles potential problems in ordering. With that change, I've executed the unit test 150 times locally and once on CI without error. 

Back to you, [~jjordan] - how strongly do you feel that the dtest is necessary?;;;","28/Mar/17 22:14;jjordan;So I did some more investigation here with some test upgrades myself and after creating a table on 2.0 and upgrading to 2.1 the schema seemingly has the correct json formatted thing in it.  But if you then immediately upgrade to 3.0 with a kill -9 stop, 3.0 fails to start.  If you instead flush before upgrading to 3.0 the json version of the schema is flushed to disk, and everything is fine.  I'm not sure it is worth adding ugly legacy conversion hacks in 3.x for this super edge case of ""upgrade as fast as I can without flushing"".;;;","28/Mar/17 22:17;jjordan;Anyway, this code works.  Adding basically dead code to 3.x seems ugly to me, but it is possible to hit this if you upgrade fast enough, so I guess it may be worth adding it, since we can drop it on merge to master.;;;","28/Mar/17 22:20;jjirsa;I intend to drop it on the merge to master - would only exist for 3.0 and 3.11
;;;","28/Mar/17 22:23;jjordan;+1;;;","29/Mar/17 00:32;jjirsa;Committed to 3.0 as {{6edc26824747b204fefc31478db833667d5d5892}}, merged into 3.11, and then skipped trunk (did {{merge -s ours}}, but only updated CHANGES, so the code will not exist for 4.0).

Thanks [~jjordan] . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cdc column addition strikes again,CASSANDRA-13382,13059334,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Mar/17 09:26,16/Apr/19 09:30,14/Jul/23 05:56,28/Mar/17 12:43,3.11.0,,,,,,,,,,,0,,,,,"This is a followup of CASSANDRA-12697, where the patch mistakenly only handled the {{system_schema.tables}} table, while the {{cdc}} column has been added to {{system_schema.views}} table.

The patch is pretty trivial, though this highlight that we don't seem to have upgrade tests for materialized views.",,aleksey,christopher.lambert,jjirsa,jjordan,JoshuaMcKenzie,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 12:43:18 UTC 2017,,,,,,,,,,"0|i3ctqf:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"27/Mar/17 09:33;slebresne;Simple patch below:
| [13382-3.11|https://github.com/pcmanus/cassandra/commits/13382-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13382-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13382-3.11-dtest] |

I'll try to write a new upgrade dtest to show this, but as I've never really been able to run upgrade tests locally, I won't promise to sink 2 days of my time into it if it doesn't go smoothly.;;;","27/Mar/17 12:59;slebresne;Alright, dtest to reproduce is [here|https://github.com/riptano/cassandra-dtest/pull/1454].;;;","27/Mar/17 15:14;JoshuaMcKenzie;Try not to --author my name on this one. :);;;","27/Mar/17 15:38;jjordan;+1 Looks good to me.  addViewToSchemaMutation calls addTableParamsToRowBuilder which was covered by the fix in CASSANDRA-12236.  So the only MV cdc thing that still needs catching is in makeUpdateForSchema which this patch covers.;;;","27/Mar/17 16:27;jjirsa;Minor nit that the comment is grammatically confusing (probably accidental if -> is):

{code}
+     * The tables to which we added the cdc column. This is used in {@link #makeUpdateForSchema} below to make sure we skip that
+     * column is cdc is disabled as the columns breaks pre-cdc to post-cdc upgrades (typically, 3.0 -> 3.X).
{code}

;;;","28/Mar/17 09:27;aleksey;+1, LGTM too.;;;","28/Mar/17 12:43;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor doc update: Replaced non-ASCII dash in command line,CASSANDRA-13374,13058776,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,24/Mar/17 01:05,15/May/20 08:02,14/Jul/23 05:56,28/Mar/17 12:23,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Minor doc update to replace non-ascii code, for copy-paste.
Not sure if it's the right way to report it, or should I use GitHub PR?",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/17 01:06;jay.zhuang;13374-3.11.patch;https://issues.apache.org/jira/secure/attachment/12860261/13374-3.11.patch",,,,,,,,,,,,,,,,,,,1.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 12:23:14 UTC 2017,,,,,,,,,,"0|i3cqaf:",9223372036854775807,3.11.x,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"28/Mar/17 12:23;jasobrown;[~jay.zhuang] while not ""official"" yet, CASSANDRA-13256 has some good instructions on contributing doc improvements.

That being said, I'm +1'ing this patch and committing it as sha {{380a614f1c10e34e456c428d9c3986991437b97f}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test,CASSANDRA-13372,13058607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,23/Mar/17 14:20,16/Apr/19 09:30,14/Jul/23 05:56,11/May/17 20:22,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1525/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test

{code}
Error Message

'Repaired at: 0' unexpectedly found in 'SSTable: /tmp/dtest-qoNeEc/test/node1/data0/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-4-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948985001\nMaximum timestamp: 1490129952789002\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -9222701292667950301 (key=5032394c323239385030)\nLast token: -3062233317334255711 (key=3032503434364f4e4f30)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16550\ntotalRows: 3310\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3310\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3309                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3310\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948985001\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-qoNeEc/test/node1/data1/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-5-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948987000\nMaximum timestamp: 1490129952789004\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -3060251208033125494 (key=33364e4b313936504b30)\nLast token: 2923054332122545251 (key=344b3634354b35353430)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16565\ntotalRows: 3313\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3313\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3312                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3313\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948987000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-qoNeEc/test/node1/data2/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-6-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948984000\nMaximum timestamp: 1490129952789003\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: 2925175199546211606 (key=3250365032354e4c3231)\nLast token: 9222137691148971235 (key=4c30334f32394d4c3031)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16885\ntotalRows: 3377\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3377\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3376                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3377\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948984000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\n'
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 332, in sstable_marking_test
    self.assertNotIn('Repaired at: 0', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 810, in assertNotIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}",,bdeggleston,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/17 14:20;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12860149/node1.log","23/Mar/17 14:20;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12860147/node1_debug.log","23/Mar/17 14:20;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12860148/node1_gc.log","23/Mar/17 14:20;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12860152/node2.log","23/Mar/17 14:20;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12860150/node2_debug.log","23/Mar/17 14:20;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12860151/node2_gc.log","23/Mar/17 14:20;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12860155/node3.log","23/Mar/17 14:20;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12860153/node3_debug.log","23/Mar/17 14:20;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12860154/node3_gc.log",,,,,,,,,,,9.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 20:22:46 UTC 2017,,,,,,,,,,"0|i3cp8v:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"11/May/17 20:22;bdeggleston;This hasn't failed since March, and CASSANDRA-13454 would have fixed any problems with sstables not being promoted to repaired after a repair;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove legacy auth tables support,CASSANDRA-13371,13058575,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,23/Mar/17 12:48,15/May/20 08:06,14/Jul/23 05:56,03/Aug/17 10:59,3.0.15,3.11.1,4.0,4.0-alpha1,,,Feature/Authorization,,,,,0,security,,,,"Starting with Cassandra 3.0, we include support for converting pre CASSANDRA-7653 user tables, until they will be dropped by the operator. Converting e.g. permissions happens by simply copying all of them from {{permissions}} -> {{role_permissions}}, until the {{permissions}} table has been dropped.

Upgrading to 4.0 will only be possible from 3.0 upwards, so I think it's safe to assume that the new permissions table has already been populated, whether the old table was dropped or not. Therefor I'd suggest to just get rid of the legacy support.",,jeromatron,jjordan,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 10:59:14 UTC 2017,,,,,,,,,,"0|i3cp1r:",9223372036854775807,,,,,,,snazy,,snazy,,,Normal,,,,,,,,,,,,,,,,,,,"04/Jul/17 09:21;spod;* [branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [test-all|https://circleci.com/gh/spodkowinski/cassandra/75]
* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/111/];;;","25/Jul/17 14:58;snazy;Code looks good to me (pretty much identical to [this branch|https://github.com/apache/cassandra/compare/trunk...snazy:13729-remove-legacy-auth-trunk], CASSANDRA-13729).

Except, it's missing a startup check and a unit test for that.
The startup check is necessary as people may still be effectively using the old auth tables.;;;","27/Jul/17 07:53;spod;I've rebased the patch and added the startup check, test and NEWS.txt of yours. 

* [branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [test-all|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/150/]

Btw, there's also CASSANDRA-13662 if you're interested in further cleanup for trunk.;;;","28/Jul/17 12:14;snazy;+1

Just two nits, feel free to do those on commit:
* {{CassandraRoleManager.getRoleFromTable}} is only used from {{getRole}} - code can be moved into {{getRole}}
* {{import org.apache.cassandra.schema.Schema;}} is no longer needed in PasswordAuthenticator
;;;","28/Jul/17 19:10;jjordan;We should probably also add a WARN on 3.0/3.11 if the legacy tables are being used, especially if we are going to not allow upgrading if you still have them around.;;;","29/Jul/17 08:26;snazy;[~spodxx@gmail.com] can you provide a patch for what [~jjordan] proposed?;;;","31/Jul/17 12:43;spod;How about using the startup check for that and make it log to warn in 3.0/3.11 and start throwing a StartupException in 4.0?

;;;","31/Jul/17 12:45;snazy;Yea - just log a warning in 3.0 & 3.x.;;;","31/Jul/17 13:53;spod;Had to change the API for the startup check a bit and had to update the corresponding test for backporting...

||trunk||3.11||3.0||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-trunk]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-3.11]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-3.0]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/158]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/157]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/156]|
|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-trunk]|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-3.11]|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-3.0]|

;;;","31/Jul/17 14:18;snazy;One thing in the 3.0 + 3.11 branches: Better don't reference the classes {{PasswordAuthenticator}}, {{CassandraRoleManager}}, {{CassandraAuthorizer}} but use the table names as strings for the {{LEGACY_AUTH_TABLES}} constant. Referencing the classes may unintentionally initialize stuff.
Beside that: +1 - feel free to  fix that on commit.;;;","03/Aug/17 10:59;spod;Committed as d74ed4b78886c to 3.0. Merged to 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unittest CipherFactoryTest failed on MacOS,CASSANDRA-13370,13058362,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,22/Mar/17 18:59,07/Mar/23 11:52,14/Jul/23 05:56,24/Mar/17 17:28,3.11.0,4.0,4.0-alpha1,,,,Legacy/Testing,,,,,0,,,,,"Seems like MacOS(El Capitan) doesn't allow writing to {{/dev/urandom}}:
{code}
$ echo 1 > /dev/urandom
echo: write error: operation not permitted
{code}
Which is causing CipherFactoryTest failed:
{code}
$ ant test -Dtest.name=CipherFactoryTest
...
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 2.184 sec
    [junit]
    [junit] Testcase: buildCipher_SameParams(org.apache.cassandra.security.CipherFactoryTest):  Caused an ERROR
    [junit] setSeed() failed
    [junit] java.security.ProviderException: setSeed() failed
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:472)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.access$300(NativePRNG.java:331)
    [junit]     at sun.security.provider.NativePRNG.engineSetSeed(NativePRNG.java:214)
    [junit]     at java.security.SecureRandom.getDefaultPRNG(SecureRandom.java:209)
    [junit]     at java.security.SecureRandom.<init>(SecureRandom.java:190)
    [junit]     at org.apache.cassandra.security.CipherFactoryTest.setup(CipherFactoryTest.java:50)
    [junit] Caused by: java.io.IOException: Operation not permitted
    [junit]     at java.io.FileOutputStream.writeBytes(Native Method)
    [junit]     at java.io.FileOutputStream.write(FileOutputStream.java:313)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:470)
...
{code}

I'm able to reproduce the issue on two Mac machines. But not sure if it's affecting all other developers.

{{-Djava.security.egd=file:/dev/urandom}} was introduced in:
CASSANDRA-9581

I would suggest to revert the [change|https://github.com/apache/cassandra/commit/ae179e45327a133248c06019f87615c9cf69f643] as {{pig-test}} is removed ([pig is no longer supported|https://github.com/apache/cassandra/commit/56cfc6ea35d1410f2f5a8ae711ae33342f286d79]).
Or adding a condition for MacOS in build.xml.

[~aweisberg] [~jasobrown] any thoughts?",,aweisberg,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/17 16:34;jay.zhuang;13370-trunk-update.txt;https://issues.apache.org/jira/secure/attachment/12860183/13370-trunk-update.txt","22/Mar/17 21:31;jay.zhuang;13370-trunk.txt;https://issues.apache.org/jira/secure/attachment/12860012/13370-trunk.txt",,,,,,,,,,,,,,,,,,2.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 17:28:41 UTC 2017,,,,,,,,,,"0|i3cnqf:",9223372036854775807,5.0,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"22/Mar/17 19:18;aweisberg;It would be nice to have tests not block on secure random in the environments where that block for an unfortunate amount of time. I looked and I couldn't find a way to have SHA1PRNG or a fast seed generator be the default. I suspect there is a configuration out there that will initialize quickly, but I couldn't find it.

I would +1 switching to something that works on OS X in the interim.

;;;","22/Mar/17 21:34;jay.zhuang;[~aweisberg] how about this fix: [718f67d|https://github.com/cooldoger/cassandra/commit/718f67d711c15b0d9dbebce3065064c73efd85e5]?;;;","23/Mar/17 09:54;spod;Jay, shouldn't simply removing the seed be enough? Do you still have to remove the egd path to get rid of the error?;;;","23/Mar/17 14:36;aweisberg;I think we should remove the seed anyways so that subsequent usage of secure random doesn't also fail only on OX X. These tests have been failing for a long time without being fixed.
;;;","23/Mar/17 16:25;aweisberg;Oh, I misunderstood. So it's removing the seed that will stop Java from writing to /dev/random. Yes I think that would be a better approach.;;;","23/Mar/17 16:37;jay.zhuang;Make sense. Thanks [~spod]
Updated the [patch|https://github.com/cooldoger/cassandra/commit/e89ac4407f387dc9607b21d3ef9ece6d4bda4bd8], passed the test locally on MacOS and Linux.;;;","23/Mar/17 17:01;aweisberg;Sorry to keep changing my mind. Still digesting the fact that we can fix just this one test and keep using /dev/urandom. I checked and we don't use seeding of SecureRandom outside of this test. So I propose going with your original solution of using SHA1PRNG, seeding it the way the test does so that the test is deterministic as originally intended, and not changing anything in build.xml.

||Code|utests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13370-3.11?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13370-3.11-testall/2/]|;;;","23/Mar/17 18:21;spod;-I don't think seeding SecureRandom does what you think it does, Ariel. The provided seed will just get ""mixed"" with the current RNG seed. This is different from e.g. seeding java.util.Random and will not make the test deterministic.-

(does only seem to apply to the native PRNG, just tested with SHA1PRNG and it worked as described by you, my mistake here)
;;;","23/Mar/17 18:46;aweisberg;Well it's still news to me that NativePRNG does mixing. Thanks for bringing it up. I guess the test author didn't rely on it being deterministic.

The right thing to do anyways is to generate a random seed and log it. So you get fuzzing but you can still reproduce a failure. I amended my original commit.;;;","23/Mar/17 19:07;spod;The initial stacktrace from the description shows that NativePRNG tries do write the seed into /dev/urandom. Any values written there will not reset the seed system wide (which would be funny), but simply provide a bit of additional entropy. SHA1RPNG seems to work differently though, but the [javadoc|https://docs.oracle.com/javase/8/docs/api/java/security/SecureRandom.html#setSeed-byte:A-] isn't very clear either what exactly to expect.;;;","24/Mar/17 16:57;aweisberg;[~jay.zhuang] does this work for you? I don't want to rewrite your patch without running it by you.;;;","24/Mar/17 17:03;jay.zhuang;[~aweisberg] yes, sure. Thanks for improving that.;;;","24/Mar/17 17:28;aweisberg;Committed as [ee7023e324cdd3b3442b04ad4b0b1f4b33921d35|https://github.com/apache/cassandra/commit/ee7023e324cdd3b3442b04ad4b0b1f4b33921d35];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If there are multiple values for a key, CQL grammar choses last value. This should not be silent or should not be allowed.",CASSANDRA-13369,13058350,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nachiket_patil,nachiket_patil,nachiket_patil,22/Mar/17 18:48,07/Mar/23 11:52,14/Jul/23 05:56,08/May/17 20:52,3.11.0,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"If through CQL, multiple values are specified for a key, grammar parses the map and last value for the key wins. This behavior is bad.
e.g. 
{code}
CREATE KEYSPACE Excalibur WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'dc1': 2, 'dc1': 5};
{code}

Parsing this statement, 'dc1' gets RF = 5. This can be catastrophic, may even result in loss of data. This behavior should not be silent or not be allowed at all.  
",,aweisberg,jjirsa,nachiket_patil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/17 22:17;nachiket_patil;3.X.diff;https://issues.apache.org/jira/secure/attachment/12860748/3.X.diff","08/May/17 18:20;aweisberg;test_stdout.txt;https://issues.apache.org/jira/secure/attachment/12866959/test_stdout.txt","27/Mar/17 22:17;nachiket_patil;trunk.diff;https://issues.apache.org/jira/secure/attachment/12860749/trunk.diff",,,,,,,,,,,,,,,,,3.0,nachiket_patil,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:49:44 UTC 2017,,,,,,,,,,"0|i3cnnr:",9223372036854775807,2.1.x,3.0.x,5.0,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"25/Mar/17 17:24;jjirsa;*This is not a review*, just a quick glance. However:

{code}
+            // Create a keyspace
+            execute(""CREATE KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 2}"");
+
+            // try modifying the keyspace
+            assertInvalidThrow(SyntaxException.class, ""CREATE KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 2, '"" + DATA_CENTER + ""' : 3 }"");
+            execute(""ALTER KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 3}"");
{code}

You're creating the keyspace, then your comment says modify, but the CQL query is another {{CREATE}}, which will definitely fail. You're checking for {{SyntaxException}}, so I suspect the test is doing the right thing, but it's a bit confusing. Would be better if that was {{ALTER}};;;","27/Mar/17 22:49;nachiket_patil;[~jjirsa] Thanks. Fixed.;;;","02/May/17 19:42;aweisberg;||Code|utests|dtests||
|[3.11|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13369-3.11?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/268]||;;;","08/May/17 18:20;aweisberg;bootstrap_test.TestBootstrap.consistent_range_movement_false_with_replica_down_should_succeed_test
bootstrap_test.TestBootstrap.simultaneous_bootstrap_test
cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_blogposts
materialized_views_test.TestMaterializedViews.clustering_column_test
materialized_views_test.TestMaterializedViews.clustering_column_test
paxos_tests.TestPaxos.contention_test_many_threads
secondary_indexes_test.TestPreJoinCallback.write_survey_test
topology_test.TestTopology.size_estimates_multidc_test
topology_test.TestTopology.size_estimates_multidc_test;;;","08/May/17 20:52;aweisberg;Committed as [https://github.com/apache/cassandra/commit/1a83efe2047d0138725d5e102cc40774f3b14641|1a83efe2047d0138725d5e102cc40774f3b14641]. Thanks.;;;","06/Jul/17 17:49;aweisberg;[~jeromatron] Another ticket hit with the materialized view hammer?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible AssertionError in UnfilteredRowIteratorWithLowerBound,CASSANDRA-13366,13058245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Mar/17 14:34,16/Apr/19 09:30,14/Jul/23 05:56,23/Mar/17 09:30,3.11.0,,,,,,,,,,,0,,,,,"In the code introduced by CASSANDRA-8180, we build a lower bound for a partition (sometimes) based on the min clustering values of the stats file. We can't do that if the sstable has and range tombston marker and the code does check that this is the case, but unfortunately the check is done using the stats {{minLocalDeletionTime}} but that value isn't populated properly in pre-3.0. This means that if you upgrade from 2.1/2.2 to 3.4+, you may end up getting an exception like
{noformat}
WARN  [ReadStage-2] 2017-03-20 13:29:39,165  AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.AssertionError: Lower bound [INCL_START_BOUND(Foo, -9223372036854775808, -9223372036854775808) ]is bigger than first returned value [Marker INCL_START_BOUND(Foo)@1490013810540999] for sstable /var/lib/cassandra/data/system/size_estimates-618f817b005f3678b8a453f3930b8e86/system-size_estimates-ka-1-Data.db
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:122)
{noformat}
and this until the sstable is upgraded.",,christopher.lambert,philipthompson,slebresne,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 15:34:45 UTC 2017,,,,,,,,,,"0|i3cn0f:",9223372036854775807,,,,,,,stefania,,stefania,,,Normal,,3.4,,,,,,,,,,,,,,,,,"22/Mar/17 14:49;slebresne;Attaching patch below:
| [13366-3.11|https://github.com/pcmanus/cassandra/commits/13366-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13366-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13366-3.11-dtest] |

Mostly, this just make sure we don't use pre-3.0 sstables for building the lower bound since it's unsafe. There was also a corner case with {{null}} in clusterings (which we only allow for compact tables for backward compatiblity and should be pretty rare) that wasn't handled so the patch adds that. And I added a bunch of comments as I felt this could be useful to future readers.

I'd like to try to write an upgrade dtest for this, but haven't taken the time yet. I'll update when that's the case, but the problem is simple enough that this probably shouldn't block review in the meantime ([~Stefania] assigning you since you wrote CASSANDRA-8180, but feel free to unassign if you don't have time).
;;;","23/Mar/17 03:23;stefania;Thanks for fixing this [~slebresne], it LGTM and the comments you've added are extremely useful.

CI results also look good.

Two typos, [here|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#commitcomment-21451015] and [here|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#commitcomment-21450989]. 

I also assume that [{{mayOverlapWith(}}|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#diff-894e091348f28001de5b7fe88e65733fL2016] was removed despite being public, because it is unreliable in the presence of range tombstones and compact tables, so I think it's justifiable.
;;;","23/Mar/17 09:33;slebresne;Committed thanks (I'm still keeping the ""write a dtest"" on my TODO list, but it may took me a few days to get to it and I don't see the point delaying the commit given this is pretty simple one).

bq. because it is unreliable in the presence of range tombstones and compact tables

Correct, it was unused and unsafe, so felt safer to just get rid of it.;;;","27/Mar/17 15:34;philipthompson;If the test is going to take more than a few days after the ticket, could we get a separate jira ticket, maybe a subtask for that? I would hate to see it be forgotten and then no test added.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cqlsh COPY fails importing Map<String,List<String>>, ParseError unhashable type list",CASSANDRA-13364,13058179,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefania,nicolaen,nicolaen,22/Mar/17 10:54,16/Apr/19 09:30,14/Jul/23 05:56,07/Apr/17 01:09,2.1.18,2.2.10,3.0.13,3.11.0,,,Legacy/Tools,,,,,0,cqlsh,,,,"When importing data with the _COPY_ command into a column family that has a _map<text, frozen<list<text>>>_ field, I get a _unhashable type: 'list'_ error. Here is how to reproduce:

{code}
CREATE TABLE table1 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<list<text>>>
);

insert into table1 (col1, col2map) values (1, {'key': ['value1']});

cqlsh:ks> copy table1 to 'table1.csv';


table1.csv file content:
1,{'key': ['value1']}


cqlsh:ks> copy table1 from 'table1.csv';
...
Failed to import 1 rows: ParseError - Failed to parse {'key': ['value1']} : unhashable type: 'list',  given up without retries
Failed to process 1 rows; failed rows written to kv_table1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.420 seconds (0 skipped).
{code}

But it works fine for Map<String, Set<String>>.

{code}
CREATE TABLE table2 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<set<text>>>
);

insert into table2 (col1, col2map) values (1, {'key': {'value1'}});

cqlsh:ks> copy table2 to 'table2.csv';


table2.csv file content:
1,{'key': {'value1'}}


cqlsh:ks> copy table2 from 'table2.csv';
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.417 seconds (0 skipped).
{code}

The exception seems to arrive in _convert_map_ function in _ImportConversion_ class inside _copyutil.py_.",,ifesdjeen,nicolaen,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 01:09:32 UTC 2017,,,,,,,,,,"0|i3cmlr:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"29/Mar/17 03:34;stefania;The problem is that we cannot parse maps into Python dictionaries because otherwise we couldn't parse dictionaries with another dictionary as key. So we use sets of tuples for dictionaries. This means that even values must be hashable and lists aren't. A trivial fix is to however parse lists as tuples, rather than lists. The driver should accept tuples for list cql types.

This is a regression for 2.1 and so the fix should be applied to 2.1 too.

||2.1||2.2||3.0||3.11||trunk||
|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-2.1]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-2.2]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-3.11]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-2.1-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-2.2-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-3.0-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-3.11-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-cqlsh-tests/]|

The patch applies cleanly to all branches.

Test is [here|https://github.com/riptano/cassandra-dtest/pull/1456].;;;","29/Mar/17 08:03;nicolaen;I see. Thanks for the fix 👍 I've just checked that the tuple type in python preserves the order, and it does.;;;","06/Apr/17 12:41;ifesdjeen;+1, the patch looks good! 

The test failures on 2.1 and trunk ({{describe_mv}} and {{test_bulk_round_trip}}) seem to be unrelated...;;;","07/Apr/17 01:09;stefania;Thank you [~ifesdjeen] and [~nicolaen].

Committed to 2.1 as 010b5f3a567663a5ceb823932a0b430848d331e3 and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix racy read command serialization,CASSANDRA-13363,13058159,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,arokhin,arokhin,22/Mar/17 09:22,25/Jun/19 10:33,14/Jul/23 05:56,30/Aug/17 16:39,3.0.15,3.11.1,,,,,,,,,,0,pull-request-available,,,,"Constantly see this error in the log without any additional information or a stack trace.

{code}
Exception in thread Thread[MessagingService-Incoming-/10.0.1.26,5,main]
{code}

{code}
java.lang.ArrayIndexOutOfBoundsException: null
{code}

Logger: org.apache.cassandra.service.CassandraDaemon
Thrdead: MessagingService-Incoming-/10.0.1.12
Method: uncaughtException
File: CassandraDaemon.java
Line: 229","CentOS 6, Cassandra 3.10",aleksey,arokhin,githubbot,ifesdjeen,jasonstack,jay.zhuang,jjirsa,KurtG,lizg,mkrupits_jb,samt,shashikantkulkarni,slebresne,zhaoyan,zmalik,zznate,,,,,,,,,,,"Github user johnyannj closed the pull request at:

    https://github.com/apache/cassandra/pull/137
;01/Oct/18 12:53;githubbot;600",,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,CASSANDRA-12435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 10:33:41 UTC 2019,,,,,,,,,,"0|i3cmhb:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"03/Apr/17 13:18;slebresne;I really don't think we can do anything about it unless you provide a full stack trace.;;;","03/Apr/17 13:22;ifesdjeen;You can turn off {{OmitStackTraceInFastThrow}} if it's on, my guess is that the stack trace might be emitted because the amount of thrown exceptions has reached a threshold and was optimised away.;;;","10/Apr/17 10:48;arokhin;[~ifesdjeen] Thank you, turned it off. [~slebresne] I'll let you know when/if the issue is reproduced. ;;;","04/May/17 17:30;arokhin;[~slebresne] Looks like we met the exception again. With a stack trace this time. Is that helpful?

{noformat}
ERROR 17:22:43 Exception in thread Thread[MessagingService-Incoming-/10.0.1.12,5,main]
java.lang.ArrayIndexOutOfBoundsException: 71
    at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:74) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1021) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.13.jar:3.0.13]
{noformat}
;;;","04/May/17 17:33;arokhin;Quick update - Cassandra version now is 3.0.13

And worth to mention that 3 nodes Cassandra cluster becomes completely unreposnsive after that exception. 

Exception itself occures on all 3 nodes.;;;","18/Jul/17 11:23;shashikantkulkarni;Hello,
I am also facing the similar issue so adding my comment here. I have Apache Cassandra v3.9, with 3 node in cluster. Replication factor 2. Same datacenter. CentOS 6.8, Java 8

{noformat}
ERROR [MessagingService-Incoming-/10.0.0.111] 2017-07-06 14:26:02,506 CassandraDaemon.java:226 - Exception in thread Thread[MessagingService-Incoming-/10.0.0.111,5,main]
java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:310) ~[guava-18.0.jar:na]
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:292) ~[guava-18.0.jar:na]
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:65) ~[guava-18.0.jar:na]
	at org.apache.cassandra.db.ClusteringPrefix$Serializer.deserializeValuesWithoutSize(ClusteringPrefix.java:359) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer.deserializeValues(ClusteringBoundOrBoundary.java:179) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer.deserialize(ClusteringBoundOrBoundary.java:161) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.Slice$Serializer.deserialize(Slice.java:322) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.Slices$Serializer.deserialize(Slices.java:336) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer.deserialize(ClusteringIndexSliceFilter.java:174) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:77) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1041) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:696) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:626) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.9.0.jar:3.9.0]
{noformat}

After this exception the cluster becomes unresponsive and start throwing errors if we try to query for web application.

Thanks;;;","05/Aug/17 10:18;zhaoyan;Hello,
I am also facing the similar issue so adding my comment here.

cassandra 3.0.14  jdk8

{code:java}
ERROR [MessagingService-Incoming-/10.xx.0.205] 2017-08-05 18:12:38,319 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/10.xx.0.205,5,main]
java.lang.ArrayIndexOutOfBoundsException: 36
        at org.apache.cassandra.db.Slice$Bound$Serializer.deserialize(Slice.java:542) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.Slice$Serializer.deserialize(Slice.java:335) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.Slices$Serializer.deserialize(Slices.java:346) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer.deserialize(ClusteringIndexSliceFilter.java:176) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:77) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1031) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.14.jar:3.0.14]
ERROR [MessagingService-Incoming-/10.xx.0.205] 2017-08-05 18:12:39,994 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/10.xx.0.205,5,main]
java.lang.ArrayIndexOutOfBoundsException: 103
        at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:74) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1031) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.14.jar:3.0.14]
{code}
;;;","05/Aug/17 18:00;jjirsa;Relevant code is:

{code}
        public ClusteringIndexFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException
        {
            Kind kind = Kind.values()[in.readUnsignedByte()]; // We read a byte, and then try to find the kind, and we're way outside of the array
            boolean reversed = in.readBoolean();

            return kind.deserializer.deserialize(in, version, metadata, reversed);
        }
{code}

And

{code}
            public Slice.Bound deserialize(DataInputPlus in, int version, List<AbstractType<?>> types) throws IOException
            {
                Kind kind = Kind.values()[in.readByte()]; // same thing, read a byte, way outside of the array
                return deserializeValues(in, kind, version, types);
            }
{code}
;;;","05/Aug/17 21:08;jjirsa;{quote}
And worth to mention that 3 nodes Cassandra cluster becomes completely unreposnsive after that exception.
Exception itself occures on all 3 nodes.
{quote}

Does a cluster restart help, or does it stay broken?

Did you recently do any changes? Changing schemas?  Do you see any signs of physical hardware errors (interface counters showing errors)?;;;","06/Aug/17 08:22;zhaoyan;please  see  CASSANDRA-12435   same question.

when I query with index  ( use expr)  by java driver,  It will appear。

as I test,  3.0.10-3.0.14  all has this problem。;;;","09/Aug/17 02:37;zhaoyan;I apply one patch here:

https://github.com/apache/cassandra/pull/137;;;","09/Aug/17 03:59;githubbot;GitHub user johnyannj opened a pull request:

    https://github.com/apache/cassandra/pull/137

    CASSANDRA-13363/CASSANDRA-12435 fix index flag before not be consistent with follow data content

    fix CASSANDRA-13363/CASSANDRA-12435
    
    when the command will be executed by local node and target node.
    the command.index will be reload by local thread.
    
    when code reach indexFlag(command.index.isPresent())  is false
    but reach if (command.index.isPresent()) , it is changed to true, because the ""command.index"" has been loaded by local execute thread.
    
    the command.index can be reload by target node, so it's serialize is optional。 but the flag must be consistent with follow data content.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/johnyannj/cassandra johnyannj-patch-fix-13363

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/137.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #137
    
----
commit 5050ff51b1f62fe7a678be01f79ff962a0fd6caa
Author: zhaoyan <zhaoyan@zhaoyanblog.com>
Date:   2017-08-09T02:35:46Z

    Update ReadCommand.java
    
    fix CASSANDRA-13363/CASSANDRA-12435
    
    when the command will be executed by local node and target node.
    the command.index will be reload by local thread.
    
    when code reach indexFlag(command.index.isPresent())  is false
    but reach if (command.index.isPresent()) , it is changed to true, because the ""command.index"" has been loaded by local execute thread.
    
    the command.index can be reload by target node, so it's serialize is optional。 but the flag must be consistent with follow data content.

----
;;;","09/Aug/17 04:24;jjirsa;Marking patch available
;;;","14/Aug/17 15:29;aleksey;The problem is real, and the patch does work, but I'm afraid it doesn't quite solve the issue completely.

There is also an issue involving {{serializedSize()}}. {{index}} field might be empty at the time when we calculate the size of the message, and switch to non-empty afterwards. It's not currently an issue since {{READ_COMMAND}} is not using a {{CallbackDeterminedSerializer}} but it's still a bug.

The proper fix would be to make sure the field never changes and is only set once at construction time. While at it, might also want to refactor it to not be {{Optional}}. {{Optional}} is reserved for return types, not object fields and method arguments.

Give me a couple hours to try work it out properly?;;;","16/Aug/17 09:14;zhaoyan;hi @Aleksey Yeschenko

thank you for your review.
the serializedSize()  is real a hidden trouble。

I do agree with you that：
""The proper fix would be to make sure the field never changes and is only set once at construction time.""

the index is designed as lazy load , load once , and not set at construction time,    so  I think its load may be very hard.

What about removing index from the readcommand‘s  serialize?
;;;","16/Aug/17 10:05;samt;[~zhaoyan] 

bq.the index is designed as lazy load , load once , and not set at construction time, so I think its load may be very hard

This was true at one point, but in CASSANDRA-10215 that changed so that we could avoid performing the lookup to determine which index to use multiple times during the query execution. That patch was didn't really go far enough though (which is my bad), as [~iamaleksey] points out there are several shortcomings with it; it shouldn't be using {{Optional}}, the field should be final and so forth. 

However, there is also another issue at play here. Queries with secondary indexes were historically always executed as partition range queries, as their results can span multiple partitions. Even when a partition key restriction is present, we convert the query to a range read command and the index field *is* always set at construction time (in {{SelectStatement::getRangeCommand}}), so it is never lazily loaded. There is a related bug though, CASSANDRA-11617, which causes queries with a partition key restriction && a custom index expression to be executed as single partition read command. When these are created (in {{SelectStatement::getSliceCommands}}), we *don't* ensure that the index is loaded at construction time (because we weren't expecting any query with an index to generate such a command). So in this case, the lazy loading happens when the command is executed locally, leading to the race causing the serialization issues you're seeing. 

bq.What about removing index from the readcommand‘s serialize?

This would partially undo CASSANDRA-10215 and require each replica to figure out which index to use, which is potentially expensive (relatively) and unnecesary.
 
So to cut a long story short, [~iamaleksey]'s diagnosis and solution are correct, making sure that where an index is appropriate it gets set on the read command during construction is the right  way to fix this. The more thorough refactoring, which should have been done in CASSANDRA-10215, is also a good idea.  ;;;","18/Aug/17 01:12;zhaoyan;Hi @Sam Tunnicliffe 

Thank you for your patient explanation.

I has one more question： 

""This would partially undo CASSANDRA-10215 and require each replica to figure out which index to use, ""

Does it take a lot of time to do ""the lookup to determine which index to use ""?

1=>send the command to other replica, must after the local replica figure out which index to use .
2=>send to all replica, all replica figure it out itself.

which looks better?;;;","18/Aug/17 10:03;samt;{quote} Does it take a lot of time to do ""the lookup to determine which index to use ""?

1=>send the command to other replica, must after the local replica figure out which index to use .
2=>send to all replica, all replica figure it out itself.

which looks better?
{quote}

Through profiling, we found that a non-trivial amount of time was spent in {{SecondaryIndexManager::getBestIndexFor}}. Part of the problem was that once found, there was no index field on the command to set, so originally this lookup was being done 3 or 4 times per-query, on every replica. 
Once that was fixed, it made sense to just do the index lookup once when the command is constructed, so it could be included in the serialization & so save the replicas from having to do the lookup. So the lookup goes from being performed (4 x num_replicas) times to once per query, which seems like a good idea generally. 

;;;","18/Aug/17 14:52;aleksey;Doing it at most once on each replica is probably not a big deal at all. But I'd rather not change current behaviour (except fix this issue). Which would also mean we'd be able to implement CASSANDRA-10214 at any point without a protocol change.;;;","21/Aug/17 12:10;aleksey;[~zhaoyan] Thanks again for the initial analysis. And that patch would 100% prevent the AIOOBE, just not fix the underlying issue entirely.

A more comprehensive refactoring to fix the race for good:
||3.0||3.11||4.0||
|[branch|https://github.com/iamaleksey/cassandra/tree/13363-3.0]|[branch|https://github.com/iamaleksey/cassandra/tree/13363-3.11]|[branch|https://github.com/iamaleksey/cassandra/tree/13363-4.0]|
|[utest|https://circleci.com/gh/iamaleksey/cassandra/2]|[utest|https://circleci.com/gh/iamaleksey/cassandra/3]|[utest|https://circleci.com/gh/iamaleksey/cassandra/4]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/211/testReport/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/212/testReport/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/213/testReport/]|
[~beobal] can you please review?

EDIT: Added test results;;;","29/Aug/17 15:12;samt;LGTM except now that {{findIndex}} is baked into {{SinglePartitionReadCommand}}, we perform a lookup through {{SIM::getBestIndexFor}} when creating the commands to actually read from index tables in {{CompositesSearcher}} & {{KeysSearcher}}. Also, because those commands are created using the base table's CFM, we actually end up finding an index (the one we're in the process of searching). In practice, I don't suppose this is much of a problem as the execution of those commands is done directly through {{queryMemtableAndDisk}}, so we don't attempt to erroneously use the index. However, it is confusing and more seriously, it breaks {{secondary_index_test:TestSecondaryIndexes.test_only_coordinator_chooses_index_for_query}}.

Aside from that, I just have a couple of tiny nits, feel free to ignore either/both:

{{getBestIndexFor(ReadCommand)}} is now only used by tests which could easily be tweaked to use the version which takes a {{RowFilter}}. OFC, that trivial method doesn't really muck up the the {{SIM}} API, so nbd if it stays, but it isn't really adding anything either so ¯\_(ツ)_/¯

In some methods of {{*ReadCommand}}, the long lists of args are formatted 1 per line, and in others are in a single line. e.g. {{SPRC::create}} vs {{SPRC::copy}} etc. All of these are already touched by this patch, so they may as well be consistently formatted.
;;;","29/Aug/17 17:10;aleksey;Thanks for the review. Pushed a commit addressing the issues to the same branch (and rebased on top of most recent 3.0 while at it). Tests scheduled/running: [utest|https://circleci.com/gh/iamaleksey/cassandra/17], [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/223/].;;;","29/Aug/17 17:50;samt;LGTM and the failing dtest is passing for me locally now, so +1 assuming CI is still happy.;;;","30/Aug/17 16:39;aleksey;The only unit test that failed was {{ViewFilteringTest}}, due to a timeout. Reran locally, it passed. No new dtests seems to be affected (11 failures total).

Committed as [7f297bcf8aced983cbc9c4103d0ebefc1789f0dd|https://github.com/apache/cassandra/commit/7f297bcf8aced983cbc9c4103d0ebefc1789f0dd] to 3.0 and merged up.

;;;","25/Jun/19 10:33;zmalik;Hi, 

we are facing similar issue in cassandra 3.0.16 where this racy condition is supposed to be fixed. In our case it is happening during bootstrap time 
{noformat}
ERROR 19:38:22 [Stream #f1c709f0-9418-11e9-8599-931b65954728] Streaming error occurred
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
	at java.util.ArrayList.rangeCheck(ArrayList.java:657) ~[na:1.8.0_162]
	at java.util.ArrayList.get(ArrayList.java:433) ~[na:1.8.0_162]
	at org.apache.cassandra.db.ClusteringPrefix$Serializer.deserializeValuesWithoutSize(ClusteringPrefix.java:346) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.RangeTombstone$Bound$Serializer.deserializeValues(RangeTombstone.java:212) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.RangeTombstone$Bound$Serializer.deserialize(RangeTombstone.java:202) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeOne(UnfilteredSerializer.java:407) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:373) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:87) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:65) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.StreamReader$StreamDeserializer.hasNext(StreamReader.java:263) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:111) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:52) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:45) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.StreamReader.writePartition(StreamReader.java:172) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.compress.CompressedStreamReader.read(CompressedStreamReader.java:107) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:54) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:43) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:59) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:294) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [apache-cassandra-3.0.16.jar:3.0.16]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_162]
{noformat}
 This causes streaming errors and as a consequence the node stays in JOINING state.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup ParentRepairSession after repairs,CASSANDRA-13359,13057894,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,21/Mar/17 13:14,15/May/20 08:00,14/Jul/23 05:56,21/Mar/17 14:45,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"As part of removing anti-compaction code in CASSANDRA-9143, the call the removeParentRepairSession was not moved into the repair complete callback.",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 14:33:12 UTC 2017,,,,,,,,,,"0|i3ckun:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"21/Mar/17 14:31;bdeggleston;patch here: https://github.com/bdeggleston/cassandra/commits/13359;;;","21/Mar/17 14:33;marcuse;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial typo in JavaDriverClient.java,CASSANDRA-13355,13057705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,jjirsa,jjirsa,20/Mar/17 21:49,15/May/20 08:03,14/Jul/23 05:56,20/Mar/17 21:51,4.0,4.0-alpha1,,,,,Tool/stress,,,,,0,,,,,Upstream github PR from Ian Macalinao https://github.com/apache/cassandra/pull/97 - simple typo fix. ,,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/cassandra/pull/97,https://github.com/apache/cassandra/pull/97,,,,,,,,,,9223372036854775807,,,Mon Mar 20 21:51:43 UTC 2017,,,,,,,,,,"0|i3cjon:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"20/Mar/17 21:51;jjirsa;Committed to trunk as {{5b8b1ce26cd073a44ddf7c7a6750da409a343eba}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCS estimated compaction tasks does not take number of files into account,CASSANDRA-13354,13057554,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Jan Karlsson,Jan Karlsson,Jan Karlsson,20/Mar/17 15:12,15/May/20 08:06,14/Jul/23 05:56,07/Apr/17 10:57,4.0,4.0-alpha1,,,,,Local/Compaction,,,,,1,,,,,"In LCS, the way we estimate number of compaction tasks remaining for L0 is by taking the size of a SSTable and multiply it by four. This would give 4*160mb with default settings. This calculation is used to determine whether repaired or repaired data is being compacted.

Now this works well until you take repair into account. Repair streams over many many sstables which could be smaller than the configured SSTable size depending on your use case. In our case we are talking about many thousands of tiny SSTables. As number of files increases one can run into any number of problems, including GC issues, too many open files or plain increase in read latency.

With the current algorithm we will choose repaired or unrepaired depending on whichever side has more data in it. Even if the repaired files outnumber the unrepaired files by a large margin.

Similarily, our algorithm that selects compaction candidates takes up to 32 SSTables at a time in L0, however our estimated task calculation does not take this number into account. These two mechanisms should be aligned with each other.

I propose that we take the number of files in L0 into account when estimating remaining tasks. 
",Cassandra 2.2.9,esimfon,Jan Karlsson,JoshuaMcKenzie,marcuse,pauloricardomg,tommy_s,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/17 15:43;Jan Karlsson;13354-trunk.txt;https://issues.apache.org/jira/secure/attachment/12859578/13354-trunk.txt","23/Mar/17 19:41;Jan Karlsson;patchedTest.png;https://issues.apache.org/jira/secure/attachment/12860216/patchedTest.png","23/Mar/17 19:42;Jan Karlsson;unpatchedTest.png;https://issues.apache.org/jira/secure/attachment/12860217/unpatchedTest.png",,,,,,,,,,,,,,,,,3.0,Jan Karlsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Apr 07 10:57:10 UTC 2017,,,,,,,,,,"0|i3cir3:",9223372036854775807,2.2.9,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"20/Mar/17 15:16;Jan Karlsson;Added patch on 4.0 to fix this. Applies cleanly to other versions as well(tested 2.2.9).
I have tested this in a cluster and will upload some graphs as well.
Comments and suggestions welcome!;;;","23/Mar/17 19:47;Jan Karlsson;I did some tests simulating traffic on a 4 node cluster. 2 of the nodes were running with my patch while the other two ran without it.
Steps to reproduce:
Traffic on
Turn one of the nodes off
Wait 7 minutes
Truncate hints on all other nodes
Turn node on
Run repair on the node

As you can see the unpatched version kept increasing as non-repaired data from ongoing traffic was prioritized. If I had more discrepancies in my data set, this would just increase to the configured FD limit or until you die from heap pressure.

Repair is completed at 8:11pm but those small repaired files are not compacted as it picks unrepaired new sstables over the small repaired sstables. However, it did show a downwards trend as compaction was slightly faster than insertion and would probably eventually end with the repaired files compacted.

During the unpatched test, it only showed 2 pending compactions with 22k~ file descriptors open/10k~ sstables. At 8:33pm I disabled the traffic completely to hurry this along.
SSTables in each level: [10347/4, 5, 0, 0, 0, 0, 0, 0, 0];;;","06/Apr/17 20:29;JoshuaMcKenzie;[~krummas]: have bandwidth for review on this one?;;;","07/Apr/17 07:03;marcuse;patch lgtm, I just pushed a tiny nit here: https://github.com/krummas/cassandra/commits/13354

I'm running dtests, not unlikely that some test relies on the old calculations, will commit if tests look good and you agree with my small change [~Jan Karlsson];;;","07/Apr/17 07:19;Jan Karlsson;yes small change lgtm;;;","07/Apr/17 10:57;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed unregistering mbean during drop keyspace,CASSANDRA-13346,13056967,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Lerh Low,gabor.auth,gabor.auth,17/Mar/17 11:15,15/May/20 07:59,14/Jul/23 05:56,06/Jun/17 18:59,3.0.14,3.11.0,4.0,4.0-alpha1,,,Feature/Materialized Views,,,,,0,lhf,,,,"All node throw exceptions about materialized views during drop keyspace:
{code}

WARN  [MigrationStage:1] 2017-03-16 16:54:25,016 ColumnFamilyStore.java:535 - Failed unregistering mbean: org.apache.cassandra.db:type=Tables,keyspace=test20160810,table=unit_by_account
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap$KeySetView.remove(ConcurrentHashMap.java:4569) ~[na:1.8.0_121]
        at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:712) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.unregisterMBean(ColumnFamilyStore.java:570) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:527) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:517) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:365) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:358) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.config.Schema.dropView(Schema.java:744) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$373(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1256) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}
",Cassandra 3.9,Andrew Efimov,cnlwsu,gabor.auth,jeromatron,jjirsa,KurtG,Lerh Low,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/17 00:01;Lerh Low;13346-3.0.X.txt;https://issues.apache.org/jira/secure/attachment/12865234/13346-3.0.X.txt","27/Apr/17 00:01;Lerh Low;13346-3.X.txt;https://issues.apache.org/jira/secure/attachment/12865235/13346-3.X.txt","18/May/17 04:55;Lerh Low;13346-trunk.txt;https://issues.apache.org/jira/secure/attachment/12868679/13346-trunk.txt",,,,,,,,,,,,,,,,,3.0,Lerh Low,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 06 18:59:04 UTC 2017,,,,,,,,,,"0|i3cf4f:",9223372036854775807,3.0.13,3.10,,,,,cnlwsu,,cnlwsu,,,Low,,,,,,,,,,,,,,,,,,,"26/Mar/17 03:21;jeromatron;Is there any context around this?  Were you doing any other schema modification at the time?  Also was there any other effect other than this error in the logs?;;;","04/Apr/17 00:59;Andrew Efimov;Hi Jeremy, Gábor
I have the same issue in 3.10, in my case, this occurs only for materialized views at the end of the test phase.
Also I am using org.cassandraunit for testing in embedded mode.
{noformat}
java.lang.NullPointerException: null
	at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentHashMap$KeySetView.remove(ConcurrentHashMap.java:4569) ~[na:1.8.0_121]
	at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:713) ~[cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.unregisterMBean(ColumnFamilyStore.java:577) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:534) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:524) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:370) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:363) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.config.Schema.dropView(Schema.java:704) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$17(SchemaKeyspace.java:1313) [cassandra-all-3.10.jar:3.10]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1313) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:535) ~[cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[cassandra-all-3.10.jar:3.10]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[cassandra-all-3.10.jar:3.10]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{noformat};;;","04/Apr/17 04:02;jjirsa;[~gabor.auth] - is {{test20160810.unit_by_account}} a materialized view as well? 

;;;","04/Apr/17 05:17;gabor.auth;[~jjirsa]: yes, as I mentioned in the description: ""All node throw exceptions about materialized views during drop keyspace"". :)

All nodes throws this exception about all materialized view of the keyspace during drop keyspace command.;;;","04/Apr/17 05:20;gabor.auth;""Is there any context around this?""

Hm... I saw only this exception.

""Were you doing any other schema modification at the time?""

No, only `DROP KEYSPACE test20160810`.

""Also was there any other effect other than this error in the logs?""

As I see, everything is okay but this exception.;;;","04/Apr/17 10:49;Andrew Efimov;I guess that the problem may be in the difference of metrics types for Materialized view and Table:
{{at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:713)}}
{{TableMetrics}} does not find the metrics for view, can only be used for Table.;;;","04/Apr/17 11:11;Andrew Efimov;{{TableMetrics.release}} does not release metric with name {{ViewReadTime}}
because it is not created, but release method tries to remove this metric with NPE:
{noformat}
C* 3.10
org.apache.cassandra.metrics.TableMetrics: 669

        // We do not want to capture view mutation specific metrics for a view
        // They only makes sense to capture on the base table
        if (cfs.metadata.isView())
        {
            viewLockAcquireTime = null;
            viewReadTime = null;
        }
        else
        {
            viewLockAcquireTime = createTableTimer(""ViewLockAcquireTime"", cfs.keyspace.metric.viewLockAcquireTime);
            viewReadTime = createTableTimer(""ViewReadTime"", cfs.keyspace.metric.viewReadTime);
        }
{noformat};;;","27/Apr/17 00:11;Lerh Low;This is quite easily reproduced, we just have to create a materialized view and try to drop keyspace. 

{code}
CREATE KEYSPACE mvtest WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1 };
CREATE TABLE mvtest.tobedropped (
    foo int,
    bar text,
    baz text,
    PRIMARY KEY (foo, bar)
);

CREATE MATERIALIZED VIEW mvtest.explosion AS
    SELECT foo, bar, baz FROM mvtest.tobedropped WHERE
    foo IS NOT NULL AND bar IS NOT NULL AND baz IS NOT NULL
PRIMARY KEY (foo, bar, baz);

INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (1, 'baz', 'bokusapp');
INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (2, 'baz', 'vitamin');
INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (2, 'backgammon', 'gin');

DROP KEYSPACE mvtest;
{code}

As [~Andrew Efimov] mentioned, this seems to be because {{ViewLockReadTime}} and {{ViewLockAcquireTime}} are both set to null for materialized views (they don't make sense for materialized views and was decided to be that way based on [CASSANDRA-10323|https://issues.apache.org/jira/browse/CASSANDRA-10323]). So the call to {{Metrics.getMetrics().get(name.getMetricName())}} returns null, which throws the Exception as the {{remove}} method does not allow {{null}} values (For the implementation of the set in {{allTableMetrics}}. I've attached a patch for both 3.0.X and 3.X since it's a relatively small change - it looks like it's just a case of trying to unregister a metric from the registry that doesn't exist so we should just ignore it when it's {{null}} (which is only when it's releasing view metrics). I've retested it on my local and it works...Any feedbacks are welcome! ([~carlyeks], [~cnlwsu]]...?), or guidance on writing tests if necessary (It doesn't seem like there are any metrics tests though there are metrics dtests, I'll try taking a look at dtests). ;;;","27/Apr/17 03:15;cnlwsu;+1 for patch. Big +1 for some dtests, the registering and removing of metrics around keyspace and table drops+creations has come up a few times (notice all hard coded entries in {{release}} that has drifted in and out of date).;;;","27/Apr/17 03:38;cnlwsu;so actually taking a second look there may be something more to this. The patch will still work though.

The {{release}} call is iterating over {{all}} registered mbeans, not just the ones for that table so whenever there are metrics that exist in some tables and not others, and gets dropped, it will throw the NPE. It really seems we should not be using a big static map to store these, but just a local one to store the ones we create for that table. The tricky thing is the ""all"" metrics which span multiple tables but that can probably be handled by getting a list of the columnfamilystore's and iterating over them instead of trying to keep a separate registry in sync.

Patch does fix it though and is a much simpler fix.;;;","27/Apr/17 04:26;Lerh Low;Hey Chris,

Thanks for answering. I'll try to fit dtests into my schedule and take a look at how it works/how to write one. 

With regards to your more recent comment, my impression is {{all}} is a hashmap of metric names to their aliases.
 {{allTableMetrics}} is a map of each metric to their set of metrics for each Table ({{String, Set<String>}}, which is used in the aggregation metrics over all Tables (e.g Readlatency key will have all the tables' read latency in its Set of values). If we had it use a local one to store the ones we create for the table, then it would be harder to aggregate over all known Tables. 

Open to any suggestions though, as usual :) ;;;","28/Apr/17 14:08;cnlwsu;the ""all"" map is actually for metrics that have a global representation thats aggregated. For example, theres a global sstables per read metric that aggregates all the individual table and keyspace metrics so you can see the entire nodes sstables per read to see if any are high instead of walking through entire table set to find out.;;;","01/May/17 06:20;Lerh Low;Hi Chris,

While debugging, {{all}} is just a map of names to values. I think what you're referring to is {{allTableMetrics}}, which in that case then yep - as you mentioned, the downside is now we would need some way of aggregating over all metrics for the global ones, but if you'd rather it that way I can look into it. 

I've also gotten ready a dtest here: https://github.com/riptano/cassandra-dtest/pull/1467 Any feedbacks/code style/whatever are more than welcome. ;;;","09/May/17 22:44;Lerh Low;Thought I should also add, this causes metrics not to be dropped properly, so there will still be metrics in the registry for tables that should have already been dropped as a result of this Exception..;;;","09/May/17 23:42;cnlwsu;which can potentially be bad since recreating same named table may cause mbean naming conflicts and failures;;;","18/May/17 02:11;Lerh Low;Hi Chris,

Looking into it a little bit more, this is another way to do it - just retain enough information in {{allTableMetrics}} so we know what metrics are held by which CF. Or, at least, we can iterate through it to find the metrics that are held for a particular CF, and release all of them. In this case, the changes will look something like this: https://github.com/apache/cassandra/compare/trunk...juiceblender:cassandra-13346

Then {{allTableMetrics}} becomes something we keep in sync with the {{MetricsRegistry}}. In fact it's more or less identical or isomorphic to {{MetricsRegistry}}. Just with the way we currently register metrics, we use this method: 
{code}
public String getMetricName()
        {
            return MetricRegistry.name(group, type, name, scope);
        }
{code}
which adds a {{.}} in between each value. 
So that eventually, an entry in my branch's {{allTableMetrics}} looks like so: 
{{org.apache.cassandra.metrics:type=Table,keyspace=system_distributed,scope=repair_history,name=TotalDiskSpaceUsed -> The actual metric}}

While {{Metrics.getMetrics()}} looks like so: 
{{org.apache.cassandra.metrics.Table.BloomFilterFalsePositives.system.built_views -> The actual metric}}

From here, there are a few options forward:
i) We stick with the original patch
ii) We go ahead with what's in the branch, the dtest still works for it.
iii) We totally get rid of {{allTableMetrics}} and just use the existing {{MetricsRegistry}}. To make it really safe for the aggregating metrics, I feel we would need some way to construct (or parse) a {{MetricName}} object from the String returned from {{Metrics.getMetrics()}} (It's a Map<String, Metric> unfortunately). In this case we will have to iterate over every metric that we ever registered compared to just the table metrics in {{allTableMetrics}}, but it should be relatively fine because we don't release metrics often
iv) We try and have each CFStore keep a local copy of its TableMetric. When constructing TableMetric for a CFStore, we will have to get a list of CFStores (I would need some guidance on this, from a viewManager somewhere?) and get all their respective gauges and aggregate them that way. Something like iii). 

I'm for either i) or iii), any thoughts? 
;;;","18/May/17 02:20;cnlwsu;I like idea of {{i}} immediately and {{iii}} being done in follow up. It could use some refactoring/cleaning up.;;;","18/May/17 04:41;Lerh Low;Let's go with {{i}} then. Would you be able to review and commit please if it's ok or if anything else needs to be done? :) The dtest is here: https://github.com/riptano/cassandra-dtest/pull/1467;;;","18/May/17 04:48;Lerh Low;Sorry, forgot about trunk. Updating. ;;;","18/May/17 04:55;Lerh Low;Attached, trunk is the same as 3.10.;;;","18/May/17 14:07;cnlwsu;I cant commit but +1 from me;;;","18/May/17 20:59;Lerh Low;Ahhh I see, thanks for going through all that with me though :);;;","05/Jun/17 23:06;jjirsa;[~cnlwsu] / [~Lerh Low] - just to be clear, which patches here are ready to commit? The ones attached to the JIRA, or the github URL https://github.com/apache/cassandra/compare/trunk...juiceblender:cassandra-13346 ? 

 ;;;","05/Jun/17 23:24;Lerh Low;[~jjirsa] The ones attached to the JIRA :) ;;;","06/Jun/17 18:59;jjirsa;Committed as {{40ad3cf4dd384bede595edce4617534ca904f1ed}}, thanks all!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong logger name in AnticompactionTask,CASSANDRA-13343,13056770,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,szhou,szhou,szhou,16/Mar/17 19:34,15/May/20 08:02,14/Jul/23 05:56,16/Mar/17 20:33,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,,,,,,0,,,,,"We have the below code in AnticompactionTask.java. The parameter is wrong.
{code}
private static Logger logger = LoggerFactory.getLogger(RepairSession.class);
{code}",,jasobrown,szhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/17 19:37;szhou;CASSANDRA-13343-v1.patch;https://issues.apache.org/jira/secure/attachment/12859153/CASSANDRA-13343-v1.patch",,,,,,,,,,,,,,,,,,,1.0,szhou,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 20:33:38 UTC 2017,,,,,,,,,,"0|i3cdwn:",9223372036854775807,3.0.10,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"16/Mar/17 19:39;szhou;[~pauloricardomg] do you mind taking a quick review? Just one line change.;;;","16/Mar/17 20:22;jasobrown;+1. This goes back to 2.2, so I'll fix 'em all. Thanks for finding this trivial one!;;;","16/Mar/17 20:33;jasobrown;committed as sha {{a69f6885556a147837f15098fd3aef5de756fad5}}. thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy deserializer can create empty range tombstones,CASSANDRA-13341,13056717,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,16/Mar/17 16:24,16/Apr/19 09:30,14/Jul/23 05:56,29/Mar/17 11:25,3.0.13,3.11.0,,,,,,,,,,0,,,,,"Range tombstones in the 2.x file format is a bit far-westy so you can actually get sequences of range tombstones like {{\[1, 4\]@3 \[1, 10\]@5}}. But the current legacy deserializer doesn't handle this correctly. On the first range, it will generate a {{INCL_START(1)@3}} open marker, but upon seeing the next tombstone it will decide to close the previously opened range and re-open with deletion time 5, so will generate {{EXCL_END_INCL_START(1)@3-5}}. That result in the first range being empty, which break future assertions in the code.",,blambov,christopher.lambert,jeromatron,jjirsa,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 29 11:25:16 UTC 2017,,,,,,,,,,"0|i3cdkv:",9223372036854775807,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"16/Mar/17 16:49;slebresne;Attaching fix and a unit test to demonstrate the problem. The basic idea is that the legacy deserializer has to wait until he has seen all range tombstones with the same start before generating an open marker.

| [13341-3.0|https://github.com/pcmanus/cassandra/commits/13341-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13341-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13341-3.0-dtest] |
| [13341-3.11|https://github.com/pcmanus/cassandra/commits/13341-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13341-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13341-3.11-dtest] |

[~blambov]: setting you as reviewer because while it's a different problem than in CASSANDRA-13237, it touches basically the same code. If you don't have time, feel free to de-assign yourself though.
;;;","21/Mar/17 10:48;blambov;Looks good with a couple of documentation nits:
- The comment to [{{popMarker}}|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR675] needs to mention that this is called multiple times; it currently leaves the false impression we would get the outstanding open marker as the last item in the iteration.
- [{{openNew}} comment text|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR690] looks wrong and at odds with the code.
- ""popping"" not [""poping""|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR640]. The whole diagnostic printout seems costly and should probably disappear.
;;;","22/Mar/17 15:19;slebresne;Thanks, pushed a new commit with fixes for those. Re-triggered CI to be extra sure even though it's mostly updates to comments. ;;;","29/Mar/17 11:25;slebresne;Tests are good, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs handling range tombstones in the sstable iterators,CASSANDRA-13340,13056710,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,16/Mar/17 16:01,16/Apr/19 09:30,14/Jul/23 05:56,23/Mar/17 16:20,3.0.13,3.11.0,,,,,,,,,,0,,,,,"There is 2 bugs in the way sstable iterators handle range tombstones:
# empty range tombstones can be returned due to a strict comparison that shouldn't be.
# the sstable reversed iterator can actually return completely bogus results when range tombstones are spanning multiple index blocks.

The 2 bugs are admittedly separate but as they both impact the same area of code and are both range tombstones related, I suggest just fixing both here (unless something really really mind).

Marking the ticket critical mostly for the 2nd bug: it can truly make use return bad results on reverse queries.",,blambov,christianmovi,christopher.lambert,jeromatron,jjirsa,jjordan,kohlisankalp,mkjellman,slebresne,Yasuharu,zzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 16:20:07 UTC 2017,,,,,,,,,,"0|i3cdjb:",9223372036854775807,,,,,,,blambov,,blambov,,,Critical,,,,,,,,,,,,,,,,,,,"16/Mar/17 16:18;slebresne;Attaching fix for both issue with unit test below. I tried to explain the problems it solves and how it solves it in the patches.
| [13340-3.0|https://github.com/pcmanus/cassandra/commits/13340-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13340-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13340-3.0-dtest] |
| [13340-3.11|https://github.com/pcmanus/cassandra/commits/13340-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13340-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13340-3.11-dtest] |
;;;","16/Mar/17 23:06;mkjellman;did you mean to commit this line in one of the unit tests commented out?

https://github.com/pcmanus/cassandra/commit/66100ffecba6ff55027f6e85b29efaf98700edaa#diff-70d3a7f61389330811d6eb2f7d2d1b76R1391;;;","17/Mar/17 08:17;slebresne;Forgot to remove it, sorry, I can do so on commit though if that's fine (it's not needed for the test to reproduce the failure and I prefer unit test to be as focused as possible, but I indeed forgot to remove it entirely). Thanks for the notice.;;;","23/Mar/17 11:19;blambov;I think this is correct but I find the terminology confusing. Previous block [sometimes|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R309] means the next we'll iterate to (previous on disk), [other times|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R327] the previous we iterated. I'd prefer a consistent meaning for these; it looks like the new code is at odds with the previous convention on these, so the meanings of {{hasPrevious/NextBlock}} need reversing. {{skipLast/First}} have the same problem, I'd at least add {{IteratedItem}} to their names to add a little clarity.

[{{canIncludeSliceStart/End}}|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R334] appear to have exactly the opposite meaning of {{hasPrevious/NextBlock}}. I can see {{loadFromDisk}} needs separate booleans for the difference in the non-indexed case, but {{readCurrentBlock}} can do without the latter, can't it?;;;","23/Mar/17 15:14;slebresne;bq. Previous block sometimes means the next we'll iterate to (previous on disk), other times the previous we iterated.

You're right, that's confusing. That said, I tried switching the newly introduced {{hasPrevious/NextBlock}} but at least to me that felt pretty confusing, so I decide to switch the existing usage. Basically feels more logical to me, though that's possibly somewhat personal. In any case it's consistent now, previous/next refer to the previous/next block we'll iterate.

bq. {{skipLast/First}} have the same problem

If you mean that first/last can be a tad confused when we're reading a block in one sense but iterating on its items afterward in the other sense, then I agree, but I didn't felt inverting those was really improving things. I did added {{IteratedItem}} and completed the comments so it's hopefully more clear.

bq. but {{readCurrentBlock}} can do without the latter, can't it?

Absolutely, removed the redundant argument, thanks.;;;","23/Mar/17 15:38;blambov;LGTM

Nit: there are a few more inverted meanings: [in these two comments|https://github.com/pcmanus/cassandra/blob/94c0a9cca6b072e5f35c666c56e7ad1eb0577e7c/src/java/org/apache/cassandra/db/columniterator/SSTableReversedIterator.java#L189] as well as [this {{lastOfPrevious}}|https://github.com/pcmanus/cassandra/blob/94c0a9cca6b072e5f35c666c56e7ad1eb0577e7c/src/java/org/apache/cassandra/db/columniterator/SSTableReversedIterator.java#L352].;;;","23/Mar/17 16:20;slebresne;Committed (with nits fixed), thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dropping column results in ""corrupt"" SSTable",CASSANDRA-13337,13056599,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jborgstrom,jborgstrom,16/Mar/17 09:43,16/Apr/19 09:30,14/Jul/23 05:56,27/Mar/17 10:05,3.0.13,3.11.0,,,,,Local/Compaction,,,,,0,,,,,"It seems like dropping a column can make SSTables containing rows with writes to only the dropped column will become uncompactable.

Also Cassandra <= 3.9 and <= 3.0.11 will even refuse to start with the same stack trace

{code}
cqlsh -e ""create keyspace test with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }""
cqlsh -e ""create table test.test(pk text primary key, x text, y text)""

cqlsh -e ""update test.test set x='1' where pk='1'""
nodetool flush

cqlsh -e ""update test.test set x='1', y='1' where pk='1'""
nodetool flush
cqlsh -e ""alter table test.test drop x""

nodetool compact test test
error: Corrupt empty row found in unfiltered partition
-- StackTrace --
java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:382)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:87)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:65)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.doCompute(SSTableIdentityIterator.java:123)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:100)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:30)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129)
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:58)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:67)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:227)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:190)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:610)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)

{code}",,ifesdjeen,jborgstrom,jjirsa,philipthompson,slebresne,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 10:05:14 UTC 2017,,,,,,,,,,"0|i3ccun:",9223372036854775807,3.0.12,3.10,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"20/Mar/17 15:25;slebresne;This is definitively wrong, attaching patch for fix (including an unit test to reproduce).
| [13337-3.0|https://github.com/pcmanus/cassandra/commits/13337-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13337-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13337-3.0-dtest] |
| [13337-3.11|https://github.com/pcmanus/cassandra/commits/13337-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13337-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13337-3.11-dtest] |

I'll note that the patch basically disable the error message seen here, instead simply ignoring empty rows from disk since they can happen. I suppose it would be possible to do a more involved checking to make sure we didn't wrote something actually empty, but I'm not sure at all it's worth the trouble (not the cost of that check on a pretty hot path) especially given that expecting non-empty rows was wrong no only due to dropping, but also because we can actually skip columns due to the {{ColumnFilter}} within {{SerializationHelper}}. I believe the latter would only potentially impact thrift queries, which may be why nobody as yet reported that problem, but it's still wrong.
;;;","20/Mar/17 16:58;ifesdjeen;There's another way to reproduce the same issue with slightly different steps:

{code}
CREATE KEYSPACE IF NOT EXISTS ""test"" WITH REPLICATION = {'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': 1 };

CREATE TABLE IF NOT EXISTS ""test"".""reproduce"" (pk1 int, ck1 int, v1 int, v2 int, v3 int, v4 int, v5 int, PRIMARY KEY(pk1, ck1));

UPDATE ""test"".""reproduce"" SET v2 = 1, v3 = 3, v4 = 4 WHERE pk1 = 1 AND ck1 = 0;

ALTER TABLE ""test"".""reproduce"" DROP v2;
ALTER TABLE ""test"".""reproduce"" DROP v3;
ALTER TABLE ""test"".""reproduce"" DROP v4;

SELECT * FROM test.reproduce;
{code}

But essentially the problem is that we do return empty rows from local storage. For example, when {{UPDATE}} was used to set only a subset of rows, then the rows that were used in {{UPDATE}} get dropped. When trying to query, we end up with an empty row. This wouldn't happen with {{INSERT}} since for {{INSERT}} we have liveness set.

I just see a single small problem: 

{code}
        createTable(""CREATE TABLE %s(k int PRIMARY KEY, x int, y int)"");
        execute(""UPDATE %s SET x = 1 WHERE k = 0"");
        flush(doFlush); // (1)
        execute(""ALTER TABLE %s DROP x"");
{code}

If we do flush at point {{1}}, we will end up with a single row {{row(1, null)}}. However, if we do not do flush and query directly from memtable, we end up with an empty result.;;;","21/Mar/17 11:04;slebresne;bq. I just see a single small problem

You're right, that is wrong. And that's because while compaction uses {{UnfilteredSerializer.deserialize()}} which the first patch changes, {{SSTableIterator/SSTableReversedIterator}} which are used by queries do not, it uses {{UnfilteredDeserializer}} (which is more lazy). Pushed an additional patch that fix that part (unfortunately, due to how {{UnfilteredDesializer}} works, we can't easily handle this within {{UnfilteredDeserializer}} itself; still not terribly hard to handle properly).;;;","24/Mar/17 15:44;ifesdjeen;Sorry for the delay, I have somehow missed the notification. 
The new patch looks great, thanks for taking care of it!

+1 ;;;","27/Mar/17 10:05;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not start on Windows due to 'JNA link failure',CASSANDRA-13333,13056345,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,blerer,blerer,blerer,15/Mar/17 16:31,16/Apr/19 09:30,14/Jul/23 05:56,28/Mar/17 15:19,,,,,,,,,,,,0,,,,,"Cassandra 3.0 HEAD does not start on Windows. The only error in the logs is: 
{{ERROR 16:30:10 JNA failing to initialize properly.}} ",,blerer,ChandraM,dbrosius,jasobrown,jeromatron,JoshuaMcKenzie,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 15:19:13 UTC 2017,,,,,,,,,,"0|i3cba7:",9223372036854775807,,,,,,,mkjellman,,mkjellman,,,Critical,,,,,,,,,,,,,,,,,,,"15/Mar/17 21:30;jasobrown;caused by CASSANDRA-13233. [~mkjellman], you don't happen have a Windows machine to investigate this, perchance? (pretty sure you do not);;;","15/Mar/17 22:43;blerer;[~jasobrown], [~mkjellman] if you want I can investigate it. It seems that I am one of the only Windows dev around here.  ;;;","15/Mar/17 22:50;jasobrown;[~blerer] Sorry to give you more work, but the help is much appreciated!;;;","16/Mar/17 20:59;mkjellman;[~blerer] I don't have a Windows machine to test this unfortunately... but, I do know what's going on, although I'm not sure about the correct fix.

The following code from {{StartupChecks}}:

{code}
public static final StartupCheck checkJnaInitialization = new StartupCheck()
    {
        public void execute() throws StartupException
        {
            // Fail-fast if JNA is not available or failing to initialize properly
            if (!CLibrary.jnaAvailable())
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, ""JNA failing to initialize properly. "");
        }
    };
{code}

This is due to the fact that when I implemented the Windows {{CLibraryWrapper}} implementation {{CLibraryWindows}}, I had it return false for jnaAvailable(). Given that we only use JNA right now to access libc calls I wasn't aware that any of those would be implemented in JNA to do something in Windows.

If that's not correct, I'll throw a very quick patch together to load JNA in the Windows implementation, but I'll still need some help to know what wrapped methods actually work on Windows.

Should the startup check just be exempted for Windows?;;;","17/Mar/17 09:26;blerer;I went to look into how the things were working before the CASSANDRA-13233 changes.
The code does a difference between JNA not being there and the fact that it could not link a library. In the first case {{jnaAvailable()}} must return {{false}} but in the second case it should return {{true}} (and an error message should have been logged). The new implementation always return {{false}} even if the library is actually there.
So, to keep the existing behaviour, we would need to check if JNA is here and throw the appropriate errors. The easier way would probably to also have the following code in {{CLibraryWindows}} but it feels a bit weird:
{code}
        try
        {
            Native.register(""c"");
        }
        catch (NoClassDefFoundError e)
        {
            logger.warn(""JNA not found. Native methods will be disabled."");
            jnaAvailable = false;
        }
        catch (UnsatisfiedLinkError e)
        {
            logger.warn(""JNA link failure, one or more native method will be unavailable."");
            logger.trace(""JNA link failure details: {}"", e.getMessage());
        }
        catch (NoSuchMethodError e)
        {
            logger.warn(""Obsolete version of JNA present; unable to register C library. Upgrade to JNA 3.2.7 or later"");
            jnaAvailable = false;
        }
{code} 

Any suggestions?;;;","17/Mar/17 13:21;blerer;I did a bit of digging. JNA is actually used on {{Windows}} by {{org.apache.cassandra.utils.WindowsTimer}}. So it makes sense to block startup if JNA is not there. 
 ;;;","17/Mar/17 13:33;JoshuaMcKenzie;It's not worth disabling startup due to an inability to access winmm.dll. I used that to access the multimedia timer to change the kernel's timer coalescing which is strictly a performance improvement and shouldn't block startup of a node.;;;","17/Mar/17 14:00;blerer;Then does it really make sense to keep the {{jnaAvailable()}} method? Right now, that check does not really bring much because if the library cannot be linked the server will start anyway.
Is a warning in the log not enough? I really wonder if anybody is really using the NativeMBean to check that jna and mlock are available.
[~jasobrown], [~mkjellman] what is your opinion on that?;;;","17/Mar/17 14:17;blerer;[~dbrosius] you are the one that exposed the information through JMX. Is it something that you really need?;;;","17/Mar/17 14:24;blerer;Sorry, I missed the comment pointing at CASSANDRA-5508.

;;;","17/Mar/17 14:38;blerer;Another approach would be to convert  {{jnaAvailable()}} into {{isAvailable()}}. This method will return {{true}} if the library has been successfully linked and {{false}} otherwise. The startup check will then check, if the operating system is not Windows, that the library has been successfully linked and prevent the system to start if it has not.;;;","17/Mar/17 14:49;jeromatron;I think it's reasonable to not hold up startup if we can still issue a warning and still keep the state whether it was able to use JNA to lock memory.;;;","17/Mar/17 16:46;dbrosius;[~blerer] i added it for someone else, so no i don't personally need it;;;","17/Mar/17 17:19;blerer;I have pushed an initial version of the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0].
The patch replace the {{jnaAvailable()}} method by the {{isAvailable()}} method which will return {{true}} only if the library has been sucessfully linked.
In the case of Windows {{isAvailable()}} will always return {{false}}.
The patch remove the startup check so the server will start even if JNA is not found (in which case a warning would have already been logged).
The {{NativeAccessMBean::isAvailable}} method will now only return {{true}} if the native library has been successfully linked.  

If nobody disagree with the approach, I will push it on CI next week. ;;;","21/Mar/17 16:52;blerer;||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13333-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-dtest/]|

[~jasobrown], [~mkjellman] could one of you review the patches. Only 3.0 and 3.11 differ a bit. ;;;","21/Mar/17 21:13;mkjellman;[~blerer]

* Just chatted quickly with [~jasobrown] and [~jjirsa] and I think hiding the fact we're using JNA under the hood and going with {{isAvailable()}} vs. {{jnaAvailable()}] is a good change, however, I think that changes the scope a bit. In the case of {{CLibraryWindows}} I used the existing Sigar based logic to get the current PID of the running JVM as I wasn't sure if the *nix APIs would work in Windows land. Thinking about it, this means {{isAvailable()}} is a bit more nuanced. For instance, in the current {{CLibraryWindows}} case, yes, the {{callGetpid()}} method is ""available"" but all the other methods that we happen to have right now use JNA, which isn't ""available"" in the Windows case.
* Do you happen to know if the native {{getpid()}} JNA will work on Windows? If so we can switch that from Sigar -> JNA for Windows too just like we're already doing for the Linux/Darwin case. If so, it makes sense to attempt to load JNA and libc in the WIndows case too?
* I don't think we should remove the checkJnaInitialization {{StartupCheck}} at minimum in the Linux and Darwin cases. Given that we ship JNA in the release, either we can't link against the library it due to an issue or someone removed it. Given the performance implications I think we should leave the hard fail in place -- but skip it if the current OS type is  Windows.;;;","22/Mar/17 20:36;blerer;I force pushed a new patch.
The new patch use the {{Kernel32}} library to support natively the {{callGetPid}} method and keep the startup check. As the Windows library is not the {{c}} one, the patch also rename {{CLibrary}} to {{NativeLibrary}} as the name was misleading.  

||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13333-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-dtest/]|

;;;","22/Mar/17 21:07;mkjellman;[~blerer] this looks great! Renaming {{CLibrary}} --> {{NativeLibrary}} helps make the intent much clearer. 

# Should the loading of {{Native.register(""winmm"")}} in {{WindowsTimer}} also be moved into NativeLibraryWindows?
# Looks like the trunk patch didn't get pushed up or potentially just a copy paste error? Currently it's just pointing at blerer/trunk.
# Thanks for putting the MSDN API URL in the method javadoc. :)
# In {{NativeLibraryWindows}} I think the following logger statements could be simplified:

{code}
catch (UnsatisfiedLinkError e)
{
    logger.warn(""JNA link failure, one or more native method will be unavailable."");
    logger.error(""JNA link failure details: {}"", e.getMessage());
}
{code}

Can be simplified to:
{code}
logger.error(""Failed to link against JNA. Native methods will be unavailable."", e);
{code};;;","23/Mar/17 09:25;blerer;[~mkjellman] Thanks for the reviews.

bq. 1. Should the loading of {{Native.register(""winmm"")}} in {{WindowsTimer}} also be moved into NativeLibraryWindows?
{{WindowsTimer}} is really specific to Windows and according to [~JoshuaMcKenzie] [comment|https://issues.apache.org/jira/browse/CASSANDRA-13333?focusedCommentId=15929978&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15929978] we should not prevent startup due to an inability to access the {{winmm.dll}} library. So, I would be in favor of keeping it separeted for now.

bq. 2. Looks like the trunk patch didn't get pushed up or potentially just a copy paste error? Currently it's just pointing at blerer/trunk.
Sorry for that. It is a copy paste mistake. I fixed it.

bq. 3.  Thanks for putting the MSDN API URL in the method javadoc.
I am pretty sure that otherwise, I will have to end up googling it in a month or two ;-) 

bq. 4. In NativeLibraryWindows I think the following logger statements could be simplified:
I have pushed a new commit to fix it in all the branches. ;;;","25/Mar/17 08:23;ChandraM;Hey , 

Is this change committed just wondering when this change will be available in the remote. 

I tried a few moments back after i took the latest changes , and it fails to start - ERROR [main] 2017-03-25 13:35:21,001 CassandraDaemon.java:663 - JNA failing to initialize properly.

Thanks in adv. ;;;","25/Mar/17 13:18;blerer;The ticket status is still {{Patch Available}} which means that the patch has not been accepted yet.;;;","27/Mar/17 21:52;mkjellman;+1 [~blerer] LGTM!;;;","28/Mar/17 15:19;blerer;Thanks for the review.
Committed into 3.0 at 9b8692c6a4c75b7df29a58b5d3d1d1ee5cb0c3a4 and merged into 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_hints_delivery_threads does not work,CASSANDRA-13329,13056184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Gerrrr,Fuud,Fuud,15/Mar/17 06:25,15/May/20 08:01,14/Jul/23 05:56,07/Apr/17 10:34,3.11.0,4.0,4.0-alpha1,,,,,,,,,0,lhf,,,,"HintsDispatchExecutor creates JMXEnabledThreadPoolExecutor with corePoolSize  == 1 and maxPoolSize==max_hints_delivery_threads and unbounded LinkedBlockingQueue.

In this configuration additional threads will not be created.

Same problem with PerSSTableIndexWriter.",,alekiv,Fuud,Gerrrr,ifesdjeen,jeromatron,mbulman,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Gerrrr,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 21 09:40:21 UTC 2017,,,,,,,,,,"0|i3caaf:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"24/Mar/17 12:36;Gerrrr;{{JMXEnabledThreadPoolExecutor}} (used by {{HintsDispatchExecutor}} and {{PerSSTableIndexWriter}}) extends {{DebuggableThreadPoolExecutor}}.

According to the docs on {{DebuggableThreadPoolExecutor}} it works in the following way:
1. If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
2. If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
3. If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.

In both {{HintsDispatchExecutor}} and {{PerSSTableIndexWriter}}, {{JMXEnabledThreadPoolExecutor}} is constructed with corePoolSize equal to 1, maximumPoolSize equal to some constant and a work queue being unbounded {{LinkedBlockingQueue}}. In that setup when there are no tasks running, the new incoming task will add a thread to the pool (according to #1). However, because the queue is unbounded, according to #2 all the consequent tasks will be added to the queue instead of adding threads to the pool. Having corePoolSize equal to maximumPoolSize solves the problem because then the pool will maintain maximumPoolSize threads and submit tasks to them before queueing.

*Link to the branch*: https://github.com/Gerrrr/cassandra/tree/13329-3.10;;;","28/Mar/17 14:58;ifesdjeen;+1, LGTM

I've written a small test to verify the behaviour [here|https://github.com/ifesdjeen/cassandra/commit/8c2a322bf0bdaf0d198384992caa793f8289b533] although I do not think we should include it into the final patch, since it's rather a configuration/initialisation problem.

And triggered CI for 3.11 and trunk: 

|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13329-trunk]|[dtest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-dtest/]|[utest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13329-3.11]|[dtest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-dtest/]|[utest|https://cassci.datastax.com/job/ifesdjeen-13329-3.11-testall/]|;;;","07/Apr/17 10:34;ifesdjeen;Committed to 3.11 as [8eeea07f5f74eb86403e84464107b75c5063cf6b|https://github.com/apache/cassandra/commit/8eeea07f5f74eb86403e84464107b75c5063cf6b] and merged to [trunk|https://github.com/apache/cassandra/commit/a0a0e823e3095b9a47a3d11b3d58b14c039e1ced];;;","10/Apr/17 19:33;alekiv;[~ifesdjeen], [~Gerrrr]: do you have plans to fix this problem(max_hints_delivery_threads part) in 3.0.x version?;;;","10/Apr/17 19:56;Gerrrr;Hey [~alekiv],

Thank you for the suggestion! I have created a branch for 3.0 - https://github.com/Gerrrr/cassandra/tree/13329-3.0
The fix there is only for HintsDispatchExecutor since there was no PerSSTableIndexWriter.;;;","04/Jul/17 09:11;alekiv;Hello [~Gerrrr],

do you have plans to create pull request for 3.0.x version?;;;","16/Jul/17 12:38;Gerrrr;Hi [~ifesdjeen],
Can you please check the patch for 3.0 https://github.com/Gerrrr/cassandra/tree/13329-3.0 ?

Thank you!;;;","21/Jul/17 09:40;ifesdjeen;Committed the backport to 3.0 with [b337c690d321f2e4d7ebbbb0a1b8a90f986d21e9|https://github.com/apache/cassandra/commit/b337c690d321f2e4d7ebbbb0a1b8a90f986d21e9], merged up to [3.11|https://github.com/apache/cassandra/commit/bf0a4b9cd6324e9c5adfe8cdd72ecb1c3a70568a] and [trunk|https://github.com/apache/cassandra/commit/8d2fa65f8a7abbc27b0c3ff0820af2945fcf7496]. I've also taken a chance to make call syntax consistent across the branches (1-arg call instead of passing same var twice).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair not streaming correct sstables,CASSANDRA-13328,13056123,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,14/Mar/17 22:51,15/May/20 08:01,14/Jul/23 05:56,16/Mar/17 22:42,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Looks like I forgot to update the logic in {{StreamSession.getSSTableSectionsForRanges}} for CASSANDRA-9143. As a result, the updated incremental repair is still streaming all unrepaired sstables for the requested token range.",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 22:42:22 UTC 2017,,,,,,,,,,"0|i3c9wv:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"14/Mar/17 22:55;bdeggleston;|[branch|https://github.com/bdeggleston/cassandra/tree/CASSANDRA-13328]|[dtest|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-CASSANDRA-13328-dtest/]|[testall|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-CASSANDRA-13328-testall/]|;;;","15/Mar/17 07:19;marcuse;+1;;;","16/Mar/17 22:42;bdeggleston;Committed as d6a701ea11c919938cb09b0fca2ea0ec7ad2123b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outbound TCP connections ignore internode authenticator,CASSANDRA-13324,13050599,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,13/Mar/17 15:49,15/May/20 08:01,14/Jul/23 05:56,27/Mar/17 17:25,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,,0,,,,,When creating an outbound connection pool and connecting from within andOutboundTcpConnection it doesn't check if internode authenticator will allow the connection. In practice this can cause a bunch of orphaned threads perpetually attempting to reconnect to an endpoint that is will never accept the connection.,,aweisberg,jeromatron,jjirsa,marcuse,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 17:25:44 UTC 2017,,,,,,,,,,"0|i3bbtb:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"16/Mar/17 15:30;aweisberg;||code|utests|dtests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13324-trunk?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13324-trunk-testall/6/]|[dtests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13324-trunk-dtest/5/]|;;;","17/Mar/17 09:11;marcuse;+1 on the code - but seems the tests didn't run, I retriggered, feel free to commit if the tests look good;;;","20/Mar/17 20:29;aweisberg;The tests found some bugs and I made a few small changes. I force pushed instead of adding a commit.

It didn't compile because {{MessagingServiceTest}} needed to pass in a {{MockBackpressureStrategy}} to construct an {{OutboundTcpConnectionPool}}. Also there were issues with {{DatabaseDescriptor.internodeAuthenticator}} being null. Rather then add null checks I added made the config not nullable with a default of the allow all authenticator.;;;","23/Mar/17 22:06;aweisberg;[~krummas] can you review the last set of changes?;;;","24/Mar/17 11:42;marcuse;The new {{public static final IInternodeAuthenticator ALLOW_ALL}} in {{IInternodeAuthenticator}} should probably be removed as it is not used (alternative being to remove {{AllowAllInternodeAuthenticator.java}}, but I guess people might be using that in config files)

Feel free to do that on commit, +1;;;","24/Mar/17 19:28;aweisberg;Committed as [732d1af866b91e5ba63e7e2a467d99d4cb90e11f|https://github.com/apache/cassandra/commit/732d1af866b91e5ba63e7e2a467d99d4cb90e11f];;;","24/Mar/17 20:23;aweisberg;{{MessagineService.getConnectionPool()}} can now return null and there are callers not checking for null.;;;","24/Mar/17 21:50;aweisberg;||code|utests|dtests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13324-trunk-2?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13324-trunk-2-testall/1/]|[dtests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13324-trunk-2-dtest/1/]|;;;","25/Mar/17 18:11;jjirsa;Did you intend to change the behavior here: [getConnectionPool(expiredCallbackInfo.target).incrementTimeout() |https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13324-trunk-2?expand=1#diff-af09288f448c37a525e831ee90ea49f9L539] ? ;;;","25/Mar/17 22:50;aweisberg;Yes. It was double counting due to a merge error. It's incrementing already on the line above but with a null check.;;;","27/Mar/17 06:37;marcuse;+1;;;","27/Mar/17 17:25;aweisberg;Merged additional changes in [c86de2a9817aa45930afe181ae1891d2363393c7|https://github.com/apache/cassandra/commit/c86de2a9817aa45930afe181ae1891d2363393c7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgradesstables fails after upgrading from 2.1.x to 3.0.11,CASSANDRA-13320,13049952,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,zzheng,zzheng,10/Mar/17 08:08,16/Apr/19 09:30,14/Jul/23 05:56,17/Mar/17 11:16,3.0.13,3.11.0,,,,,,,,,,1,,,,,"I tried to execute {{nodetool upgradesstables}} after upgrading cluster from 2.1.16 to 3.0.11, but it fails when upgrading a table with 2i.

This problem can be reproduced as follows.
{code}
$ ccm create test -v 2.1.16 -n 1 -s
$ ccm node1 cqlsh  -e ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}""
$ ccm node1 cqlsh  -e ""CREATE TABLE test.test(k1 text, k2 text, PRIMARY KEY( k1 ));""
$ ccm node1 cqlsh  -e ""CREATE INDEX k2 ON test.test(k2);""
 
$ ccm node1 cqlsh  -e ""INSERT INTO test.test (k1, k2 ) VALUES ( 'a', 'a') ;""
$ ccm node1 cqlsh  -e ""INSERT INTO test.test (k1, k2 ) VALUES ( 'a', 'b') ;""
 
$ ccm node1 nodetool flush
 
$ for i in `seq 1 `; do ccm node${i} stop; ccm node${i} setdir -v3.0.11;ccm node${i} start; done
$ ccm node1 nodetool upgradesstables test test
Traceback (most recent call last):
  File ""/home/y/bin/ccm"", line 86, in <module>
    cmd.run()
  File ""/home/y/lib/python2.7/site-packages/ccmlib/cmds/node_cmds.py"", line 267, in run
    stdout, stderr = self.node.nodetool("" "".join(self.args[1:]))
  File ""/home/y/lib/python2.7/site-packages/ccmlib/node.py"", line 742, in nodetool
    raise NodetoolError("" "".join(args), exit_status, stdout, stderr)
ccmlib.node.NodetoolError: Nodetool command '/home/zzheng/.ccm/repository/3.0.11/bin/nodetool -h localhost -p 7100 upgradesstables test test' failed; exit status: 2; stderr: WARN  06:29:08 Only 10476 MB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:197)
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:415)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:307)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)
{code}

The result of dumping the 2i sstable is as follows.
{code}
[
{""key"": ""a"",
 ""cells"": [[""61"",1488961273,1488961269822817,""d""]]},
{""key"": ""b"",
 ""cells"": [[""61"","""",1488961273015759]]}
]
{code}

This problem is occurred by the tombstone row. When this row is processed in {{LegacyLayout.java}}, it will be treated as a row maker.
https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L1195
Then the deletion info will be lost.

As a result, the row will be a empty row, which causes the assertion error.

To avoid this, I added the code to add row deletion info when the row is a tombstone and *not* a row marker, and it works as I expect, which means that {{upgradesstables}} succeeds and row deletion info is remained.

However I don't understand whether this change will cause another problem. Anyway, I submit my patch as a reference.",,blerer,jeromatron,jjirsa,mnantern,samt,slebresne,Yasuharu,ytakata,zzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13347,,,CASSANDRA-13059,CASSANDRA-12620,,,,CASSANDRA-13059,,,,,"10/Mar/17 08:10;zzheng;13320.patch;https://issues.apache.org/jira/secure/attachment/12857268/13320.patch",,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 17 11:16:38 UTC 2017,,,,,,,,,,"0|i3b88n:",9223372036854775807,3.0.11,,,,,,blerer,,blerer,,,Normal,,3.0.11,,,,,,,,,,,,,,,,,"10/Mar/17 19:02;jjirsa;[~slebresne] or [~blerer] - given your work on CASSANDRA-12620 , either of you available to comment on the correctness of this? 
;;;","10/Mar/17 20:15;jjirsa;For troubleshooting in case it helps someone, here's some dumps of the sstable data post-upgradesstables with 3.0.10:

{code}
$ ~/.ccm/repository/3.0.10/tools/bin/sstabledump ~/.ccm/test/node1/data0/test/test-0c0e762005c511e7990409d9d370a92a/mc-2-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""a"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 15,
        ""liveness_info"" : { ""tstamp"" : ""2017-03-10T19:09:14.478850Z"" },
        ""cells"" : [
          { ""name"" : ""k2"", ""value"" : ""b"" }
        ]
      }
    ]
  }
]

$ ~/.ccm/repository/3.0.10/tools/bin/sstabledump ~/.ccm/test/node1/data0/test/test-0c0e762005c511e7990409d9d370a92a/.k2/mc-2-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""a"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 15,
        ""clustering"" : [ ""61"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-03-10T19:09:13.979340Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""b"" ],
      ""position"" : 23
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 38,
        ""clustering"" : [ ""61"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-03-10T19:09:14.478850Z"" },
        ""cells"" : [ ]
      }
    ]
  }
]
{code}

and 3.0.11 + the patch from [~zzheng] 

{code}
$ ~/.ccm/repository/3.0.11/tools/bin/sstabledump ~/.ccm/test/node1/data0/test/test-a4feee4005cb11e79e0709d9d370a92a/mc-2-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""a"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 22,
        ""liveness_info"" : { ""tstamp"" : ""2017-03-10T20:10:03.973045Z"" },
        ""cells"" : [
          { ""name"" : ""k2"", ""value"" : ""b"" }
        ]
      }
    ]
  }
]
$ ~/.ccm/repository/3.0.11/tools/bin/sstabledump ~/.ccm/test/node1/data0/test/test-a4feee4005cb11e79e0709d9d370a92a/.k2/mc-2-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""a"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 26,
        ""clustering"" : [ ""61"" ],
        ""deletion_info"" : { ""marked_deleted"" : ""2017-03-10T20:09:59.667091Z"", ""local_delete_time"" : ""2017-03-10T20:10:03Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""b"" ],
      ""position"" : 27
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 52,
        ""clustering"" : [ ""61"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-03-10T20:10:03.973045Z"" },
        ""cells"" : [ ]
      }
    ]
  }
]

{code}

;;;","10/Mar/17 20:25;blerer;I will try to look into that next week.;;;","14/Mar/17 19:21;samt;I think that the change is slightly incorrect and can cause a regression along the lines of what [~slebresne] describes [here|https://issues.apache.org/jira/browse/CASSANDRA-12620?focusedCommentId=15751089&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15751089]. 2i tables are the exception to the assertion that we never actually delete row markers in 2.x (sort of), so I think the correct fix should be:

{code}
diff --git a/src/java/org/apache/cassandra/db/LegacyLayout.java b/src/java/org/apache/cassandra/db/LegacyLayout.java
index 972bb9f..bfe3bff 100644
--- a/src/java/org/apache/cassandra/db/LegacyLayout.java
+++ b/src/java/org/apache/cassandra/db/LegacyLayout.java
@@ -1196,8 +1196,12 @@ public abstract class LegacyLayout
                 assert !cell.value.hasRemaining();
                 // In 2.1, the row marker expired cell might have been converted into a deleted one by compaction. So,
                 // we need to set the primary key liveness info only if the cell is not a deleted one.
+                // The only time in 2.x that we actually delete a row marker is in 2i tables, so in that case we do
+                // want to actually propagate the row deletion. (CASSANDRA-13320)
                 if (!cell.isTombstone())
                     builder.addPrimaryKeyLivenessInfo(LivenessInfo.create(cell.timestamp, cell.ttl, cell.localDeletionTime));
+                else if (metadata.isIndex())
+                    builder.addRowDeletion(Row.Deletion.regular(new DeletionTime(cell.timestamp, cell.localDeletionTime)));
             }
             else
             {
{code};;;","15/Mar/17 13:57;slebresne;Agreed, [~beobal]'s patch lgtm.;;;","15/Mar/17 14:02;blerer;I still have some tests that I would like to run on top of the upgrade tests. ;;;","15/Mar/17 16:02;blerer;I discussed off line with [~beobal]. One of my concerned was with expired TTLs on the index rows that have been compacted (converted in to deleted cells). Sam pointed out that as the index tables only have row markers and no regular data, it should be fine to convert them into deleted rows.
So, I am +1 with the patch if the upgrade tests look good.
  ;;;","17/Mar/17 11:16;samt;Thanks, I had some problems running the upgrade tests on cassci, but I finally managed to get a [pretty clean run|https://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13320-3.0-upgrade-upgrade/5/testReport/]. The supercolumn test failures are also present upstream, which looks like an issue with a no-longer-supported sstable version. I was slightly concerned by the failure in {{test_cell_TTL_expiry_during_paging}} but I've run that locally several times without any problem. We did actually have a failing test indicating this problem (which is no longer failing) tracked by CASSANDRA-13059, so I'm marking that resolved too. 

Committed to 3.0 in {{5918375e88bcacfab47e44cbfa2dd202ec634725}} 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Default logging we ship will incorrectly print ""?:?"" for ""%F:%L"" pattern due to includeCallerData being false by default no appender",CASSANDRA-13317,13049779,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,09/Mar/17 22:30,15/May/20 08:00,14/Jul/23 05:56,23/Mar/17 16:39,3.11.0,4.0,4.0-alpha1,,,,Legacy/Core,,,,,0,,,,,"We specify the logging pattern as ""%-5level [%thread] %date{ISO8601} %F:%L - %msg%n"". 

%F:%L is intended to print the Filename:Line Number. For performance reasons logback (like log4j2) disables tracking line numbers as it requires the entire stack to be materialized every time.

This causes logs to look like:
WARN  [main] 2017-03-09 13:27:11,272 ?:? - Protocol Version 5/v5-beta not supported by java driver
INFO  [main] 2017-03-09 13:27:11,813 ?:? - No commitlog files found; skipping replay
INFO  [main] 2017-03-09 13:27:12,477 ?:? - Initialized prepared statement caches with 14 MB
INFO  [main] 2017-03-09 13:27:12,727 ?:? - Initializing system.IndexInfo

When instead you'd expect something like:
INFO  [main] 2017-03-09 13:23:44,204 ColumnFamilyStore.java:419 - Initializing system.available_ranges
INFO  [main] 2017-03-09 13:23:44,210 ColumnFamilyStore.java:419 - Initializing system.transferred_ranges
INFO  [main] 2017-03-09 13:23:44,215 ColumnFamilyStore.java:419 - Initializing system.views_builds_in_progress

The fix is to add ""<includeCallerData>true</includeCallerData>"" to the appender config to enable the line number and stack tracing.",,aweisberg,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/17 22:35;mkjellman;13317_v1.diff;https://issues.apache.org/jira/secure/attachment/12857114/13317_v1.diff",,,,,,,,,,,,,,,,,,,1.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:48:52 UTC 2017,,,,,,,,,,"0|i3b7hb:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"09/Mar/17 22:36;mkjellman;[~aweisberg] wanna +1 this?;;;","09/Mar/17 23:44;mkjellman;p.s. [~aweisberg] there is a performance penalty here which is why it's disabled by default in logback and log4j2.. I think it's helpful to have the filename and line number personally while debugging so I think it makes sense to keep %F:%L in the pattern for logback-test.xml...

For the actual default conf we ship I wonder if it might make sense to remove ""%F:%L"" from the pattern instead of fixing the issue.... 

The thing is though that we ship this ""ASYNCDEBUGLOG"" appender enabled by default though which already has <includeCallerData>true</includeCallerData>..... so if we decide it's not worth the performance overhead to log the filename and line number for the actual default (non-test) logback config we ship we should also make ASYNCDEBUGLOG disabled by default...
 
{code}
  <root level=""INFO"">
    <appender-ref ref=""SYSTEMLOG"" />
    <appender-ref ref=""STDOUT"" />
    <appender-ref ref=""ASYNCDEBUGLOG"" /> <!-- Comment this line to disable debug.log -->
    <!--
    <appender-ref ref=""LogbackMetrics"" />
    -->
  </root>
{code};;;","22/Mar/17 18:21;aweisberg;{{includeCallerData}} as far as I can tell is only a property on {{ch.qos.logback.classic.AsyncAppender}}. The other appenders don't have that tunable and I suspect they always fetch the caller data. I'll add this to logback-test.xml's async appender which was missing it.;;;","22/Mar/17 19:42;aweisberg;||Code|utests|dtests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13317-3.11?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13317-3.11-testall/1/]|[dtests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13317-3.11-dtest/1/]|;;;","23/Mar/17 16:39;aweisberg;Committed as [3e95c5b0c574383e7da9a5e152b7be8aa122af9f|https://github.com/apache/cassandra/commit/3e95c5b0c574383e7da9a5e152b7be8aa122af9f];;;","06/Jul/17 17:48;aweisberg;[~jeromatron] is this really materialized view related? Seems purely log config related to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build error because of dependent jar (byteman-install-3.0.3.jar) currupted ,CASSANDRA-13316,13049759,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,samding,samding,09/Mar/17 21:54,15/May/20 08:00,14/Jul/23 05:56,27/Mar/17 17:17,2.2.10,3.11.0,4.0,4.0-alpha1,,,Legacy/Testing,,,,,0,,,,,"When build  cassandra 3.10 on amd64, CentOS Linux 7, there is a build error caused by corrupted jar file (byteman-install-3.0.3.jar).

Here is the replicated steps:

After install necessary dependent packages and apache-ant, git clone cassandra 3.10:

1)
  git clone https://github.com/apache/cassandra.git
  cd cassandra
  git checkout cassandra-3.10
  ant 

Then gets errors like:
""
build-project:
     [echo] apache-cassandra: /cassandra/build.xml
    [javac] Compiling 45 source files to /cassandra/build/classes/thrift
    [javac] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
    [javac] error: error reading /cassandra/build/lib/jars/byteman-install-3.0.3.jar; error in opening zip file
    [javac] Compiling 1474 source files to /cassandra/build/classes/main
    [javac] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
    [javac] error: error reading /cassandra/build/lib/jars/byteman-install-3.0.3.jar; error in opening zip file
    [javac] Creating empty /cassandra/build/classes/main/org/apache/cassandra/hints/package-info.class 
""
2) 
To check the jar and get:

# jar -i /cassandra/build/lib/jars/byteman-install-3.0.3.jar
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
java.util.zip.ZipException: error in opening zip file
        at java.util.zip.ZipFile.open(Native Method)
        at java.util.zip.ZipFile.<init>(ZipFile.java:219)
        at java.util.zip.ZipFile.<init>(ZipFile.java:149)
        at java.util.jar.JarFile.<init>(JarFile.java:166)
        at java.util.jar.JarFile.<init>(JarFile.java:103)
        at sun.tools.jar.Main.getJarPath(Main.java:1163)
        at sun.tools.jar.Main.genIndex(Main.java:1195)
        at sun.tools.jar.Main.run(Main.java:317)
        at sun.tools.jar.Main.main(Main.java:1288)

3) if download the jar and replace it, the build will be successful.

wget http://downloads.jboss.org/byteman/3.0.3/byteman-download-3.0.3-bin.zip
  unzip byteman-download-3.0.3-bin.zip -d /tmp
  rm  -f  build/lib/jars/byteman-install-3.0.3.jar
  cp  /tmp/byteman-download-3.0.3/lib/byteman-install.jar build/lib/jars/byteman-install-3.0.3.jar
  
  ant

....
BUILD SUCCESSFUL
Total time: 36 seconds
","Platform: Amd64
OS: CentOS Linux 7",jay.zhuang,jjirsa,samding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 17:15:49 UTC 2017,,,,,,,,,,"0|i3b7cv:",9223372036854775807,3.10,,,,,,,,,,,Normal,,3.10,,,,,,,,,,,,,,,,,"27/Mar/17 17:15;jay.zhuang;The problem is fixed [c0f99c|https://github.com/apache/cassandra/commit/c0f99c4e68da8e4e7f3d430b2cb45a762c1ff9f2] by [~mshuler];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Gossip breaks, Hint files not being deleted on nodetool decommission",CASSANDRA-13308,13049224,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,arijit91,arijit91,08/Mar/17 10:09,15/May/20 07:59,14/Jul/23 05:56,19/Apr/17 16:02,3.0.14,3.11.0,4.0,4.0-alpha1,,,Consistency/Hints,Legacy/Streaming and Messaging,,,,0,,,,,"How to reproduce the issue I'm seeing:
Shut down Cassandra on one node of the cluster and wait until we accumulate a ton of hints. Start Cassandra on the node and immediately run ""nodetool decommission"" on it.

The node streams its replicas and marks itself as DECOMMISSIONED, but other nodes do not seem to see this message. ""nodetool status"" shows the decommissioned node in state ""UL"" on all other nodes (it is also present in system.peers), and Cassandra logs show that gossip tasks on nodes are not proceeding (number of pending tasks keeps increasing). Jstack suggests that a gossip task is blocked on hints dispatch (I can provide traces if this is not obvious). Because the cluster is large and there are a lot of hints, this is taking a while. 

On inspecting ""/var/lib/cassandra/hints"" on the nodes, I see a bunch of hint files for the decommissioned node. Documentation seems to suggest that these hints should be deleted during ""nodetool decommission"", but it does not seem to be the case here. This is the bug being reported.

To recover from this scenario, if I manually delete hint files on the nodes, the hints dispatcher threads throw a bunch of exceptions and the decommissioned node is now in state ""DL"" (perhaps it missed some gossip messages?). The node is still in my ""system.peers"" table

Restarting Cassandra on all nodes after this step does not fix the issue (the node remains in the peers table). In fact, after this point the decommissioned node is in state ""DN""",Using Cassandra version 3.0.9,aleksey,arijit91,jasonstack,jay.zhuang,jjirsa,jjordan,rha,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13562,,,,,,,,,,,,,,,,,,,,,"09/Mar/17 02:34;arijit91;28207.stack;https://issues.apache.org/jira/secure/attachment/12856917/28207.stack","09/Mar/17 02:34;arijit91;logs;https://issues.apache.org/jira/secure/attachment/12856918/logs","09/Mar/17 02:39;arijit91;logs_decommissioned_node;https://issues.apache.org/jira/secure/attachment/12856919/logs_decommissioned_node",,,,,,,,,,,,,,,,,3.0,jjirsa,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 16:02:32 UTC 2017,,,,,,,,,,"0|i3b41z:",9223372036854775807,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"09/Mar/17 00:55;jjirsa;Just tried a trivial repro using 3.0.11 and ccm, didn't reproduce (not really surprising). 

 How big (approximately) is the cluster?
 Do you have any other range movements happening at the same time? 
 Can you post the jstack with the blocked gossip thread? 
 How much data did you have on the nodes? 
 Did the streams actually finish (do you see the streams complete in {{nodetool netstats}} ) ? 

We've seen some other recent bugs where gossip gets blocked (CASSANDRA-12281, for example), so I'm curious if you can still reproduce on 3.0.11. 

;;;","09/Mar/17 02:34;arijit91;Thanks for looking into this!

The cluster is 10 nodes in size, with about 2 GB of metadata on each node right now. Although surprisingly, when this happened yesterday,
I saw that nodes on average had 500 MB of hints for the decommissioned node with one node storing 3 GB of hints.

I don't think there were any range movements happening. I would guess that this is not CASSANDRA-12281, since I don't see the stack trace for that bug in my jstack output.

I've attached the jstack output (relevant threads from what I could figure out are 28548 and 5832) and a snippet of the log messages during this time.

I didn't think to look at `nodetool netstats`, but it looked like hinted handoff was happening, albeit slowly (a 100 MB file was getting replayed every 30 minutes according to logs, even though the node was decommissioned). The streaming for decommission must have completed, from the fact that logs on the node said it was DECOMMISSIONED?

Please let me know if you need any more logs.;;;","09/Mar/17 03:54;jjirsa;Definitely not 12281. I'm not sure how you're getting 3G of hints on 2G of data. The stack+both logs you uploaded were for the leaving node, yes? Had you recently decommissioned another node in the recent'ish past (before you decommissioned this node)? 


;;;","09/Mar/17 05:10;arijit91;The stack and ""logs"" were for a non-leaving node. The ""logs_decommissioned_node"" file was for the leaving node. If you look at the timestamps, you will see that on 06:04:33, the leaving node says DECOMMISSIONED, but the ""logs"" file shows hinted handoff occurring at 07:01:43. The host id in the hints file corresponds to that of the leaving node.

And you are correct! The cluster had a history of stopping Cassandra on nodes for a while before starting and running ""nodetool decommission"" on them. I believe this was done a few times before, and it caused the same condition described above at least twice. The nodes might have been down for several hours before the decommission.;;;","09/Mar/17 07:26;jjirsa;
{code}
Thread 28548: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.FutureTask.awaitDone(boolean, long) @bci=165, line=429 (Compiled frame)
 - java.util.concurrent.FutureTask.get() @bci=13, line=191 (Compiled frame)
 - org.apache.cassandra.hints.HintsDispatchExecutor.completeDispatchBlockingly(org.apache.cassandra.hints.HintsStore) @bci=22, line=112 (Interpreted frame)
 - org.apache.cassandra.hints.HintsService.excise(java.util.UUID) @bci=75, line=323 (Interpreted frame)
 - org.apache.cassandra.service.StorageService.excise(java.util.Collection, java.net.InetAddress) @bci=35, line=2229 (Interpreted frame)
 - org.apache.cassandra.service.StorageService.excise(java.util.Collection, java.net.InetAddress, long) @bci=9, line=2242 (Interpreted frame)
 - org.apache.cassandra.service.StorageService.handleStateLeft(java.net.InetAddress, java.lang.String[]) @bci=58, line=2146 (Interpreted frame)
 - java.util.concurrent.ConcurrentHashMap.get(java.lang.Object) @bci=1, line=936 (Compiled frame)
 - org.apache.cassandra.gms.Gossiper.getEndpointStateForEndpoint(java.net.InetAddress) @bci=5, line=817 (Compiled frame)
 - org.apache.cassandra.service.StorageService.onChange(java.net.InetAddress, org.apache.cassandra.gms.ApplicationState, org.apache.cassandra.gms.VersionedValue) @bci=418, line=1685 (Compiled frame)
 - org.apache.cassandra.gms.Gossiper.doOnChangeNotifications(java.net.InetAddress, org.apache.cassandra.gms.ApplicationState, org.apache.cassandra.gms.VersionedValue) @bci=38, line=1200 (Compiled frame)
 - org.apache.cassandra.gms.Gossiper.applyNewStates(java.net.InetAddress, org.apache.cassandra.gms.EndpointState, org.apache.cassandra.gms.EndpointState) @bci=164, line=1183 (Compiled frame)
 - org.apache.cassandra.gms.Gossiper.applyStateLocally(java.util.Map) @bci=366, line=1146 (Compiled frame)
 - org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(org.apache.cassandra.net.MessageIn, int) @bci=143, line=58 (Compiled frame)
 - org.apache.cassandra.net.MessageDeliveryTask.run() @bci=82, line=67 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=745 (Compiled frame)
{code}

{{excise}} [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L287-L327] attempts to complete the running dispatch if it exists (for example, if the host was just down, but came up, and hint delivery is in progress), even though that endpoint is going away (was just decom'd). 

[~iamaleksey] - I'm not very familiar with this code - are we really gaining much from this? Do we need to block trying to deliver hints we know aren't going to be deliverable, risking getting into this situation where we're blocking waiting for {{isHostAlive()}} to finally fail (which won't happen if Gossip is blocked and thus FD won't kick in), when the very next thing we do is {{exciseStore()}}? 

;;;","09/Mar/17 14:07;aleksey;We don't need to. I guess reusing {{completeDispatchBlockingly}} there was chosen as an option to simplify dealing with leftovers, to avoid the race between hints still replaying and dropping the files for the departing node.

What we minimally need to do is to cancel blockingly - rather than wait for completion - and then remove the leftovers (excise).;;;","11/Mar/17 01:55;arijit91;My workaround for now is to delete hint files for a node before starting Cassandra and running ""nodetool decommission"" on it (since it is taking quite long). Does that sound legitimate?;;;","17/Mar/17 20:42;jjirsa;So we'll just grab the future, cancel if we're running, remove it from the map of tasks, and let the cleanup continue:

|| branch || utests || dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13308] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13308-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13308-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13308] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13308-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13308-dtest/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13308] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-13308-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-13308-dtest/] |

New dtest demonstrating this failure mode is @ https://github.com/jeffjirsa/cassandra-dtest/tree/hint-gossip (and dtests linked in this branch have been started against this dtest repo/branch). 

;;;","19/Apr/17 13:33;aleksey;- {{HintsDispatchExecutor.interruptDispatch()}} only uses the {{hostId}} field of the passed {{HintsStore}} instance, so we might as well just pass the host id directly
- in the same method, you should replace {{scheduledDispatches.get()}} call with a call to {{remove()}}, thus eliminating a redundant {{remove()}} later down the line.
- no need for the racy {{isDone()}} check either, it doesn't save us anything

So, ultimately, just

{code}
    void interruptDispatch(UUID hostId)
    {
        Future future = scheduledDispatches.remove(hostId);
        if (null != future)
            future.cancel(true);
    }
{code}

should be enough.

But these are nits, can address on commit. LGTM overall, +1.;;;","19/Apr/17 16:02;jjirsa;Thanks Aleksey. Committed with nits as [5089e74ef4a0eaeb1c439d57f074de1c496421f2|https://github.com/apache/cassandra/commit/5089e74ef4a0eaeb1c439d57f074de1c496421f2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The specification of protocol version in cqlsh means the python driver doesn't automatically downgrade protocol version.,CASSANDRA-13307,13049086,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mbyrd,mbyrd,mbyrd,07/Mar/17 23:51,15/May/20 08:03,14/Jul/23 05:56,19/Apr/17 06:21,3.11.0,4.0,4.0-alpha1,,,,Legacy/Tools,,,,,0,doc-impacting,,,,"Hi,
Looks like we've regressed on the issue described in:
https://issues.apache.org/jira/browse/CASSANDRA-9467
In that we're no longer able to connect from newer cqlsh versions
(e.g trunk) to older versions of Cassandra with a lower version of the protocol (e.g 2.1 with protocol version 3)

The problem seems to be that we're relying on the ability for the client to automatically downgrade protocol version implemented in Cassandra here:
https://issues.apache.org/jira/browse/CASSANDRA-12838
and utilised in the python client here:
https://datastax-oss.atlassian.net/browse/PYTHON-240

The problem however comes when we implemented:
https://datastax-oss.atlassian.net/browse/PYTHON-537
""Don't downgrade protocol version if explicitly set"" 
(included when we bumped from 3.5.0 to 3.7.0 of the python driver as part of fixing: https://issues.apache.org/jira/browse/CASSANDRA-11534)

Since we do explicitly specify the protocol version in the bin/cqlsh.py.

I've got a patch which just adds an option to explicitly specify the protocol version (for those who want to do that) and then otherwise defaults to not setting the protocol version, i.e using the protocol version from the client which we ship, which should by default be the same protocol as the server.
Then it should downgrade gracefully as was intended. 
Let me know if that seems reasonable.
Thanks,
Matt
",,colinkuo,githubbot,jeromatron,jjirsa,jkni,mbyrd,mck,sludwig,tjake,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13311,,,,,,,,,,,CASSANDRA-9467,CASSANDRA-12150,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mbyrd,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 18:40:53 UTC 2017,,,,,,,,,,"0|i3b37b:",9223372036854775807,3.11.x,,,,,,mck,,mck,,,Low,,3.10,,,,,,,,,,,,,,,,,"08/Mar/17 22:06;githubbot;GitHub user Jollyplum opened a pull request:

    https://github.com/apache/cassandra/pull/96

    Fix cqlsh automatic protocol downgrade regression CASSANDRA-13307

    Patch by Matt Byrd for CASSANDRA-13307

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Jollyplum/cassandra 13307

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/96.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #96
    
----
commit b52b27810bf0d3bb9caafe21fde6120cf53c7382
Author: Matt Byrd <matthew_byrd@apple.com>
Date:   2017-03-08T21:55:01Z

    Fix cqlsh automatic protocol downgrade regression
    Patch by Matt Byrd for CASSANDRA-13307

----
;;;","08/Mar/17 22:09;mbyrd;https://github.com/Jollyplum/cassandra/commit/b52b27810bf0d3bb9caafe21fde6120cf53c7382
https://github.com/apache/cassandra/pull/96
https://github.com/Jollyplum/cassandra/tree/13307;;;","08/Mar/17 22:20;jjirsa;[~tjake] - since this was on your mind having recently opened CASSANDRA-13311 , any chance you're anxious to review? 
;;;","29/Mar/17 18:36;mbyrd;Hey [~tjake] are you at all keen to review? or shall I see if someone else can?
Thanks;;;","05/Apr/17 07:41;mck;Am taking a look at this ([~tjake] you're too slow and the Caribbean is no excuse :-)

I'm not sure if there's a jira field i'm supposed put my name in as a/the reviewer, [~jjirsa]?;;;","05/Apr/17 14:12;jkni;There's one you can set through the Edit button, if you scroll down. If you don't have permissions to access/edit that somehow, come complain in #cassandra-dev on IRC.

Thanks for volunteering to review!;;;","09/Apr/17 01:00;mck;+1
The code looks good, makes sense to me, and been manually tested.

Test results are 
|| Branch || Unit Tests || DTests ||
| [trunk|https://github.com/apache/cassandra/pull/96/commits/c36a4e5547af3967976144f7b553d70873503f77] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/3] \\ [circleci|https://circleci.com/gh/michaelsembwever/cassandra/3] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/7/] |

^i don't have access to put builds/branches on cassci, and i understand from [this|http://cassci.datastax.com/userContent/cassci-usage.html], and the mailing list, no one will be added in lei of waiting for the new ASF build infra.

For the meantime, the unit tests all pass on my machine.;;;","09/Apr/17 12:19;mck;{quote} ^i don't have access to put builds/branches on cassci, and i understand from this, and the mailing list, no one will be added in lei of waiting for the new ASF build infra.{quote}

Have added unit test builds to both asf jenkins and circle. The former passed.;;;","12/Apr/17 06:07;jjirsa;[~mck] you able to commit? Or easier for me to push it for you?
;;;","12/Apr/17 10:05;mck;{quote}mck you able to commit? Or easier for me to push it for you?{quote}

Hey [~jjirsa], first up trunk is broken so i'm hesitant to push to it. Secondly it looks like this patch breaks a number of dtests, and it's taking a ""little"" time verifying it one way or another…;;;","12/Apr/17 10:08;mck;A number of dtests _appear_ broken, [~mbyrd] are you able to check any these failures?

 - bootstrap_test.TestBootstrap.simultaneous_bootstrap_test
 - paging_test.TestPagingWithDeletions.test_ttl_deletions
 - paxos_tests.TestPaxos.contention_test_many_threads
 - secondary_indexes_test.TestPreJoinCallback.manual_join_test
 - secondary_indexes_test.TestPreJoinCallback.manual_join_test

More info on running dtests is at http://cassandra.apache.org/doc/latest/development/testing.html#dtests;;;","12/Apr/17 12:30;mck;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/7/testReport/
seems to indicate most of the tests are just flakey, and of those that are not (ie ""Failed 1 times in the last 7 runs"") pass locally.

;;;","13/Apr/17 21:34;mbyrd;Hey [~mck] Did you still want me to take a look? 
sounds like the failures can be explained by flakiness? ;;;","14/Apr/17 06:30;mck;{quote}Hey mck Did you still want me to take a look? {quote}
No [~mbyrd], flakiness on that particular build configuration on the asf jenkins is to blame.

I will push the commit as soon as trunk it green again.;;;","18/Apr/17 07:55;mck;Committed to trunk.

[~tjake], ([~jjirsa]), i missed that you marked this for 3.11.x. Would you like me to commit it to the cassandra-3.11 branch as well? The patch applies cleanly there.;;;","19/Apr/17 03:01;tjake;yes please;;;","19/Apr/17 06:15;mck;
|| Branch || Unit Tests || DTests ||
| [3.11.x|https://github.com/michaelsembwever/cassandra/commit/32835b0919c5d89b565f0adff15a845fe392c270] | [circleci|https://circleci.com/gh/michaelsembwever/cassandra/14] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/24] |
| [trunk|https://github.com/apache/cassandra/pull/96/commits/c36a4e5547af3967976144f7b553d70873503f77] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/3] \\ [circleci|https://circleci.com/gh/michaelsembwever/cassandra/3] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/15/] |
;;;","19/Apr/17 06:21;mck;committed now in both cassandra-3.11 branch and trunk.;;;","29/Jun/17 21:30;githubbot;Github user Jollyplum commented on the issue:

    https://github.com/apache/cassandra/pull/96
  
    (committed elsewhere)
;;;","29/Jun/17 21:30;githubbot;Github user Jollyplum closed the pull request at:

    https://github.com/apache/cassandra/pull/96
;;;","10/Nov/17 18:40;sludwig;Is this fix already in a version available via pip from https://pypi.python.org/pypi/cqlsh/ ? If not, can you publish it there?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Builds fetch source jars for build dependencies, not just source dependencies",CASSANDRA-13306,13048962,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,jkni,jkni,07/Mar/17 18:00,07/Mar/23 11:52,14/Jul/23 05:56,16/Mar/17 04:51,4.0,4.0-alpha1,,,,,Build,,,,,0,,,,,"A recent commit without a linked JIRA cleaned up dead imports and also added a {{sourcesFilesetId}} to artifact fetching for the build-deps-pom. This causes ant to fetch source jars for the build deps, but we have an explicit separate build-deps-pom-sources that fetches sources.

This happened in commit {{e96ce6d132129025ff6b923129cb67eed2f97931}}.

Was this an intentional change, [~dbrosius]? It seems to conflate the separate build-deps-pom and build-deps-pom-sources.",,dbrosius,jjirsa,jkni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 04:50:52 UTC 2017,,,,,,,,,,"0|i3b2fr:",9223372036854775807,,,,,,,dbrosius@apache.org,,dbrosius@apache.org,,,Normal,,5.0,,,,,,,,,,,,,,,,,"08/Mar/17 00:29;dbrosius;agreed, that can be reverted.;;;","16/Mar/17 04:50;jkni;Thanks! I ran a round of CI and tested builds with empty and populated .m2.

Committed to trunk as {{fe08463c3b7135a0f1b121bb0d148c80b8c7e123}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slice.isEmpty() returns false for some empty slices,CASSANDRA-13305,13048799,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Mar/17 08:57,16/Apr/19 09:30,14/Jul/23 05:56,09/Mar/17 11:01,3.0.13,3.11.0,,,,,,,,,,0,,,,,"{{Slice.isEmpty}} is currently defined as {{comparator.compare(end, start) < 0}} but this shouldn't be a strict inequality. Indeed, the way {{Slice.Bound}} is defined, having a start equal to an end implies a range like {{[1, 1)}}, but that range is definitively empty and something we shouldn't let in as that would break merging and other range tombstone related code.

In practice, if you can currently insert such empty range (with something like {{DELETE FROM t WHERE k = 'foo' AND i >= 1 AND i < 1}}), and that can trigger assertions in {{RangeTomstoneList}} (and possibly other problem).",,blambov,christianmovi,christopher.lambert,JoshuaMcKenzie,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 09 11:01:13 UTC 2017,,,,,,,,,,"0|i3b1kv:",9223372036854775807,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"07/Mar/17 09:06;slebresne;Patch is trivial, but attaching a unit test to demonstrate the problem too.

| [13305-3.0|https://github.com/pcmanus/cassandra/commits/13305-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13305-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13305-3.0-dtest] |
| [13305-3.11|https://github.com/pcmanus/cassandra/commits/13305-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13305-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13305-3.11-dtest] |
;;;","07/Mar/17 17:11;JoshuaMcKenzie;[~blambov] to review;;;","08/Mar/17 12:38;blambov;The patch looks good, but the failures in {{RowsTest}} on 3.0 appear to be new. Could you verify that they are not genuine regressions?;;;","08/Mar/17 14:39;slebresne;Yes, saw that, and that's a bit weird. Ran the tests locally quite a few times with no failure, and as importantly, I'm reasonably confident the failing tests do not use {{Slice.isEmpty}} (which is the only method the patch modifies) in any way, since that latter method is only used in {{ModificationStatement}} and {{RangeTombstoneList}}, but the failing tests are low-level Row tests that relatively clearly don't use these.
Anyway, I've re-triggered a CI run on 3.0 to make sure this isn't a CI thing, but if it's clean, I'll probably just go ahead and commit unless you have reason to think there is something genuinely going on here.;;;","09/Mar/17 07:18;blambov;Second run is clean, marking ready to commit.;;;","09/Mar/17 11:01;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
last row of previous page == first row of next page while querying data using SASI index,CASSANDRA-13302,13048565,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,andrew.tolbert,andrew.tolbert,06/Mar/17 16:03,01/Aug/21 11:18,14/Jul/23 05:56,12/May/17 07:53,3.11.7,4.0,4.0-alpha1,,,,,,,,,0,,,,,"Apologies if this is a duplicate (couldn't track down an existing bug).

Similarly to [CASSANDRA-11208], it appears it is possible to retrieve duplicate rows when paging using a SASI index as documented in [JAVA-1413|https://datastax-oss.atlassian.net/browse/JAVA-1413], the following test demonstrates that data is repeated while querying using a SASI index:

{code:java}
public class TestPagingBug
{
	public static void main(String[] args)
	{
		Cluster.Builder builder = Cluster.builder();
		Cluster c = builder.addContactPoints(""192.168.98.190"").build();		
		Session s = c.connect();
		
		s.execute(""CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 }"");
		s.execute(""CREATE TABLE IF NOT EXISTS test.test_table_sec(sec BIGINT PRIMARY KEY, id INT)"");
                //create secondary index on ID column, used for select statement
                String index = ""CREATE CUSTOM INDEX test_table_sec_idx ON test.test_table_sec (id) USING 'org.apache.cassandra.index.sasi.SASIIndex' ""
                + ""WITH OPTIONS = { 'mode': 'PREFIX' }"";
                s.execute(index);
		
		PreparedStatement insert = s.prepare(""INSERT INTO test.test_table_sec (id, sec) VALUES (1, ?)"");		
		for (int i = 0; i < 1000; i++)
			s.execute(insert.bind((long) i));
		
		PreparedStatement select = s.prepare(""SELECT sec FROM test.test_table_sec WHERE id = 1"");
		
		long lastSec = -1;		
		for (Row row : s.execute(select.bind().setFetchSize(300)))
		{
			long sec = row.getLong(""sec"");
			if (sec == lastSec)
				System.out.println(String.format(""Duplicated id %d"", sec));
			
			lastSec = sec;
		}
		System.exit(0);
	}
}
{code}

The program outputs the following:

{noformat}
Duplicated id 23
Duplicated id 192
Duplicated id 684
{noformat}

Note that the simple primary key is required to reproduce this.",Tested with C* 3.9 and 3.10.,adelapena,adutra,andrew.tolbert,blind_oracle,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13379,,,,,,CASSANDRA-11208,,,,,,,CASSANDRA-13332,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 07:53:35 UTC 2017,,,,,,,,,,"0|i3b053:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,,,,,,,,,,,,,,,,,,"06/Apr/17 15:06;ifesdjeen;Attaching a simple patch and test reproducing the problem with paging. It is happening only in tables without clustering, because of the left bound inclusion on subsequent pages.

|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13379-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13379-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13379-trunk-dtest/]|;;;","10/Apr/17 13:47;adelapena;The patch looks good to me, +1. There are some unused imports in {{SASICQLTest}} that can be removed while committing the patch.;;;","12/May/17 07:53;ifesdjeen;Thank you for the review! I've fixed the imports on commit as you have suggested. 

Committed to 3.11 with [9723db27171c014911a8a3f2b0db9454a91dd936|https://github.com/apache/cassandra/commit/9723db27171c014911a8a3f2b0db9454a91dd936] and merged up to [trunk|https://github.com/apache/cassandra/commit/c7b77eb60b56ec612622809fa871919cacd91a43];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataOutputBuffer.asNewBuffer broken,CASSANDRA-13298,13048358,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,05/Mar/17 12:43,16/Apr/19 09:30,14/Jul/23 05:56,28/Mar/17 07:50,3.11.0,,,,,,,,,,,0,,,,,"The implementation of {{DataOutputBuffer.asNewBuffer()}} reuses the underlying {{ByteBuffer}} array. This is probably not an issue, but it is definitely incorrect and may lead to incorrect/overwritten information returned by {{SystemKeyspace.truncationAsMapEntry}}.",,jeromatron,jjirsa,snazy,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 07:50:42 UTC 2017,,,,,,,,,,"0|i3ayv3:",9223372036854775807,,,,,,,stefania,,stefania,,,Low,,,,,,,,,,,,,,,,,,,"05/Mar/17 12:45;snazy;||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13298-asNewBuffer-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13298-asNewBuffer-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13298-asNewBuffer-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13298-asNewBuffer-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13298-asNewBuffer-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13298-asNewBuffer-trunk-dtest/lastSuccessfulBuild/]
;;;","28/Mar/17 07:14;stefania;+1, nice catch!;;;","28/Mar/17 07:50;snazy;Thanks!
Committed as [62abe46c5efc47812899219f2e38af94c34fa49a |https://github.com/apache/cassandra/commit/62abe46c5efc47812899219f2e38af94c34fa49a] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible data loss on upgrade 2.1 - 3.0,CASSANDRA-13294,13047968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stefania,marcuse,marcuse,03/Mar/17 10:03,16/Apr/19 09:30,14/Jul/23 05:56,07/Mar/17 01:22,3.0.12,3.11.0,,,,,Legacy/Local Write-Read Paths,,,,,1,,,,,"After finishing a compaction we delete the compacted away files. This is done [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/lifecycle/LogFile.java#L328-L337] which uses [this|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/lifecycle/LogRecord.java#L265-L271] to get the files - we get all files starting with {{absoluteFilePath}}. Absolute file path is generated [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L142-L153]. For 3.0 version files the filename looks like this: {{/blabla/keyspace1/standard1-bdb031c0ff7b11e6940fdd0479dd8912/mc-1332-big}} but for 2.1 version files, they look like this: {{/blabla/keyspace1/standard1-bdb031c0ff7b11e6940fdd0479dd8912/keyspace1-standard1-ka-2}}.

The problem is then that if we were to finish a compaction including the legacy file, we would actually delete all legacy files having a generation starting with '2'",,colinkuo,flightc,jane.deng@datastax.com,jeromatron,jjirsa,jjordan,marcuse,mshuler,rha,stefania,vinaykumarcse,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 04:41:18 UTC 2017,,,,,,,,,,"0|i3awi7:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Critical,,,,,,,,,,,,,,,,,,,"03/Mar/17 17:20;marcuse;patch that appends the separator when getting sstables: https://github.com/krummas/cassandra/commits/marcuse/13294
http://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-13294-testall/
http://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-13294-dtest/

Needs upgrade dtest as well, will work on that;;;","04/Mar/17 00:10;weideng;If this problem is caused by the file name change, would we need to port to 2.2 as well, because the file name change was introduced in 2.2?;;;","04/Mar/17 08:17;marcuse;It was not caused by the filename change, it was caused by CASSANDRA-7066 / CASSANDRA-10421

And it does not affect trunk since we don't support reading 2.1 files there;;;","06/Mar/17 02:34;stefania;I think we still have a problem with the proposed patch because for legacy files it can also get the wrong [record files|https://github.com/stef1927/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/lifecycle/LogFile.java#L368], which would cause an incorrect classification of files in a folder, for example by considering unrelated sstable files as temporary.

Therefore I've added the descriptor separator to the absolute path. However, we must be careful not to write it to the txn files, to make sure that we can still upgrade from 3.0.x versions onward. In case of a non clean shutdown, there could be some txn files from the old version to be processed on startup by the upgraded version, I think it's easier not to write the additional separator rather than handling parsing and checksum differently. This way we can simply avoid committing this patch to trunk and carry on without the additional separator since it is not required for 3.0+ files.

Revised patch is here,  what do you think [~krummas]?

||3.0||3.11||
|[patch|https://github.com/stef1927/cassandra/tree/13294-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13294-3.11]|
|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13294-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13294-3.11-testall/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13294-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13294-3.11-dtest/]|
;;;","06/Mar/17 13:41;marcuse;+1 on the patch and [here|https://github.com/riptano/cassandra-dtest/pull/1449] is an upgrade dtest which reproduces this;;;","07/Mar/17 01:22;stefania;Thank you for the review and the test. 

Committed to 3.0 as {{1ba68a1e5d681c091e2c53e7720029f10591e7ef}} and merged into 3.11. Then merged into trunk with {{-s ours}}.;;;","07/Mar/17 04:41;mshuler;Thanks all! I started all the cassandra-3.0 branch tests on commit {{1ba68a1}} and will check on them in the morning for a look at a release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog replay may fail if last mutation is within 4 bytes of end of segment,CASSANDRA-13282,13047190,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,01/Mar/17 05:02,15/May/20 08:06,14/Jul/23 05:56,13/Mar/17 05:01,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,Legacy/Core,,,,,0,,,,,"Following CASSANDRA-9749 , stricter correctness checks on commitlog replay can incorrectly detect ""corrupt segments"" and stop commitlog replay (and potentially stop cassandra, depending on the configured policy). In {{CommitlogReplayer#replaySyncSection}} we try to read a 4 byte int {{serializedSize}}, and if it's 0 (which will happen due to zeroing when the segment was created), we continue on to the next segment. However, it appears that if a mutation is sized such that it ends with 1, 2, or 3 bytes remaining in the segment, we'll pass the {{isEOF}} on the while loop but fail to read the {{serializedSize}} int, and fail. ",,aleksey,blambov,jay.zhuang,jjirsa,JoshuaMcKenzie,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 19:30;jjirsa;whiteboard.png;https://issues.apache.org/jira/secure/attachment/12855676/whiteboard.png",,,,,,,,,,,,,,,,,,,1.0,jjirsa,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 05:01:43 UTC 2017,,,,,,,,,,"0|i3arpj:",9223372036854775807,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"02/Mar/17 19:23;jjirsa;Will kick off CI in a bit, not going to do all 8 tests at once because I don't want to monopolize cassci, but links should be accurate once tests start. 

|| Branch || Unit Tests || DTests ||
| [2.2|https://github.com/jeffjirsa/cassandra/commits/cassandra-2.2-13282] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-2.2-13282-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-2.2-13282-dtest/] |
| [3.0|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.0-13282] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13282-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13282-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.11-13282] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13282-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13282-dtest/] |
| [trunk|https://github.com/jeffjirsa/cassandra/commits/cassandra-13282] | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-13282-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-13282-dtest/] |
;;;","02/Mar/17 19:33;jjirsa;Attaching a drawing for whichever reviewer wants this ticket - drawing created while discussing this offline because it's somewhat nuanced apparently. Basically when we allocate in {{sync()}}, if we're at the end of a file, we return -1, and then the end marker for the segment gets set to the end of the file. Therefore within the while loop as we replay an individual sync section, we can get to a point where we throw trying to read an int from the unused tail of the section. ;;;","07/Mar/17 14:03;JoshuaMcKenzie;[~blambov] to review.;;;","08/Mar/17 13:15;blambov;Patch LGTM, but I want to make sure this isn't showing a more serious problem. Is an upgrade involved in triggering this issue?

In 2.2+ we write the exact end of written data ({{endOfBuffer}}) on {{sync()}} and sync sections are thus always precisely sized, thus this should not be happening. We could end up in this situation upgrading from 2.1 and earlier, though. Perhaps we should only ignore this if the descriptor version is pre-2.2?;;;","08/Mar/17 14:46;jjirsa;[~blambov] It does appear to require 2.1 commitlog segments, yes. I'm not sure it's worth adding a descriptor guard onto it - it should be safe in both situations, though (as you mention) less meaningful in 2.2+. I'm not opposed to it, but I'm also not convinced it's necessary. Will defer to your opinion on that.;;;","09/Mar/17 07:21;blambov;Not adding a guard is fine. I would change the comment a little, though, to mention such sections come from 2.1 and earlier.;;;","13/Mar/17 05:01;jjirsa;Committed to 2.2 as [beb9658dd5e18e3a6a4e8431b6549ae4c33365a9|https://github.com/apache/cassandra/commit/beb9658dd5e18e3a6a4e8431b6549ae4c33365a9] and merged up to trunk with the slightly more verbose comment to clarify it's from 2.1 era commitlog segments. 

Thanks for the quick review, [~blambov] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update build.xml and build.properties.default maven repos,CASSANDRA-13278,13046760,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,mshuler,mshuler,27/Feb/17 21:05,15/May/20 08:04,14/Jul/23 05:56,01/Mar/17 21:51,2.1.18,2.2.10,3.0.12,3.11.0,4.0,4.0-alpha1,Build,,,,,0,lhf,,,,"Only 2 of the 5 urls in build.properties.default are currently valid.

java.net2, jclouds, and oauth urls all 404.

{noformat}
$ git grep remoteRepository
build.properties.default:artifact.remoteRepository.central:     http://repo1.maven.org/maven2
build.properties.default:artifact.remoteRepository.java.net2:   http://download.java.net/maven/2
build.properties.default:artifact.remoteRepository.apache:      https://repository.apache.org/content/repositories/releases
build.properties.default:artifact.remoteRepository.jclouds:     http://jclouds.googlecode.com/svn/repo
build.properties.default:artifact.remoteRepository.oauth:       http://oauth.googlecode.com/svn/code/maven
build.xml:      <artifact:remoteRepository id=""central""   url=""${artifact.remoteRepository.central}""/>
build.xml:      <artifact:remoteRepository id=""java.net2"" url=""${artifact.remoteRepository.java.net2}""/>
build.xml:      <artifact:remoteRepository id=""apache""    url=""${artifact.remoteRepository.apache}""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:          <remoteRepository refid=""apache""/>
build.xml:          <remoteRepository refid=""java.net2""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:          <remoteRepository refid=""apache""/>
build.xml:          <remoteRepository refid=""java.net2""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:        <remoteRepository refid=""apache""/>
build.xml:        <remoteRepository refid=""central""/>
build.xml:        <remoteRepository refid=""oauth""/>
{noformat}",,christopher.lambert,jjirsa,mshuler,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 21:51:27 UTC 2017,,,,,,,,,,"0|i3ap27:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"01/Mar/17 13:39;snazy;Also fixed an issue that 3.0, 3.11 and trunk builds fail with an empty maven repo, because the ow2 repository, which is referenced from byteman-root.pom, seems to no longer contains byteman artifacts ([BYTEMAN-337|https://issues.jboss.org/browse/BYTEMAN-337]). Fixed by adding {{byteman-install}} dependency before all other byteman deps.

||cassandra-2.1|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...snazy:13278-remove-repos-2.1]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-2.1-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-2.1-dtest/lastSuccessfulBuild/]
||cassandra-2.2|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...snazy:13278-remove-repos-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-2.2-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-2.2-dtest/lastSuccessfulBuild/]
||cassandra-3.0|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...snazy:13278-remove-repos-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-3.0-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-3.0-dtest/lastSuccessfulBuild/]
||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13278-remove-repos-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13278-remove-repos-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13278-remove-repos-trunk-dtest/lastSuccessfulBuild/]

CI triggered;;;","01/Mar/17 17:52;mshuler;+1
Thanks!;;;","01/Mar/17 21:51;jjirsa;Thanks guys. Committing  per conversation in IRC with [~mshuler] so I can rebase other patches and get CI working again. Committed as {{301f7c5b7bfbb73ecbfb65a6ac7b393a4d968e78}} and merged with patches provided from 2.1 through trunk. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate results with secondary index on static column,CASSANDRA-13277,13046699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,rha,rha,27/Feb/17 18:06,15/May/20 07:59,14/Jul/23 05:56,28/Mar/17 11:24,3.11.0,4.0,4.0-alpha1,,,,Feature/2i Index,Legacy/Local Write-Read Paths,,,,0,2i,,,,"As a follow up of http://www.mail-archive.com/user@cassandra.apache.org/msg50816.html 

Duplicate results appear with secondary index on static column with RF > 1.
Number of results vary depending on consistency level.

Here is a CCM session to reproduce the issue:
{code}
romain@debian:~$ ccm create 39 -n 3 -v 3.9 -s
Current cluster is now: 39
romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(2 rows)
cqlsh> CONSISTENCY ALL 
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(3 rows)
{code}

When RF matches the number of nodes, it works as expected.

Example with RF=3 and 3 nodes:
{code}
romain@debian:~$ ccm create 39 -n 3 -v 3.9 -s
Current cluster is now: 39

romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.

cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
cqlsh> CONSISTENCY all
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
{code}

Example with RF = 2 and 2 nodes:

{code}
romain@debian:~$ ccm create 39 -n 2 -v 3.9 -s
Current cluster is now: 39
romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
cqlsh> CONSISTENCY ALL 
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
{code}",,adelapena,blerer,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 11:24:17 UTC 2017,,,,,,,,,,"0|i3aoon:",9223372036854775807,3.9,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"24/Mar/17 13:06;adelapena;The underlying problem can be reproduced with a single node:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

CREATE TABLE k.c (
	pk int, 
	ck int, 
	sc int static
	primary key (pk, ck)
);

CREATE index ON k.c (sc);

INSERT INTO k.c (pk, ck, sc) values (1, 2, 3);
INSERT INTO k.c (pk, ck, sc) values (-1, 2, 3);

SELECT token(pk), pk, ck, sc FROM k.c where sc = 3 AND token(pk) > 0;
 system.token(pk)     | pk | ck | sc
----------------------+----+----+----
 -4069959284402364209 |  1 |  2 |  3
  7297452126230313552 | -1 |  2 |  3

SELECT token(pk), pk, ck, sc FROM k.c where sc = 3 AND token(pk) <= 0;
 system.token(pk)     | pk | ck | sc
----------------------+----+----+----
 -4069959284402364209 |  1 |  2 |  3
  7297452126230313552 | -1 |  2 |  3
{code}
This is produced because {{CompositesSearcher}} doesn't verify that index hits satisfy command's key constraint when dealing with static columns, as it is done with regular columns.

The provided examples don't specify key restrictions but they fail when RF is lesser than the number of nodes because they are internally split into subqueries directed to specific token ranges. Replicas ignore the token range restriction and the coordinator receives duplicate rows from unexpected token ranges, as it is shown in the previous example.

An initial version of the patch can be found here.
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13277-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13277-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13277-trunk-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13277-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13277-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13277-3.11-dtest/]|;;;","28/Mar/17 10:07;blerer;Nice finding! The patch looks good. Thanks.;;;","28/Mar/17 11:24;blerer;Comitted into 3.11 at 41befde2273724e2070a28cd6c47a407e3e4426a and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression on CASSANDRA-11416: can't load snapshots of tables with dropped columns,CASSANDRA-13276,13046696,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,matt.kopit,matt.kopit,27/Feb/17 17:46,15/May/20 07:59,14/Jul/23 05:56,20/Apr/17 16:00,3.0.14,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,"I'm running Cassandra 3.10 and running into the exact same issue described in CASSANDRA-11416: 

1. A table is created with columns 'a' and 'b'
2. Data is written to the table
3. Drop column 'b'
4. Take a snapshot
5. Drop the table
6. Run the snapshot schema.cql to recreate the table and the run the alter
7. Try to restore the snapshot data using sstableloader

sstableloader yields the error:
java.lang.RuntimeException: Unknown column b during deserialization",,adelapena,ifesdjeen,matt.kopit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 12:36:56 UTC 2017,,,,,,,,,,"0|i3aonz:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"06/Apr/17 12:17;adelapena;It seems that {{NativeSSTableLoaderClient.createTableMetadata}} creates table metadata without taking dropped columns into account. Working in a patch.;;;","10/Apr/17 21:28;matt.kopit;Thanks Andrés. Will this be patched for Cassandra ver. 3.0.x, too?;;;","11/Apr/17 11:14;adelapena;Here is the patch for 3.0.x, 3.x and trunk:

||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13276-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13276-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13276-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13276-trunk-dtest/]|

And a new dtest can be found [here|https://github.com/riptano/cassandra-dtest/compare/master...adelapena:CASSANDRA-13276].

Thanks for reporting the bug.;;;","20/Apr/17 09:12;ifesdjeen;+1, the patch looks good!;;;","20/Apr/17 12:36;adelapena;Committed:

||3.0|[175e4f8ce868ea04a5e11a8d5212d8c397ea0d12|https://github.com/apache/cassandra/commit/175e4f8ce868ea04a5e11a8d5212d8c397ea0d12]||
||3.11|[42904c65381d69351b130f64f9b2ba2425513a04|https://github.com/apache/cassandra/commit/42904c65381d69351b130f64f9b2ba2425513a04]||
||trunk|[b687641f78c85f266bae2475da8622c06c39dd0f|https://github.com/apache/cassandra/commit/b687641f78c85f266bae2475da8622c06c39dd0f]||

Thanks for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra throws an exception during CQL select query filtering on map key,CASSANDRA-13275,13046651,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,abdelx,abdelx,27/Feb/17 14:35,16/Apr/19 09:30,14/Jul/23 05:56,12/May/17 07:40,,,,,,,Legacy/CQL,,,,,0,,,,,"Env: cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4

Using this table structure:
{code}CREATE TABLE mytable (
    mymap frozen<map<uuid, frozen<set<text>>>> PRIMARY KEY
)
{code}
Executing:
{code} select * from mytable where mymap contains key UUID;
{code}
Within cqlsh shows this message:

{code}
ServerError: java.lang.UnsupportedOperationException

system.log:
java.lang.UnsupportedOperationException: null
        at org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$ContainsRestriction.appendTo(SingleColumnRestriction.java:456) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet.values(PartitionKeySingleRestrictionSet.java:86) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getPartitionKeys(StatementRestrictions.java:585) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:474) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.statements.SelectStatement.getQuery(SelectStatement.java:262) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:227) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:76) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:219) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:204) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.9.jar:3.9]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}",,abdelx,blerer,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri May 12 07:40:41 UTC 2017,,,,,,,,,,"0|i3aodz:",9223372036854775807,3.9,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"18/Apr/17 17:28;ifesdjeen;Partition key filtering was introduced in [CASSANDRA-11031], although {{CONTAINS}} didn't trigger filtering, the read path was trying to convert {{CONTAINS}} restriction to bounds.

|[3.11|https://github.com/apache/cassandra/compare/3.11...ifesdjeen:13275-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13275-3.11-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13275-3.11-dtest/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13275-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13275-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13275-trunk-dtest/]|
|[dtest branch|https://github.com/riptano/cassandra-dtest/compare/master...ifesdjeen:13275-master]|

This is not applicable to 3.0 since we do not allow partition key filtering there.;;;","10/May/17 07:31;blerer;Thanks. The patch looks good to me.

I would just replace:
{code}
String idx = tableName + ""_c_idx"";
createIndex(""CREATE INDEX "" + idx + "" ON %s (c)"");
{code}

by 

{code}
String idx = createIndex(""CREATE INDEX ON %s (c)"");
{code}

in {{trunk}} (it does not work in 3.11). Which can be done on commit.;;;","12/May/17 07:40;ifesdjeen;Thank you for the review! I've made the change you suggested on commit, applied to trunk only.

Committed to 3.11 with [f0319c88fd9a0e70cd8900ba7431724285886f9e|https://github.com/apache/cassandra/commit/f0319c88fd9a0e70cd8900ba7431724285886f9e], merged up to [trunk|https://github.com/apache/cassandra/commit/501d0441c26a8b7f4266a423bde4cee7b0ac6f36];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix code to not exchange schema across major versions,CASSANDRA-13274,13046607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,27/Feb/17 10:47,16/Apr/19 09:30,14/Jul/23 05:56,28/Mar/17 07:49,3.0.13,3.11.0,,,,,,,,,,0,,,,,"A rolling upgrade from 3.* to 4.0 (messaging version {{11}}) unveils a regression caused by CASSANDRA-11128.

Generally, we store all possible options/attributes including the default values in the schema. This causes (expected) schema-version-mismatches during rolling upgrades and therefore we prevent schema pulls/pushes in this situation, which has been broken by CASSANDRA-11128.",,aleksey,jay.zhuang,jjirsa,slebresne,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10520,,,CASSANDRA-11128,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 07:49:22 UTC 2017,,,,,,,,,,"0|i3ao47:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Critical,,,,,,,,,,,,,,,,,,,"27/Feb/17 14:06;aleksey;Not sure I understand the issue.

Could you please be more clear regarding what is broken, what the expected behaviour is, and what is the behaviour you are observing?;;;","27/Feb/17 15:13;snazy;CASSANDRA-11138 sets the messaging-version for any node to {{Math.min(version, current_version}}. I.e. a newer node (aka C*4.0 using messaging-version 11) would appear as messaging-version 10 in 3.* nodes. That basically allows migration-tasks (schema pull/push) between different messaging versions.;;;","27/Feb/17 15:34;snazy;||cassandra-3.0|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...snazy:13274-rolling-upgrade-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-3.0-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-3.0-dtest/lastSuccessfulBuild/]
||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13274-rolling-upgrade-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13274-rolling-upgrade-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13274-rolling-upgrade-trunk-dtest/lastSuccessfulBuild/]

CI triggered.

Also tested a few CQL upgrade tests from ""legacy"" 3.0 to current trunk (to verify the upgrade issue locally) and this patch to trunk (to verify the fix).

Also added an entry to {{NEWS.txt}} in the trunk branch regarding rolling upgrades.

Reviewer hint: for {{rolling_upgrade_test}}, it's necessary to disable the {{nodetool upgradesstables }} (comment out {{node.nodetool('upgradesstables -a')}} in {{upgrade_through_versions_test.py}} line#446) invocation until CASSANDRA-13059 is fixed.;;;","27/Mar/17 13:50;slebresne;+1, but make sure to update NEWS file for trunk with the actual version this is going to make it in. Also, completely a nit, but for the changelog I'd probably rephrase to something like ""Fix code to not exchange schema across major versions"" as that's more precise and avoid referencing a version that doesn't exists yet.;;;","28/Mar/17 07:49;snazy;Thanks!
Committed as [6da41ed047ed5ef8a5d11e7a60e73dfeb129a72a|https://github.com/apache/cassandra/commit/6da41ed047ed5ef8a5d11e7a60e73dfeb129a72a] to [cassandra-3.0|https://github.com/apache/cassandra/tree/cassandra-3.0] and merged up to trunk.
Trunk commit contains the updated changes in NEWS.txt.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""nodetool bootstrap resume"" does not exit",CASSANDRA-13272,13046521,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Wimtie,tvdw,tvdw,27/Feb/17 01:10,15/May/20 08:01,14/Jul/23 05:56,14/Jul/17 09:49,2.2.11,3.0.15,3.11.1,4.0,4.0-alpha1,,Legacy/Streaming and Messaging,Local/Startup and Shutdown,,,,0,lhf,,,,"I have a script that calls ""nodetool bootstrap resume"" after a failed join (in my environment some streams sometimes fail due to mis-tuning of stream bandwidth settings). However, if the streams fail again, nodetool won't exit.

Last lines before it just hangs forever :

{noformat}
[2017-02-26 07:02:42,287] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-Data.db (progress: 1112%)
[2017-02-26 07:02:42,287] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-Data.db (progress: 1112%)
[2017-02-26 07:02:59,843] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12671-big-Data.db (progress: 1112%)
[2017-02-26 09:25:51,000] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:33:45,017] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:39:27,216] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:53:33,084] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:55:07,115] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 10:06:49,557] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 10:40:55,880] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 11:09:21,025] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 12:44:35,755] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 12:49:18,867] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 13:23:50,611] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 13:23:50,612] Stream failed
{noformat}

At that point (""Stream failed"") I would expect nodetool to exit with a non-zero exit code. Instead, it just wants me to ^C it.",,blerer,jjirsa,rha,tvdw,Wimtie,yogeshnachnani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Wimtie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 14 09:49:55 UTC 2017,,,,,,,,,,"0|i3anl3:",9223372036854775807,3.0.10,,,,,,blerer,,blerer,,,Normal,,2.2.0 beta 1,,,,,,,,,,,,,,,,,"29/May/17 14:38;Wimtie;{noformat}
diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java
index 9bc046f..d7c1aa5 100644
--- a/src/java/org/apache/cassandra/service/StorageService.java
+++ b/src/java/org/apache/cassandra/service/StorageService.java
@@ -1287,8 +1287,9 @@ public class StorageService extends NotificationBroadcasterSupport implements IE
                 @Override
                 public void onFailure(Throwable e)
                 {
-                    String message = ""Error during bootstrap: "" + e.getCause().getMessage();
-                    logger.error(message, e.getCause());
+                    Throwable cause = Throwables.getRootCause(e);
+                    String message = ""Error during bootstrap: "" + cause.getMessage();
+                    logger.error(message, cause);
                     progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.ERROR, 1, 1, message));
                     progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.COMPLETE, 1, 1, ""Resume bootstrap complete""));
                 }
diff --git a/src/java/org/apache/cassandra/utils/Throwables.java b/src/java/org/apache/cassandra/utils/Throwables.java
index 5ad9686..30fc9f4 100644
--- a/src/java/org/apache/cassandra/utils/Throwables.java
+++ b/src/java/org/apache/cassandra/utils/Throwables.java
@@ -30,6 +30,18 @@ import org.apache.cassandra.io.FSWriteError;

 public final class Throwables
 {
+    public static Throwable getRootCause(Throwable t)
+    {
+        Throwable cause = t.getCause();
+        if (cause == null) {
+            return t;
+        }
+        while (cause.getCause() != null) {
+            cause = cause.getCause();
+        }
+        return cause;
+    }
+
     public enum FileOpType { READ, WRITE }

     public interface DiscreteAction<E extends Exception>
{noformat};;;","05/Jul/17 15:48;blerer;Thanks for the patch.

I guess, based on the patch, that the problem came from the fact that {{e.getCause().getMessage()}} was causing a {{NPE}}?

Regarding the patch, could you add some javadoc and a unit test for {{Throwables::getRootCause}}.
Could you also attach the patch as a file?;;;","05/Jul/17 15:55;blerer;I just realize that your patch change the current behavior. The cause should be the cause of the Exception or the Exception itself not the root cause of the all chain.;;;","06/Jul/17 08:17;blerer;I had a look at the sources and I am not sure to understand the purpose of the original code. Everywhere else in {{StorageService}} the full chain of Exceptions is being logged.

If the wrapping Exception was created without a message it will automatically use the message of the wrapped exception. Due to that, we should probably use:
{code}
                @Override
                public void onFailure(Throwable e)
                {
                    String message = ""Error during bootstrap: "" + e.getMessage();
                    logger.error(message, e);
                    progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.ERROR, 1, 1, message));
                    progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.COMPLETE, 1, 1, ""Resume bootstrap complete""));
                }
{code}

[~yukim] Do you remember why you used the exception cause instead of the exception?;;;","07/Jul/17 06:20;yukim;I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.
I'm not sure what's causing its cause to be null, but it looks it can happen.
I think we need proper null handling there.;;;","10/Jul/17 09:28;blerer;bq. I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.
In this case, we should probably retrieve the cause only if {{e}} is an {{ExecutionException}} and the cause is not {{null}}. It should never be the case if {{e}} is an {{ExecutionException}} but better be safe than sorry.

[~Wimtie] could you update your patch? 
  ;;;","10/Jul/17 10:30;Wimtie;[~blerer], I'll update it to retrieve the cause in case of ExecutionException. It'll then pass that message back to the client, but I'll use the original throwable in the call to log.error, so we don't lose the original stacktrace in the logs.

Cheerio,

Tim;;;","10/Jul/17 11:49;Wimtie;{noformat}
From 1fd3c4eef678eda01f1f33c95760c1976675455f Mon Sep 17 00:00:00 2001
From: Timothy George Lamballais Tessensohn <timothy.lamballais@booking.com>
Date: Mon, 10 Jul 2017 13:47:19 +0200
Subject: [PATCH] fix NPE when stream fails

https://issues.apache.org/jira/browse/CASSANDRA-13272
---
 src/java/org/apache/cassandra/service/StorageService.java | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java
index 143b402d78..919df0cda3 100644
--- a/src/java/org/apache/cassandra/service/StorageService.java
+++ b/src/java/org/apache/cassandra/service/StorageService.java
@@ -1287,8 +1287,11 @@ public class StorageService extends NotificationBroadcasterSupport implements IE
                 @Override
                 public void onFailure(Throwable e)
                 {
-                    String message = ""Error during bootstrap: "" + e.getCause().getMessage();
-                    logger.error(message, e.getCause());
+                    String message = e.getMessage();
+                    logger.error(message, e);
+                    if (e instanceof ExecutionException && e.getCause() != null) {
+                        message = e.getCause().getMessage();
+                    }
                     progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.ERROR, 1, 1, message));
                     progressSupport.progress(""bootstrap"", new ProgressEvent(ProgressEventType.COMPLETE, 1, 1, ""Resume bootstrap complete""));
                 }
{noformat};;;","14/Jul/17 09:45;blerer;Thanks for the patch. I ran CI on the 2.2 branch for extra safety and the failing tests are unrelated.
 
+1;;;","14/Jul/17 09:49;blerer;Committed into 2.2 at 5b982d790bffbf1beb92fd605f6f213914ba4b63 and merged into 3.0, 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expiration in OutboundTcpConnection can block the reader Thread,CASSANDRA-13265,13046063,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cesken,cesken,cesken,24/Feb/17 16:16,15/May/20 08:00,14/Jul/23 05:56,03/May/17 20:55,3.0.14,3.11.0,4.0,4.0-alpha1,,,Messaging/Internode,,,,,0,,,,,"I observed that sometimes a single node in a Cassandra cluster fails to communicate to the other nodes. This can happen at any time, during peak load or low load. Restarting that single node from the cluster fixes the issue.

Before going in to details, I want to state that I have analyzed the situation and am already developing a possible fix. Here is the analysis so far:

- A Threaddump in this situation showed  324 Threads in the OutboundTcpConnection class that want to lock the backlog queue for doing expiration.
- A class histogram shows 262508 instances of OutboundTcpConnection$QueuedMessage.

What is the effect of it? As soon as the Cassandra node has reached a certain amount of queued messages, it starts thrashing itself to death. Each of the Thread fully locks the Queue for reading and writing by calling iterator.next(), making the situation worse and worse.
- Writing: Only after 262508 locking operation it can progress with actually writing to the Queue.
- Reading: Is also blocked, as 324 Threads try to do iterator.next(), and fully lock the Queue

This means: Writing blocks the Queue for reading, and readers might even be starved which makes the situation even worse.

-----
The setup is:
 - 3-node cluster
 - replication factor 2
 - Consistency LOCAL_ONE
 - No remote DC's
 - high write throughput (100000 INSERT statements per second and more during peak times).
 ","Cassandra 3.0.9
Java HotSpot(TM) 64-Bit Server VM version 25.112-b15 (Java version 1.8.0_112-b15)
Linux 3.16",alekiv,aweisberg,cesken,githubbot,jasobrown,jjirsa,kohlisankalp,szhou,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13159,,,,,,,,,,,,,,,,,,,"28/Apr/17 22:02;aweisberg;cassandra-13265-2.2-dtest_stdout.txt;https://issues.apache.org/jira/secure/attachment/12865631/cassandra-13265-2.2-dtest_stdout.txt","28/Apr/17 22:02;aweisberg;cassandra-13265-trun-dtest_stdout.txt;https://issues.apache.org/jira/secure/attachment/12865632/cassandra-13265-trun-dtest_stdout.txt","24/Feb/17 16:38;cesken;cassandra.pb-cache4-dus.2017-02-17-19-36-26.chist.xz;https://issues.apache.org/jira/secure/attachment/12854505/cassandra.pb-cache4-dus.2017-02-17-19-36-26.chist.xz","24/Feb/17 16:38;cesken;cassandra.pb-cache4-dus.2017-02-17-19-36-26.td.xz;https://issues.apache.org/jira/secure/attachment/12854504/cassandra.pb-cache4-dus.2017-02-17-19-36-26.td.xz",,,,,,,,,,,,,,,,4.0,cesken,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 09 16:02:34 UTC 2017,,,,,,,,,,"0|i3al5r:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"24/Feb/17 16:38;cesken;Thread Dump;;;","24/Feb/17 16:38;cesken;Class Histogram;;;","24/Feb/17 16:39;cesken;The Thread dumps show, that several Threads park on the same objects.
- 324 Threads are waiting on the same object, trying to iterate over Queue (expiration)
- 24 Threads  wait on a different object, as far as we see they try to read from the Queue

{code}
--- cassandra.pb-cache4-dus.2017-02-20-01-41-14.td -------------------------------------------------------------------
      1         - parking to wait for  <0x00000001c04b1748> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c056d4f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c0579c60> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
     24         - parking to wait for  <0x00000001c058ce50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c058e520> (a java.util.concurrent.Semaphore$NonfairSync)
      1         - parking to wait for  <0x00000001c058ee50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c0592bc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c0593058> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c0593ae0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c05958d0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c059f788> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      4         - parking to wait for  <0x00000001c07f5ea8> (a java.util.concurrent.SynchronousQueue$TransferStack)
      1         - parking to wait for  <0x00000001c0df0548> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c4b52790> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c56a7ca8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c56beea8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c56bf2d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    324         - parking to wait for  <0x00000001c5d5a150> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
      1         - parking to wait for  <0x00000001c628edb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c6290b78> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c62958a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c6295b08> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c72343a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c7581d58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001c8dd5738> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001ccdc3b80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001cd22e1b0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001f3c39428> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000001fb43f5d0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      1         - parking to wait for  <0x00000002003b6018> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
{code}
;;;","24/Feb/17 17:41;cesken;Link to the current patch: https://github.com/apache/cassandra/compare/trunk...christian-esken:13265-3.0?expand=1;;;","24/Feb/17 20:15;aweisberg;Thanks for catching this. I am not clear on you end up with so many threads blocked. There shouldn't be that many threads since only 128 request threads can be in flight in a default configuration. What are these threads and what pool are they coming from?

This fix creates a thread to do the scheduled work per outbound TCP connection and unfortunately we have a lot of them. For a typical thousand node cluster we will create 6k threads (3 inbound, 3 outbound) and this would add another 3k.

I think the ideal fix would commit a bounded number of threads to processing expiration work. It would also only attempt to expire on a fixed interval so excessive time isn't spent checking for expirations that won't occur because no time has passed.

Initially I was going to suggest another thread pool based approach to handle expiration, but I don't see much to be gained doing expiration in a separate thread. Certainly it would reduce outliers and be more non-blocking. You could do the expiration work in the thread submitting a message and just take care that only one thread does expiration work every X milliseconds/seconds.

So the general requirements as I see it whether you use another thread or not.:

If there is currently no expiration processing task for a connection and it looks like we should expire then we should atomically perform expiration. It would be okay to occasionally miss executing expiration since the next message will have an opportunity to perform expiration. We should not perform expiration if the time since the last expiration finished is < some threshold that is configurable via cassandra.yaml and JMX (also queryable via JMX).

If we do decide to use a thread pool I think you want allocate a bounded thread pool that is not that big. Maybe min(availableProcessors, 4). Configurable via a property.

This also needs to be done on 2.1, 2.2, 3.0, 3.11, and trunk, but to start just get it done in one version so we can get the design right.;;;","28/Feb/17 09:50;cesken;I see your argument. On larger clusters this may get problematic. I will try to summarize the alternative solutions:
- Offload expiration to a ""random"" regular Thread, but only a single one. If one Thread already expires ...
 -- ... let the other Threads continue  (1)
 -- ... let the other Threads wait  (2)
- Use an ""Expiration Thread Pool"" (3). I am not (currently) in favor for it, and if I understood you correctly then it is also not your preference.

I will implement option (1) today.


Please see the attached Thread Dump to see which Threads are blocking. Here are two examples from the Thread Dumps. Mainly they are SharedPool-Worker threads, that either do iterator.remove() or iterator.next(). I think in the Threaddump there is also a HintDispatcher Thread that is parking on the same lock.

java.util.concurrent.LinkedBlockingQueue$Itr.remove:
{code}
""SharedPool-Worker-294"" #587 daemon prio=5 os_prio=0 tid=0x00007fb69b11e260 nid=0x6090 waiting on condition [0x00007fb162c0e000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000023a426218> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
        at java.util.concurrent.LinkedBlockingQueue.fullyLock(LinkedBlockingQueue.java:225)
        at java.util.concurrent.LinkedBlockingQueue$Itr.remove(LinkedBlockingQueue.java:840)
        at org.apache.cassandra.net.OutboundTcpConnection.expireMessages(OutboundTcpConnection.java:555)
        at org.apache.cassandra.net.OutboundTcpConnection.enqueue(OutboundTcpConnection.java:165)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:771)
        at org.apache.cassandra.net.MessagingService.sendReply(MessagingService.java:744)
        at org.apache.cassandra.hints.HintVerbHandler.reply(HintVerbHandler.java:99)
        at org.apache.cassandra.hints.HintVerbHandler.doVerb(HintVerbHandler.java:94)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
        at java.lang.Thread.run(Thread.java:745)
{code}

java.util.concurrent.LinkedBlockingQueue$Itr.next:
{code}
""SharedPool-Worker-295"" #590 daemon prio=5 os_prio=0 tid=0x00007fb69b1135b0 nid=0x608d waiting on condition [0x00007fb162cd1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000023a426218> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
        at java.util.concurrent.LinkedBlockingQueue.fullyLock(LinkedBlockingQueue.java:225)
        at java.util.concurrent.LinkedBlockingQueue$Itr.next(LinkedBlockingQueue.java:823)
        at org.apache.cassandra.net.OutboundTcpConnection.expireMessages(OutboundTcpConnection.java:550)
        at org.apache.cassandra.net.OutboundTcpConnection.enqueue(OutboundTcpConnection.java:165)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:771)
        at org.apache.cassandra.net.MessagingService.sendReply(MessagingService.java:744)
        at org.apache.cassandra.hints.HintVerbHandler.reply(HintVerbHandler.java:99)
        at org.apache.cassandra.hints.HintVerbHandler.doVerb(HintVerbHandler.java:94)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","28/Feb/17 16:58;cesken;Here is one possibly very important observation. It looks like Coalescing is doing an infinite loop while doing maybeSleep(). I checked 10 Thread dumps, and in each of them the Thread was at the same location. Is it possible that averageGap is 0? This would lead to infinite recursion.
{code}
    private static boolean maybeSleep(int messages, long averageGap, long maxCoalesceWindow, Parker parker)
    {
        // only sleep if we can expect to double the number of messages we're sending in the time interval
        long sleep = messages * averageGap; // TODO can averageGap be 0 ?
        if (sleep > maxCoalesceWindow)
            return false;

        // assume we receive as many messages as we expect; apply the same logic to the future batch:
        // expect twice as many messages to consider sleeping for ""another"" interval; this basically translates
        // to doubling our sleep period until we exceed our max sleep window
        while (sleep * 2 < maxCoalesceWindow)
            sleep *= 2;     // <<<<<<<<<<<<<<<< CoalescingStrategies:106
        parker.park(sleep);
        return true;
    }
{code}

If sum is bigger than MEASURED_INTERVAL, then averageGap() returns 0. I am aware that this is highly unlikely, but I cannot explain the likely hanging in maybeSleep() line 106.
{code}
        private long averageGap()
        {
            if (sum == 0)
                return Integer.MAX_VALUE;
            return MEASURED_INTERVAL / sum;
        }
{code};;;","28/Feb/17 17:09;aweisberg;This bug was noticed recently and fixed as part of CASSANDRA-13159.;;;","01/Mar/17 06:50;jjirsa;Closing;;;","01/Mar/17 12:45;cesken;Reopening.

While the ""averageGap == 0"" issue has been fixed, I would still want to fix the issue from the description. That issue is that multiple Threads do the expiration, which leads to unnecessary locking, more CPU usage  and possible starvation of the reader Thread.

I will prepare a patch, that fixes that.;;;","01/Mar/17 13:26;cesken;Here is the patch for reducing contention in the queue expiration. The patch wraps expireMessages() in expireMessagesConditionally(), which makes sure that only a single Thread will do expiration at the same time:
  https://github.com/apache/cassandra/compare/trunk...christian-esken:13265-3.0?expand=1

PS: Commits in this patch are not yet squashed. If the patch is good, I will create a proper branch to have a more clear history.;;;","01/Mar/17 13:51;jasobrown;I think this patch is reasonable, so please clean it up for review. As a quick comment, you can collapse the functionality of {{expireMessagesConditionally()}} into {{expireMessages()}} and leave out the {{BACKLOG_EXPIRATION_DEBUG}}.;;;","01/Mar/17 13:54;cesken;Will do. Thanks for your quick feedback.;;;","01/Mar/17 14:58;githubbot;GitHub user christian-esken opened a pull request:

    https://github.com/apache/cassandra/pull/95

    Expire messages by a single Thread

    When queue expiration is done, one single Thread is elected to do the
    work. Previously, all Threads would go in and do the same work,
    producing high lock contention. The Thread reading from the Queue could
    even be starved by not be able to acquire the read lock. CASSANDRA-13265

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/christian-esken/cassandra 13265b-3.0

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/95.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #95
    
----
commit 448d7a9c1430d9f23bd819895fc618bb227ca833
Author: Christian Esken <christian.esken@trivago.com>
Date:   2017-03-01T14:56:36Z

    Expire messages by a single Thread
    
    When queue expiration is done, one single Thread is elected to do the
    work. Previously, all Threads would go in and do the same work,
    producing high lock contention. The Thread reading from the Queue could
    even be starved by not be able to acquire the read lock. CASSANDRA-13265

----
;;;","01/Mar/17 14:59;cesken;I created a fresh branch to have a clean commit. The Pull request is opened: 
 https://github.com/apache/cassandra/pull/95;;;","01/Mar/17 16:01;jasobrown;- remove {{BACKLOG_EXPIRATION_DEBUG}} logging and constant.
- {{backlogExpireAt}} should be a class constant ({{private static final BACKLOG_EXPIRE_AT}}. Also, maybe rename this to something like {{BACKLOG_PURGE_SIZE}}.
;;;","01/Mar/17 16:19;aweisberg;[~jjirsa] Seems like wires got crossed. This issue still exists. It's just the infinite loop in the comments that was already addressed.

[~cesken] This still allows back to back expirations to continue. I think we only want to attempt expiration every 200 milliseconds or something.;;;","01/Mar/17 19:14;jasobrown;bq. This still allows back to back expirations to continue

This happens in the existing code already. We exit out of the loop when we find the first message that is not timed out: [code|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L563], however, you could (and can) have multiple threads each iterating over the LBQ. Where things get really nuts is in the iterator in LBQ ({{LinkedBlockingQueue.Itr}}), all of the methods need to acquire both the put and take locks for the instance, combined with the OTC thread itself trying to acquire the take lock. So it's pretty clear how you can get into lock acquisition hell here if the messages are not being consumed fast enough.

[~cesken]'s patch relieves the callers of {{OutboundTcpConnection#enqueue()}} from having to contend for the LBQ locks, and I think that's the biggest benefit. I don't have a strong enough opinion about only expiring after some length of time has elapsed. tbh, I'd prefer to defer that kind of extra behavior until we can prove that the current patch doesn't satisfy what is needed here.
;;;","01/Mar/17 19:26;aweisberg;bq. This happens in the existing code already. 
That doesn't make it not a bad behavior that blocks the reader thread while it goes to do some potentially large task that doesn't actually accomplish anything. Iterating a list with 262508 elements is not going to be very fast. The list can actually be longer now that threads aren't going to be slowed down appending to the list by the need to iterate for expiration.

We have no way of knowing if this is actually fixed. How often does this issue occur? This expiration code is ancient, but it hasn't been reported until now.

I don't think we should leave it to silently afflict people later on who may not know what's going on or how to address it.

Expiration is based on time. There is no point in attempting expiration again immediately because almost nothing will have expired. It allows one bad connection to consume resources it shouldn't in the form of hijacking a thread to iterate a list.

I don't see the downside of switching from a boolean to a long and CASing that instead. If we aren't confident in it we can set a small interval so that it still checks for expiration often although I think that just generates useless work. We can't make timeouts pass faster.;;;","02/Mar/17 00:23;zznate;bq. How often does this issue occur?

Not very often, IME, but it does happen. You can see this if you monitor the {{ConnectionMetrics#commandDroppedTasks}} gauge. ;;;","02/Mar/17 05:09;aweisberg;I didn't notice that it already bails out and doesn't iterate the entire list once it encounters the first message that isn't timed out. In that case it's fine to attempt to iterate the queue frequently because the cost to check is low since it will fail on the first element.

I am actually surprised this a performance issue. I guess that there is some pathological behavior once you start trying to have multiple threads iterate and remove at the same time and they actually end up needing to remove elements.

I'll kick off utests and dtests for this tomorrow.;;;","02/Mar/17 09:20;cesken;bq. How often does this issue occur?

Not very often, but it happens. I assume that special scenarios trigger this:
- High write-throughput  (especially when you have write spikes it is easy to get above 1024 messages)
- A long Stop-the-World GC phase (because then even more Threads could start to write and iterate the Queue)
- Temporary network overload to the target node (because nothing is taken from the Queue in that case).
- Many non-droppable entries in the Queue (because then the loop does not bail out:  ""if (! qm.droppable)  continue;"" )

Temporary overloads usually resolve themselves, but in this case it does not. As soon as the Queue has reached a certain size limit, most time is spent in iterating the Queue, and the reader is starved (1 reader Thread fights against 324 Threads that do a read-lock by calling iterator.next()).
;;;","02/Mar/17 09:25;cesken;Ariel wrote:
{quote}
Expiration is based on time. There is no point in attempting expiration again immediately because almost nothing will have expired. It allows one bad connection to consume resources it shouldn't in the form of hijacking a thread to iterate a list.

I don't see the downside of switching from a boolean to a long and CASing that instead. If we aren't confident in it we can set a small interval so that it still checks for expiration often although I think that just generates useless work. We can't make timeouts pass faster.
{quote}

[~aweisberg], I understand that you want to CAS on ""lastExpirationTime"", right? I am also for doing this. Its fitting better and still keeps the change simple. In that case the Thread should iterate the whole Queue, and not bail out on the first hit. I will change it in the PR.;;;","02/Mar/17 11:40;cesken;I updated the PR with the following changes:
 - Variable names / modifiers (static)
 - Expiration is based on time
 - Expiration inspects the whole Queue (no bailing out)

This is really hard to reproduce and to test. Because of that I did not yet remove the BACKLOG_EXPIRATION_DEBUG. If you have a hint about test possibilities, let me know.;;;","02/Mar/17 13:07;jasobrown;Some comments on the patch

- use {{System.nanoTime()}} instead of {{System.currentTimeMillis()}}. nanoTime is monotonic, currentTimeMillis is not.
- for testing, I'd give {{expireMessages}} default visibility, and write a test that enqueues a bunch of messages the expire in n seconds, immediately calls {{expireMessages}} and ensure no messages were purged, wait the {{BACKLOG_EXPIRATION_INTERVAL_MILLIS}}, then invoke {{expireMessages}} and make sure everything was purged. You don't need to run OTC as a thread or with the consumer part of the class executing.
- then you can remove the {[BACKLOG_EXPIRATION_DEBUG}} :);;;","02/Mar/17 13:12;jasobrown;tbh, I don't think we want to traverse the entire {{backlog}}. If we go with the assumption that items are added to the backlog 'reasonably' in ascending timestamp order, then even a few out of order entries won't make much of a difference to the overload load. Thus, bailing out on the first non-expired message is still a sensible thing.;;;","02/Mar/17 14:33;aweisberg;I don't want to traverse the whole backlog I just thought we were so we should avoid doing it to frequently. Then I realized we don't have to traverse the entire backlog and sure enough that is what it was already doing. I prefer the version from yesterday with Jason's suggestions.;;;","02/Mar/17 14:33;cesken;bq. use System.nanoTime() instead of System.currentTimeMillis().
Agreed, {{System.nanoTime()}} is slightly better here. In real life it won't make a terrific difference even with the worst clocks, but ""_lets do things right""_. :-) . I never looked up the native code for nanoTIme(), but I bet on Unix it uses the POSIX {{clock_gettime(CLOCK_MONOTONIC, ...)}}.

bq. I don't think we want to traverse the entire backlog. [...]
Your argument  ""reasonably in ascending timestamp order"" makes sense, if all entries would have the same expiration time. But the Verbs have different timeouts, the defaults ranging from 2 to 60 seconds. Thus iterating the whole Queue should be done, as in the worst case we will remove nothing even though most entries are timed out.
;;;","02/Mar/17 14:49;aweisberg;bq. Your argument ""reasonably in ascending timestamp order"" makes sense, if all entries would have the same expiration time. But the Verbs have different timeouts, the defaults ranging from 2 to 60 seconds. Thus iterating the whole Queue should be done, as in the worst case we will remove nothing even though most entries are timed out.
Oh you are right. That's a pretty serious bug in and of itself. That sucks!

If you want to leave the trace code in it's not the end of the world just use the trace functionality in the logger? You can grab whether trace is enabled once inside the expire method. Do reduce it to a single statement that prints the timing after expiration finishes.;;;","02/Mar/17 14:57;jasobrown;bq. But the Verbs have different timeouts, the defaults ranging from 2 to 60 seconds

Oh, dang! Nice find. Yeah, I guess we should traverse the {{backlog}} in that case.

bq. In real life it won't make a terrific difference even with the worst clocks

Clocks go wrong all the time at scale, so timestamping must be done as correctly as possible. {{System.nanoTime()}} does indeed call {{clock_gettime(CLOCK_MONOTONIC, ...)}};;;","02/Mar/17 15:08;aweisberg;A nit that has been pointed out to me for System.nanoTime(). It can wrap so you should use {{now - lastExpirationTime > interval}}.;;;","02/Mar/17 16:08;cesken;Change to System.nanoTime() is done. I kept the logging, but stripped it down and guarded it with a {{isTraceEnabled()}};;;","02/Mar/17 16:52;aweisberg;Sorry one other nit. Instead of retrieving System.nanoTime() twice when enqueuing a message can you retrieve it once in {{enqueue}} and then pass it as a parameter to {{QueuedMessage}}? 

I think it's actually OK if it's slightly old because expiration runs for a while. In that scenario we want to be timing out messages sooner not later.;;;","03/Mar/17 09:41;cesken;Your ""nit"" totally makes sense, as {{nanoTime()}} is expensive if called very often. Good catch, actually I should have seen that myself. I committed the change, including the required change to RetriedQueuedMessage. I also removed two warnings by adding the generic types.

As a side note, one could even use an estimated time here, like EstimatorTimeSource which works with a regularly updated cached time. I was able to boost the performance of Triava Cache by factor 2 - 10 depending on the test. An example is in https://github.com/trivago/triava/blob/master/src/examples/java/com/trivago/examples/TimeSourceExample.java, estimatorTimeSource().;;;","03/Mar/17 18:09;jasobrown;I'm +1 on the ticket, but [~aweisberg] should also weigh in.

As a note, we have {{ApproximateTime}} for an estimated time, but that works off of {{System.currentTimeMillis()}} rather than {{System.nanoTime()}}, so monotonicity.

UPDATE: Actually, I'd like to see a unit test implemented, like what I described earlier.;;;","06/Mar/17 11:23;cesken;I was already looking into doing a unit test it but it requires access to the queue which means making it package level access and using {{@VisibleForTesting}}. I will do that tomorrow, unless there are arguments against it. I will also check alternatives.;;;","06/Mar/17 16:54;cesken;I have one question about a code fragment. When the socket is not available the backlog is cleared, but no drops are counted. Looks like an omission to me, or is it intentional? {{dropped.addAndGet(backlog.size(); )}} would be an approximation. We likely cannot get closer as {{backlog.clear();}} does not tell how much elements were removed.

{code}
                    if (qm.isTimedOut())
                        dropped.incrementAndGet();
                    else if (socket != null || connect())
                        writeConnected(qm, count == 1 && backlog.isEmpty());
                    else
                    {
                        // clear out the queue, else gossip messages back up.
                        drainedMessages.clear();
                        // dropped.addAndGet(backlog.size()); //  TODO Should dropped statistics be counted in this case?
                        backlog.clear();
                        break inner;
                    }
{code};;;","06/Mar/17 22:13;aweisberg;I think you are correct that we were supposed to mark those as dropped. [~jasobrown] do you agree?

I think the approximate nanoTime/currenTimeMillis approach where a single thread periodically updates the time is reasonable. If you added nanoTime to ApproximateTime with it's own configuration I think it would be fine to use it in this context.;;;","06/Mar/17 22:22;jasobrown;bq. we were supposed to mark those as dropped

We probably should count them as dropped as we are dropping them, huh ;) fwiw, looks like it's always been implemented this way, since CASSANDRA-3005. So, yes, please update the counter.;;;","07/Mar/17 17:01;cesken;I added three changes:
- Implemented unit test
- Count dropped messages if Cassandra cannot write to the socket 
- Fix the QueuedMessage.isTimedOut(), which was prone to a System.nanoTime() wrap bug, as it used a check of type: aNanos < bNanos;;;","07/Mar/17 19:27;aweisberg;Awesome.
* [This needs to be configurable from the YAML and via JMX. Add it to {{StorageProxyMBean}}/{{StorageProxy}}, store the value in {{Config}} then get it from {{DatabaseDescriptor}} it will need to be a volatile field in {{Config}}. Add a setter and a getter. I have the setter return the previous value, but a lot of the existing ones don't. 10 seconds is also way too high as a default. I propose 200 milliseconds. It's an improvement over today, but still aggressive at trying to free memory. Also use {{java.util.concurrent.TimeUnit}} to convert to the value you want.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R134]
* [It should include drained messages as well. To be 100% correct you would have to count how many messages you have iterated over and sent and then subtract.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R261]
* [Typo, ""thus letting""|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R604]
* [Extra line break|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R625]
* [We don't do/allow author tags|https://github.com/apache/cassandra/pull/95/files#diff-c16fff43d2aafe61a4219656d1ab6f9eR26]
* [Use TimeUnit|https://github.com/apache/cassandra/pull/95/files#diff-c16fff43d2aafe61a4219656d1ab6f9eR36]
* [It isn't using the constant.|https://github.com/apache/cassandra/pull/95/files#diff-c16fff43d2aafe61a4219656d1ab6f9eR118]
* [Just in case maybe assert the droppable/non-droppable status of the verbs. Or does it not matter since the tests will fail anyways?|https://github.com/apache/cassandra/pull/95/files#diff-c16fff43d2aafe61a4219656d1ab6f9eR33]

I had a sad and unfortunate thought. We are going about expiring wrong by counting messages. We really want to expire based on the weight of the queue. Also I really really hate that bounded queues that aren't weighted are a thing. Let's not do that here though since I want to backport this to other versions.
;;;","08/Mar/17 10:36;cesken;bq. It should include drained messages as well
OMG, right. I thought about that, and concluded it is correct to exclude. But obviously the messages are already drained from the queue, so they must be added.

bq. We don't do/allow author tags
OOPS, always this oversmart IDE :-)

bq. This needs to be configurable
Oh. Even more work. This is getting bigger than anticipated, but I am happy to do it. Thanks for the hints on how do do the Configuration. I will work on it later this day or tomorrow.;;;","08/Mar/17 15:22;cesken;I will update the status while working on the individual topics:
(x)    This needs to be configurable from the YAML and via JMX. 
(/)    It should include drained messages as well.
(/)    Typo, ""thus letting""
(/)    Extra line break
(/)    We don't do/allow author tags
(/)    Use TimeUnit
  => Additionally I am now determining the timeout value automatic
(/)    It isn't using the constant.
(/)    Just in case maybe assert the droppable/non-droppable status of the verbs. Or does it not matter since the tests will fail anyways?
   => It wouldn't matter, but I added a check make it more explicit.
;;;","08/Mar/17 16:13;cesken;I pushed the changes noted in the former comment.
I am plannig to do ""Configurabilty and default value"" tomorrow.;;;","10/Mar/17 11:45;cesken;I am nearly done with the configuration, and have two questions about it:

1.  How to handle the default value? My approach is to pre-configure the default value in Config:
{code}
    public static final int otc_backlog_expiration_interval_in_ms_default = 200;
    public volatile Integer otc_backlog_expiration_interval_in_ms = otc_backlog_expiration_interval_in_ms_default;
{code}

Additionally I will handle null values, that might have been set via JMX in the getter of DatabaseDescriptor:
{code}
    public static Integer getOtcBacklogExpirationInterval()
    {
        Integer confValue = conf.otc_backlog_expiration_interval_in_ms;
        return confValue != null ? confValue : Config.otc_backlog_expiration_interval_in_ms_default;
    }
{code}
Is that OK? Should I also handle other illegal values in that getter (negative values), or reject them in the setter?  I have not found a  code example in Cassandra that handles bad values uniformly for MBean and Config.

2. How to read the config value? I am seeing some {{Integer.getInteger(propName, defaultValue)}}, but this looks strange to me. I think changes from JMX would not even be reflected. Thus I am calling the getter from above: {{DatabaseDescriptor.getOtcBacklogExpirationInterval()}}. Is the latter OK?
;;;","10/Mar/17 17:05;cesken;I committed the change containing the configuration, as I would like some feedback whether I am on the right path. Please note that I did not yet have time for tests (planned for next Monday), but I thought it is better to give a chance to review the current changes.

I also had to add back ""AtomicBoolean backlogExpirationActive"" Otherwise I cannot guarantee that only a single Thread is iterating the Queue, especially if a small expiration interval (1ms, or 0ms) is configured. The ""AtomicLong backlogNextExpirationTime"" could now be ""volatile long"".;;;","13/Mar/17 13:56;cesken;The change is now finished, including configuration, MBean and testing. I also tested an interval of 0 ms, which is close to what Cassandra does today.

Please have  a look. What is the recommended way of getting this into the official Cassandra repo?  A patch, or would someone with write access take it directly from Github? This also has impact on how the merge conflicts will be resolved, as there are now 3 files with merge conflicts according to https://github.com/apache/cassandra/pull/95. I did not want to rebase my branch without asking.;;;","13/Mar/17 17:44;aweisberg;The right way to do it is create a branch for all the versions where this is going to be fixed. Start at 2.2, merge to 3.0, merge to 3.11, then merge to trunk. 

You can get away with one field. Check the next expiration time, CAS it to {{Long.MAX_VALUE}}, then when you are done store the next expiration time in it. Doing it with two fields also works. I wouldn't bother changing it.

If you set the default value via a property it will work fine. It will set it once when it loads the class at startup and then overwrite it with YAML contents or JMX invocations. Generally we do set the default value in config via assignment. Doing it via property gives yet another way to set the value, but it's the least important. It's more useful for things which aren't in the YAML. Adding YAML properties adds a bit of boiler plate.

* [A smaller value could potentially expire messages slightly sooner at the expense of more CPU time and queue contention while iterating the backlog of messages.|https://github.com/apache/cassandra/pull/95/files#diff-bdaab1104a93e723ce0b609a6477c9c4R992]
* [You shouldn't need the check for null? Usually we ""just"" make sure its not null and skip the boilerplate.|https://github.com/apache/cassandra/pull/95/files#diff-a8a9935b164cd23da473fd45784fd1ddR1973]
* [Avoid unrelated whitespace changes.| https://github.com/apache/cassandra/pull/95/files#diff-a8a9935b164cd23da473fd45784fd1ddL2034]
*  [I still think it's a good idea to avoid hard coding this kind of value so operators have options without recompiling.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R139]
* Fun fact. You don't need {{backlogNextExpirationTime}} to be volatile. You can piggyback on {{backlogExpirationActive}} to get the desired effects from the Java memory model. A store to {{backlogExpirationActive}} makes a prior stores (by the current thread) to {{backlogNextExpirationTime}} visible. A read of {{backlogExpirationActive}} would make prior stores to {{backlogNextExpirationTimeVisible}} by the last writer to {{backlogExpirationActive}} visible. The volatile read is close to free so I wouldn't change it just so it's not sensitive to the order the fields are accessed in.
* [Breaking out the uber bike shedding this could be maybeExpireMessages.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R600]
* [Swap the order of these two stores so it doesn't do extra expirations.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R636]
* [Using a boxed integer makes it a bit confusing because now everyone has to know how null is handled. What's the diff between null and 0? Better to let 0 be disabled and not have null.|https://github.com/apache/cassandra/pull/95/files#diff-097eb77c5d9d80a48e7547fbb81822caR62]
* [This is not quite correct you can't count drainCount as dropped because some of the drained messages may have been sent during iteration.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R270];;;","14/Mar/17 10:05;cesken;bq. This is not quite correct you can't count drainCount as dropped because some of the drained messages may have been sent during iteration.
I looked in more detail, and I think this a flaw in the original code ""suggested"" me to do this:  {{drainedMessages.clear()}} is called twice, and one time would be enough. IMO it would be better to only keep the one at the end of the method and also do the drop-counting for the drained messages there. This would also cover a rather exotic case of the {{catch (Exception e)}} in the {{run()}} method. If an Exception is thrown, then there is a danger of nothing being counted.

bq. Using a boxed integer
bq. You shouldn't need the check for null?
From a brief check, this refers to a similar point. I saw many configuration options to allow null and followed that route. I am absolutely happy to make it non-boxed.

bq. The right way to do it is create a branch for all the versions where this is going to be fixed. Start at 2.2, merge to 3.0, merge to 3.11, then merge to trunk. 
At Github? I can do so. But no PR, right? I saw it mentioned that one should not open PR's for Cassandra on Github as they cannot be handled (it's just a mirror).
;;;","14/Mar/17 13:05;cesken;Done. Hint: Not everything is committed yet, as I have to remove my debug code from it.
- (/) A smaller value could potentially ...
- (/)  You shouldn't need the check for null? Usually we ""just"" make sure its not null
    OK.  I thought it might be possible to set this to null, but even JConsole refuses it.
- (/) Using a boxed integer makes it a bit confusing ...
  ACK. Happily changed that. Looks like I followed bad examples.
- (/) Avoid unrelated whitespace changes.
  OK. I missed that after moving the field.
- (?)  I still think it's a good idea to avoid hard coding this kind of value so operators have options without recompiling.
 I would like BACKLOG_PURGE_SIZE to be kept hard coded for now. It has been there for quite some time hard coded, and in the long term I do not think it should be kept as-is. For example it would better to purge on the number of actually DROPPABLE messages in the queue (or their weight if you want to extend even further)
- (/) Fun fact. You don't need backlogNextExpirationTime to be volatile. You can piggyback on backlogExpirationActive to get the desired effects from the Java memory model. [...] I wouldn't change it ...
Yes.  I am  aware of that and using that technique often. Here I did not like it as visibility effects would not be obvious, unless explicitly documented. You are probably aware what Brian Goetz says about piggybacking in his JCIP book. BTW: A more obvious usage is for me in status fields, e.g. to make the results of a Future visible. I won't change it either, so marking this as done.
- (?)    Breaking out the uber bike shedding this could be maybeExpireMessages.
Nope, I am not going back that road. I had expireMessagesConditionally() before and changed it on request. If we do this, then a Set should not have an add() method but only a maybeAdd(), because it might not add the entry. Also I added clear documentation, so it should be fine. 
- (/)    Swap the order of these two stores so it doesn't do extra expirations.
  Ouch. That hurts. I wanted to protect from Exceptions inside the throw-Block which would disable expiration infinitely. I was quite tired yesterday. I am swapping it back, TimeUnit conversions never throw Exceptions, so it is safe. :-|
- (/)  This is not quite correct you can't count drainCount as dropped because some of the drained messages may have been sent during iteration.
  In progress. I am wondering if we should include fixing the drop count it in this patch, as it will likely create even more conflicts. OTOH I have to touch some related methods anyhow. I will think about it.
;;;","14/Mar/17 17:40;aweisberg;bq. I would like BACKLOG_PURGE_SIZE to be kept hard coded for now. It has been there for quite some time hard coded, and in the long term I do not think it should be kept as-is. For example it would better to purge on the number of actually DROPPABLE messages in the queue (or their weight if you want to extend even further)
I agree but I don't want to add more to this change set and I want to backport it to at least 2.2. I suggest it primarily because it's very low effort to add a property as opposed to a full YAML + JMX config option.

bq. At Github? I can do so. But no PR, right? I saw it mentioned that one should not open PR's for Cassandra on Github as they cannot be handled (it's just a mirror).
Project policy is to not use PRs even for comments. Code review comments go in JIRA. What I and some others do is link to a compare view of what we intend to merge. Don't delete the branches your links point to because it invalidates the information in JIRA.

bq. I looked in more detail, and I think this a flaw in the original code ""suggested"" me to do this: drainedMessages.clear() is called twice, and one time would be enough. IMO it would be better to only keep the one at the end of the method and also do the drop-counting for the drained messages there. This would also cover a rather exotic case of the catch (Exception e) in the run() method. If an Exception is thrown, then there is a danger of nothing being counted.
So there is the issue of not counting drops as they happen. The thread can block for a long time writing messages so if we wait to count drops they won't show up quite as promptly. Not a huge deal, but I think we should log the drops especially due to timeouts as they happen rather than at the end. It's definitely true that we don't need to clear the backlog twice and we could log the drops due to items remaining in the backlog there. The loop decrements a counter every time it runs so you know how many remaining elements are being dropped if you hit break inner.;;;","15/Mar/17 09:49;cesken;bq. it's very low effort to add a property
I see. I thought it would just clutter the cassanda.yaml, as nobody ever would change the value. But if you feel it is important enough, I can do so.

bq. Not a huge deal, but I think we should log the drops especially due to timeouts as they happen
OK. I can follow your argument. I will rewrite it, also adding comments with explanation. 

PS: I will be soon on vacation for two weeks, so please don't wonder why you do not see any updates from me.;;;","15/Mar/17 12:05;aweisberg;bq. I see. I thought it would just clutter the cassanda.yaml, as nobody ever would change the value. But if you feel it is important enough, I can do so.
So not a property in the YAML. A java property. So something like ""Integer.getInteger(""property"", 1024);"" instead of just ""1024"".
bq. PS: I will be soon on vacation for two weeks, so please don't wonder why you do not see any updates from me.
Enjoy
;;;","15/Mar/17 15:20;cesken;bq.  I still think it's a good idea to avoid hard coding this kind of value so operators have options without recompiling. [...] A java property. 
(/) static final int BACKLOG_PURGE_SIZE = Integer.getInteger(""OTC_BACKLOG_PURGE_SIZE"", 1024);

bq. I think we should log the drops especially due to timeouts as they happen rather than at the end.
(/) I agree, and I did not change that. After the loop I simply add the unprocessed messages, which happen due to {{break inner;}}. I'll push today, so you have a chance to see that. ;;;","16/Mar/17 15:44;aweisberg;Great! Almost there.

* [Use Config.PROPERTY_PREFIX with the property name.|https://github.com/apache/cassandra/pull/95/files#diff-c7ef124561c4cde1c906f28ad3883a88R139]
* [Still using big I integer here.|https://github.com/apache/cassandra/pull/95/files#diff-71f06c193f5b5e270cf8ac695164f43aR2685]
* [And here.|https://github.com/apache/cassandra/pull/95/files#diff-097eb77c5d9d80a48e7547fbb81822caR62]

After that the last step is to create 2.2, 3.0, and 3.11 branches. Then I'll run the tests for each of the branches and if they look good I'll commit.;;;","10/Apr/17 16:13;cesken;Done. My highest priority is the 3.0 branch. I created a patch (single file, squashed) for 3.0, that I also applied to my Github fork https://github.com/christian-esken/cassandra/commits/cassandra-3.0 . I attached the patch using the Submit Patch button on the top.;;;","10/Apr/17 16:17;cesken;From 6bd3f3fc3b2da3a66b53a94a819446a9ea8ea2cf Mon Sep 17 00:00:00 2001
From: Christian Esken <Christian.Esken@trivago.com>
Date: Wed, 1 Mar 2017 15:56:36 +0100
Subject: [PATCH] Expire OTC messages by a single Thread

This patch consists of the following aspects related to OutboundTcpConnection:
- Backlog queue expiration by a single Thread
- Drop count statistics
- QueuedMessage.isTimedOut() fix

When backlog queue expiration is done, one single Thread is elected to do the
work. Previously, all Threads would go in and do the same work,
producing high lock contention. The Thread reading from the Queue could
even be starved by not be able to acquire the read lock.
Backlog queue is inspected every otc_backlog_expiration_interval_ms
milliseconds if its size exceeds BACKLOG_PURGE_SIZE. Added unit tests
for OutboundTcpConnection.

Timed out messages are counted in the dropped statistics. Additionally
count the dropped messages when it is not possible to write to the
socket, e.g. if there is no connection because a target node is down.

Fix QueuedMessage.isTimedOut(), which had used a ""a < b"" comparison on
nano time values, which can be wrong due to wrapping of System.nanoTime().

CASSANDRA-13265
---
 conf/cassandra.yaml                                |   9 ++
 src/java/org/apache/cassandra/config/Config.java   |   6 +
 .../cassandra/config/DatabaseDescriptor.java       |  10 ++
 .../cassandra/net/OutboundTcpConnection.java       | 113 +++++++++++---
 .../org/apache/cassandra/service/StorageProxy.java |  10 +-
 .../cassandra/service/StorageProxyMBean.java       |   3 +
 .../cassandra/net/OutboundTcpConnectionTest.java   | 170 +++++++++++++++++++++
 7 files changed, 294 insertions(+), 27 deletions(-)
 create mode 100644 test/unit/org/apache/cassandra/net/OutboundTcpConnectionTest.java

diff --git a/conf/cassandra.yaml b/conf/cassandra.yaml
index 790dfd743b..9c1510b66a 100644
--- a/conf/cassandra.yaml
+++ b/conf/cassandra.yaml
@@ -985,3 +985,12 @@ windows_timer_interval: 1
 
 # Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.
 # otc_coalescing_enough_coalesced_messages: 8
+
+# How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.
+# Expiration is done if messages are piling up in the backlog. Droppable messages are expired to free the memory
+# taken by expired messages. The interval should be between 0 and 1000, and in most installations the default value
+# will be appropriate. A smaller value could potentially expire messages slightly sooner at the expense of more CPU
+# time and queue contention while iterating the backlog of messages.
+# An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.
+#
+# otc_backlog_expiration_interval_ms: 200
diff --git a/src/java/org/apache/cassandra/config/Config.java b/src/java/org/apache/cassandra/config/Config.java
index 9aaf7ae33e..6a99cd3cbd 100644
--- a/src/java/org/apache/cassandra/config/Config.java
+++ b/src/java/org/apache/cassandra/config/Config.java
@@ -298,6 +298,12 @@ public class Config
     public int otc_coalescing_window_us = otc_coalescing_window_us_default;
     public int otc_coalescing_enough_coalesced_messages = 8;
 
+    /**
+     * Backlog expiration interval in milliseconds for the OutboundTcpConnection.
+     */
+    public static final int otc_backlog_expiration_interval_ms_default = 200;
+    public volatile int otc_backlog_expiration_interval_ms = otc_backlog_expiration_interval_ms_default;
+ 
     public int windows_timer_interval = 0;
 
     public boolean enable_user_defined_functions = false;
diff --git a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
index 602214f3c6..e9e54c3e20 100644
--- a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
+++ b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
@@ -1967,6 +1967,16 @@ public class DatabaseDescriptor
         conf.otc_coalescing_enough_coalesced_messages = otc_coalescing_enough_coalesced_messages;
     }
 
+    public static int getOtcBacklogExpirationInterval()
+    {
+        return conf.otc_backlog_expiration_interval_ms;
+    }
+
+    public static void setOtcBacklogExpirationInterval(int intervalInMillis)
+    {
+        conf.otc_backlog_expiration_interval_ms = intervalInMillis;
+    }
+ 
     public static int getWindowsTimerInterval()
     {
         return conf.windows_timer_interval;
diff --git a/src/java/org/apache/cassandra/net/OutboundTcpConnection.java b/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
index 46083994df..99ad194b94 100644
--- a/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
+++ b/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
@@ -31,6 +31,7 @@ import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.zip.Checksum;
@@ -62,6 +63,7 @@ import org.xerial.snappy.SnappyOutputStream;
 import org.apache.cassandra.config.Config;
 import org.apache.cassandra.config.DatabaseDescriptor;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.util.concurrent.Uninterruptibles;
 
 public class OutboundTcpConnection extends Thread
@@ -116,9 +118,14 @@ public class OutboundTcpConnection extends Thread
         if (coalescingWindow < 0)
             throw new ExceptionInInitializerError(
                     ""Value provided for coalescing window must be greather than 0: "" + coalescingWindow);
+
+        int otc_backlog_expiration_interval_in_ms = DatabaseDescriptor.getOtcBacklogExpirationInterval();
+        if (otc_backlog_expiration_interval_in_ms != Config.otc_backlog_expiration_interval_ms_default)
+            logger.info(""OutboundTcpConnection backlog expiration interval set to to {}ms"", otc_backlog_expiration_interval_in_ms);
+
     }
 
-    private static final MessageOut CLOSE_SENTINEL = new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE);
+    private static final MessageOut<?> CLOSE_SENTINEL = new MessageOut<MessagingService.Verb>(MessagingService.Verb.INTERNAL_RESPONSE);
     private volatile boolean isStopped = false;
 
     private static final int OPEN_RETRY_DELAY = 100; // ms between retries
@@ -128,6 +135,11 @@ public class OutboundTcpConnection extends Thread
     static final int LZ4_HASH_SEED = 0x9747b28c;
 
     private final BlockingQueue<QueuedMessage> backlog = new LinkedBlockingQueue<>();
+    private static final String BACKLOG_PURGE_SIZE_PROPERTY = PREFIX + ""otc_backlog_purge_size"";
+    @VisibleForTesting
+    static final int BACKLOG_PURGE_SIZE = Integer.getInteger(BACKLOG_PURGE_SIZE_PROPERTY, 1024);
+    private final AtomicBoolean backlogExpirationActive = new AtomicBoolean(false);
+    private volatile long backlogNextExpirationTime;
 
     private final OutboundTcpConnectionPool poolReference;
 
@@ -164,11 +176,11 @@ public class OutboundTcpConnection extends Thread
 
     public void enqueue(MessageOut<?> message, int id)
     {
-        if (backlog.size() > 1024)
-            expireMessages();
+        long nanoTime = System.nanoTime();
+        expireMessages(nanoTime);
         try
         {
-            backlog.put(new QueuedMessage(message, id));
+            backlog.put(new QueuedMessage(message, id, nanoTime));
         }
         catch (InterruptedException e)
         {
@@ -176,6 +188,18 @@ public class OutboundTcpConnection extends Thread
         }
     }
 
+    /**
+     * This is a helper method for unit testing. Disclaimer: Do not use this method outside unit tests, as
+     * this method is iterating the queue which can be an expensive operation (CPU time, queue locking).
+     * 
+     * @return true, if the queue contains at least one expired element
+     */
+    @VisibleForTesting // (otherwise = VisibleForTesting.NONE)
+    boolean backlogContainsExpiredMessages(long nowNanos)
+    {
+        return backlog.stream().anyMatch(entry -> entry.isTimedOut(nowNanos));
+    }
+
     void closeSocket(boolean destroyThread)
     {
         isStopped = destroyThread; // Exit loop to stop the thread
@@ -214,9 +238,8 @@ public class OutboundTcpConnection extends Thread
                 throw new AssertionError(e);
             }
 
-            currentMsgBufferCount = drainedMessages.size();
+            int count = currentMsgBufferCount = drainedMessages.size();
 
-            int count = drainedMessages.size();
             //The timestamp of the first message has already been provided to the coalescing strategy
             //so skip logging it.
             inner:
@@ -233,14 +256,16 @@ public class OutboundTcpConnection extends Thread
                         continue;
                     }
 
-                    if (qm.isTimedOut())
+                    if (qm.isTimedOut(System.nanoTime()))
                         dropped.incrementAndGet();
                     else if (socket != null || connect())
                         writeConnected(qm, count == 1 && backlog.isEmpty());
                     else
                     {
-                        // clear out the queue, else gossip messages back up.
-                        drainedMessages.clear();
+                        // Not connected! Clear out the queue, else gossip messages back up. Update dropped
+                        // statistics accordingly. Hint: The statistics may be slightly too low, if messages
+                        // are added between the calls of backlog.size() and backlog.clear()
+                        dropped.addAndGet(backlog.size());
                         backlog.clear();
                         break inner;
                     }
@@ -254,6 +279,8 @@ public class OutboundTcpConnection extends Thread
                 }
                 currentMsgBufferCount = --count;
             }
+            // Update dropped statistics by the number of unprocessed drainedMessages
+            dropped.addAndGet(currentMsgBufferCount);
             drainedMessages.clear();
         }
     }
@@ -343,7 +370,7 @@ public class OutboundTcpConnection extends Thread
         }
     }
 
-    private void writeInternal(MessageOut message, int id, long timestamp) throws IOException
+    private void writeInternal(MessageOut<?> message, int id, long timestamp) throws IOException
     {
         out.writeInt(MessagingService.PROTOCOL_MAGIC);
 
@@ -563,18 +590,53 @@ public class OutboundTcpConnection extends Thread
         return version.get();
     }
 
-    private void expireMessages()
+    /**
+     * Expire elements from the queue if the queue is pretty full and expiration is not already in progress.
+     * This method will only remove droppable expired entries. If no such element exists, nothing is removed from the queue.
+     * 
+     * @param timestampNanos The current time as from System.nanoTime()
+     */
+    @VisibleForTesting
+    void expireMessages(long timestampNanos)
     {
-        Iterator<QueuedMessage> iter = backlog.iterator();
-        while (iter.hasNext())
+        if (backlog.size() <= BACKLOG_PURGE_SIZE)
+            return; // Plenty of space
+
+        if (backlogNextExpirationTime - timestampNanos > 0)
+            return; // Expiration is not due.
+
+        /**
+         * Expiration is an expensive process. Iterating the queue locks the queue for both writes and
+         * reads during iter.next() and iter.remove(). Thus letting only a single Thread do expiration.
+         */
+        if (backlogExpirationActive.compareAndSet(false, true))
         {
-            QueuedMessage qm = iter.next();
-            if (!qm.droppable)
-                continue;
-            if (!qm.isTimedOut())
-                return;
-            iter.remove();
-            dropped.incrementAndGet();
+            try
+            {
+                Iterator<QueuedMessage> iter = backlog.iterator();
+                while (iter.hasNext())
+                {
+                    QueuedMessage qm = iter.next();
+                    if (!qm.droppable)
+                        continue;
+                    if (!qm.isTimedOut(timestampNanos))
+                        continue;
+                    iter.remove();
+                    dropped.incrementAndGet();
+                }
+
+                if (logger.isTraceEnabled())
+                {
+                    long duration = TimeUnit.NANOSECONDS.toMicros(System.nanoTime() - timestampNanos);
+                    logger.trace(""Expiration of {} took {}μs"", getName(), duration);
+                }
+            }
+            finally
+            {
+                long backlogExpirationIntervalNanos = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getOtcBacklogExpirationInterval());
+                backlogNextExpirationTime = timestampNanos + backlogExpirationIntervalNanos;
+                backlogExpirationActive.set(false);
+            }
         }
     }
 
@@ -586,18 +648,19 @@ public class OutboundTcpConnection extends Thread
         final long timestampNanos;
         final boolean droppable;
 
-        QueuedMessage(MessageOut<?> message, int id)
+        QueuedMessage(MessageOut<?> message, int id, long timestampNanos)
         {
             this.message = message;
             this.id = id;
-            this.timestampNanos = System.nanoTime();
+            this.timestampNanos = timestampNanos;
             this.droppable = MessagingService.DROPPABLE_VERBS.contains(message.verb);
         }
 
         /** don't drop a non-droppable message just because it's timestamp is expired */
-        boolean isTimedOut()
+        boolean isTimedOut(long nowNanos)
         {
-            return droppable && timestampNanos < System.nanoTime() - TimeUnit.MILLISECONDS.toNanos(message.getTimeout());
+            long messageTimeoutNanos = TimeUnit.MILLISECONDS.toNanos(message.getTimeout());
+            return droppable && nowNanos - timestampNanos  > messageTimeoutNanos;
         }
 
         boolean shouldRetry()
@@ -615,7 +678,7 @@ public class OutboundTcpConnection extends Thread
     {
         RetriedQueuedMessage(QueuedMessage msg)
         {
-            super(msg.message, msg.id);
+            super(msg.message, msg.id, msg.timestampNanos);
         }
 
         boolean shouldRetry()
diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java
index cffd63cd8d..ea082d5f20 100644
--- a/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -72,8 +72,6 @@ import org.apache.cassandra.triggers.TriggerExecutor;
 import org.apache.cassandra.utils.*;
 import org.apache.cassandra.utils.AbstractIterator;
 
-import static com.google.common.collect.Iterables.contains;
-
 public class StorageProxy implements StorageProxyMBean
 {
     public static final String MBEAN_NAME = ""org.apache.cassandra.db:type=StorageProxy"";
@@ -2683,4 +2681,12 @@ public class StorageProxy implements StorageProxyMBean
     public long getReadRepairRepairedBackground() {
         return ReadRepairMetrics.repairedBackground.getCount();
     }
+
+    public int getOtcBacklogExpirationInterval() {
+        return DatabaseDescriptor.getOtcBacklogExpirationInterval();
+    }
+
+    public void setOtcBacklogExpirationInterval(int intervalInMillis) {
+        DatabaseDescriptor.setOtcBacklogExpirationInterval(intervalInMillis);
+    }
 }
diff --git a/src/java/org/apache/cassandra/service/StorageProxyMBean.java b/src/java/org/apache/cassandra/service/StorageProxyMBean.java
index 0db0ca60ff..ee82a5b1dd 100644
--- a/src/java/org/apache/cassandra/service/StorageProxyMBean.java
+++ b/src/java/org/apache/cassandra/service/StorageProxyMBean.java
@@ -59,6 +59,9 @@ public interface StorageProxyMBean
     public long getReadRepairRepairedBlocking();
     public long getReadRepairRepairedBackground();
 
+    public int getOtcBacklogExpirationInterval();
+    public void setOtcBacklogExpirationInterval(int intervalInMillis);
+
     /** Returns each live node's schema version */
     public Map<String, List<String>> getSchemaVersions();
 }
diff --git a/test/unit/org/apache/cassandra/net/OutboundTcpConnectionTest.java b/test/unit/org/apache/cassandra/net/OutboundTcpConnectionTest.java
new file mode 100644
index 0000000000..c09ae0f07e
--- /dev/null
+++ b/test/unit/org/apache/cassandra/net/OutboundTcpConnectionTest.java
@@ -0,0 +1,170 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.net;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.MessagingService.Verb;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * The tests check whether Queue expiration in the OutboundTcpConnection behaves properly for droppable and
+ * non-droppable messages.
+ */
+public class OutboundTcpConnectionTest
+{
+    AtomicInteger messageId = new AtomicInteger(0);
+
+    final static Verb VERB_DROPPABLE = Verb.MUTATION; // Droppable, 2s timeout
+    final static Verb VERB_NONDROPPABLE = Verb.GOSSIP_DIGEST_ACK; // Not droppable
+
+    final static long NANOS_FOR_TIMEOUT = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getTimeout(VERB_DROPPABLE)*2);
+
+    
+    /**
+     * Verifies our assumptions whether a Verb can be dropped or not. The tests make use of droppabilty, and
+     * may produce wrong test results if their droppabilty is changed. 
+     */
+    @BeforeClass
+    public static void assertDroppability()
+    {
+        if (!MessagingService.DROPPABLE_VERBS.contains(VERB_DROPPABLE))
+            throw new AssertionError(""Expected "" + VERB_DROPPABLE + "" to be droppable"");
+        if (MessagingService.DROPPABLE_VERBS.contains(VERB_NONDROPPABLE))
+            throw new AssertionError(""Expected "" + VERB_NONDROPPABLE + "" not to be droppable"");
+    }
+
+    /**
+     * Tests that non-droppable messages are never expired
+     */
+    @Test
+    public void testNondroppable() throws UnknownHostException
+    {
+        OutboundTcpConnection otc = getOutboundTcpConnectionForLocalhost();
+        long nanoTimeBeforeEnqueue = System.nanoTime();
+
+        assertFalse(""Fresh OutboundTcpConnection contains expired messages"",
+                otc.backlogContainsExpiredMessages(nanoTimeBeforeEnqueue));
+
+        fillToPurgeSize(otc, VERB_NONDROPPABLE);
+        fillToPurgeSize(otc, VERB_NONDROPPABLE);
+        otc.expireMessages(expirationTimeNanos());
+
+        assertFalse(""OutboundTcpConnection with non-droppable verbs should not expire"",
+                otc.backlogContainsExpiredMessages(expirationTimeNanos()));
+    }
+
+    /**
+     * Tests that droppable messages will be dropped after they expire, but not before.
+     * 
+     * @throws UnknownHostException
+     */
+    @Test
+    public void testDroppable() throws UnknownHostException
+    {
+        OutboundTcpConnection otc = getOutboundTcpConnectionForLocalhost();
+        long nanoTimeBeforeEnqueue = System.nanoTime();
+
+        initialFill(otc, VERB_DROPPABLE);
+        assertFalse(""OutboundTcpConnection with droppable verbs should not expire immediately"",
+                otc.backlogContainsExpiredMessages(nanoTimeBeforeEnqueue));
+
+        otc.expireMessages(nanoTimeBeforeEnqueue);
+        assertFalse(""OutboundTcpConnection with droppable verbs should not expire with enqueue-time expiration"",
+                otc.backlogContainsExpiredMessages(nanoTimeBeforeEnqueue));
+
+        // Lets presume, expiration time have passed => At that time there shall be expired messages in the Queue
+        long nanoTimeWhenExpired = expirationTimeNanos();
+        assertTrue(""OutboundTcpConnection with droppable verbs should have expired"",
+                otc.backlogContainsExpiredMessages(nanoTimeWhenExpired));
+
+        // Using the same timestamp, lets expire them and check whether they have gone
+        otc.expireMessages(nanoTimeWhenExpired);
+        assertFalse(""OutboundTcpConnection should not have expired entries"",
+                otc.backlogContainsExpiredMessages(nanoTimeWhenExpired));
+
+        // Actually the previous test can be done in a harder way: As expireMessages() has run, we cannot have
+        // ANY expired values, thus lets test also against nanoTimeBeforeEnqueue
+        assertFalse(""OutboundTcpConnection should not have any expired entries"",
+                otc.backlogContainsExpiredMessages(nanoTimeBeforeEnqueue));
+
+    }
+
+    /**
+     * Fills the given OutboundTcpConnection with (1 + BACKLOG_PURGE_SIZE), elements. The first
+     * BACKLOG_PURGE_SIZE elements are non-droppable, the last one is a message with the given Verb and can be
+     * droppable or non-droppable.
+     */
+    private void initialFill(OutboundTcpConnection otc, Verb verb)
+    {
+        assertFalse(""Fresh OutboundTcpConnection contains expired messages"",
+                otc.backlogContainsExpiredMessages(System.nanoTime()));
+
+        fillToPurgeSize(otc, VERB_NONDROPPABLE);
+        MessageOut<?> messageDroppable10s = new MessageOut<>(verb);
+        otc.enqueue(messageDroppable10s, nextMessageId());
+        otc.expireMessages(System.nanoTime());
+    }
+
+    /**
+     * Returns a nano timestamp in the far future, when expiration should have been performed for VERB_DROPPABLE.
+     * The offset is chosen as 2 times of the expiration time of VERB_DROPPABLE.
+     * 
+     * @return The future nano timestamp
+     */
+    private long expirationTimeNanos()
+    {
+        return System.nanoTime() + NANOS_FOR_TIMEOUT;
+    }
+
+    private int nextMessageId()
+    {
+        return messageId.incrementAndGet();
+    }
+
+    /**
+     * Adds BACKLOG_PURGE_SIZE messages to the queue. Hint: At BACKLOG_PURGE_SIZE expiration starts to work.
+     * 
+     * @param otc
+     *            The OutboundTcpConnection
+     * @param verb
+     *            The verb that defines the message type
+     */
+    private void fillToPurgeSize(OutboundTcpConnection otc, Verb verb)
+    {
+        for (int i = 0; i < OutboundTcpConnection.BACKLOG_PURGE_SIZE; i++)
+        {
+            otc.enqueue(new MessageOut<>(verb), nextMessageId());
+        }
+    }
+
+    private OutboundTcpConnection getOutboundTcpConnectionForLocalhost() throws UnknownHostException
+    {
+        InetAddress lo = InetAddress.getByName(""127.0.0.1"");
+        OutboundTcpConnectionPool otcPool = new OutboundTcpConnectionPool(lo);
+        OutboundTcpConnection otc = new OutboundTcpConnection(otcPool);
+        return otc;
+    }
+}
-- 
2.12.0

;;;","10/Apr/17 17:32;aweisberg;Great! Most regular contributors are linking to a branch in their fork, but there are issues with that (people force update, delete branches). You don't have to attach a patch to get it to submit patch. 

I do need a patch for each branch. 2.2, 3.0, 3.11 and trunk. We try to push the work of dealing with the merge conflicts to the assignees over the committers. Use whatever method you prefer to submit the code.

When you have all of the patches I'll kick off test runs. It's going to take a while for the dtests to run right now.
;;;","11/Apr/17 15:28;cesken;Done.  2 organizational topics left:

I will add the required line to the commit message. Looks OK?!?
 bq. patch by Christian Esken; reviewed by Ariel Weisberg  and Jason Brown for CASSANDRA-13265
My proposals for the CHANGES.txt would be the following text. Can you do that, Ariel? I do not know in which versions to add that, as they are upcoming versions.
 bq. Expire OutboundTcpConnection messages by a single Thread 

Here are the branches.The cassandra-3.0 is already squashed. If that branch is OK, I will also squash the other 3 branches.

https://github.com/christian-esken/cassandra/commits/cassandra-3.0
https://github.com/christian-esken/cassandra/commits/cassandra-3.11
https://github.com/christian-esken/cassandra/commits/trunk
https://github.com/christian-esken/cassandra/commits/cassandra-2.2;;;","11/Apr/17 16:34;aweisberg;Squashing is preferred, but I like to keep the history. When I squash I create a second -squashed branch to hold the squash commit. I never delete branches and tolerate the cluttered namespace. If we used pull requests I would delete branches since the pull request preserves the information, but we don't :-( Since people tend to work on multiple tickets they don't name the branch cassandra-3.0 they do something like cassandra-13625-3.0. The commit process I follow is http://cassandra.apache.org/doc/latest/development/how_to_commit.html.

For the commit message don't list multiple reviewers just the one in the JIRA (me). I have been told that line is automatically parsed so we want to stick to the expected format. Also some OCD people want a line break in between the first and last line of the commit message.

Having CHANGES.TXT in the patch is helpful so I don't forget to add it in one branch. If it's not there and I follow the commit process I have to add it at each branch. For the entry also include the ticket number in parens at the end.

I'll kick off the tests now.;;;","11/Apr/17 20:50;aweisberg;There seem to be some build issues in various branches? Maybe because I rebased?

You should register with CircleCI so it will automatically build and run the unit tests for you out of your repo when you commit. When you rebase there will be a circle.yml in each branch that will automatically have it run the build.

||Code|utests|dtests||
|[2.2|https://github.com/aweisberg/cassandra/pull/new/cassandra-13265-2.2]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13265-2%2E2]||
|[3.0|https://github.com/aweisberg/cassandra/pull/new/cassandra-13265-3.0]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13265-3%2E0]||
|[3.11|https://github.com/aweisberg/cassandra/pull/new/cassandra-13265-3.11]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13265-3%2E11]||
|[trunk|https://github.com/aweisberg/cassandra/pull/new/cassandra-13265-trunk]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13265-trunk]||
;;;","13/Apr/17 11:57;cesken;There were different reasons why the build failed, e.g. somehow Eclipse did not pick up the build parameters for 2.2 after ""ant generate-eclipse-files"" and the build was done with Java 8 language level (lambdas). Looks like building and testing in Eclipse alone is not enough, so I redid everything manually in the console and fixed the issues. As you recommended, I have created branches that follow your naming  (cassandra-13625-3.0) with squashed commits. The new branches are:

https://github.com/christian-esken/cassandra/commits/cassandra-13625-2.2
https://github.com/christian-esken/cassandra/commits/cassandra-13625-3.11
https://github.com/christian-esken/cassandra/commits/cassandra-13625-3.0
https://github.com/christian-esken/cassandra/commits/cassandra-13625-trunk

About CHANGES.TXT: I added changes in the ""matching"" release versions that were listed in the individual branches. Please check, as the naming conventions within Cassandra are still not clear to me (e.g. there exists a 3.11 branch, a 3.0.11 release and a 3.11.0 changelog entry).;;;","13/Apr/17 16:44;aweisberg;OK, don't forget to get set up with CircleCI and post the block with the test results.

Also you transposed 13625 and 13265 :-);;;","13/Apr/17 16:50;aweisberg;Nevermind I'll run them. I have to anyways for the dtests until we get the dtests running in CircleCI. I updated my copies. 

For CHANGES.TXT the entry should go at the top of the list of entries for the version the change is for. I don't know why.;;;","19/Apr/17 13:00;cesken;bq. For CHANGES.TXT the entry should go at the top of the list of entries for the version the change is for. I don't know why.
I also haven't seen this mentioned. Probably someone could add that to https://wiki.apache.org/cassandra/HowToContribute or http://cassandra.apache.org/doc/latest/development/how_to_commit.html . Anyhow I have fixed that.

bq. set up with CircleCI [...] Also you transposed 13625 and 13265 
I changed the branches to correct the transposing 13625 and 13265. I didn't find any other place than the branch names. I will try to find out about how to do the CircleCI stuff. Meanwhile here are the updated links:

https://github.com/christian-esken/cassandra/commits/cassandra-13265-2.2
https://github.com/christian-esken/cassandra/commits/cassandra-13265-3.0
https://github.com/christian-esken/cassandra/commits/cassandra-13265-3.11
https://github.com/christian-esken/cassandra/commits/cassandra-13265-trunk;;;","19/Apr/17 13:35;aweisberg;Sorry I just had a really busy week last week and I've been trying to get Circle to the point it can run the dtests. I'm mostly there it's just a few failing tests that remain.;;;","19/Apr/17 15:13;cesken;No problem. I was away for Easter, so I did not even notice you being busy. 
I just started my CircleCI test for the first time. Its working on the first branch (trunk) for an hour and is not complete yet, so I guess with all the branches it can take a day to complete. I have restarted the build with more parallelism and hopefully that will create a more acceptable turnaround time. I will send an update whenever that is complete.  https://circleci.com/gh/christian-esken/cassandra/3;;;","20/Apr/17 08:08;cesken;Unfortunately some test failed, not because of bugs but due to technical issues, mostly with ""com.datastax.driver.core.exceptions.NoHostAvailableException"". Are these the ""dtest"" issues in CircleCI you mentioned?

I tried to run the tests locally, but even ""ant test"" runs > 1 hour and keeps failing  with Timeout, NoHostAvailableException, or similar. I don't know why the tests fail, as my Laptop should be capable of doing it. I am frequently running a 3-node Cassandra on it via ccm and that works properly.

Currently I think I did all I can do. Let me know if I can check something else. What is your proposal how do we continue here?;;;","20/Apr/17 15:08;aweisberg;That circle.yml is broken. For instance OutboundTcpConnectionTest failed to start because it NPEed in DatabaseDescriptor but it's not in the summary. Or maybe it's Circle. I'm not sure. I'm also not sure why your are only getting builds for trunk.

If you fix the unit test I think it's ready to commit. The dtests ran on Apache Jenkins for everything except trunk. Trunk timed out on one test so I'm going to try it again.;;;","20/Apr/17 16:57;aweisberg;The branches are out of date. 3.11 ones doesn't have a circle.yml. They really need to be rebased onto something relatively recent so they can run in Circle.;;;","21/Apr/17 13:28;cesken;Hmm, I rebased on 2.2 and trunk. I am surprised that 3.11 is not current, as 3.11 is not that old. I will now clean my repo, including deleting the bad ""13625"" branches and rebasing 3.0 and 3.11. ;;;","21/Apr/17 13:39;cesken;I pulled the changes, fixed the CHANGES.txt and pushed everything again. Now CircleCI is kicking off the builds for the branches. Looks like we are getting somewhere. :-);;;","24/Apr/17 14:31;cesken;First here is a summary and the question I have: The tests work if I add ""DatabaseDescriptor.daemonInitialization();"" to the unit test of the affected branches. Is this a good idea, [~aweisberg]?

Now the long story:

This is the status for branch cassandra-13265-3.0:
- (/) Running unit tests in Eclipse: Works
 - (/)/(?) CircleCI: All normal tests work fine. ""Your build ran 4754 tests in junit with 0 failures"".  The build fails for me with: Target ""stress-test"" does not exist in the project ""apache-cassandra"". As ""ant test"" worked, I would guess that the patch is fine. I will reverify the specific unit test locally


This is the status for branch cassandra-13265-3.11 and cassandra-13265-trunk:
- (/) Running unit tests in Eclipse: Works
- (x) Running unit tests with CircleCI or ""ant test"" fails, due to non-initialized DatabaseDescriptor.
  When I add the following to the unit test of cassandra-13265-3.11, the unit test works. 
{code}
   DatabaseDescriptor.daemonInitialization();
{code}

{code}
    [junit] Null Test:  Caused an ERROR
    [junit] null
    [junit] java.lang.ExceptionInInitializerError
    [junit]     at java.lang.Class.forName0(Native Method)
    [junit]     at java.lang.Class.forName(Class.java:264)
    [junit] Caused by: java.lang.NullPointerException
    [junit]     at org.apache.cassandra.config.DatabaseDescriptor.getWriteRpcTimeout(DatabaseDescriptor.java:1400)
    [junit]     at org.apache.cassandra.net.MessagingService$Verb$1.getTimeout(MessagingService.java:121)
    [junit]     at org.apache.cassandra.net.OutboundTcpConnectionTest.<clinit>(OutboundTcpConnectionTest.java:43)
{code};;;","26/Apr/17 15:31;aweisberg;I think the stress-test target just doesn't exist in 3.0.

Adding {{DatabaseDescriptor.daemonInitialization()}} is the correct fix I think.

I a struggling to just get a run of the dtests for 2.2 and trunk before I merge. This runs output seems corrupted https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/30/

The trunk trunk run doesn't even appear.;;;","27/Apr/17 11:48;cesken;I am fixing the branches, while you work on the dtests. I will continue updating this comment as long as I work on it.
|| branch || sqaushed? || Unit Tests OK? || comment ||
|  cassandra-13265-3.0 |  yes | (/) / (?) | No stress-test in build.xml. I patched circle.yml to match that: https://github.com/christian-esken/cassandra/commit/1a776e299c76093eb3edf20e0d9054e14549a667 . CircleCI still kicks off a 4th test, which fails but can likely be ignored for now. |
|  cassandra-13265-3.11 | yes | CircleCI  (/) | |
|  cassandra-13265-2.2  | yes | ant test   (/) | CicrleCI hasn't kicked off tests for the branch | 
|  cassandra-13265-trunk  | yes | CircleCI (/) / (?)  | My unit test works. But there is a strange unrelated unit test failure: ClassNotFoundException: org.apache.cassandra.stress.CompactionStress |
;;;","27/Apr/17 15:06;aweisberg;You don't need to get all the tests passing. Only the ones broken specifically by your changes. So you have to run the tests on the base branch (cassandra-2.2, cassandra-3.0, cassandra-3.11, cassandra-trunk) or compare the results to https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-test-all/ or the one specific to the version of your branch.

You also need to be aware that a test might just be unreliable in CircleCI. The unit tests seem to be reliable to me when I was running them with my circle.yml, but I am not sure about the one on trunk.

Certainly it runs additional tests which weren't running all the time before Circle and I don't think they were reliable when we started running them.;;;","28/Apr/17 22:02;aweisberg;trunk
{noformat}
Test Result (12 failures / +2)
auth_test.TestAuth.system_auth_ks_is_alterable_test
bootstrap_test.TestBootstrap.decommissioned_wiped_node_can_join_test
paging_test.TestPagingWithDeletions.test_ttl_deletions
repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test
repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test
snapshot_test.TestSnapshot.test_snapshot_and_restore_dropping_a_column
repair_tests.incremental_repair_test.TestIncRepair.multiple_full_repairs_lcs_test
repair_tests.incremental_repair_test.TestIncRepair.multiple_full_repairs_lcs_test
sstableutil_test.SSTableUtilTest.compaction_test
topology_test.TestTopology.size_estimates_multidc_test
topology_test.TestTopology.size_estimates_multidc_test
bootstrap_test.TestBootstrap.simultaneous_bootstrap_test
{noformat}
2.2
{noformat}
Test Result (4 failures / -8)
batch_test.TestBatch.logged_batch_throws_uae_test
snapshot_test.TestSnapshot.test_snapshot_and_restore_dropping_a_column
topology_test.TestTopology.size_estimates_multidc_test
bootstrap_test.TestBootstrap.simultaneous_bootstrap_test
{noformat};;;","01/May/17 17:31;aweisberg;So the dtests failing on the branch don't match the failures on trunk, but I looked at some of the other branches and they are also failing similar random tests. So I am running trunk using the branch job to see what the test failures look like in that scenario. I don't think you broke anything it's just doing due diligence is a pain right now.

I looked at the utests and they look fine. Right now it runs 4 containers, but the first container is most important as that is where the tests are closest to zero failures since we only used to run those on every commit. The others failures don't look related (famous last words).

Meanwhile can you squash the branches that aren't squashed?;;;","03/May/17 13:04;cesken;Done. Squashed and pushed. 

I also removed my ""stress-test"" patch in the 3.0 branch, as it is not related and also does not look like a proper fix. As a reference, here is the patch:
{code}
-    - case $CIRCLE_NODE_INDEX in 0) ant eclipse-warnings; ant test ;; 1) ant long-test ;; 2) ant test-compression ;; 3) ant stress-test ;;esac:
+    - case $CIRCLE_NODE_INDEX in 0) ant eclipse-warnings; ant test ;; 1) ant long-test ;; 2) ant test-compression ;;esac:
{code}
;;;","03/May/17 20:55;aweisberg;Committed as [617c8ebadb6c4df99c35a913184760e82172b1f5|https://github.com/apache/cassandra/commit/617c8ebadb6c4df99c35a913184760e82172b1f5]. Thanks!;;;","04/May/17 08:03;cesken;Thanks. I have seen your commit in three branches. I did not yet see the changes in cassandra-2.2, when looking at  https://github.com/apache/cassandra/commits/cassandra-2.2 . Is this an omission, or is the github repo is not current?;;;","04/May/17 15:47;aweisberg;I decided to start from 3.0 because 2.2 is really critical fixes only and this issue has been broken since 2.1 without eliciting a lot of complaints until now. It's a judgement call. I took a quick poll and the response was that 3.0+ was the place to start.

Is this something that's impacting you in 2.2?;;;","05/May/17 12:37;cesken;It is fine, I do not use 2.2. I was just wondering because you asked me to start with 2.2, which required more effort and made things a bit more complicated. If you hadn't asked, I would have done patches just for the HEAD of version 3 (cassandra-3.0) and version 4 (trunk). 

So, mission complete. Thanks Ariel for guiding me through my first Cassandra patch. :-);;;","05/May/17 13:17;aweisberg;I'm sorry :-( I'll try to get a more concrete handle on fix versions earlier next time. It's always been something I've struggled calibrating because different committers are all over the map in terms of how far back they will backport things.;;;","07/Oct/17 15:51;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/95
  
    Hi @christian-esken , CASSANDRA-13265 has been merged, do you mind closing this PR? (we can't close it for you)

;;;","09/Oct/17 16:02;githubbot;Github user christian-esken commented on the issue:

    https://github.com/apache/cassandra/pull/95
  
    Closing PR, as it has been merged in all relevant banches. See https://issues.apache.org/jira/browse/CASSANDRA-13265
;;;","09/Oct/17 16:02;githubbot;Github user christian-esken closed the pull request at:

    https://github.com/apache/cassandra/pull/95
;;;","09/Oct/17 16:02;cesken;PR closed: https://github.com/apache/cassandra/pull/95;;;"
Incorrect ComplexColumnData hashCode implementation,CASSANDRA-13263,13046000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,24/Feb/17 11:51,15/May/20 08:06,14/Jul/23 05:56,05/Sep/17 11:04,3.0.15,3.11.1,4.0,4.0-alpha1,,,,,,,,0,,,,,"I went through some of the logs from CASSANDRA-13175 and one of the more serious issues that we should address seem to be the {{ComplexColumnData.hashCode()}} implementation. As Objects.hashCode is not using deep hashing for array arguments, hashed will be based on the identity instead of the array's content. See patch for simple fix.
",,colinkuo,slebresne,spod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 11:04:06 UTC 2017,,,,,,,,,,"0|i3akrr:",9223372036854775807,,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"24/Feb/17 11:57;spod;||3.0||3.11||trunk||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13263-3.0]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13263-3.11]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13263-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-3.11-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13263-trunk-testall/]|
;;;","21/Aug/17 09:18;slebresne;Sorry that this kind of felt through the cracks. I'm fine changing that thought the ""correct"" fix here is to use {{BTree.hashCode}} here, not {{Arrays.hashCode}}. If you change that, I'm +1 on the change, though I'd personally stick to just trunk for this as it's definitively good future-proofing but, I'm relatively confident, doesn't matter with the current code: I don't think that method is called in practice and I see that unlikely to change, at least on 3.X. Definitively won't fight it if it makes you sleep better to have it in 3.0/3.11 though for basically the exact same reasons (and it's a pretty safe change anyway).;;;","05/Sep/17 11:04;spod;I'm pretty sure that fixing a hashCode function that isn't even used yet will cause much harm. Merged as 1e80e35 for 3.0 upwards with your suggested changes. Thanks for reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect cqlsh results when selecting same columns multiple times,CASSANDRA-13262,13045960,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,muru,spod,spod,24/Feb/17 09:27,15/May/20 08:02,14/Jul/23 05:56,05/May/17 17:19,2.2.14,3.0.18,3.11.4,4.0,4.0-alpha1,,Legacy/Tools,,,,,0,lhf,,,,"Just stumbled over this on trunk:

{quote}
cqlsh:test1> select a, b, c from table1;

 a | b    | c
---+------+-----
 1 |    b |   2
 2 | null | 2.2

(2 rows)
cqlsh:test1> select a, a, b, c from table1;

 a | a    | b   | c
---+------+-----+------
 1 |    b |   2 | null
 2 | null | 2.2 | null

(2 rows)
cqlsh:test1> select a, a, a, b, c from table1;

 a | a    | a             | b    | c
---+------+---------------+------+------
 1 |    b |           2.0 | null | null
 2 | null | 2.20000004768 | null | null
{quote}

My guess is that his is on the Python side, but haven't really looked into it.
",,Anthony Grasso,blerer,mck,muru,philipthompson,PnP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14594,CASSANDRA-13690,,,,,,,,,,,,,,,,,,,,"13/Mar/17 05:37;muru;0001-Fix-incorrect-cqlsh-results-when-selecting-same-colu.patch;https://issues.apache.org/jira/secure/attachment/12857557/0001-Fix-incorrect-cqlsh-results-when-selecting-same-colu.patch","08/May/17 02:53;Anthony Grasso;CASSANDRA-13262-v2.2.txt;https://issues.apache.org/jira/secure/attachment/12866815/CASSANDRA-13262-v2.2.txt","08/May/17 02:53;Anthony Grasso;CASSANDRA-13262-v3.0.txt;https://issues.apache.org/jira/secure/attachment/12866816/CASSANDRA-13262-v3.0.txt","08/May/17 02:53;Anthony Grasso;CASSANDRA-13262-v3.11.txt;https://issues.apache.org/jira/secure/attachment/12866817/CASSANDRA-13262-v3.11.txt",,,,,,,,,,,,,,,,4.0,muru,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 05 01:31:26 UTC 2018,,,,,,,,,,"0|i3akiv:",9223372036854775807,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"13/Mar/17 05:29;muru;This is on the Python side, specifically because the results are converted to an OrderedDict ([bin/cqlsh.py#L500|https://github.com/apache/cassandra/blob/trunk/bin/cqlsh.py#L500]):
{code}
self.session.row_factory = ordered_dict_factory
{code}

Dictionaries of course don't support duplicate keys. The default row_factory is a named tuple, which also doesn't like duplicate keys, so we have changes to the key names:
{code}
Row(rack=u'rack1', timeout=5000, rack_=u'rack1')
OrderedDict([(u'rack', u'rack1'), (u'timeout', 5000)])
{code}

The simple fix would be explicitly list the values corresponding to each column in [print_static_result()|https://github.com/apache/cassandra/blob/trunk/bin/cqlsh.py#L1115]:

{code}
formatted_values = [map(self.myformat_value, [row[c] for c in column_names], cql_types) for row in result.current_rows]
{code}

And that sort of negates the point of using an OrderedDict in the first place.;;;","13/Mar/17 05:37;muru;Sorry for the double upload, there was noise due to some whitespace differences in the previous one.;;;","05/May/17 17:19;spod;The provided patch fixes the issue and has been committed as aaf201128764. 
The changes look good to me, but I'm not that familiar with the python driver and cqlsh code to be comfortable enough committing this to anything but trunk. Anyone else, feel free to backport.

Test results: [ [testall|https://builds.apache.org/user/spod/my-views/view/Cassandra%20List%20View/job/Cassandra-devbranch-testall/7/] (clean) | [dtest|https://builds.apache.org/user/spod/my-views/view/Cassandra%20List%20View/job/Cassandra-devbranch-dtest/37/#showFailuresLink] (7 flaky) ];;;","08/May/17 02:53;Anthony Grasso;[~spodxx@gmail.com] I have created patches for [v2.2 | ^CASSANDRA-13262-v2.2.txt], [v3.0 | ^CASSANDRA-13262-v3.0.txt] and [v3.11 | ^CASSANDRA-13262-v3.11.txt]. I was unable to reproduced this issue in 2.1, hence no patch was created.;;;","15/May/17 04:32;mck;Putting [~Anthony Grasso]'s patches in…


|| branch || testall || dtest ||
| [cassandra-2.2_13262|https://github.com/michaelsembwever/cassandra/tree/mck/cassandra-2.2_13262]	| [testall|https://circleci.com/gh/michaelsembwever/cassandra/tree/mck%2Fcassandra-2.2_13262]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/46/] |
| [cassandra-3.0_13262|https://github.com/michaelsembwever/cassandra/tree/mck/cassandra-3.0_13262]	| [testall|https://circleci.com/gh/michaelsembwever/cassandra/tree/mck%2Fcassandra-3.0_13262]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/47/] |
| [cassandra-3.11_13262|https://github.com/michaelsembwever/cassandra/tree/mck/cassandra-3.11_13262]	| [testall|https://circleci.com/gh/michaelsembwever/cassandra/tree/mck%2Fcassandra-3.11_13262]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/48/] |

;;;","15/May/17 04:57;mck;Some nits:
 - in cassandra-3.0_13262 CHANGES should be those merged in from 2.2.x
 - similar in cassandra-3.11;;;","18/May/17 10:37;mck;[~Anthony Grasso], the {{cqlsh_tests.cqlsh_tests.TestCqlsh.test_with_empty_values}} failed in the dtest run¹ against the 3.11 patch. (the history of the runs before that has been lost before i got a chance to check it.) Could you take a look at this to see if it's actually caused from the patch? (i would get a chance to before next tuesday.)

¹ https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/48/;;;","30/Aug/18 08:38;blerer;It is really unclear from the comments when and where the patch was committed. Only 4.0 appears even if it seems that it should been committed into the 2.2, 3.0 and 3.11 branches.

Can somebody update Fix/versions?;;;","30/Aug/18 09:02;spod;Fix version is correct. See my [previous comment|https://issues.apache.org/jira/browse/CASSANDRA-13262?focusedCommentId=15998605&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15998605]. ;;;","30/Aug/18 09:16;blerer;Sorry, [~spodxx@gmail.com] I missed that part. The last comments confused me.

[~mck] what is the status of the patch for the 2.2, 3.0 and 3.11 branches?;;;","30/Aug/18 10:01;mck;{quote} what is the status of the patch for the 2.2, 3.0 and 3.11 branches?
{quote}
[~blerer], the issue was resolved before I addded the comment with anthony's patches. They were never committed because of waiting on the [above comment|https://issues.apache.org/jira/browse/CASSANDRA-13262?focusedCommentId=16015553&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16015553].

Would you like me to go ahead and commit the back-ports? Or run them through the tests again?;;;","30/Aug/18 10:13;blerer;[~mck] It would be great if you could run them through the tests again. If it still fails I will look into it.;;;","30/Aug/18 11:02;mck;New dtests running…

|| branch || testall || dtest ||
| [cassandra-2.2_13262|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-2.2_13262]	| [!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_13262.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_13262]	| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/628/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/628/] |
| [cassandra-3.0_13262|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.0_13262]	| [!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.0_13262.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.0_13262]	| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/632/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/632/] |
| [cassandra-3.11_13262|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13262]	| [!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13262.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13262]	| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/630/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/630/] |


EDIT: rebased branches.;;;","04/Sep/18 06:28;mck;[~spodxx@gmail.com], [~blerer], the tests look must better now.  (Still waiting on 3.0 but it looks fine, and those failed against 3.11 and 2.2 are also failed on those branches or known to be flakey.)

Any objections if i commit these backports (and update the 'Fix Versions' accordingly)?;;;","04/Sep/18 08:04;blerer;+1 for me. The patches look good. 
[~mck] Thanks for your efforts.;;;","05/Sep/18 01:31;mck;Committed to 2.2, 3.0 and 3.11, with 62e48c5f3f818d1e841178d7365d208435a63537;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index on udt built failed and no data could be inserted,CASSANDRA-13247,13045157,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,adelapena,mashudong,mashudong,22/Feb/17 09:19,15/May/20 08:06,14/Jul/23 05:56,23/Mar/17 17:12,3.11.0,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"index on udt built failed and no data could be inserted

steps to reproduce:

CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '2'}  AND durable_writes = true;

CREATE TYPE ks1.address (
    street text,
    city text,
    zip_code int,
    phones set<text>
);

CREATE TYPE ks1.fullname (
    firstname text,
    lastname text
);

CREATE TABLE ks1.users (
    id uuid PRIMARY KEY,
    addresses map<text, frozen<address>>,
    age int,
    direct_reports set<frozen<fullname>>,
    name fullname
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';


SELECT * FROM users where name = { firstname : 'first' , lastname : 'last'} allow filtering;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

WARN  [ReadStage-2] 2017-02-22 16:59:33,392 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.AssertionError: Only CONTAINS and CONTAINS_KEY are supported for 'complex' types
	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:683) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:303) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:110) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:162) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:128) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:292) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:281) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:289) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:145) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:138) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:134) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:323) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1803) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2486) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]


CREATE INDEX users_name_idx ON ks1.users (name);

ERROR [CompactionExecutor:776] 2017-02-22 16:49:41,934 CassandraDaemon.java:226 - Exception in thread Thread[CompactionExecutor:776,1,main]
java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$indexPartition$17(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.index.SecondaryIndexManager.indexPartition(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.indexPartition(Keyspace.java:566) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:70) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.compaction.CompactionManager$12.run(CompactionManager.java:1468) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 16 common frames omitted
ERROR [SecondaryIndexManagement:3] 2017-02-22 16:49:41,934 CassandraDaemon.java:226 - Exception in thread Thread[SecondaryIndexManagement:3,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:403) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:715) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$5(CassandraIndex.java:685) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_121]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:399) ~[apache-cassandra-3.9.jar:3.9]
	... 6 common frames omitted
Caused by: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$indexPartition$17(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.index.SecondaryIndexManager.indexPartition(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.indexPartition(Keyspace.java:566) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:70) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.compaction.CompactionManager$12.run(CompactionManager.java:1468) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	... 4 common frames omitted
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 16 common frames omitted


SELECT * FROM users where name = { firstname : 'first' , lastname : 'last'};
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

WARN  [ReadStage-2] 2017-02-22 16:55:43,139 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.RuntimeException: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'users_name_idx' is not yet available
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2490) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'users_name_idx' is not yet available
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:390) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1801) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2486) ~[apache-cassandra-3.9.jar:3.9]
	... 5 common frames omitted


insert into users (id, name) values (uuid(), {firstname: 'a', lastname: 'b'});
WriteFailure: Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

ERROR [MutationStage-2] 2017-02-22 17:04:34,355 StorageProxy.java:1353 - Failed to apply mutation locally : {}
java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx for ks: ks1, table: users
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:526) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:396) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:215) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1347) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2539) [apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:808) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:335) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:137) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:175) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:258) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 12 common frames omitted
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 26 common frames omitted





",,adelapena,blerer,jjirsa,mashudong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/17 09:19;mashudong;udt_index.txt;https://issues.apache.org/jira/secure/attachment/12853916/udt_index.txt",,,,,,,,,,,,,,,,,,,1.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 16:48:33 UTC 2017,,,,,,,,,,"0|i3aflj:",9223372036854775807,3.9,,,,,,blerer,,blerer,,,Critical,,,,,,,,,,,,,,,,,,,"21/Mar/17 10:43;adelapena;It seems there are two separate problems here. 

At first,  without any index created, the query using {{ALLOW FILTERING}} over a non frozen UDT should either work or, if it is not supported, show a meaningfull CQL message:

{code}
CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};

CREATE TYPE ks1.fullname (
	firstname text,
	lastname text
);

CREATE TABLE ks1.users (
	id uuid PRIMARY KEY,
	name fullname
);

INSERT INTO ks1.users(id, name) VALUES (now(), {firstname : 'first', lastname : 'last'});

SELECT * FROM ks1.users WHERE name = {firstname : 'first', lastname : 'last'} ALLOW FILTERING;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}

Also, as a second problem, index creation over a non frozen UDT should either work or, if it is not supported, fail during CQL validation showing a meaningfull message:

{code}
CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};

CREATE TYPE ks1.fullname (
	firstname text,
	lastname text
);

CREATE TABLE ks1.users (
	id uuid PRIMARY KEY,
	name fullname
);

CREATE INDEX users_name_idx ON ks1.users (name);

INSERT INTO ks1.users(id, name) VALUES (now(), {firstname : 'first', lastname : 'last'});
WriteFailure: Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code};;;","22/Mar/17 19:01;adelapena;I'm working on an initial version of the patch [here|https://github.com/apache/cassandra/compare/trunk...adelapena:13247-trunk].

The patch makes CQL validation layer to forbid {{SELECT}} restrictions and {{CREATE INDEX}} over non-frozen UDT columns, which are not supported operations. Both operations are still perfectly possible with frozen UDTs.;;;","23/Mar/17 14:21;adelapena;||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13247-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13247-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13247-trunk-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13247-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13247-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13247-3.11-dtest/]|;;;","23/Mar/17 15:32;blerer;The patch looks good. Nice work :-).

I just have two minor nits:
* Can you remove the {{TODO}} comment. If you think that adding such a feature might be usefull it is probably better to open a JIRA to keep track of it.
* If you want to check that a query will not return any results in the unit tests it is better to use {{assertEmpty}} as it is more explicite.

No need to re-trigger CI for those changes.;;;","23/Mar/17 16:47;blerer;+1;;;","23/Mar/17 16:48;blerer;Committed into 3.11 at 82d3cdcd6cfeff043c92ea7a060498942130feb5 and merged into trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Querying by secondary index on collection column returns NullPointerException sometimes,CASSANDRA-13246,13044989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,hochung,hochung,21/Feb/17 22:24,15/May/20 08:06,14/Jul/23 05:56,20/Mar/17 11:43,3.0.13,3.11.0,4.0,4.0-alpha1,,,Feature/2i Index,Legacy/Local Write-Read Paths,,,,1,easyfix,,,,"Not sure if this is the absolute minimal case that produces the bug, but here are the steps for reproducing.

1. Create table
{code}
CREATE TABLE test (
id text,
ck1 text,
ck2 text,
static_value text static,
set_value set<text>,
primary key (id, ck1, ck2)
);
{code}
2. Create secondary indices on the clustering columns, static column, and collection column
{code}
create index on test (set_value);
create index on test (static_value);
create index on test (ck1);
create index on test (ck2);
{code}
3. Insert a null value into the `set_value` column
{code}
insert into test (id, ck1, ck2, static_value, set_value) values ('id', 'key1', 'key2', 'static', {'one', 'two'} );
{code}
Sanity check: 
{code}
select * from test;

 id | ck1  | ck2  | static_value | set_value
----+------+------+--------------+----------------
 id | key1 | key2 |       static | {'one', 'two'}
{code}
4. Set the set_value to be empty
{code}
update test set set_value = {} where id = 'id' and ck1 = 'key1' and ck2 = 'key2';
{code}
5. Make a select query that uses `CONTAINS` in the `set_value` column
{code}
select * from test where ck2 = 'key2' and static_value = 'static' and set_value contains 'one' allow filtering;
{code}
Here we get a ReadFailure:
{code}
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}
Logs show a NullPointerException
{code}
java.lang.RuntimeException: java.lang.NullPointerException
       	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2470) ~[apache-cassandra-3.7.jar:3.7]
       	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
       	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.7.jar:3.7]
       	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.NullPointerException: null
       	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:720) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:303) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:120) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:293) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:281) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:76) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:289) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:134) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:127) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:123) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:65) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:292) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1799) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2466) ~[apache-cassandra-3.7.jar:3.7]
       	... 5 common frames omitted
{code}","[cqlsh 5.0.1 | Cassandra 3.7 | CQL spec 3.4.2 | Native protocol v4] 
One cassandra node up, with consistency ONE",blerer,githubbot,hochung,jjirsa,mikkel.t.andersen@gmail.com,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/17 19:56;mikkel.t.andersen@gmail.com;cassandra-13246.diff;https://issues.apache.org/jira/secure/attachment/12858747/cassandra-13246.diff",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 11:43:37 UTC 2017,,,,,,,,,,"0|i3aek7:",9223372036854775807,3.0.x,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"13/Mar/17 12:44;githubbot;Github user MikkelTAndersen commented on the issue:

    https://github.com/apache/cassandra/pull/98
  
    Ahh sorry - it seems to complicated, its a simple null check - any chance
    you can add it Alex ? I have attached the patch.
    
    On Mon, Mar 13, 2017 at 1:38 PM, Alex Petrov <notifications@github.com>
    wrote:
    
    > Hi @MikkelTAndersen <https://github.com/MikkelTAndersen>. Cassandra does
    > not use pull requests for Apache Cassandra. Please use JIRA directly.
    >
    > You can get more information on the contribution process here
    > https://wiki.apache.org/cassandra/HowToContribute
    >
    > —
    > You are receiving this because you were mentioned.
    > Reply to this email directly, view it on GitHub
    > <https://github.com/apache/cassandra/pull/98#issuecomment-286095410>, or mute
    > the thread
    > <https://github.com/notifications/unsubscribe-auth/AACsNo9UVdqa6Ygr4N2jwfUj-0ithzS7ks5rlTi8gaJpZM4MbArn>
    > .
    >
    
    
    
    -- 
    Venlig Hilsen
    Mikkel T. Andersen
    Skjoldborgvej 8
    7100 Vejle
    Mobil: +45 40 26 79 26
    
    From dac39d0268ba82b6be033e0c63ebd653ae0517cc Mon Sep 17 00:00:00 2001
    From: Mikkel Andersen <mikkel.t.andersen@gmail.com>
    Date: Mon, 13 Mar 2017 10:21:27 +0100
    Subject: [PATCH] added null check - see CASSANDRA-13246
    
    ---
     .../org/apache/cassandra/db/filter/RowFilter.java  | 22 ++++++++++++----------
     1 file changed, 12 insertions(+), 10 deletions(-)
    
    diff --git a/src/java/org/apache/cassandra/db/filter/RowFilter.java b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    index bf65e96..c26c1ad 100644
    --- a/src/java/org/apache/cassandra/db/filter/RowFilter.java
    +++ b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    @@ -611,17 +611,19 @@ public abstract class RowFilter implements Iterable<RowFilter.Expression>
                         if (column.isComplex())
                         {
                             ComplexColumnData complexData = row.getComplexColumnData(column);
    -                        for (Cell cell : complexData)
    -                        {
    -                            if (type.kind == CollectionType.Kind.SET)
    -                            {
    -                                if (type.nameComparator().compare(cell.path().get(0), value) == 0)
    -                                    return true;
    -                            }
    -                            else
    +                        if (complexData != null) {
    +                            for (Cell cell : complexData)
                                 {
    -                                if (type.valueComparator().compare(cell.value(), value) == 0)
    -                                    return true;
    +                                if (type.kind == CollectionType.Kind.SET)
    +                                {
    +                                    if (type.nameComparator().compare(cell.path().get(0), value) == 0)
    +                                        return true;
    +                                }
    +                                else
    +                                {
    +                                    if (type.valueComparator().compare(cell.value(), value) == 0)
    +                                        return true;
    +                                }
                                 }
                             }
                             return false;
    -- 
    2.0.1
    
    From 145087e3b5d748ce25e8792f91c249d2a05de3e5 Mon Sep 17 00:00:00 2001
    From: Mikkel Andersen <mikkel.t.andersen@gmail.com>
    Date: Mon, 13 Mar 2017 10:22:53 +0100
    Subject: [PATCH] fixed formatting
    
    ---
     src/java/org/apache/cassandra/db/filter/RowFilter.java | 3 ++-
     1 file changed, 2 insertions(+), 1 deletion(-)
    
    diff --git a/src/java/org/apache/cassandra/db/filter/RowFilter.java b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    index c26c1ad..d3fc301 100644
    --- a/src/java/org/apache/cassandra/db/filter/RowFilter.java
    +++ b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    @@ -611,7 +611,8 @@ public abstract class RowFilter implements Iterable<RowFilter.Expression>
                         if (column.isComplex())
                         {
                             ComplexColumnData complexData = row.getComplexColumnData(column);
    -                        if (complexData != null) {
    +                        if (complexData != null)
    +                        {
                                 for (Cell cell : complexData)
                                 {
                                     if (type.kind == CollectionType.Kind.SET)
    -- 
    2.0.1
    

;;;","13/Mar/17 14:09;githubbot;Github user MikkelTAndersen commented on the issue:

    https://github.com/apache/cassandra/pull/98
  
    Thanks Benjamin - I added a test for list, set and map. its all in this
    patch
    
    On Mon, Mar 13, 2017 at 2:02 PM, Benjamin Lerer <notifications@github.com>
    wrote:
    
    > You can look into org.apache.cassandra.cql3.validation.entities.
    > SecondaryIndexTest for some examples. It is where you test should go.
    >
    > Otherwise just generate a patch using: git format-patch and attach the
    > output to the JIRA ticket. I guess that the problem must be there since 3.0
    > so the patch should be for this version.
    >
    > —
    > You are receiving this because you were mentioned.
    > Reply to this email directly, view it on GitHub
    > <https://github.com/apache/cassandra/pull/98#issuecomment-286100435>, or mute
    > the thread
    > <https://github.com/notifications/unsubscribe-auth/AACsNmhSUn2vC3WqcTP_BjutuvUp3KRQks5rlT5UgaJpZM4MbArn>
    > .
    >
    
    
    
    -- 
    Venlig Hilsen
    Mikkel T. Andersen
    Skjoldborgvej 8
    7100 Vejle
    Mobil: +45 40 26 79 26
    
    From dac39d0268ba82b6be033e0c63ebd653ae0517cc Mon Sep 17 00:00:00 2001
    From: Mikkel Andersen <mikkel.t.andersen@gmail.com>
    Date: Mon, 13 Mar 2017 10:21:27 +0100
    Subject: [PATCH] added null check - see CASSANDRA-13246
    
    ---
     .../org/apache/cassandra/db/filter/RowFilter.java  | 22 ++++++++++++----------
     1 file changed, 12 insertions(+), 10 deletions(-)
    
    diff --git a/src/java/org/apache/cassandra/db/filter/RowFilter.java b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    index bf65e96..c26c1ad 100644
    --- a/src/java/org/apache/cassandra/db/filter/RowFilter.java
    +++ b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    @@ -611,17 +611,19 @@ public abstract class RowFilter implements Iterable<RowFilter.Expression>
                         if (column.isComplex())
                         {
                             ComplexColumnData complexData = row.getComplexColumnData(column);
    -                        for (Cell cell : complexData)
    -                        {
    -                            if (type.kind == CollectionType.Kind.SET)
    -                            {
    -                                if (type.nameComparator().compare(cell.path().get(0), value) == 0)
    -                                    return true;
    -                            }
    -                            else
    +                        if (complexData != null) {
    +                            for (Cell cell : complexData)
                                 {
    -                                if (type.valueComparator().compare(cell.value(), value) == 0)
    -                                    return true;
    +                                if (type.kind == CollectionType.Kind.SET)
    +                                {
    +                                    if (type.nameComparator().compare(cell.path().get(0), value) == 0)
    +                                        return true;
    +                                }
    +                                else
    +                                {
    +                                    if (type.valueComparator().compare(cell.value(), value) == 0)
    +                                        return true;
    +                                }
                                 }
                             }
                             return false;
    -- 
    2.0.1
    
    From 145087e3b5d748ce25e8792f91c249d2a05de3e5 Mon Sep 17 00:00:00 2001
    From: Mikkel Andersen <mikkel.t.andersen@gmail.com>
    Date: Mon, 13 Mar 2017 10:22:53 +0100
    Subject: [PATCH] fixed formatting
    
    ---
     src/java/org/apache/cassandra/db/filter/RowFilter.java | 3 ++-
     1 file changed, 2 insertions(+), 1 deletion(-)
    
    diff --git a/src/java/org/apache/cassandra/db/filter/RowFilter.java b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    index c26c1ad..d3fc301 100644
    --- a/src/java/org/apache/cassandra/db/filter/RowFilter.java
    +++ b/src/java/org/apache/cassandra/db/filter/RowFilter.java
    @@ -611,7 +611,8 @@ public abstract class RowFilter implements Iterable<RowFilter.Expression>
                         if (column.isComplex())
                         {
                             ComplexColumnData complexData = row.getComplexColumnData(column);
    -                        if (complexData != null) {
    +                        if (complexData != null)
    +                        {
                                 for (Cell cell : complexData)
                                 {
                                     if (type.kind == CollectionType.Kind.SET)
    -- 
    2.0.1
    
    From ab8e52a26c361483c9af9990037c9fb7abdd7aba Mon Sep 17 00:00:00 2001
    From: Mikkel Andersen <mikkel.t.andersen@gmail.com>
    Date: Mon, 13 Mar 2017 15:07:11 +0100
    Subject: [PATCH] added test and fixed documentation
    
    ---
     src/java/org/apache/cassandra/db/rows/Row.java     |  2 +-
     .../validation/entities/SecondaryIndexTest.java    | 39 ++++++++++++++++++++++
     2 files changed, 40 insertions(+), 1 deletion(-)
    
    diff --git a/src/java/org/apache/cassandra/db/rows/Row.java b/src/java/org/apache/cassandra/db/rows/Row.java
    index 04092a6..7449e51 100644
    --- a/src/java/org/apache/cassandra/db/rows/Row.java
    +++ b/src/java/org/apache/cassandra/db/rows/Row.java
    @@ -132,7 +132,7 @@ public interface Row extends Unfiltered, Collection<ColumnData>
          * The returned object groups all the cells for the column, as well as it's complex deletion (if relevant).
          *
          * @param c the complex column for which to return the complex data.
    -     * @return the data for {@code c} or {@code null} is the row has no data for this column.
    +     * @return the data for {@code c} or {@code null} if the row has no data for this column.
          */
         public ComplexColumnData getComplexColumnData(ColumnMetadata c);
     
    diff --git a/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java b/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java
    index c00688d..68a3378 100644
    --- a/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java
    +++ b/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java
    @@ -430,6 +430,45 @@ public class SecondaryIndexTest extends CQLTester
             });
         }
     
    +    @Test
    +    public void testSelectOnMultiIndexOnCollectionsWithNull() throws Throwable
    +    {
    +        createTable("" CREATE TABLE %s ( k int, v int, x text, l list<int>, s set<text>, m map<text, int>, PRIMARY KEY (k, v))"");
    +
    +        createIndex(""CREATE INDEX ON %s (x)"");
    +        createIndex(""CREATE INDEX ON %s (v)"");
    +        createIndex(""CREATE INDEX ON %s (s)"");
    +        createIndex(""CREATE INDEX ON %s (m)"");
    +
    +
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (0, 0, 'x', [1, 2],    {'a'},      {'a' : 1})"");
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (0, 1, 'x', [3, 4],    {'b', 'c'}, {'a' : 1, 'b' : 2})"");
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (0, 2, 'x', [1],       {'a', 'c'}, {'c' : 3})"");
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (1, 0, 'x', [1, 2, 4], {},         {'b' : 1})"");
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (1, 1, 'x', [4, 5],    {'d'},      {'a' : 1, 'b' : 3})"");
    +        execute(""INSERT INTO %s (k, v, x, l, s, m) VALUES (1, 2, 'x', null,      null,       null)"");
    +
    +        beforeAndAfterFlush(() -> {
    +            // lists
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND l CONTAINS 1 ALLOW FILTERING""), row(1, 0), row(0, 0), row(0, 2));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND k = 0 AND l CONTAINS 1 ALLOW FILTERING""), row(0, 0), row(0, 2));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND l CONTAINS 2 ALLOW FILTERING""), row(1, 0), row(0, 0));
    +            assertEmpty(execute(""SELECT k, v FROM %s WHERE x = 'x' AND l CONTAINS 6 ALLOW FILTERING""));
    +
    +            // sets
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND s CONTAINS 'a' ALLOW FILTERING"" ), row(0, 0), row(0, 2));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND k = 0 AND s CONTAINS 'a' ALLOW FILTERING""), row(0, 0), row(0, 2));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND s CONTAINS 'd' ALLOW FILTERING""), row(1, 1));
    +            assertEmpty(execute(""SELECT k, v FROM %s  WHERE x = 'x' AND s CONTAINS 'e' ALLOW FILTERING""));
    +
    +            // maps
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND m CONTAINS 1 ALLOW FILTERING""), row(1, 0), row(1, 1), row(0, 0), row(0, 1));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND k = 0 AND m CONTAINS 1 ALLOW FILTERING""), row(0, 0), row(0, 1));
    +            assertRows(execute(""SELECT k, v FROM %s WHERE x = 'x' AND m CONTAINS 2 ALLOW FILTERING""), row(0, 1));
    +            assertEmpty(execute(""SELECT k, v FROM %s  WHERE x = 'x' AND m CONTAINS 4 ALLOW FILTERING""));
    +        });
    +    }
    +
         /**
          * Migrated from cql_tests.py:TestCQL.map_keys_indexing()
          */
    -- 
    2.0.1
    

;;;","14/Mar/17 19:56;mikkel.t.andersen@gmail.com;Attaching the file so its easier to apply the patch.;;;","14/Mar/17 19:57;mikkel.t.andersen@gmail.com;I hope is easy for you to review and apply otherwise please contact me.;;;","17/Mar/17 15:23;blerer;Thanks for the patch. It looks good to me.
I am just waiting for the CI results.;;;","20/Mar/17 08:40;blerer;[~mikkel.t.andersen@gmail.com] Be carefull, the patch should be marked as {{Ready To Commit}} only when the reviewer give his green light. Which I did not do yet as I was waiting for the CI results as mentioned in my comment.;;;","20/Mar/17 08:49;mikkel.t.andersen@gmail.com;Sorry Benjamin - did not mean to cause problems... could you send me the
link to where the workflow is described?

On Mon, Mar 20, 2017 at 9:40 AM, Benjamin Lerer (JIRA) <jira@apache.org>;;;","20/Mar/17 09:25;blerer;Do not worry, I am just mentionning it for you to know (for your next patches ;-) )
{{Ready to commit}} should normally be set by the reviewer to tell that the review is complete. If the reviewer is also a committer, he might simply commit the patch and skip that state.
So the workflow is simply:  

{noformat}
                                                      +-----------------+     +----------+
                                        +--- ok ----> | READY TO COMMIT | --> | RESOLVED | or something like that
+------+        +------------------+    |             +-----------------+     +----------+
| OPEN | -----> | PATCH AVAILABLE  | ---+
+------+        +------------------+    |              +------+     
                                        +-- not ok --> | OPEN |
                                                       +------+     
{noformat}
  
                                                         ;;;","20/Mar/17 09:50;blerer;CI results look good.
|| 3.0 | [utests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-3.0-testall/] | [dtests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-3.0-dtest/]|
|| 3.11 | [utests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-3.11-testall/] | [dtests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-3.11-dtest/]|
|| trunk | [utests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-trunk-testall/] | [dtests| http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13246-trunk-dtest/]|

I just added an extra unit test to test filtering without secondary index.;;;","20/Mar/17 11:43;blerer;Committed into 3.0 at 0eebc6e6b7cd7fc801579e57701608e7bf155ee0 and merged into 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy deserializer can create unexpected boundary range tombstones,CASSANDRA-13237,13044000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Feb/17 13:53,16/Apr/19 09:30,14/Jul/23 05:56,23/Feb/17 14:22,3.0.12,3.11.0,,,,,,,,,,0,,,,,"Most of the code don't generate a range tombstone boundary with the same deletion time on both side as this is basically useless, and there is some assertion in {{DataResolver}} that actually expect this. However, the deserializer for legacy sstable doesn't always properly avoid their creation and we can thus generate them (and break the {{DataResolver}} assertion.",,blambov,jeromatron,slebresne,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 14:22:25 UTC 2017,,,,,,,,,,"0|i3a8o7:",9223372036854775807,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"17/Feb/17 14:43;slebresne;Attaching patches for the problem below:
| [13237-3.0|https://github.com/pcmanus/cassandra/commits/13237-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13237-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13237-3.0-dtest] |
| [13237-3.11|https://github.com/pcmanus/cassandra/commits/13237-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13237-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13237-3.11-dtest] |
| [13237-trunk|https://github.com/pcmanus/cassandra/commits/13237-trunk] | [utests|http://cassci.datastax.com/job/pcmanus-13237-trunk-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13237-trunk-dtest] |

The first commit of the 3.0/3.11 patches is actually not doing anything but just adding a unit test that shows we can indeed create a range tombstone boundary that have the same deletion on both sides. For that, and because we don't have an easy way to actually write old sstable, the patch also refactor {{UnfilteredDeserializer.OldFormatDeserializer}} so that it can be unit tested. So while the diff may look ""large-ish"", it's really just moving some code around for the test purposes.

The 2nd commit does 2 things:
# it fixes the legacy deserializer to stop doing this.
# it update the assertion in {{DataResolver}} that breaks with such boundaries (again, same deletion on both side of the boundary).

Not that while it may sound like the 1st part is enough, we can't guarantee that some use having already upgraded haven't some bad boundaries already (let's clarify when I say ""bad"" that it's really just an small inefficiency so not really a big deal) so we shouldn't break on those. Still, no point in creating such useless boundaries, hence the 1st part.

The trunk patch really only include the {{DataResolver}} change since legacy code has been removed there.;;;","20/Feb/17 11:16;blambov;The deserializer fix and test look good, but there are some problems in {{DataResolver}}:
- Shouldn't we be looking at {{openDeletionTime}} [here|https://github.com/pcmanus/cassandra/commit/ba7d1763c108a4a7d84b91ab9eb9b36d04efb0f5#diff-8781f9483cca1cfc87145c767295cc79R341]?
- Regardless of the result of that test, you are still setting {{markerToRepair}} on the [next line|https://github.com/pcmanus/cassandra/commit/ba7d1763c108a4a7d84b91ab9eb9b36d04efb0f5#diff-8781f9483cca1cfc87145c767295cc79R344] -- I don't think this is the intended behaviour.
- Is it too hard to write a test for the above?
;;;","20/Feb/17 15:55;slebresne;You're right, I didn't re-read the patch and botched it, sorry about that. I've pushed an update to all branches that fixes both of your point in {{DataResolver}} (both were indeed typos) and add a new test in {{DataResolverTest}} to test this.

I did rebased and force pushed the update mostly because I wanted the new test to be in the first commit so I could easily test it with and without the fixes (crappy excuse, I know) and I hope this isn't too much trouble (but I truly only did the changes of your points above).
;;;","21/Feb/17 06:49;blambov;The trunk patch doesn't appear to have any changes?

Apart from that it looks good to me.;;;","21/Feb/17 08:52;slebresne;bq. The trunk patch doesn't appear to have any changes?

Forgot to commit before pushing, sorry :(. Done now so I'll wait on the CI to make sure.;;;","23/Feb/17 14:22;slebresne;Alright, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
corrupt flag error after upgrade from 2.2 to 3.0.10,CASSANDRA-13236,13043967,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,ingard,ingard,17/Feb/17 11:59,16/Apr/19 09:30,14/Jul/23 05:56,10/May/17 10:36,3.0.14,3.11.0,,,,,,,,,,0,,,,,"After upgrade from 2.2.5 to 3.0.9/10 we're getting a bunch of errors like this:

{code}
ERROR [SharedPool-Worker-1] 2017-02-17 12:58:43,859 Message.java:617 - Unexpected exception during request; channel = [id: 0xa8b98684, /10.0.70.104:56814 => /10.0.80.24:9042]
java.io.IOError: java.io.IOException: Corrupt flags value for unfiltered partition (isStatic flag set): 160
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:222) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:210) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:749) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:711) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:265) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:224) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:76) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:487) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:464) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.10.jar:3.0.10]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.10.jar:3.0.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
Caused by: java.io.IOException: Corrupt flags value for unfiltered partition (isStatic flag set): 160
        at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:374) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:217) ~[apache-cassandra-3.0.10.jar:3.0.10]
        ... 23 common frames omitted
{code}",cassandra 3.0.10,ifesdjeen,ingard,ingardm,jasobrown,jjirsa,mshuler,samt,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12088,,,,,,,,,,,CASSANDRA-12088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 10:36:50 UTC 2017,,,,,,,,,,"0|i3a8gv:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Critical,,,,,,,,,,,,,,,,,,,"17/Feb/17 12:27;jasobrown;Can you share the schema and query?;;;","17/Feb/17 12:32;ingard;I'm not sure which queries causes this atm. Is there a way to find out from enabling logging or something else?;;;","17/Feb/17 12:47;ingard;CREATE KEYSPACE keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;

CREATE TABLE keyspace.public_uri_share (
    uri text PRIMARY KEY,
    collection__username__type__id text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.photo_collections (
    photo__username__id text,
    collection__username__type__id text,
    collection_photo_order bigint,
    is_cover_photo boolean,
    PRIMARY KEY (photo__username__id, collection__username__type__id)
) WITH CLUSTERING ORDER BY (collection__username__type__id ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.comments_counts (
    collection__username__type__id text,
    collection_photo_order bigint,
    counter_value counter,
    PRIMARY KEY (collection__username__type__id, collection_photo_order)
) WITH CLUSTERING ORDER BY (collection_photo_order ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.comments (
    collection__username__type__id text,
    collection_photo_order bigint,
    comment_id_order bigint,
    comment text,
    commenter_username text,
    PRIMARY KEY (collection__username__type__id, collection_photo_order, comment_id_order)
) WITH CLUSTERING ORDER BY (collection_photo_order ASC, comment_id_order ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.photos (
    username text,
    id_shard int,
    id text,
    json text,
    main_collection__username__type__id text,
    u_id text,
    PRIMARY KEY ((username, id_shard), id)
) WITH CLUSTERING ORDER BY (id ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collection_types (
    collection_type int PRIMARY KEY,
    name text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collections (
    username text,
    collection_type int,
    collection_id bigint,
    collection_data blob,
    comments_lazy bigint,
    comments_lazy_updated bigint,
    json text,
    last_modified bigint,
    max_exif_timestamp bigint,
    min_exif_timestamp bigint,
    share_date bigint,
    share_default_granted_authorization text,
    share_non_public_share boolean,
    share_remove_geo_data boolean,
    share_secret text,
    share_uri text,
    subscribed_collection_started_date bigint,
    subscribed_collection_username__type__id text,
    type_last_modified bigint static,
    PRIMARY KEY ((username, collection_type), collection_id)
) WITH CLUSTERING ORDER BY (collection_id ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collection_photos_order (
    username text,
    collection_id bigint,
    collection_photo_order bigint,
    comments_lazy bigint,
    comments_lazy_updated bigint,
    last_modified bigint static,
    likes_lazy bigint,
    likes_lazy_updated bigint,
    photo__username__id text,
    photo_metadata blob,
    PRIMARY KEY ((username, collection_id), collection_photo_order)
) WITH CLUSTERING ORDER BY (collection_photo_order DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.main_photos (
    id text PRIMARY KEY,
    json text,
    main_collection__username__type__id text,
    u_id text,
    username text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collection_titles (
    username text,
    collection_type int,
    collection_title text,
    collection__username__type__id text,
    PRIMARY KEY ((username, collection_type), collection_title)
) WITH CLUSTERING ORDER BY (collection_title ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.account_info (
    username text PRIMARY KEY,
    activated boolean,
    import_state int,
    last_read_accessed bigint,
    last_write_accessed bigint,
    locked_for_read boolean,
    locked_for_write boolean,
    u_id text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.likes_counter (
    id_a text,
    id_b text,
    counter_value counter,
    PRIMARY KEY (id_a, id_b)
) WITH CLUSTERING ORDER BY (id_b ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collection_subscribing_usernames (
    collection__username__type__id text,
    subscribing_username text,
    granted_authorization text,
    subscription_start bigint,
    PRIMARY KEY (collection__username__type__id, subscribing_username)
) WITH CLUSTERING ORDER BY (subscribing_username ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.collection_user_likes (
    id_a text,
    id_b text,
    id_c text,
    PRIMARY KEY (id_a, id_b, id_c)
) WITH CLUSTERING ORDER BY (id_b ASC, id_c ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE TABLE keyspace.likes (
    id_a text,
    id_b text,
    PRIMARY KEY (id_a, id_b)
) WITH CLUSTERING ORDER BY (id_b ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
;;;","17/Feb/17 12:48;jasobrown;The stack trace above indicates a select statement is happening. Is this occurring on ""every"" request, or only infrequently (as in a few times a minute or less)? Does this happen on all nodes, or a subset? Also, is the cluster completely transitioned to 3.0, or are you in middle of upgrading?

As far as logging goes, I'm not sure we have a way to dump error-inducing statements.;;;","17/Feb/17 12:53;ingard;We started the upgrade process and got that error repeatedly when the server came back up. We found another similar ticket CASSANDRA-12088, which again referred to CASSANDRA-12582. Atm we've upgraded all nodes to 3.0.10 and are in the process of running upgradesstables on them.
The error is still occuring on all nodes, including the ones which have finished the upgradesstables process.;;;","17/Feb/17 12:58;jasobrown;Is the exception always/only about the ""Corrupt flags value for unfiltered partition (isStatic flag set): 160""?;;;","17/Feb/17 13:04;ingard;yes;;;","17/Feb/17 13:42;jasobrown;[~ifesdjeen] or [~Stefania] any ideas what might be going here? It seems to be failing at {{UnfilteredSerializer#deserialize()}}, where it checks the extended flags to see if the row is static (apparently the flags indicate the row is static).;;;","17/Feb/17 13:49;ingard;as the upgradesstables process is running we're seeing a change (a bit too soon to conclude maybe). For instance:

cqlsh> SELECT * FROM collections WHERE username = 'redacted' AND collection_type in(0,1,2,3,4,5,6,7);
ServerError: java.io.IOError: java.io.IOException: Corrupt flags value for unfiltered partition (isStatic flag set): 160

Its failing there, but sometimes it actually returns data. So maybe the problem is related to one or more of the replicated data sets not having completed the upgradesstables for that specific table yet?;;;","17/Feb/17 16:28;stefania;My guess is that either the sstable(s) are corrupt, or it's incorrectly trying to read a static row as a regular row, like for CASSANDRA-12582. If you don't have too many sstables, you could try to reproduce it with {{tools/bin/sstabledump}}. If you can reproduce it and can share the sstable with the problem, then we should be able to debug what is going on.;;;","17/Feb/17 21:13;ifesdjeen;+1 on what [~Stefania] said.

Sometimes it's also useful to compare the outputs from the old and new sstables, might be helpful to understand how to fix the upgrade path.

If you can't show the sstables, you could also try hex-dumping the part of buffer on the exception (with {{ByteBufferUtil#bytesAsHex}}), this way you can understand if the sstable is completely off, or it's just this single byte is written incorrectly. Sometimes the offsets might be wrong, which leads into reader jumping and starting reading from the wrong point in the sstable. ;;;","17/Feb/17 23:27;jjirsa;Likely a dupe of CASSANDRA-12088 , marking as related. ;;;","18/Feb/17 07:33;ingardm;How do I go about comparing the tables? I've run upgradesstables on every
node now, but I still have snapshops from before the upgrade.;;;","20/Feb/17 08:31;ingard;[~jjirsa] Yes it might be. We found a change in the scema for one of the tables with 2 deleted static columns from back in april. However the query i mentioned earlier, was not reading from that table.;;;","20/Feb/17 08:37;ingard;The error is completely gone after upgradesstables finished on all nodes.;;;","31/Mar/17 21:04;jjirsa;[~ingard] - do you recall if your errors occurred on a cluster that started out as 2.0 and was upgraded 2.0 -> 2.2 -> 3.0 ? 

;;;","05/May/17 01:43;samt;The error is being thrown when the coordinator deserialises the response from a replica (which may actually be itself) that is reading an old format sstable. The bug is in the serialisation of the replica response which does a double read of the static row, first as the static row, then incorrectly as the partition's regular row. This only affects tables with column indexes, so doesn't repro with narrow partitions. With support for pre-3.0 sstables going away in 4.0, the problem only affects 3.0 & 3.x.
 
I've pushed 3.0 & 3.11 branches and added a dtest to catch this scenario [here|https://github.com/riptano/cassandra-dtest/pull/1469], but I need to get up to speed on the new CI workflows, especially running dtests with custom a repo/branch
||branch||
|[13236-3.0|https://github.com/beobal/cassandra/tree/13236-3.0]|
|[13236-3.11|https://github.com/beobal/cassandra/tree/13236-3.11]|;;;","05/May/17 18:16;jjirsa;Queued you up as builds #44 and #45 in dtests here: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/

Unit tests you can do on your own by linking your github account with circleci.com 
;;;","09/May/17 17:02;jjirsa;[~beobal] - Dtests have completed. I'm copying the output below because ASF jenkins doesn't keep history forever.

#44 (3.0) shows 14 failures: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/44/

{quote}
 auth_test.TestAuth.system_auth_ks_is_alterable_test (Failed 19 times in the last 30 runs. Flakiness: 65%, Stability: 36%)	2 min 4 sec	4
 bootstrap_test.TestBootstrap.resumable_bootstrap_test (Failed 1 times in the last 10 runs. Flakiness: 11%, Stability: 90%)	2 min 31 sec	1
 bootstrap_test.TestBootstrap.simultaneous_bootstrap_test (Failed 26 times in the last 30 runs. Flakiness: 27%, Stability: 13%)	3 min 15 sec	7
 consistency_test.TestConsistency.short_read_test (Failed 3 times in the last 27 runs. Flakiness: 11%, Stability: 88%)	13 min	1
 consistency_test.TestConsistency.short_read_test (Failed 3 times in the last 27 runs. Flakiness: 11%, Stability: 88%)	14 min	1
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_dc_disabled_test (Failed 3 times in the last 10 runs. Flakiness: 44%, Stability: 70%)	1 min 30 sec	1
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_dc_reenabled_test (Failed 5 times in the last 13 runs. Flakiness: 58%, Stability: 61%)	1 min 51 sec	1
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_disabled_test (Failed 5 times in the last 13 runs. Flakiness: 58%, Stability: 61%)	1 min 36 sec	1
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_enabled_test (Failed 5 times in the last 13 runs. Flakiness: 58%, Stability: 61%)	1 min 34 sec	1
 paxos_tests.TestPaxos.contention_test_many_threads (Failed 9 times in the last 30 runs. Flakiness: 44%, Stability: 70%)	3 min 15 sec	1
 repair_tests.incremental_repair_test.TestIncRepair.multiple_full_repairs_lcs_test (Failed 3 times in the last 30 runs. Flakiness: 17%, Stability: 90%)	57 sec	1
 repair_tests.incremental_repair_test.TestIncRepair.multiple_full_repairs_lcs_test (Failed 3 times in the last 30 runs. Flakiness: 17%, Stability: 90%)	58 sec	1
 repair_tests.repair_test.TestRepair.dc_parallel_repair_test (Failed 2 times in the last 13 runs. Flakiness: 25%, Stability: 84%)	2 min 2 sec	1
 repair_tests.repair_test.TestRepair.dc_repair_test (Failed 3 times in the last 30 runs. Flakiness: 17%, Stability: 90%)	2 min 3 sec	1
{quote}


#45 (3.11) shows 7 https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/45/testReport/

{quote}
 bootstrap_test.TestBootstrap.simultaneous_bootstrap_test (Failed 26 times in the last 30 runs. Flakiness: 24%, Stability: 13%)	3 min 19 sec	8
 cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_with_timeouts (Failed 2 times in the last 30 runs. Flakiness: 10%, Stability: 93%)	2 min 57 sec	1
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_dc_disabled_test (Failed 4 times in the last 11 runs. Flakiness: 40%, Stability: 63%)	2 min 34 sec	2
 hintedhandoff_test.TestHintedHandoffConfig.hintedhandoff_dc_disabled_test (Failed 4 times in the last 11 runs. Flakiness: 40%, Stability: 63%)	2 min 34 sec	2
 replace_address_test.TestReplaceAddress.fail_without_replace_test (Failed 4 times in the last 10 runs. Flakiness: 33%, Stability: 60%)	3 min 29 sec	2
 topology_test.TestTopology.size_estimates_multidc_test (Failed 23 times in the last 30 runs. Flakiness: 41%, Stability: 23%)	2 min 8 sec	1
 topology_test.TestTopology.size_estimates_multidc_test (Failed 23 times in the last 30 runs. Flakiness: 41%, Stability: 23%)	2 min 11 sec	1
{quote}

I see [an upgrade dtest|https://github.com/beobal/cassandra-dtest/commit/923b8d8d4f6738ac1afbab9221e7ec67cad09bf1] for this issue - feel that's sufficient, or were you planning on adding a unit test? ;;;","09/May/17 18:40;jjirsa;Patch looks good to me. I don't see any indication that any of the dtest failures are related to this patch (they all seem reasonably flakey on their own, except resumable bootstrap test, which seems to have completed a bootstrap while the dtest thought it would still be in progress).

+1

;;;","10/May/17 10:36;samt;Thanks, committed to 3.0 in {{415d06b1da7062c48e735c1c20ded031fa0349d2}} and merged to 3.11 & trunk (with {{--s ours}} in the latter case);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve testing on macOS by eliminating sigar logging,CASSANDRA-13233,13043791,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,16/Feb/17 23:14,15/May/20 08:00,14/Jul/23 05:56,16/Mar/17 20:50,3.0.12,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,"The changes introduced in CASSANDRA-7838 (Resolved; Fixed; 2.2.0 beta 1): ""Warn user when OS settings are poor / integrate sigar"" are not Mac friendly.

{code}

INFO  [main] 2016-10-18T11:20:10,330 SigarLibrary.java:44 - Initializing SIGAR library
DEBUG [main] 2016-10-18T11:20:10,342 SigarLog.java:60 - no libsigar-universal64-macosx.dylib in java.library.path
org.hyperic.sigar.SigarException: no libsigar-universal64-macosx.dylib in java.library.path
        at org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172) ~[sigar-1.6.4.jar:?]
        at org.hyperic.sigar.Sigar.<clinit>(Sigar.java:100) [sigar-1.6.4.jar:?]
        at org.apache.cassandra.utils.SigarLibrary.<init>(SigarLibrary.java:47) [main/:?]
        at org.apache.cassandra.utils.SigarLibrary.<clinit>(SigarLibrary.java:28) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.hash(UUIDGen.java:363) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.makeNode(UUIDGen.java:342) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.makeClockSeqAndNode(UUIDGen.java:291) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:42) [main/:?]
        at org.apache.cassandra.config.CFMetaData$Builder.build(CFMetaData.java:1278) [main/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:369) [classes/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:356) [classes/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:351) [classes/:?]
        at org.apache.cassandra.batchlog.BatchTest.defineSchema(BatchTest.java:59) [classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) [junit-4.6.jar:?]
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) [junit-4.6.jar:?]
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) [junit-4.6.jar:?]
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27) [junit-4.6.jar:?]
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) [junit-4.6.jar:?]
        at org.junit.runners.ParentRunner.run(ParentRunner.java:220) [junit-4.6.jar:?]
        at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39) [junit-4.6.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535) [ant-junit.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) [ant-junit.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) [ant-junit.jar:?]
INFO  [main] 2016-10-18T11:20:10,350 SigarLibrary.java:57 - Could not initialize SIGAR library org.hyperic.sigar.Sigar.getFileSystemListNative()[Lorg/hyperic/sigar/FileSystem;
{code}

There are 2 issues addressed by the attached patch:
# Create platform aware (windows, Darwin, linux) implementations of CLibrary (for instance CLibrary today assumes all platforms have support for posix_fadvise but this doesn't exist in the Darwin kernel). If methods are defined with the ""native"" JNI keyword in java when the class is loaded it will cause our jna check to fail incorrectly making all of CLibrary ""disabled"" even though because jnaAvailable = false even though on a platform like Darwin all of the native methods except posix_fadvise are supported.
# Replace sigar usage to get current pid with calls to CLibrary/native equivalent -- and fall back to Sigar for platforms like Windows who don't have that support with JDK8 (and without a CLibrary equivalent)",,blerer,jasobrown,krisden,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 23:14;mkjellman;28827709.diff;https://issues.apache.org/jira/secure/attachment/12853155/28827709.diff","02/Mar/17 23:04;mkjellman;CASSANDRA-13233-trunk-v2.diff;https://issues.apache.org/jira/secure/attachment/12855721/CASSANDRA-13233-trunk-v2.diff",,,,,,,,,,,,,,,,,,2.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Mar 15 16:32:33 UTC 2017,,,,,,,,,,"0|i3a7dr:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"02/Mar/17 22:18;jasobrown;Created branches for testing

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13233-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13233-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13233-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-3.11-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13233-trunk-testall/]|
;;;","02/Mar/17 22:34;jasobrown;On the whole, this is pretty good, but why did you remove the class member constants {{FILE_DESCRIPTOR_FD_FIELD}} and {{FILE_CHANNEL_FD_FIELD}} from {{CLIbrary}}? The code now needs to get the protected field on every invocation of {{getfd(FileChannel)}} and  {{getfd(FileDescriptor)}}. Is there something inherently more safe in fetching the reference to the {{Field}} on every invocation?;;;","02/Mar/17 22:51;mkjellman;No, I don't know why I did that looking at it now. Weird I'd do that though... Testing it now...;;;","02/Mar/17 23:07;mkjellman;Attaching an updated patch to add the class member constants back. Looks like this was changed to use a static constant vs calling getField each time in some 3.x version -- which was after I did the initial patch back on 2.1 and I just didn't catch it when I rebased the patch for trunk.;;;","02/Mar/17 23:25;jasobrown;Alright cool, looks like those changes came in CASSANDRA-12342. As I'm planning to fix this for 3.0+, in 3.0 we'll leave it as the functions keep fetching the {{Field}} ref on every invocation, and 3.11/trunk we'll use the class member constants. I'll update branches for cassci and rerun the tests shortly.;;;","03/Mar/17 19:23;jasobrown;Committed as sha {{e3968cfd1e786f260029cf8cde4164719ae56c53}}

[~mkjellman] feel free to change to subject of this ticket to something more apropos. It'll be the current name in the git history and CHANGES.txt, but the jira could have more more descriptive if you think it'll help. naming is hard ;);;;","15/Mar/17 14:08;blerer;Reopening as the patch prevent Cassandra to start on Windows.;;;","15/Mar/17 16:00;mkjellman;[~blerer] Can you create/clone to another JIRA for us to track the Windows regression you're seeing instead please? Do you have a stack trace?;;;","15/Mar/17 16:32;blerer;I have opened: CASSANDRA-13333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""multiple versions of ant detected in path for junit"" printed for every junit test case spawned by ""ant test""",CASSANDRA-13232,13043786,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,16/Feb/17 23:02,15/May/20 08:05,14/Jul/23 05:56,01/Mar/17 13:08,2.2.10,3.0.12,3.11.0,4.0,4.0-alpha1,,Build,,,,,0,,,,,"There is a super annoying junit warning logged before every junit test case when you run ""ant test"". This is due to the fact that the ant junit task that we have configured in our build.xml sources the system class path and most importantly what's in ant.library.dir.

    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/local/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mkjellman/Documents/mkjellman-cie-cassandra-trunk/build/lib/jars/ant-1.9.6.jar!/org/apache/tools/ant/Project.class

The fix here is to explicitly exclude the ant jar downloaded from the maven tasks that ends up in ${build.lib} and ${build.dir.lib} so only the ant libraries from the system class path are used.

I played around with excluding the ant classes/jars from the system class path in favor of using the ones we copy into ${build.lib} and ${build.dir.lib} with no success. After reading the documentation it seems you always want to use the libs that shipped with whatever is in $ANT_HOME so i believe excluding the jars from the build lib directories is the correct change anyways.",,aweisberg,jasobrown,mkjellman,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 23:02;mkjellman;673.diff;https://issues.apache.org/jira/secure/attachment/12853152/673.diff",,,,,,,,,,,,,,,,,,,1.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Mar 01 13:08:24 UTC 2017,,,,,,,,,,"0|i3a7cn:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"21/Feb/17 22:13;zznate;Marking this as patch available. ;;;","22/Feb/17 15:23;aweisberg;||code|utests|dtests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13232?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13232-testall/1/]|[dtests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13232-dtest/1/]|;;;","01/Mar/17 13:08;jasobrown;Looks like [~aweisberg] +1'd, and committed as sha {{aa66c999ad18e63f9c6b53a2da0750099ec7132c}} to 2.2, 3.0, 3.11, and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.cassandra.db.DirectoriesTest(testStandardDirs) unit test failing,CASSANDRA-13231,13043785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,16/Feb/17 22:59,07/Mar/23 11:52,14/Jul/23 05:56,04/Mar/17 23:41,2.2.10,3.0.12,3.11.0,4.0,4.0-alpha1,,,,,,,0,,,,,"The testStandardDirs(org.apache.cassandra.db.DirectoriesTest) unit test always fails. This appears to be due to a commit by Yuki for CASSANDRA-10587 which switched the SSTable descriptor to use the canonical path.

From one of Yuki's comments in CASSANDRA-10587:
""I ended up fixing Descriptor object to always have canonical path as its directory.
This way we don't need to think about given directory is relative or absolute.
In fact, right now Desctiptor (and corresponding SSTable) is not considered equal between Descriptor's directory being relative and absolute. (Added simple unit test to DescriptorTest).""

The issue here is that canonical path will expand out differently than even absolute path. In this case /var/folders -> /private/var/folders. The unit test is looking for /var/folders/... but the Descriptor expands out to /private/var/folders and the unit test fails.

Descriptor#L88 seems to be the real root cause.

   [junit] Testcase: testStandardDirs(org.apache.cassandra.db.DirectoriesTest):	FAILED
    [junit] expected:</var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42> but was:</private/var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42>
    [junit] junit.framework.AssertionFailedError: expected:</var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42> but was:</private/var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42>
    [junit] 	at org.apache.cassandra.db.DirectoriesTest.testStandardDirs(DirectoriesTest.java:159)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.DirectoriesTest FAILED

I'm guessing given we went to canonicalPath() on purpose the ""fix"" here is to call .getCanonicalFile() on both expected Files generated (snapshotDir and backupsDir) for the junit assert.",,jasobrown,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 22:59;mkjellman;674.diff;https://issues.apache.org/jira/secure/attachment/12853151/674.diff",,,,,,,,,,,,,,,,,,,1.0,mkjellman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Sat Mar 04 23:42:20 UTC 2017,,,,,,,,,,"0|i3a7cf:",9223372036854775807,2.2.9,5.0,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"04/Mar/17 13:04;jasobrown;As this is a change only to a unit test, not running the dtests:

||2.2||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13231-2.2]|[branch|https://github.com/jasobrown/cassandra/tree/13231-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13231-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13231-trunk]|
|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13231-2.2-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13231-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13231-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13231-trunk-testall/]|
;;;","04/Mar/17 13:07;jasobrown;I'll note that I was able to repo the error on macOS for cassandra 2.2 and up, but not on linux for the same branches.;;;","04/Mar/17 23:41;jasobrown;committed as sha {{44fefeffa53cf95223c4c7b6f79afaafe9b93e2e}} to 2.2, 3.0, 3.11, and trunk. Thanks!;;;","04/Mar/17 23:42;jasobrown;Note: I didn't update CHANGES.txt as this was only a fix for unit tests, and then only on one platform.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in topology_test.TestTopology.size_estimates_multidc_test,CASSANDRA-13229,13043673,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,sean.mccarthy,sean.mccarthy,16/Feb/17 15:43,01/Aug/21 11:17,14/Jul/23 05:56,12/May/17 06:54,3.11.7,4.0,4.0-alpha1,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/508/testReport/topology_test/TestTopology/size_estimates_multidc_test

{code}
Standard Output

Unexpected error in node1 log, error: 
ERROR [MemtablePostFlush:1] 2017-02-15 16:07:33,837 CassandraDaemon.java:211 - Exception in thread Thread[MemtablePostFlush:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) [main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Unexpected error in node1 log, error: 
ERROR [MigrationStage:1] 2017-02-15 16:07:33,853 CassandraDaemon.java:211 - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$496(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$222/1949434065.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_45]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1265) ~[main/:na]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:577) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager$1.runMayThrow(MigrationManager.java:402) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) [main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 16 common frames omitted
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	... 5 common frames omitted
Unexpected error in node1 log, error: 
ERROR [main] 2017-02-15 16:07:33,857 CassandraDaemon.java:663 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager.announce(MigrationManager.java:384) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager.announceNewKeyspace(MigrationManager.java:176) ~[main/:na]
	at org.apache.cassandra.service.StorageService.maybeAddKeyspace(StorageService.java:1066) ~[main/:na]
	at org.apache.cassandra.service.StorageService.maybeAddOrUpdateKeyspace(StorageService.java:1091) ~[main/:na]
	at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:1048) ~[main/:na]
	at org.apache.cassandra.service.StorageService.finishJoiningRing(StorageService.java:1043) ~[main/:na]
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:966) ~[main/:na]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:649) ~[main/:na]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:581) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:364) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:557) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:646) [main/:na]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$496(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$222/1949434065.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_45]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1265) ~[main/:na]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:577) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager$1.runMayThrow(MigrationManager.java:402) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) ~[main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) ~[na:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 16 common frames omitted
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	... 5 common frames omitted
Unexpected error in node1 log, error: 
ERROR [StorageServiceShutdownHook] 2017-02-15 16:07:35,972 AbstractCommitLogSegmentManager.java:311 - Failed to force-recycle all segments; at least one segment is still in use with dirty CFs.
{code}",,ifesdjeen,jeromatron,marcuse,pauloricardomg,philipthompson,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9639,,,,,,,"16/Feb/17 15:43;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12853079/node1.log","16/Feb/17 15:43;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12853077/node1_debug.log","16/Feb/17 15:43;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12853078/node1_gc.log","16/Feb/17 15:43;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12853082/node2.log","16/Feb/17 15:43;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12853080/node2_debug.log","16/Feb/17 15:43;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12853081/node2_gc.log","16/Feb/17 15:43;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12853085/node3.log","16/Feb/17 15:43;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12853083/node3_debug.log","16/Feb/17 15:43;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12853084/node3_gc.log",,,,,,,,,,,9.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 06:54:32 UTC 2017,,,,,,,,,,"0|i3a6nj:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"27/Mar/17 09:03;ifesdjeen;The test in [CASSANDRA-9639] has surfaced a problem with the size estimates (which is although not very realistic given we have more startup tokens). When the tokens are given manually in the test, we have to split 3 ranges into 3 parts (in dtest we use 3 data directories to ensure we check multi-disk setups). Since the given tokens aren't distributed evenly, we end up having just 1 range instead of three.

In reality (however improbable the scenario of this happening is), this fix would make sure that all the given disks are utilised. 

|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:9639-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-dtest/]|[novnode|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-novnode_dtest/]|;;;","30/Mar/17 18:58;pauloricardomg;Nice catch! I'm afraid we can't fallback to split the token ranges evenly given it's expected that a single vnode range should not span more than 1 disk (CASSANDRA-6696).

Actually in this specific case, given it's the system keyspace which spans the whole token range we could probably split the token ranges evenly (and probably should for better distribution), but when {{dontSplitRanges}} flag is passed we should always assign at least 1 vnode range per disk even if one of the disks becomes unbalanced (cases like this will become very rare after CASSANDRA-7032, but we should still protect against it).  

Although this will probably happen in rare cases when the token ranges are unbalanced and the vnode-to-disk ratio is low, we can probably tweak the {{splitOwnedRangesNoPartialRanges}} algorithm to only add more ranges to the current disk if the # of remaining tokens > # remaining parts. Does this sound reasonable or can you think of a simpler/better approach [~krummas]?;;;","31/Mar/17 11:12;marcuse;bq. it's expected that a single vnode range should not span more than 1 disk 
If there are more disks than vnode ranges, I think we should fall back to splitting vnodes over the available disks, otherwise we would leave some disks unused. This should be very rare in real production scenarios. I would assume that this is less surprising to a user than the fact that a vnode was split over several disks? Maybe even do something like avoiding to split vnode ranges onto multiple ranges if there are less than 16 tokens on the node (reason being that it is very hard to get a good balance). So, instead of checking if we run vnodes or single-token, we check if there are less than 16 tokens?

The optimisation we wanted to do in the future was to be able to take a bunch of vnode ranges offline if the disk backing the ranges fails, but not sure that is planned anymore.;;;","31/Mar/17 12:19;ifesdjeen;bq. If there are more disks than vnode ranges, I think we should fall back to splitting vnodes over the available disks, otherwise we would leave some disks unused

But wouldn't this lead to the problems with disk blacklisting and resurrecting data like Paulo said in [CASSANDRA-6696]?.. If not, we could go with ""try dividing with {{splitOwnedRangesNoPartialRanges}} first, if some disks are still unutilized, fall back to splitting vnode ranges. ;;;","31/Mar/17 15:52;pauloricardomg;Thanks for the feedback Marcus!

bq. But wouldn't this lead to the problems with disk blacklisting and resurrecting data like Paulo said in CASSANDRA-6696?

There's no actual problem from what Marcus said, we just wanted to keep vnodes tied to a single disk to be able to take vnodes offline if a specific disk fails but this hasn't been implemented yet. Furthermore CASSANDRA-10540 will probably benefit if vnodes are kept in a single disk.

With this said, we should probably keep your approach of falling back to split evenly if not all disks are used and also raise the minimum number of tokens to split by vnode from 2 to 8 to ensure a better balance (Marcus suggested 16 but 8 should be good already since there are 24 local ranges with RF=3 and CASSANDRA-7032 should also improve load balancing). We can probably reconsider this if we ever implement the optimization of blacklisting specific vnodes when disks fail.

In your initial approach, I'd just move the fallback out of {{splitOwnedRanges}} because it's a bit wrong to pass {{dontSplitRanges=true}} and have your ranges splitted. :-);;;","07/Apr/17 07:05;ifesdjeen;Thank you for clarifications. 

I've updated the branch according to your suggestions and re-triggered tests. Since fallback is now pulled one level up, I didn't write unit test for it, but it's still covered by the test that has failed. We can add an auxiliary method that handles fallback and cover it with a unit test if you think it's necessary.

|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:9639-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-dtest/]|[novnode|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-9639-trunk-novnode_dtest/]|

The patch is applicable to 3.11 and up only.
;;;","09/May/17 14:47;pauloricardomg;this fell through the cracks, sorry. LGTM, +1.
Thanks!;;;","12/May/17 06:54;ifesdjeen;Committed to 3.11 as [5af7c5ff5b287b10a5b49b2bf2890469cb627f2a|https://github.com/apache/cassandra/commit/5af7c5ff5b287b10a5b49b2bf2890469cb627f2a] and merged up to [trunk|https://github.com/apache/cassandra/commit/c66044f7818a964d60d3f265bb4cdc922b0ff39a].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI index on partition key part doesn't match,CASSANDRA-13228,13043439,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,hkroger,hkroger,16/Feb/17 01:24,16/Apr/19 09:30,14/Jul/23 05:56,10/May/17 09:22,3.11.0,,,,,,Feature/SASI,,,,,0,sasi,,,,"I created a SASI index on first part of multi-part partition key. Running query using that index doesn't seem to work.

I have here a log of queries that should indicate the issue:

{code}cqlsh:test> CREATE TABLE test1(name text, event_date date, data_type text, bytes int, PRIMARY KEY ((name, event_date), data_type));
cqlsh:test> CREATE CUSTOM INDEX test_index ON test1(name) USING 'org.apache.cassandra.index.sasi.SASIIndex';
cqlsh:test> INSERT INTO test1(name, event_date, data_type, bytes) values('1234', '2010-01-01', 'sensor', 128);
cqlsh:test> INSERT INTO test1(name, event_date, data_type, bytes) values('abcd', '2010-01-02', 'sensor', 500);
cqlsh:test> select * from test1 where NAME = '1234';

 name | event_date | data_type | bytes
------+------------+-----------+-------

(0 rows)
cqlsh:test> CONSISTENCY ALL;
Consistency level set to ALL.
cqlsh:test> select * from test1 where NAME = '1234';

 name | event_date | data_type | bytes
------+------------+-----------+-------

(0 rows){code}

Note! Creating a SASI index on single part partition key, SASI index creation fails. Apparently this should not work at all, so is it about missing validation on index creation?",,adelapena,hkroger,ifesdjeen,jeromatron,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 09:21:43 UTC 2017,,,,,,,,,,"0|i3a57j:",9223372036854775807,3.9,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"16/Feb/17 01:26;hkroger;There is a related ticket where a more thorough fix for the problem is proposed. However this could be resolved with an additional validation.;;;","16/Feb/17 13:38;ifesdjeen;We can (and probably should) just add a validation before the real solution is implemented. Creating an index should fail if it's not supported.;;;","11/Apr/17 16:39;adelapena;Here is a patch forbidding the creation of SASI indexes over partition key columns during index options validation:
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13228-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13228-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13228-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13228-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13228-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13228-trunk-dtest/]|;;;","09/May/17 13:51;ifesdjeen;""yet"" suggest we're going to support them one day :) jk 

+1, thank you for the patch!;;;","10/May/17 09:21;adelapena;Committed as [fbf14a5ebde737eff3b4e6c06ddfb6a4652a6c77|https://github.com/apache/cassandra/commit/fbf14a5ebde737eff3b4e6c06ddfb6a4652a6c77], thanks for the review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamPlan for incremental repairs flushing memtables unnecessarily,CASSANDRA-13226,13043394,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,15/Feb/17 21:50,15/May/20 08:00,14/Jul/23 05:56,17/Feb/17 22:00,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"Since incremental repairs are run against a fixed dataset, there's no need to flush memtables when streaming for them.",,bdeggleston,brstgt,marcuse,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 15:54:13 UTC 2017,,,,,,,,,,"0|i3a4xj:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"15/Feb/17 21:59;bdeggleston;|[branch|https://github.com/bdeggleston/cassandra/tree/13226]|[dtest|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13226-dtest/]|[testall|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13226-testall/]|

The only functional change is in {{src/java/org/apache/cassandra/repair/StreamingRepairTask.java}}, everything else is just rearranging some of the base repair test classes to make testing unit testing the change possible / easier.

[~krummas], [~jjirsa]: could one of you review?;;;","17/Feb/17 06:58;marcuse;+1 - maybe just add a comment why we don't need to flush on incremental repairs?;;;","17/Feb/17 22:00;bdeggleston;committed as c878b6968be88fa89fb1d1d0212411bcbc4fae7c;;;","28/Feb/17 09:29;brstgt;Isn't this also true for non-incremental repairs?
Merkle tree calculation also triggers a flush and any repair begins with a merkle tree. So there is no need to flush as the inconsistent dataset to be streamed for repair is always contained in SSTables flushed by MT calculation before.;;;","28/Feb/17 13:01;brstgt;I am referring to this ""stacktrace"":

RepairMessageVerbHandler.doVerb (case VALIDATION_REQUEST)
CompactionManager.instance.submitValidation(store, validator) 
CompactionManager.doValidationCompaction
=> StorageService.instance.forceKeyspaceFlush

After that merkle trees are calculated and based on that streams are triggered. Thats why all data that is electable for transfer has already been flushed.

Also avoiding a flush locally is only the half way. Streams REQUESTED by a stream plan also cause a flush on the sender side. But that sender also has already validated (and so flushed) the requested data.

Maybe I missed sth but from what I can see, a REPAIR stream never requires a flush.;;;","28/Feb/17 13:07;brstgt;Sorry for that many comments, just another thought:

Flushes can be optimized very easily in that way that a flush is only executed if the memtable contains mutations for the requested range OR if the memtable exceeds a certain size, so that the check is still cheap. I implemented this just for fun some months ago but did never create a ticket for it.

See patch here https://github.com/Jaumo/cassandra/commit/983514b0d3e15cea042533273ead5ea33c00bacf

Just saw it also disabled pre-repair flush as proposed before.;;;","23/Mar/17 15:23;bdeggleston;[~brstgt] I think the idea behind flushing on stream for full is that you'll be streaming even more recent data than when the merkle tree was generated, which there's really no harm in doing.;;;","23/Mar/17 15:32;brstgt;That does not make sense to me. Why should be streamed more than requested? Sounds like waste of resources to me. Streaming more than a repair requires assumes that the system is still creating inconsistent data during the repair.;;;","23/Mar/17 15:54;pauloricardomg;I think the idea behind flushing on stream was to send the most up-to-date data during bootstrap/rebuild/decommission/replace, but this doesn't apply to repair since you will end up overstreaming non-validated data as pointed out by [~brstgt]. In any case this minor improvement is subject to another ticket since this ticket is already closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testall failure in org.apache.cassandra.db.compaction.CompactionStrategyManagerPendingRepairTest.cleanupCompactionFinalized,CASSANDRA-13224,13043301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,sean.mccarthy,sean.mccarthy,15/Feb/17 16:15,16/Apr/19 09:30,14/Jul/23 05:56,06/Apr/17 13:19,,,,,,,Legacy/Testing,,,,,0,testall,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_testall/1407/testReport/org.apache.cassandra.db.compaction/CompactionStrategyManagerPendingRepairTest/cleanupCompactionFinalized

{code}
Stacktrace

junit.framework.AssertionFailedError: 
	at org.apache.cassandra.db.compaction.CompactionStrategyManagerPendingRepairTest.cleanupCompactionFinalized(CompactionStrategyManagerPendingRepairTest.java:235)
{code}",,bdeggleston,pauloricardomg,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13207,CASSANDRA-13248,CASSANDRA-13206,,,,CASSANDRA-13415,,,,,,,,,,,,,"15/Feb/17 16:15;sean.mccarthy;TEST-org.apache.cassandra.db.compaction.CompactionStrategyManagerPendingRepairTest.log;https://issues.apache.org/jira/secure/attachment/12852845/TEST-org.apache.cassandra.db.compaction.CompactionStrategyManagerPendingRepairTest.log",,,,,,,,,,,,,,,,,,,1.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 13:18:03 UTC 2017,,,,,,,,,,"0|i3a4cv:",9223372036854775807,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"04/Apr/17 23:16;pauloricardomg;It seems that the flaky tests [PendingRepairManagerTest|http://cassci.datastax.com/view/trunk/job/trunk_testall/lastCompletedBuild/testReport/org.apache.cassandra.db.compaction/PendingRepairManagerTest/history/] (CASSANDRA-13207, CASSANDRA-13248) and [CompactionStrategyManagerPendingRepairTest|http://cassci.datastax.com/view/trunk/job/trunk_testall/lastCompletedBuild/testReport/org.apache.cassandra.db.compaction/CompactionStrategyManagerPendingRepairTest/history/] (this and CASSANDRA-13206) have all a common root cause:
{noformat}
DEBUG [CompactionExecutor:4] 2017-02-20 15:10:45,045 Removing compaction strategy for pending repair c0921d40-f77e-11e6-9d70-e9c36734696d on  ks_1487603444905.tbl
{noformat}

Which seems to be caused by a race with auto-compactions triggered by flush, so the simple fix is to just disable autocompaction during these tests. I managed to reproduce this with [this multiplexer run|https://cassci.datastax.com/job/pauloricardomg-testall-multiplex/4/] and fixed by disabling autocompaction during tests: [fixed multiplexer run|https://cassci.datastax.com/job/pauloricardomg-testall-multiplex/5/]

Besides this, I also aborted transactions created by {{PendingRepairManager.getNextBackgroundTask}} after the assertions since they are causing {{LEAK DETECTED}} errors.

I am assuming (and hoping) you haven't started working on this yet so if you haven't done so would you mind reviewing this [~bdeggleston]? Otherwise I'm happy to review your patch if you already have one in place. Thanks!

||trunk||
|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13224]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-13224-testall/lastCompletedBuild/testReport/]|;;;","05/Apr/17 14:47;bdeggleston;Awesome, thank you Paulo. +1;;;","06/Apr/17 13:18;pauloricardomg;Merged as {{30820eacb9c565d06260031260910f020e16e83d}} to trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paging with reverse queries and static columns may return incorrectly sized pages,CASSANDRA-13222,13043255,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,15/Feb/17 13:08,16/Apr/19 09:30,14/Jul/23 05:56,20/Feb/17 12:16,2.2.10,,,,,,Legacy/CQL,Legacy/Local Write-Read Paths,,,,0,,,,,"There are 2 specialisations of {{ColumnCounter}} that deal with static columns differently depending on the order of iteration through the column family and which impl is used generally depends on whether or not the {{ColumnFilter}} in use is reversed. However, the base method {{ColumnCounter::countAll}} always uses forward iteration, which can result in overcounting when the query is reversed and there are statics involved. In turn, this leads to incorrectly sized pages being returned to the client.",,jasobrown,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 20 12:16:20 UTC 2017,,,,,,,,,,"0|i3a42n:",9223372036854775807,2.1.16,2.2.8,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"15/Feb/17 13:58;samt;As expected, this doesn't repro on 3.0+ so I've haven't pushed branches for 3.11 or trunk as like the 3.0 branch they'll only contain the new test.

||branch||testall||dtest||
|[13222-2.1|https://github.com/beobal/cassandra/tree/13222-2.1]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-2.1-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-2.1-dtest]|
|[13222-2.2|https://github.com/beobal/cassandra/tree/13222-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-2.2-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-2.2-dtest]|
|[13222-3.0|https://github.com/beobal/cassandra/tree/13222-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-13222-3.0-dtest]|
;;;","16/Feb/17 13:09;jasobrown;3.0 dtest hadn't run yet so I kicked it off. The 2.1/2.2 dtests have failures wrt the paging test:

{code}
""'TestPagingData' object has no attribute 'create_ks'
{code}

That appears to be an incorrectly written dtest. I'll submit a PR to fix that later.

I'm +1 on the code. The only tiny nit I would have is to add a comment to the 3.0+ versions of {{QueryPagerTest#pagingReversedQueriesWithStaticColumnsTest}} to indicate that we had a problem with reverse paging/statics/et al in pre-3.0, but that it shouldn't be problem now (and the test is to make sure everything stays legit into the future).

As for which versions to commit to, this doesn't seem critical enough for 2.1, so let's do 2.2+.;;;","20/Feb/17 12:16;samt;Thanks, committed to 2.2 in {{3f450107749c637ba133e479e7b6e5cd02e1153f}} and merged (test only) to 3.0/3.11/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra.yaml now unicode instead of ascii after 13090,CASSANDRA-13219,13043051,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aweisberg,philipthompson,philipthompson,14/Feb/17 20:00,15/May/20 08:06,14/Jul/23 05:56,14/Feb/17 21:50,2.2.9,3.0.11,3.11.0,4.0,4.0-alpha1,,Local/Config,,,,,0,,,,,"After CASSANDRA-13090, which was commit 5725e2c422d21d8efe5ae3bc4389842939553650, cassandra.yaml now has unicode characters, specifically [0xe2|http://utf8-chartable.de/unicode-utf8-table.pl?start=8320&number=128&names=2&utf8=0x]. Previously, it was only ascii.

This is an admittedly minor change, but it is breaking. It affects (at least) a subset of python yaml parsing tools (which is a large number of tools that use C*).",,aweisberg,jasobrown,jjordan,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 20:21;mshuler;utf8-to-ascii_yaml.patch;https://issues.apache.org/jira/secure/attachment/12852648/utf8-to-ascii_yaml.patch",,,,,,,,,,,,,,,,,,,1.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 21:50:25 UTC 2017,,,,,,,,,,"0|i3a2tb:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"14/Feb/17 20:26;jjordan;+1 fix LGTM

{code}
$ pcregrep --color='auto' -n '[^\x00-\x7F]' conf/cassandra.yaml
964:# On bare metal, the floor for packet processing throughput is high enough that many applications won���t notice, but in
966:# surprisingly low compared to the throughput of task processing that is possible inside a VM. It���s not that bare metal
967:# doesn���t benefit from coalescing messages, it���s that the number of packets a bare metal network interface can process
$ git apply ~/Downloads/utf8-to-ascii_yaml.patch
$ pcregrep --color='auto' -n '[^\x00-\x7F]' conf/cassandra.yaml
$
{code};;;","14/Feb/17 21:05;jasobrown;+1. I checked it out via hexdump and in the original I can see the utf-encoded value, and with the new patch i see the ascii values.;;;","14/Feb/17 21:20;aweisberg;This merged forward cleanly, but just to make sure it compiles I had cassci run the unit tests and dtests for 2.2, 3.0, and 3.11. 

https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-2.2-testall/1/
https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-3.0-testall/1/
https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-3.11-testall/1/

The dtests failed when I started them initially:
https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-3.0-dtest/1/
{noformat}
[artifact:dependencies] Transferring 4K from central
[artifact:dependencies] Downloading: commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.pom from repository apache at https://repository.apache.org/content/repositories/releases
Err: 




Build step 'Execute shell' marked build as failure
Performing Post build task...
Could not match :Aborted by  : False
Logical operation result is FALSE
{noformat}
It's not clear why but I restarted one of them and that is running and seems happy.

I'll wait for the one to pass and if that is good enough for review I'll commit.
https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-3.0-dtest/2/;;;","14/Feb/17 21:50;aweisberg;Committed as [9a80f803c2ec9a4a74cb8a99293dc81ef3dc183d|https://github.com/apache/cassandra/commit/9a80f803c2ec9a4a74cb8a99293dc81ef3dc183d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duration validation error is unclear in case of overflow.,CASSANDRA-13218,13042965,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,14/Feb/17 14:41,15/May/20 07:59,14/Jul/23 05:56,11/May/17 08:44,3.11.0,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"If a user try to insert a {{duration}} with a number of months or days that cannot fit in an {{int}} (for example: {{9223372036854775807mo1d}}), the error message is confusing.",,aboudreault,adelapena,adutra,blerer,stamhankar999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 08:44:06 UTC 2017,,,,,,,,,,"0|i3a2a7:",9223372036854775807,,,,,,,adelapena,,adelapena,,,Normal,,,,,,,,,,,,,,,,,,,"14/Feb/17 16:32;stamhankar999;Where is it specified that these are supposed to be int's? I was under the impression that since we're transmitting zigzag-encoded vint values and that format supports a 64-bit range, these attributes are intended to have signed 64-bit range. Is there a particular reason to restrict it?;;;","15/Feb/17 12:44;blerer;The encoding format support effectively a 64-bit range but {{months}} and {{days}} are stored as {{int}} to limit the memory usage and because larger numbers do not really make sense.
The duration format is not intended to store things like the age of the Earth as you do not need a nanoseconds precision for those type of information.
A {{days}} value of 2^32 (roughly 5,883,516 year) is far more than the age of mankind.

Now, I fully agree that it should have been documented properly and that the validation message should be more expicit about the problem. ;;;","15/Feb/17 16:31;stamhankar999;Ok, sounds reasonable. Thanks.;;;","26/Apr/17 00:07;stamhankar999;Found a few other bad side effects of the current behavior:

1. The validated (truncated) values are not actually re-encoded to vint's and stored; the user-provided bytes are stored. This is bad because it introduces an inconsistency of behavior -- a client can insert/select 64-bit values for months and days, but when C* exports the data as json (for example), the Duration.deserialize method will produce an object with truncated values.
2. Validation doesn't check for truncation error and reject the insert.
3. I think validation will succeed for this illegal set of values provided by a driver: (-9223372036854775807, 1, 1) because the months attribute will be coerced to 1 (since the highest 4 bytes will be dropped).

*Proposal 1*: accept and handle 64-bit values for months and days consistently. I don't know how often these values are used in C*, but an extra 8 bytes doesn't sound like it's worth the hassle of treating months and days differently than nanos. It makes C* code a little more complex; it makes driver code a little more complex (for drivers that are type-aware at least).
*Proposal 2*: beef up the validation logic to reject values that would overflow/underflow during long->int truncation. Write a tool that users can use to identify illegal rows they inserted because of the holes in the validation logic and give users an opportunity to clean up such bad data.

Admittedly, the chances that users have inserted illegal duration's is relatively small, particularly since this new type hasn't been around very long. But it seems much simpler to make those values legal, so I'd lean towards my first proposal.;;;","10/May/17 13:30;blerer;Unfortunately, there are other factors that force us to use 32-bit values for months and days. Java utilities like {{Calendar}} accept only {{int}} for months or days value and we already use {{Calendar}} for data arithmetics. Due to that the solution is simply to reject to values at write time.

I pushed a patch for that [here|https://github.com/apache/cassandra/compare/trunk...blerer:13128-3.11].

I think that the chances that users have inserted illegal durations is rather null. 
The 2 reasons for that are that:
# duration is not really usefull in itself and was mainly added for simplifying the life of user querying timeseries data
# the invalid values are greater than the age of mankind and by consequence have not much sense in the real life

[~adelapena] could you review? ;;;","10/May/17 14:41;adelapena;[~blerer] sure, can you set me as reviewer?;;;","10/May/17 16:28;adelapena;Indeed, a 32-bit integer seems more than enough for months. I either can't figure out any use case involving time intervals over 5,883,516 years and a precision higher than millennia.

The patch looks good to me, +1.

||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13128-3.11.patch]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/adelapena-13128-3.11-testall/]|;;;","11/May/17 08:43;blerer;Thanks for the review.;;;","11/May/17 08:44;blerer;Committed into 3.11 at 8693357109a6e59117a641e109c3865501e3eee6 and merged into trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minor bugs related to CASSANDRA-9143,CASSANDRA-13217,13042724,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,13/Feb/17 23:25,15/May/20 08:03,14/Jul/23 05:56,14/Feb/17 02:19,4.0,4.0-alpha1,,,,,,,,,,0,,,,,"We found a few minor bugs found in an internal review:
* -incorrect log argument order [here|https://github.com/apache/cassandra/blob/edcbef3e343778b4d5affe019f64c89da2a13aa2/src/java/org/apache/cassandra/streaming/compress/CompressedStreamReader.java#L75]-
* {{SSTableReader#intersects}} should use Bounds, not Range ([here|https://github.com/apache/cassandra/blob/edcbef3e343778b4d5affe019f64c89da2a13aa2/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L1761])
* {{CompactionStrategyManager#validateForCompaction}} doesn't prevent sstables from different repair session from being compacted together [here|https://github.com/apache/cassandra/blob/edcbef3e343778b4d5affe019f64c89da2a13aa2/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L1761]",,bdeggleston,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 16:23:56 UTC 2017,,,,,,,,,,"0|i3a0sv:",9223372036854775807,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"13/Feb/17 23:28;bdeggleston;|[trunk|https://github.com/bdeggleston/cassandra/tree/13217] | [dtests|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13217-dtest/] | [testall|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13217-testall/] |;;;","13/Feb/17 23:36;jjirsa;Looks like {{e1dda71}} ninja fixed the log ordering, so your log order fix isn't needed (because Dave already flipped it for you). ;;;","13/Feb/17 23:37;bdeggleston;bq. Looks like e1dda71 ninja fixed the log ordering, so your flipping is wrong in this patch (because Dave already flipped it for you).

Removed the log order unfixing commit;;;","13/Feb/17 23:43;jjirsa;The other 2 commits look good to me, +1
;;;","14/Feb/17 16:23;jjirsa;Blake committed as {{3f3db2d40d6b5edbf079b917953a30bcc1209d25}} ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testall failure in org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages,CASSANDRA-13216,13042588,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,sean.mccarthy,sean.mccarthy,13/Feb/17 14:51,15/May/20 08:02,14/Jul/23 05:56,13/Jun/17 09:27,3.0.14,3.11.0,4.0,4.0-alpha1,,,Legacy/Testing,,,,,0,testall,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-3.11_testall/81/testReport/org.apache.cassandra.net/MessagingServiceTest/testDroppedMessages

{code}
Error Message

expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:83)
{code}",,ifesdjeen,jasobrown,jjirsa,mkjellman,mshuler,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/17 14:34;mshuler;TEST-org.apache.cassandra.net.MessagingServiceTest.log;https://issues.apache.org/jira/secure/attachment/12860880/TEST-org.apache.cassandra.net.MessagingServiceTest.log","13/Feb/17 14:51;sean.mccarthy;TEST-org.apache.cassandra.net.MessagingServiceTest.log;https://issues.apache.org/jira/secure/attachment/12852364/TEST-org.apache.cassandra.net.MessagingServiceTest.log",,,,,,,,,,,,,,,,,,2.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 13 09:27:31 UTC 2017,,,,,,,,,,"0|i39zyn:",9223372036854775807,,,,,,,mkjellman,,mkjellman,,,Normal,,,,,,,,,,,,,,,,,,,"17/Feb/17 10:08;ifesdjeen;Problem is that when the test is running subsequently or there was some test that has already modified values of dropped messages metrics, since they're stored in mbean, they'll still be retrieved. 

For example, on the subsequent run we'd get the following metrics: 
{code}
 {READ=7500, RANGE_SLICE=7500... }
{code}

Instead, we should have just gotten 0es. 

Unfortunately, dropwizard metrics does not give any simple mechanism for resetting the metrics. The path of least resistance I've currently found is just to assign a unique name to the scope while testing the metrics that have to be test-unique and resettable.;;;","06/Mar/17 07:49;ifesdjeen;|[3.0|https://github.com/ifesdjeen/cassandra/tree/13216-3.0]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-3.0-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-3.0-dtest/]|
|[3.11|https://github.com/ifesdjeen/cassandra/tree/13216-3.11]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-3.11-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-3.11-dtest/]|
|[trunk|https://github.com/ifesdjeen/cassandra/tree/13216-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-trunk-dtest/]|;;;","10/Mar/17 05:23;mkjellman;who's reviewing this? would be great to get all our tests passing on trunk! [~jasobrown] [~aweisberg] I agree this seems like a reasonable approach because MessagingService is a singleton and we can't reset most of the metrics core objects.... without any other ideas on this i'm +1 on alex's commit.;;;","10/Mar/17 14:35;jasobrown;[~mkjellman] Looks like you are reviewing it :P. I'll give this look over, as well.;;;","10/Mar/17 14:39;jasobrown;OK, looked it over, and I'm +1, as well.;;;","17/Mar/17 22:44;jjirsa;Since this was routinely breaking unit tests, I've gone ahead and committed it as {{1dcb3131a4d7417634551456f1fe3f519fa17fd0}} . Hope nobody minds.
;;;","28/Mar/17 14:32;mshuler;Reopening, since this was so recent. Trunk run that just finished recently shows a similar error on
http://cassci.datastax.com/job/trunk_testall/1477/testReport/org.apache.cassandra.net/MessagingServiceTest/testDroppedMessages_compression/

{noformat}
Error Message

expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
Stacktrace

junit.framework.AssertionFailedError: expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:114)
Standard Output

ERROR [main] 2017-03-28 11:07:24,199 SubstituteLogger.java:250 - SLF4J: stderr
INFO  [main] 2017-03-28 11:07:24,668 YamlConfigurationLoader.java:89 - Configuration location: file:/home/automaton/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2017-03-28 11:07:24,669 YamlConfigurationLoader.java:108 - Loading settings from file:/home/automaton/cassandra/test/conf/cassandra.yaml
INFO  [main] 2017-03-28 11:07:25,547 Config.java:446 - Node configuration:[allocate_tokens_for_keyspace=null; authentica
...[truncated 10545 chars]...
close for /127.0.0.250
DEBUG [main] 2017-03-28 11:07:28,551 MessagingService.java:698 - Not resetting pool for /127.0.0.250 because internode authenticator said not to connect
DEBUG [main] 2017-03-28 11:07:28,583 OutboundTcpConnection.java:184 - Enqueuing socket close for /127.0.0.2
DEBUG [main] 2017-03-28 11:07:28,583 OutboundTcpConnection.java:184 - Enqueuing socket close for /127.0.0.2
DEBUG [main] 2017-03-28 11:07:28,584 OutboundTcpConnection.java:184 - Enqueuing socket close for /127.0.0.2
{noformat};;;","28/Mar/17 14:34;mshuler;new log from above attached;;;","29/Mar/17 11:43;ifesdjeen;This is extremely weird. I've ran the test 100 times [here|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-trunk-testall/lastCompletedBuild/consoleFull] and it passed every time. Checked for the interference (by ensuring that each test class is running in it's own forked JVM), and confirmed it (by outputting and comparing PIDs of the processes in the log files). I've checked for possible races / reordering and could not find any problem so far. Looks like we can only reproduce this issue when running a full test suite, not an individual test.;;;","29/Mar/17 12:39;ifesdjeen;Found the problem. I didn't anticipate initially that this test is time-dependent. The initial fix is still applicable. It's reproducible quite easily by adding a {{sleep}} of as few as 100 milliseconds around [here|https://github.com/apache/cassandra/blob/732d1af866b91e5ba63e7e2a467d99d4cb90e11f/test/unit/org/apache/cassandra/net/MessagingServiceTest.java#L112]. YMMV with an exact sleep number. 

However, I do not think there's any way we can reliably fetch latency numbers, since dropwizard metrics reservoirs (used within [timers|https://github.com/dropwizard/metrics/blob/15dde825de1843927898a7ad3c3bb11b2913a931/metrics-core/src/main/java/com/codahale/metrics/Timer.java#L64] are tracking real time, and snapshots we're doing (however precise) won't ever be perfect. I've mocked the clock:

||[3.11|https://github.com/ifesdjeen/cassandra/tree/13216-followup-3.11]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-3.11-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-3.11-dtest/]|
||[trunk|https://github.com/ifesdjeen/cassandra/tree/13216-followup-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-trunk-dtest/]|

3.0 branch is not susceptible to this problem, since we use time-independent [Meter|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/metrics/DroppedMessageMetrics.java#L31] instead of timer there.
Let's wait for 24 hours, I've put the utest on retry.;;;","30/Mar/17 08:26;ifesdjeen;CI looks good, no failures for this test in 10 builds. On trunk, 11th build contained 64 failures, but since there were no changes inbetween and given the nature of errors there I tend to believe it's an enviroment issue. ;;;","03/Apr/17 22:19;mkjellman;Yes, I can review again.;;;","08/May/17 08:23;ifesdjeen;Do you think we can get that reviewed any time soon? It'd be great, as this is one of the few test failures we get quite often.;;;","08/May/17 20:31;mkjellman;Yup. I'm really sorry I let this slip. I owe you a beer the next time I see you. Will do it today.;;;","08/May/17 21:54;mkjellman;With this change, the ""mocked"" Clock in MessagingServiceTest will always return 0 for getTick()
{code}
    private static long time = System.currentTimeMillis();
    private static Clock clock = new Clock()
    {
        public long getTick()
        {
            return 0;
        }

        public long getTime()
        {
            return time;
        }
    };
{code}

But if we look at the implementation of time() in metrics-core Timer, it does the following:
{code}
    /**
     * Times and records the duration of event.
     *
     * @param event a {@link Callable} whose {@link Callable#call()} method implements a process
     *              whose duration should be timed
     * @param <T>   the type of the value returned by {@code event}
     * @return the value returned by {@code event}
     * @throws Exception if {@code event} throws an {@link Exception}
     */
    public <T> T time(Callable<T> event) throws Exception {
        final long startTime = clock.getTick();
        try {
            return event.call();
        } finally {
            update(clock.getTick() - startTime);
        }
    }
{code}

So, from my understanding this means we will always just do 0-0 for the update() call on the Timer... right?

However, I don't think any of this matters in retospect. Took a big step back and looked over the actual unit test and what this thing is testing with [~jjirsa] and [~jasobrown] and all 3 of us think this magic number a bit questionable.

If we look at the original patch that added these magic numbers in the first place for CASSANDRA-10580 (https://github.com/pcmanus/cassandra/commit/c9ef25fd81501005b6484baf064081efc557f3f4) there is nothing in the ticket or test or commit that justifies testing for these magic numbers and it looks like this is just going to be dependent on how fast your system can iterate thru the logic 5000 times.

So: I'd like to propose that we throw away the 2nd assert in this test. The first and last are good (counting the number that we expect to get) but doing a literal string compare on the entire log message is kinda unhelpful. Instead, we should throw a regex here on the log message, parse out the times and just check that they are > 0. Thoughts?;;;","08/May/17 22:16;jjirsa;+1 to killing magic numbers.
;;;","09/May/17 09:58;ifesdjeen;No problem, I was hesitant to remind as I realise you have a bunch of other things to do.

bq. With this change, the ""mocked"" Clock in MessagingServiceTest will always return 0 for getTick()

This is exactly what we do, yes. Main reason for that change was because we do not rely on the wall clock, otherwise because of interference the test ends up taking more time and was time-dependent. 

Replaced string comparisons with parsing and number checks, looks cleaner now.;;;","09/May/17 10:01;mkjellman;Awesome! Are you going to push that to the same 13216-followup-3.11 branch on your GitHub fork?;;;","09/May/17 11:40;ifesdjeen;Oh yes I already did, was just waiting for CI results (which by now came clean):

||[3.11|https://github.com/ifesdjeen/cassandra/tree/13216-followup-3.11]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-3.11-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-3.11-dtest/]|
||[trunk|https://github.com/ifesdjeen/cassandra/tree/13216-followup-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13216-followup-trunk-dtest/]|;;;","16/May/17 15:47;mkjellman;Looks great! +1 Thanks Alex;;;","13/Jun/17 09:27;ifesdjeen;Thank you! 

Committed to 3.11 as [d8fb9349df818659c54d57b2d2c95ebbd0405d49|https://github.com/apache/cassandra/commit/d8fb9349df818659c54d57b2d2c95ebbd0405d49] and merged up to [trunk|https://github.com/apache/cassandra/commit/09c837f75b5814fc92d3d1576f698537ef28ee2c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use portable stderr for java error in startup,CASSANDRA-13211,13042281,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mshuler,maxb,maxb,11/Feb/17 13:20,15/May/20 08:04,14/Jul/23 05:56,16/Feb/17 00:03,2.1.17,2.2.9,3.0.11,3.11.0,4.0,4.0-alpha1,,,,,,0,,,,,"The cassandra startup shell script contains this line:

    echo Unable to find java executable. Check JAVA_HOME and PATH environment variables. > /dev/stderr

The problem here is the construct ""> /dev/stderr"". If the user invoking Cassandra has changed user (for example, by SSHing in as a personal user, and then sudo-ing to an application user responsible for executing the Cassandra daemon), then the attempt to open /dev/stderr will fail, because it will point to a PTY node under /dev/pts/ owned by the original user.

Ultimately this leads to the real problem being masked by the confusing error message ""bash: /dev/stderr: Permission denied"".

The correct technique is to replace ""> /dev/stderr"" with "">&2"" which will write to the already open stderr file descriptor, instead of resolving the chain of symlinks starting at /dev/stderr, and attempting to reopen the target by name.",,maxb,mshuler,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/17 18:54;mshuler;13211_use-portable-stderr.patch;https://issues.apache.org/jira/secure/attachment/12852402/13211_use-portable-stderr.patch",,,,,,,,,,,,,,,,,,,1.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 00:03:58 UTC 2017,,,,,,,,,,"0|i39y2n:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"13/Feb/17 03:19;zznate;I have a vague recollection of dealing with pseudoterminal ownership issues when doing {{su}}, so I could see this. [~brandon.williams] probably has a lot better bash foo than me. ;;;","13/Feb/17 17:50;brandon.williams;Ping [~mshuler];;;","13/Feb/17 18:35;mshuler;I think this change to {{echo ... >&2}} would be fine and accomplishes the same. (When I move my /usr/bin/java out of the way, I get trapped earlier in the version check in cassandra-env.sh, since no java can be found);;;","13/Feb/17 18:54;mshuler;cassandra-2.1-based patch attached. A test job will never hit this, since we set up java properly on test servers before doing any testing.;;;","16/Feb/17 00:03;mshuler;Commit {{cb090791c3d16011665f0f56afd66bbce2a0e40f}} pushed to cassandra-2.1 and up. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges,CASSANDRA-13210,13042211,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,mshuler,mshuler,10/Feb/17 23:04,16/Apr/19 09:30,14/Jul/23 05:56,14/Feb/17 22:53,,,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_large_dtest/53/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

{noformat}
Error Message

'Repaired at: 0' unexpectedly found in 'SSTable: /tmp/dtest-N7zjo6/test/node1/data0/keyspace1/standard1-a79a0c50efa211e6bf211330662f36ef/md-6-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486739263230000\nMaximum timestamp: 1486739263230000\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: 296988783704308703 (key=30503337373039503231)\nLast token: 296988783704308703 (key=30503337373039503231)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: b099d1f0-efa2-11e6-89ec-d14624f1e47e\nReplay positions covered: {CommitLogPosition(segmentId=1486739234777, position=46796)=CommitLogPosition(segmentId=1486739234777, position=50819)}\ntotalColumnsSet: 5\ntotalRows: 1\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0                 2\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                        0                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 1\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486739263230000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-N7zjo6/test/node1/data0/keyspace1/standard1-a79a0c50efa211e6bf211330662f36ef/md-4-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486739263253000\nMaximum timestamp: 1486739263253000\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -2998312467040114775 (key=4f503030314c35393330)\nLast token: -2998312467040114775 (key=4f503030314c35393330)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: b099d1f0-efa2-11e6-89ec-d14624f1e47e\nReplay positions covered: {CommitLogPosition(segmentId=1486739234777, position=46796)=CommitLogPosition(segmentId=1486739234777, position=50819)}\ntotalColumnsSet: 5\ntotalRows: 1\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0                 2\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                        0                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 1\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486739263253000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-N7zjo6/test/node1/data0/keyspace1/standard1-a79a0c50efa211e6bf211330662f36ef/md-8-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486739263242000\nMaximum timestamp: 1486739263242000\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: 3151688024208981352 (key=4f384c4b37394c4f3631)\nLast token: 3151688024208981352 (key=4f384c4b37394c4f3631)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: b099d1f0-efa2-11e6-89ec-d14624f1e47e\nReplay positions covered: {CommitLogPosition(segmentId=1486739234777, position=46796)=CommitLogPosition(segmentId=1486739234777, position=50819)}\ntotalColumnsSet: 5\ntotalRows: 1\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0                 2\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                        0                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 1\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486739263242000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\n'
{noformat}

{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 644, in sstable_marking_test_not_intersecting_all_ranges
    self.assertNotIn('Repaired at: 0', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 810, in assertNotIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{noformat}",,bdeggleston,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/17 23:04;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12852144/node1.log","10/Feb/17 23:04;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12852145/node1_debug.log","10/Feb/17 23:04;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12852146/node1_gc.log","10/Feb/17 23:04;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12852147/node2.log","10/Feb/17 23:04;mshuler;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12852148/node2_debug.log","10/Feb/17 23:04;mshuler;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12852149/node2_gc.log","10/Feb/17 23:04;mshuler;node3.log;https://issues.apache.org/jira/secure/attachment/12852150/node3.log","10/Feb/17 23:04;mshuler;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12852151/node3_debug.log","10/Feb/17 23:04;mshuler;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12852152/node3_gc.log","10/Feb/17 23:04;mshuler;node4.log;https://issues.apache.org/jira/secure/attachment/12852153/node4.log","10/Feb/17 23:04;mshuler;node4_debug.log;https://issues.apache.org/jira/secure/attachment/12852154/node4_debug.log","10/Feb/17 23:04;mshuler;node4_gc.log;https://issues.apache.org/jira/secure/attachment/12852155/node4_gc.log",,,,,,,,12.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 22:53:16 UTC 2017,,,,,,,,,,"0|i39xn3:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"13/Feb/17 21:53;bdeggleston;filed https://github.com/riptano/cassandra-dtest/pull/1442 to fix;;;","14/Feb/17 22:53;bdeggleston;dtest PR was merged, and this is no longer failing

http://cassci.datastax.com/job/trunk_large_dtest/55/testReport/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_blogposts_with_max_connections,CASSANDRA-13209,13042209,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,mshuler,mshuler,10/Feb/17 22:59,16/Apr/19 09:30,14/Jul/23 05:56,06/Jun/17 00:58,2.2.10,3.0.14,3.11.0,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/528/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_blogposts_with_max_connections

{noformat}
Error Message

errors={'127.0.0.4': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.4
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-792s6j
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: removing ccm cluster test at: /tmp/dtest-792s6j
dtest: DEBUG: clearing ssl stores from [/tmp/dtest-792s6j] directory
dtest: DEBUG: cluster ccm directory: /tmp/dtest-uNMsuW
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.5 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 datacenter1> discovered
dtest: DEBUG: Running stress with user profile /home/automaton/cassandra-dtest/cqlsh_tests/blogposts.yaml
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 1090, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2571, in test_bulk_round_trip_blogposts_with_max_connections
    copy_from_options={'NUMPROCESSES': 2})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2500, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2473, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
""errors={'127.0.0.4': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.4\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-792s6j\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: removing ccm cluster test at: /tmp/dtest-792s6j\ndtest: DEBUG: clearing ssl stores from [/tmp/dtest-792s6j] directory\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-uNMsuW\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.5 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 datacenter1> discovered\ndtest: DEBUG: Running stress with user profile /home/automaton/cassandra-dtest/cqlsh_tests/blogposts.yaml\n--------------------- >> end captured logging << ---------------------""
{noformat}",,KurtG,mshuler,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/17 23:49;KurtG;13209.patch;https://issues.apache.org/jira/secure/attachment/12870888/13209.patch","10/Feb/17 22:59;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12852138/node1.log","10/Feb/17 22:59;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12852139/node2.log","10/Feb/17 22:59;mshuler;node3.log;https://issues.apache.org/jira/secure/attachment/12852140/node3.log","10/Feb/17 22:59;mshuler;node4.log;https://issues.apache.org/jira/secure/attachment/12852141/node4.log","10/Feb/17 22:59;mshuler;node5.log;https://issues.apache.org/jira/secure/attachment/12852142/node5.log",,,,,,,,,,,,,,6.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 06 00:58:23 UTC 2017,,,,,,,,,,"0|i39xmn:",9223372036854775807,,,,,,,stefania,,stefania,,,Normal,,,,,,,,,,,,,,,,,,,"16/May/17 08:23;KurtG;although timeouts do appear in the failures, most of them seem to be occurring because the CSV's don't have the expected number of lines. sometimes it's more, sometimes less.
A better example is https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-dtest/134/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_blogposts_with_max_connections/



seems that the blogposts tests have previously been an issue that wasn't resolved https://issues.apache.org/jira/browse/CASSANDRA-10938

;;;","19/May/17 01:49;KurtG;actually it's always less, and it appears the majority of the issues come from the COPY FROM. Not every test failure seems to be caused by the same thing, but the majority appear to be because when timeouts occur in the COPY FROM they don't actually get retried, so some rows don't get written. Also once the COPY FROM code in cqlsh gets over 1000 failed rows it exits, which kind of explains why a lot of the failures are because of a difference of a 1000 rows.

Corresponding error is the following, note it mentions the # of attempts, however it will never go past 1.
{code}<stdin>:2:Failed to import 9 rows: OperationTimedOut - errors={'127.0.0.3': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.3,  will retry later, attempt 1 of 5{code}

Going to fix the error handling in the COPY FROM command to actually do retries (like COPY TO does), that should make the tests less flaky, as COPY TO also suffers from the timeouts but effectively retries and thus rarely has issues. There is still the underlying problem of why COPY is timing out in the first place, but to be honest I'd put it down to the command and nodes simply using too much resources on the servers. If it's still very flaky after fixing the retries we can look into performance issues more.

;;;","22/May/17 05:34;stefania;bq. however it will never go past 1.

If it succeeds importing rows in a following attempt, it would not log attempt no. 2.

bq. Going to fix the error handling in the COPY FROM command to actually do retries (like COPY TO does),

COPY FROM does retry on timeouts, otherwise {{test_bulk_round_trip_with_timeouts}} would always fail. The code that retries is in the [error callback|https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L2583].  These [two lines|https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L2579] just above are because of [PYTHON-652|https://datastax-oss.atlassian.net/browse/PYTHON-652], which doesn't seem fixed yet. You may want to try and see if by any chance these lines cause some time outs not to get retried but I doubt it. The reason why COPY TO and COPY FROM have difference retry mechanisms is performance (CASSANDRA-11053).

bq. Also once the COPY FROM code in cqlsh gets over 1000 failed rows it exits,

This is configurable, see [here|https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L362].;;;","24/May/17 09:21;KurtG;Thanks [~Stefania]. That's awfully misleading... 

It seems to me that this means that if there enough first attempt failures that trigger the log message to add up to over 1000 rows the COPY FROM will stop. Is that correct? If so it could account for _some_ of the failures but not all, as some of the failed jobs don't have the {{Exceeded maximum number of insert errors 1000}} error.;;;","25/May/17 01:44;stefania;bq. That's awfully misleading...

I'm not sure what you are referring to but, if it is the print statement on the number of attempts you could consider adding a statement when a batch completes with number of attempts > 0, I would keep it at debug level though.

bq. It seems to me that this means that if there enough first attempt failures that trigger the log message to add up to over 1000 rows the COPY FROM will stop. Is that correct?

Yes.

bq. If so it could account for some of the failures but not all,

Also correct, if it completed with fewer rows than expected then there is something else we don't understand yet.;;;","01/Jun/17 23:49;KurtG;{quote}I'm not sure what you are referring to but, if it is the print statement on the number of attempts you could consider adding a statement when a batch completes with number of attempts > 0, I would keep it at debug level though.
{quote}
One could say that the bug made it misleading. Now that I'm more familiar with the error handling between the tasks and the processes it makes sense, however the fact that insert errors were counted for every attempt made it misleading.

So I think the attached 1 line patch is the way to go, and from my testing it seems to make the test a lot more stable. Occasionally still get client read timeout errors early on which break the test, but this appears to be a resource issue, and doesn't happen when I run the test on a machine more powerful than my laptop.

Patch simply makes it so we count the total insert errors based on failing all attempts, rather than just one attempt. It might be nice to have a dtest that does the same blogpost COPY test but with MAXATTEMPTS=1(or more?) and forced failure to test that it is calculating correctly, but I'll only go down that path if people agree this makes sense.

The alternative is to increase the dtest's MAXINSERTERRORS but personally I don't think it makes sense to count failed attempts as errors.;;;","02/Jun/17 01:53;stefania;bq. Patch simply makes it so we count the total insert errors based on failing all attempts, rather than just one attempt.

+1, it is clearly a mistake to count attempts as insert errors. I've applied your patch to [this branch|https://github.com/stef1927/cassandra/tree/13209-3.0] and I'm currently running the full cqlsh tests on our CI system. I will post the results later.

Where do you think we should commit this, technically 2.1 is critical fixes only, and this isn't critical. I'm not sure about 2.2 either. I would suggest 3.0+, WDYT? We can set {{MAXINSERTERRORS}} to -1 to stabilize the test on the branches without this fix.

bq. It might be nice to have a dtest that does the same blogpost COPY test but with MAXATTEMPTS=1(or more?) and forced failure to test that it is calculating correctly

We have [{{test_reading_max_insert_errors}}|https://github.com/stef1927/cassandra-dtest/blob/master/cqlsh_tests/cqlsh_copy_tests.py#L1176] for this. It's currently setting {{MAXATTEMPTS}} to 1, you could extend it to use {{MAXATTEMPTS}} > 1 if you wanted to test that attempts are not counted as insert errors.;;;","02/Jun/17 07:14;KurtG;The whole critical fixes only has always annoyed me as an operator of many clusters of many versions so I'm biased in this regard. I don't see any harm in applying the fix to 2.2 (or 2.1 for that matter). The way I see it is 2.2 will be around for a long time as people won't move to 3, so at the very least we should apply it there. Plus it is a very small patch after all.

I'll have a look at extending on {{test_reading_max_insert_errors}}, thanks for the pointer.;;;","02/Jun/17 09:47;stefania;I've run the entire cqlsh tests internally for 3.0, 3.11 and trunk, no problems found. 

OK for committing in 2.2+, I'll run CI for this on Monday.;;;","05/Jun/17 02:10;stefania;CI for 2.2 looks good as far as the proposed patch is concerned: only the known failures in cqlshlib and a client request timeout in {{test_bulk_round_trip_blogposts}} when invoking {{SELECT COUNT}} at line 2475, unrelated to this patch. I imagine we would need to reduce the number of records in the bulk tests to fix this sort of problems if they happen on ASF infra, or remove the {{SELECT COUNT}} altogether, if at all possible.

So \+1 for the patch in 2.2\+, even though if may not stabilize the bulk tests fully, I expect the tests can be stabilized with changes in the tests. Do you need me to commit?

;;;","05/Jun/17 06:13;KurtG;Yeah not sure why the counts are flaky on ASF infra. I rarely ever got the client request timeouts running on a standalone machine, even with a smaller heap. Anyway, probably best those tests are fixed in their own tickets/in dtests where applicable so may as well finish up with this ticket.

Thanks for the review Stefania, and yes I need you to commit.

;;;","06/Jun/17 00:58;stefania;Committed to 2.2 as [5807ec|https://github.com/apache/cassandra/commit/5807eca8bbb30091b632e031441939c763b3e054] and merged upwards.

Thank you for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread Leak in OutboundTcpConnection,CASSANDRA-13204,13041790,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,kohlisankalp,kohlisankalp,09/Feb/17 18:22,15/May/20 08:01,14/Jul/23 05:56,11/Feb/17 16:06,2.1.17,2.2.9,3.0.11,3.11.0,4.0,4.0-alpha1,Legacy/Streaming and Messaging,Messaging/Internode,,,,0,,,,,"We found threads leaking from OutboundTcpConnection to machines which are not part of the cluster and still in Gossip for some reason. There are two issues here, this JIRA will cover the second one which is most important. 



1) First issue is that Gossip has information about machines not in the ring which has been replaced out. It causes Cassandra to connect to those machines but due to internode auth, it wont be able to connect to them at the socket level.  

2) Second issue is a race between creating a connection and closing a connections which is triggered by the gossip bug explained above. Let me try to explain it using the code

In OutboundTcpConnection, we are calling closeSocket(true) which will set isStopped=true and also put a close sentinel into the queue to exit the thread. On the ack connection, Gossip tries to send a message which calls connect() which will block for 10 seconds which is RPC timeout. The reason we will block is because Cassandra might not be running there or internode auth will not let it connect. During this 10 seconds, if Gossip calls closeSocket, it will put close sentinel into the queue. When we return from the connect method after 10 seconds, we will clear the backlog queue causing this thread to leak. 

Proofs from the heap dump of the affected machine which is leaking threads 
1. Only ack connection is leaking and not the command connection which is not used by Gossip. 
2. We see thread blocked on the backlog queue, isStopped=true and backlog queue is empty. This is happening on the threads which have already leaked. 
3. A running thread was blocked on the connect waiting for timeout(10 seconds) and we see backlog queue to contain the close sentinel. Once the connect will return false, we will clear the backlog and this thread will have leaked.  


Interesting bits from j stack 
1282 number of threads for ""MessagingService-Outgoing-/<IP-Address>""

Thread which is about to leak:
""MessagingService-Outgoing-/<IP Address>"" 
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:137)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:119)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:381)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:217)

Thread already leaked:
""MessagingService-Outgoing-/<IP Address>""
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy.coalesceInternal(CoalescingStrategies.java:482)
	at org.apache.cassandra.utils.CoalescingStrategies$CoalescingStrategy.coalesce(CoalescingStrategies.java:213)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:190)
",,aweisberg,bdeggleston,dikanggu,jasobrown,jeromatron,kohlisankalp,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 11 16:06:16 UTC 2017,,,,,,,,,,"0|i39v1j:",9223372036854775807,2.1.16,,,,,,aweisberg,,aweisberg,,,Normal,,2.1.0,,,,,,,,,,,,,,,,,"09/Feb/17 18:32;jasobrown;Looks like this is due to a regression with CASSANDRA-1632, where I naively clear the {{backlog}}, which had previously been the {{active}} queue. (we used to have two LBQs in {{OutboundTcpConnection}} in pre-2.1);;;","09/Feb/17 22:52;jasobrown;A small change in the branches linked below should resolve the race condition. (The change identical across the branches as not much has changed in {{OutboundTcpConnection}} in quite a while).

Also fixed another minor problem: when we fail to connect ({{#connect()}} returns {{false}}), we want to drop *all* messages. Currently we clear the {{backlog}}, but if there's any remaining messages in the {{drainedMessages}}, we'll keep trying to process those. We should drop those messages, as well. 

Note: I thought about how to try to test this via a unit test, but as the interweaving of the of the shared state occurs in spots that are not easy to inject to, I'm not sure how best to test this change :-/ , Lacking anything else, running the standard unit tests and dtests on cassci:

||2.1||2.2||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13204-2.1]|[branch|https://github.com/jasobrown/cassandra/tree/13204-2.2]|[branch|https://github.com/jasobrown/cassandra/tree/13204-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13204-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13204-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-2.1-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-2.2-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-3.11-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-2.1-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-2.2-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13204-trunk-testall/]|
;;;","10/Feb/17 02:30;aweisberg;* [Did you finish this comment?|https://github.com/apache/cassandra/compare/trunk...jasobrown:13204-trunk?expand=1#diff-c7ef124561c4cde1c906f28ad3883a88R184]
* --[This would cause a concurrent modification exception with the iterator|https://github.com/apache/cassandra/compare/cassandra-2.1...jasobrown:13204-2.1?expand=1#diff-c7ef124561c4cde1c906f28ad3883a88R225]-- Never mind forgot about the jump.
* [It's not clear to me that you need to move the backlog.clear()?|https://github.com/apache/cassandra/compare/cassandra-2.1...jasobrown:13204-2.1?expand=1#diff-c7ef124561c4cde1c906f28ad3883a88L164]

I think I understand the issue. A failed connection clobbers the sentinel with backlog.clear(). You fixed the clobbering by relegating the sentinel to just a tool to wake up the thread. The flag is controlling the loop and the break will make it out to check the loop condition if a connection fails.;;;","10/Feb/17 18:15;jasobrown;bq. It's not clear to me that you need to move the backlog.clear()?

I think you are correct (that it probably does not need to move). I have a (possibly unfounded) preference for referencing shared state variables like this in the same order from different spots. wdyt?

And, yes, your analysis is correct :) ;;;","10/Feb/17 19:33;aweisberg;+1;;;","11/Feb/17 16:06;jasobrown;committed as sha {{a6237bf65a95d654b7e702e81fd0d353460d0c89}} to 2.1, 2.2, 3.0, 3.11, and trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test,CASSANDRA-13202,13041444,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,08/Feb/17 17:56,16/Apr/19 09:30,14/Jul/23 05:56,14/Feb/17 22:54,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test

{code}
Error Message

'Repaired at: 0' unexpectedly found in 'SSTable: /tmp/dtest-9PYhKy/test/node1/data0/keyspace1/standard1-17eaf440edbb11e68d99c3f653778b71/md-1-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486529856104000\nMaximum timestamp: 1486529859637013\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -9222701292667950301 (key=5032394c323239385030)\nLast token: -3134717340917976237 (key=304b3338324b324b3430)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 26e751a0-edbb-11e6-accb-61d17d26194a\nReplay positions covered: {CommitLogPosition(segmentId=1486529830270, position=41626)=CommitLogPosition(segmentId=1486529830270, position=2604016)}\ntotalColumnsSet: 16365\ntotalRows: 3273\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              6546\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        2                 0\n258                     3271                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3273\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486529856104000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-9PYhKy/test/node1/data1/keyspace1/standard1-17eaf440edbb11e68d99c3f653778b71/md-2-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486529856104001\nMaximum timestamp: 1486529859637014\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -3132058048208985688 (key=314f4d374b3631393730)\nLast token: 3156024197334322031 (key=3033384b323531303430)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 26e751a0-edbb-11e6-accb-61d17d26194a\nReplay positions covered: {CommitLogPosition(segmentId=1486529830270, position=41626)=CommitLogPosition(segmentId=1486529830270, position=2604016)}\ntotalColumnsSet: 17315\ntotalRows: 3463\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              6926\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        2                 0\n258                     3461                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3463\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486529856104000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-9PYhKy/test/node1/data2/keyspace1/standard1-17eaf440edbb11e68d99c3f653778b71/md-3-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1486529856105007\nMaximum timestamp: 1486529859637012\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: 3157556839665586758 (key=3136344d4c314d343631)\nLast token: 9222137691148971235 (key=4c30334f32394d4c3031)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 26e751a0-edbb-11e6-accb-61d17d26194a\nReplay positions covered: {CommitLogPosition(segmentId=1486529830270, position=41626)=CommitLogPosition(segmentId=1486529830270, position=2604016)}\ntotalColumnsSet: 16320\ntotalRows: 3264\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              6528\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        0                 0\n258                     3264                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3264\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1486529856104000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\n'
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 55, in sstable_marking_test
    self.assertNotIn('Repaired at: 0', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 810, in assertNotIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}

Related failures:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test/",,bdeggleston,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 17:56;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12851702/node1.log","08/Feb/17 17:56;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851700/node1_debug.log","08/Feb/17 17:56;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851701/node1_gc.log","08/Feb/17 17:56;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12851705/node2.log","08/Feb/17 17:56;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851703/node2_debug.log","08/Feb/17 17:56;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851704/node2_gc.log","08/Feb/17 17:56;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12851708/node3.log","08/Feb/17 17:56;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12851706/node3_debug.log","08/Feb/17 17:56;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12851707/node3_gc.log",,,,,,,,,,,9.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 22:54:35 UTC 2017,,,,,,,,,,"0|i39swn:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"09/Feb/17 16:12;bdeggleston;This is caused by CASSANDRA-9143, and should be addressed by https://github.com/riptano/cassandra-dtest/pull/1436;;;","14/Feb/17 22:54;bdeggleston;This was fixed in dtest, and is no long failing in the most recent run

http://cassci.datastax.com/job/trunk_offheap_dtest/428/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.repair_test.TestRepair.test_failure_during_anticompaction,CASSANDRA-13201,13041443,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,08/Feb/17 17:53,16/Apr/19 09:30,14/Jul/23 05:56,14/Feb/17 22:55,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.repair_test/TestRepair/test_failure_during_anticompaction

{code}
Error Message

08 Feb 2017 04:42:14 [node3] Missing: ['Got anticompaction request']:
INFO  [main] 2017-02-08 04:31:15,447 YamlConfigura.....
See debug.log for remainder
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1056, in test_failure_during_anticompaction
    self._test_failure_during_repair(phase='anticompaction',)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1131, in _test_failure_during_repair
    node_to_kill.watch_log_for(msg_to_wait, filename='debug.log')
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 471, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
{code}",,bdeggleston,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 17:53;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12851693/node1.log","08/Feb/17 17:53;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851691/node1_debug.log","08/Feb/17 17:53;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851692/node1_gc.log","08/Feb/17 17:53;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12851696/node2.log","08/Feb/17 17:53;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851694/node2_debug.log","08/Feb/17 17:53;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851695/node2_gc.log","08/Feb/17 17:53;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12851699/node3.log","08/Feb/17 17:53;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12851697/node3_debug.log","08/Feb/17 17:53;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12851698/node3_gc.log",,,,,,,,,,,9.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 22:55:02 UTC 2017,,,,,,,,,,"0|i39swf:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"09/Feb/17 16:12;bdeggleston;This is caused by CASSANDRA-9143, and should be addressed by https://github.com/riptano/cassandra-dtest/pull/1436;;;","14/Feb/17 22:55;bdeggleston;This was fixed in dtest, and is no long failing in the most recent run

http://cassci.datastax.com/job/trunk_offheap_dtest/428/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.repair_test.TestRepair.test_dead_sync_participant,CASSANDRA-13200,13041442,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,08/Feb/17 17:52,16/Apr/19 09:30,14/Jul/23 05:56,14/Feb/17 22:55,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.repair_test/TestRepair/test_dead_sync_participant

{code}
Error Message

08 Feb 2017 04:31:07 [node1] Missing: ['Endpoint .* died']:
INFO  [main] 2017-02-08 04:28:51,776 YamlConfigura.....
See system.log for remainder
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1049, in test_dead_sync_participant
    self._test_failure_during_repair(phase='sync', initiator=False,)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1139, in _test_failure_during_repair
    node1.watch_log_for('Endpoint .* died', timeout=60)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 471, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
{code}",,bdeggleston,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 17:52;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12851684/node1.log","08/Feb/17 17:52;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851682/node1_debug.log","08/Feb/17 17:52;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851683/node1_gc.log","08/Feb/17 17:52;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12851687/node2.log","08/Feb/17 17:52;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851685/node2_debug.log","08/Feb/17 17:52;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851686/node2_gc.log","08/Feb/17 17:52;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12851690/node3.log","08/Feb/17 17:52;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12851688/node3_debug.log","08/Feb/17 17:52;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12851689/node3_gc.log",,,,,,,,,,,9.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 22:55:19 UTC 2017,,,,,,,,,,"0|i39sw7:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"09/Feb/17 16:12;bdeggleston;This is caused by CASSANDRA-9143, and should be addressed by https://github.com/riptano/cassandra-dtest/pull/1436;;;","14/Feb/17 22:55;bdeggleston;This was fixed in dtest, and is no long failing in the most recent run

http://cassci.datastax.com/job/trunk_offheap_dtest/428/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.repair_test.TestRepair.no_anticompaction_after_dclocal_repair_test,CASSANDRA-13199,13041440,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,08/Feb/17 17:50,16/Apr/19 09:30,14/Jul/23 05:56,14/Feb/17 22:55,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.repair_test/TestRepair/no_anticompaction_after_dclocal_repair_test

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['repair', '-local', 'keyspace1', 'standard1']] exited with non-zero status; exit status: 2; 
stderr: error: Incremental repairs cannot be run against a subset of tokens or ranges
-- StackTrace --
java.lang.IllegalArgumentException: Incremental repairs cannot be run against a subset of tokens or ranges
	at org.apache.cassandra.repair.messages.RepairOption.parse(RepairOption.java:242)
	at org.apache.cassandra.service.StorageService.repairAsync(StorageService.java:3258)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1466)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:828)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:323)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$241(TCPTransport.java:683)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$335/1485984579.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 170, in no_anticompaction_after_dclocal_repair_test
    node1_1.nodetool(""repair -local keyspace1 standard1"")
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 789, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 2002, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}

Related failures:

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.repair_test/TestRepair/no_anticompaction_after_hostspecific_repair_test/

http://cassci.datastax.com/job/trunk_offheap_dtest/427/testReport/repair_tests.repair_test/TestRepair/no_anticompaction_after_subrange_repair_test/",,bdeggleston,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 17:50;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12851672/node1.log","08/Feb/17 17:50;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851670/node1_debug.log","08/Feb/17 17:50;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851671/node1_gc.log","08/Feb/17 17:50;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12851675/node2.log","08/Feb/17 17:50;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851673/node2_debug.log","08/Feb/17 17:50;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851674/node2_gc.log","08/Feb/17 17:50;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12851678/node3.log","08/Feb/17 17:50;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12851676/node3_debug.log","08/Feb/17 17:50;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12851677/node3_gc.log","08/Feb/17 17:50;sean.mccarthy;node4.log;https://issues.apache.org/jira/secure/attachment/12851681/node4.log","08/Feb/17 17:50;sean.mccarthy;node4_debug.log;https://issues.apache.org/jira/secure/attachment/12851679/node4_debug.log","08/Feb/17 17:50;sean.mccarthy;node4_gc.log;https://issues.apache.org/jira/secure/attachment/12851680/node4_gc.log",,,,,,,,12.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 22:55:48 UTC 2017,,,,,,,,,,"0|i39svr:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"09/Feb/17 16:13;bdeggleston;This is caused by CASSANDRA-9143, and should be addressed by https://github.com/riptano/cassandra-dtest/pull/1436;;;","14/Feb/17 22:55;bdeggleston;This was fixed in dtest, and passed in the most recent run

http://cassci.datastax.com/job/trunk_offheap_dtest/428/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in snitch_test.TestGossipingPropertyFileSnitch.test_prefer_local_reconnect_on_listen_address,CASSANDRA-13196,13041254,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mshuler,mshuler,08/Feb/17 03:06,31/Jul/21 21:35,14/Jul/23 05:56,26/Mar/21 13:36,4.0,4.0-rc1,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1487/testReport/snitch_test/TestGossipingPropertyFileSnitch/test_prefer_local_reconnect_on_listen_address

{code}
{novnode}
Error Message

Error from server: code=2200 [Invalid query] message=""keyspace keyspace1 does not exist""
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-k6b0iF
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 dc1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snitch_test.py"", line 87, in test_prefer_local_reconnect_on_listen_address
    new_rows = list(session.execute(""SELECT * FROM {}"".format(stress_table)))
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""keyspace keyspace1 does not exist""\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-k6b0iF\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.policies: INFO: Using datacenter \'dc1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 dc1> discovered\n--------------------- >> end captured logging << ---------------------'
{novnode}
{code}",,e.dimitrova,Gerrrr,jay.zhuang,jjirsa,maedhroz,mshuler,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 03:06;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12851544/node1.log","08/Feb/17 03:06;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851539/node1_debug.log","08/Feb/17 03:06;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851540/node1_gc.log","08/Feb/17 03:06;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12851541/node2.log","08/Feb/17 03:06;mshuler;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851542/node2_debug.log","08/Feb/17 03:06;mshuler;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851543/node2_gc.log",,,,,,,,,,,,,,6.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 13:59:07 UTC 2021,,,,,,,,,,"0|i39rqf:",9223372036854775807,,,,,,,,,e.dimitrova,,,Normal,,NA,,,https://github.com/apache/cassandra-dtest/commit/f9c3d78b7845b7dd87f49fedd7287281041aac76,,,,,,,,,run CI,,,,,"16/Feb/17 21:34;Gerrrr;Hello,

I have been checking out what happened with this test and found out that the keyspace1 does not exist on the node2 because all 3 submitted migration tasks failed to complete.
After there were no more migration tasks in progress {{MigrationManager.waitUntilReadyForBootstrap}} returned and the node bootstrapped.

From the discussion in CASSANDRA-10731 I understood that bootstrapping a node with out of sync schema is undesired behavior.
In case the {{MigrationTask.inflightTasks}} queue is empty and the schema wasn't pulled yet, does it make sense to schedule more MigrationTasks one by one until at least one of them succeeds?

What do you think?;;;","12/Mar/17 14:26;Gerrrr;The failure in the test (""keyspace keyspace1 does not exist"") happened because during the pre-bootstrap schema migration all the migration tasks failed to complete and the node was bootstrapped with schema being out of sync.
{{MigrationManager.waitUntilReadyForBootstrap}} (which is invoked by {{StorageService.waitForSchema}}) just waits for the inflight tasks to finish and discards ones that take longer than {{MIGRATION_TASK_WAIT_IN_SECONDS}} to complete.
Schema migration tasks are scheduled when there is a big change in an endpoint state - it joins the cluster, becomes alive or its schema version has changed.

The idea is that it is safe to restart the migration task if it has timed out because either the task will succeed on one of the next retries or will be eventually killed by {{FailureDetector}} if the endpoint is marked as unreachable.
AFAIU there will be at least one migration task per endpoint. With the retry mechanism {{MigrationManager.waitUntilReadyForBootstrap}} will run until migration tasks to all the reachable nodes succeed.
This means that either we will receive the migration data from at least one of the nodes or all the nodes will be unreachable, but then the bootstrap is supposed to fail anyway.

*Steps to reproduce*

To test the retry, I commented out sending reply in {{org.apache.cassandra.schema.SchemaPullVerbHandler.doVerb}} and ran the original {{snitch_test.TestGossipingPropertyFileSnitch.test_prefer_local_reconnect_on_listen_address}} test.
_NB:_ the test will run forever because without response the migration requests timeout and then being restarted.

*Code*
https://github.com/Gerrrr/cassandra/tree/13196-3.11

*CI builds*:

* https://cassci.datastax.com/job/ifesdjeen-13196-trunk-dtest/
* https://cassci.datastax.com/job/ifesdjeen-13196-trunk-testall/;;;","12/Mar/17 22:14;jjirsa;There's a real risk (in large clusters, or in clusters with large schemas, or when upgrading versions where we run in a mixed-version state) that we can have a lot of migrationtasks in flight, so much so that we can actually kill nodes ( see CASSANDRA-11748 for example ) - re-queueing more migration tasks when one times out is a good way to make the problem worse, not better. I'm very concerned with the approach [here|https://github.com/Gerrrr/cassandra/commit/463f3fecd9348ea0a4ce6eeeb30141527b8b10eb#diff-f484a759f797776d9cc5d8af92b29e5eR156] where we just blindly schedule another poll. 

Do we even know why this failed in the first place? Isn't the right fix understanding why all 3 migration tasks failed, not just making more and more and more migration tasks?;;;","01/Apr/17 21:02;Gerrrr;Hi Jeff,

Thank you for the feedback and the reference to CASSANDRA-11748, I agree that blind retry might not be the best approach here. I checked available logs once again, compared them against the logs of the passing tests and found the following details. Here are the selected parts including 2 out of 3 failing migration tasks (node2_debug.log):

{NOFORMAT}
DEBUG [GossipStage:1] 2017-02-06 22:13:17,494 Gossiper.java:1524 - Received a regular ack from /127.0.0.3, can now exit shadow round
DEBUG [RequestResponseStage-1] 2017-02-06 22:13:17,544 Gossiper.java:1013 - removing expire time for endpoint : /127.0.0.3
INFO  [RequestResponseStage-1] 2017-02-06 22:13:17,544 Gossiper.java:1014 - InetAddress /127.0.0.3 is now UP

...

DEBUG [MessagingService-Outgoing-/127.0.0.3-Small] 2017-02-06 22:13:19,540 OutboundTcpConnection.java:389 - Attempting to connect to /127.0.0.3
INFO  [HANDSHAKE-/127.0.0.3] 2017-02-06 22:13:19,542 OutboundTcpConnection.java:502 - Handshaking version with /127.0.0.3
DEBUG [MessagingService-Outgoing-/127.0.0.3-Small] 2017-02-06 22:13:19,547 OutboundTcpConnection.java:474 - Done connecting to /127.0.0.3
INFO  [ScheduledTasks:1] 2017-02-06 22:13:20,175 TokenMetadata.java:498 - Updating topology for all endpoints that have changed
DEBUG [GossipStage:1] 2017-02-06 22:13:20,582 FailureDetector.java:457 - Ignoring interval time of 3091069037 for /127.0.0.3
INFO  [GossipStage:1] 2017-02-06 22:13:20,583 Gossiper.java:1050 - Node /127.0.0.3 is now part of the cluster
DEBUG [GossipStage:1] 2017-02-06 22:13:20,585 StorageService.java:2183 - Node /127.0.0.3 state NORMAL, token [-1004975124483382337, -1043780946254161012, -1171220283294501102, -1633901128898670626, -2841540313718334770, -3101553897858755861, -3621837948233406461, -3885385423954957338, -4732725481887392373, -6179581950298736502, -6981606078877819247, -7346004435634569033, -8212720784299798818, -8540770738419743277, -9144476684623172343, -9177835532129132670, 2008413755392462902, 2811132152336883532, 3686445661174337927, 3782543114949044043, 4077664166716898608, 4223225346596811804, 4829686036948015467, 5983186107673554538, 6452742755557526311, 6973900262646615849, 7254341221965284892,
8057149706767269849, 8062244717118882837, 8473826416444317568, 8957680003056564562, 9045604815824506670]

...

DEBUG [GossipStage:1] 2017-02-06 22:13:20,622 MigrationManager.java:85 - Submitting migration task for /127.0.0.3
INFO  [GossipStage:1] 2017-02-06 22:13:20,623 TokenMetadata.java:479 - Updating topology for /127.0.0.3
DEBUG [MigrationStage:1] 2017-02-06 22:13:20,623 MigrationTask.java:74 - Can't send schema pull request: node /127.0.0.3 is down.
INFO  [GossipStage:1] 2017-02-06 22:13:20,628 TokenMetadata.java:479 - Updating topology for /127.0.0.3
DEBUG [GossipStage:1] 2017-02-06 22:13:20,631 MigrationManager.java:85 - Submitting migration task for /127.0.0.3
DEBUG [MigrationStage:1] 2017-02-06 22:13:20,631 MigrationTask.java:74 - Can't send schema pull request: node /127.0.0.3 is down.
...
DEBUG [RequestResponseStage-1] 2017-02-06 22:13:20,665 Gossiper.java:1013 - removing expire time for endpoint : /127.0.0.3
INFO  [RequestResponseStage-1] 2017-02-06 22:13:20,665 Gossiper.java:1014 - InetAddress /127.0.0.3 is now UP
{NOFORMAT}

Following the reconnect (""Attempting to connect to /127.0.0.3"") the migration tasks were submitted after the endpoint is considered NORMAL by the StorageService, but before it was marked alive by Gossiper.realMarkAlive. Can there be a rare race condition based on the order of these actions?
Can you please tell me if this might make sense or provide with any suggestions on what to look at?

Thanks,
Aleksandr;;;","01/Apr/17 23:35;jjirsa;Wouldn't be surprised if there was a race condition there - there was a not-dissimilar race solved recently in CASSANDRA-12653 where the race was in setting up token metadata as the node came out of shadow round, and this is fairly similar - we come out of shadow round at {{2017-02-06 22:13:17,494}} , we submit the migration tasks at {{2017-02-06 22:13:20,622}} and immediately {{2017-02-06 22:13:20,623}} decide not to send it, and FD finally sees the nodes come up at {{2017-02-06 22:13:20,665}} - at the very least, I'm not sure why we'd even try to submit the migration task knowing the instance was down - requeueing the schema pull immediately on failure here definitely wouldn't have helped (we'd have failed to send it again, as the instance was still down). I'm sort of wondering if this test still fails with #12653 committed - it doesn't seem like it's the exact same issue, but maybe the changes from 12653 also help with this race? 

I'm not sure of the history here, but it seems like [MigrationManager#shouldPullSchemaFrom|https://github.com/Gerrrr/cassandra/blob/463f3fecd9348ea0a4ce6eeeb30141527b8b10eb/src/java/org/apache/cassandra/schema/MigrationManager.java#L125] could potentially check that endpoint's UP/DOWN in addition to messaging version.  
;;;","05/Mar/21 17:43;maedhroz;Failed again here: https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/459/tests/;;;","18/Mar/21 14:53;brandon.williams;Patch to simplify a good portion of this test with a sprinkle of regular expressions.  I believe the flakiness may have been looking for the INTERNAL_IP state on 4.0, which could be affected by CASSANDRA-16381 and CASSANDRA-16525, but it is neither necessary nor logical to look for on a clean 4.0+ cluster.

[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/497/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/497/pipeline]
;;;","25/Mar/21 23:46;e.dimitrova;I am +1 on the new version of the test. The patterns are way more reliable and remove the need of updating the previous strings all the time. 

I haven't been able to reproduce the test failure or find recent CI failures. Looking at the log from Caleb what you say seems reasonable to me from code inspection. 

I suggest we commit the new version and close the ticket. If the issue reappears we will reopen it. WDYT?;;;","25/Mar/21 23:49;e.dimitrova;PS I ran the new version also locally with 3.0 and 3.11 and it passes successfully;;;","26/Mar/21 13:36;brandon.williams;Sounds like a plan, committed.;;;","26/Mar/21 13:59;e.dimitrova;Thank you :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in repair_tests.incremental_repair_test.TestIncRepair.compaction_test,CASSANDRA-13194,13041060,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,mshuler,mshuler,07/Feb/17 18:08,16/Apr/19 09:30,14/Jul/23 05:56,26/Nov/18 23:08,,,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/333/testReport/repair_tests.incremental_repair_test/TestIncRepair/compaction_test

{noformat}
Error Message

errors={'127.0.0.3': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.3
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-2JszEQ
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 225, in compaction_test
    create_ks(session, 'ks', 3)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 725, in create_ks
    session.execute(query % (name, ""'class':'SimpleStrategy', 'replication_factor':%d"" % rf))
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
""errors={'127.0.0.3': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.3\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-2JszEQ\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\n--------------------- >> end captured logging << ---------------------""
{noformat}",,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/17 18:08;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12851436/node1.log","07/Feb/17 18:08;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12851435/node1_debug.log","07/Feb/17 18:08;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12851434/node1_gc.log","07/Feb/17 18:08;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12851433/node2.log","07/Feb/17 18:08;mshuler;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12851428/node2_debug.log","07/Feb/17 18:08;mshuler;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12851429/node2_gc.log","07/Feb/17 18:08;mshuler;node3.log;https://issues.apache.org/jira/secure/attachment/12851430/node3.log","07/Feb/17 18:08;mshuler;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12851431/node3_debug.log","07/Feb/17 18:08;mshuler;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12851432/node3_gc.log",,,,,,,,,,,9.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-02-07 18:08:07.0,,,,,,,,,,"0|i39qjb:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in org.apache.cassandra.hints.HintsBufferPoolTest.testBackpressure,CASSANDRA-13191,13041051,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,mshuler,mshuler,07/Feb/17 17:57,31/Jul/21 21:35,14/Jul/23 05:56,16/Apr/21 20:25,4.0,4.0-rc1,,,,,Legacy/Testing,Test/unit,,,,0,test-failure,,,,"example failure:

http://cassci.datastax.com/job/trunk_testall/1392/testReport/org.apache.cassandra.hints/HintsBufferPoolTest/testBackpressure

{noformat}
Error Message

Connection reset
Stacktrace

java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:209)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.jboss.byteman.agent.submit.Submit$Comm.readResponse(Submit.java:941)
	at org.jboss.byteman.agent.submit.Submit.submitRequest(Submit.java:790)
	at org.jboss.byteman.agent.submit.Submit.addScripts(Submit.java:603)
	at org.jboss.byteman.contrib.bmunit.BMUnit.loadScriptText(BMUnit.java:268)
	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$10.evaluate(BMUnitRunner.java:369)
	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$6.evaluate(BMUnitRunner.java:241)
	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$1.evaluate(BMUnitRunner.java:75)
Standard Output

ERROR [main] 2017-02-07 11:03:07,465 ?:? - SLF4J: stderr
INFO  [main] 2017-02-07 11:03:07,650 ?:? - Configuration location: file:/home/automaton/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2017-02-07 11:03:07,651 ?:? - Loading settings from file:/home/automaton/cassandra/test/conf/cassandra.yaml
INFO  [main] 2017-02-07 11:03:08,225 ?:? - Node configuration:[allocate_tokens_for_keyspace=null; authenticator=null; authorizer=null; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=null; batch_size_fail_threshold_in_kb=50; batch_size_warn_threshold_in_kb=5; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=build/test/cassandra/cdc_raw:222; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=4; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=build/test/cassandra/commitlog:222; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=5; commitlog_sync=batch; commitlog_sync_batch_window_in_ms=1.0; commitlog_sync_period_in_ms=0; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=0; concurrent_compactors=4; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@e7edb54; disk_access_mode=mmap; disk_failure_policy=ignore; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=true; enable_user_defined_functions=true; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=org.apache.cassandra.locator.SimpleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=0; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=build/test/cassandra/hints:222; hints_flush_period_in_ms=10000; incremental_backups=true; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=true; internode_authenticator=null; internode_compression=none; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=offheap_objects; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9264; native_transport_port_ssl=null; num_tokens=1; otc_coalescing_strategy=TIMEHORIZON; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.ByteOrderedPartitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_timeout_in_ms=10000; role_manager=null; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=16; rpc_address=null; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; saved_caches_directory=build/test/cassandra/saved_caches:222; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; storage_port=7232; stream_throughput_outbound_megabits_per_sec=200; streaming_connections_per_host=1; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@378542de; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=0; write_request_timeout_in_ms=2000]
DEBUG [main] 2017-02-07 11:03:08,225 ?:? - Syncing log with a batch window of 1.0
INFO  [main] 2017-02-07 11:03:08,225 ?:? - DiskAccessMode is mmap, indexAccessMode is mmap
INFO  [main] 2017-02-07 11:03:08,226 ?:? - Global memtable on-heap threshold is enabled at 227MB
INFO  [main] 2017-02-07 11:03:08,226 ?:? - Global memtable off-heap threshold is enabled at 227MB
INFO  [main] 2017-02-07 11:03:08,618 ?:? - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
INFO  [main] 2017-02-07 11:03:08,618 ?:? - Back-pressure is disabled with strategy null.
INFO  [ScheduledTasks:1] 2017-02-07 11:03:08,761 ?:? - Overriding RING_DELAY to 1000ms
DEBUG [main] 2017-02-07 11:03:08,763 ?:? - Loading settings from file:/home/automaton/cassandra/test/conf/cassandra.yaml
WARN  [main] 2017-02-07 11:03:08,910 ?:? - Protocol Version 5/v5-beta not supported by java driver
DEBUG [COMMIT-LOG-ALLOCATOR] 2017-02-07 11:03:09,187 ?:? - No segments in reserve; creating a fresh one
DEBUG [COMMIT-LOG-ALLOCATOR] 2017-02-07 11:03:09,242 ?:? - No segments in reserve; creating a fresh one
DEBUG [main] 2017-02-07 11:03:09,249 ?:? - CLSM closing and clearing existing commit log segments...
INFO  [main] 2017-02-07 11:03:09,263 ?:? - No commitlog files found; skipping replay
DEBUG [COMMIT-LOG-ALLOCATOR] 2017-02-07 11:03:09,273 ?:? - No segments in reserve; creating a fresh one
DEBUG [main] 2017-02-07 11:03:09,792 ?:? - Using SLF4J as the default logging framework
DEBUG [main] 2017-02-07 11:03:09,795 ?:? - -Dio.netty.recycler.maxCapacity.default: 262144
DEBUG [main] 2017-02-07 11:03:09,800 ?:? - -Dio.netty.recycler.maxSharedCapacityFactor: 2
DEBUG [main] 2017-02-07 11:03:09,800 ?:? - -Dio.netty.recycler.linkCapacity: 16
INFO  [main] 2017-02-07 11:03:10,106 ?:? - Initialized prepared statement caches with 10 MB
INFO  [main] 2017-02-07 11:03:10,439 ?:? - Initializing SIGAR library
DEBUG [main] 2017-02-07 11:03:10,500 ?:? - no libsigar-amd64-linux.so in java.library.path
org.hyperic.sigar.SigarException: no libsigar-amd64-linux.so in java.library.path
	at org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172) ~[sigar-1.6.4.jar:na]
	at org.hyperic.sigar.Sigar.<clinit>(Sigar.java:100) ~[sigar-1.6.4.jar:na]
	at org.apache.cassandra.utils.SigarLibrary.<init>(SigarLibrary.java:47) [main/:na]
	at org.apache.cassandra.utils.SigarLibrary.<clinit>(SigarLibrary.java:28) [main/:na]
	at org.apache.cassandra.utils.UUIDGen.hash(UUIDGen.java:388) [main/:na]
	at org.apache.cassandra.utils.UUIDGen.makeNode(UUIDGen.java:367) [main/:na]
	at org.apache.cassandra.utils.UUIDGen.makeClockSeqAndNode(UUIDGen.java:300) [main/:na]
	at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:41) [main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.bytes(ByteBufferUtil.java:602) [main/:na]
	at org.apache.cassandra.schema.TableId.toHexString(TableId.java:77) [main/:na]
	at org.apache.cassandra.db.Directories.<init>(Directories.java:201) [main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:609) [main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:601) [main/:na]
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:420) [main/:na]
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:333) [main/:na]
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:133) [main/:na]
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110) [main/:na]
	at org.apache.cassandra.db.Keyspace.openAndGetStore(Keyspace.java:168) [main/:na]
	at org.apache.cassandra.cql3.statements.UpdatesCollector.validateIndexedColumns(UpdatesCollector.java:91) [main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:623) [main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:573) [main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:568) [main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.executeOnceInternal(QueryProcessor.java:339) [main/:na]
	at org.apache.cassandra.db.SystemKeyspace.persistLocalMetadata(SystemKeyspace.java:388) [main/:na]
	at org.apache.cassandra.cql3.CQLTester.prepareServer(CQLTester.java:199) [classes/:na]
	at org.apache.cassandra.SchemaLoader.prepareServer(SchemaLoader.java:70) [classes/:na]
	at org.apache.cassandra.hints.HintsBufferTest.defineSchema(HintsBufferTest.java:60) [classes/:na]
	at org.apache.cassandra.hints.HintsBufferPoolTest.defineSchema(HintsBufferPoolTest.java:42) [classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_45]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_45]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_45]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_45]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) [junit-4.6.jar:na]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) [junit-4.6.jar:na]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) [junit-4.6.jar:na]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27) [junit-4.6.jar:na]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) [junit-4.6.jar:na]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:220) [junit-4.6.jar:na]
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39) [junit-4.6.jar:na]
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535) [ant-junit-1.9.6.jar:na]
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) [ant-junit-1.9.6.jar:na]
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) [ant-junit-1.9.6.jar:na]
INFO  [main] 2017-02-07 11:03:10,502 ?:? - Could not initialize SIGAR library org.hyperic.sigar.Sigar.getFileSystemListNative()[Lorg/hyperic/sigar/FileSystem; 
INFO  [main] 2017-02-07 11:03:10,708 ?:? - Initializing system.IndexInfo
INFO  [main] 2017-02-07 11:03:10,956 ?:? - Initializing system.batches
INFO  [main] 2017-02-07 11:03:10,990 ?:? - Initializing system.paxos
INFO  [main] 2017-02-07 11:03:11,026 ?:? - Initializing system.local
INFO  [main] 2017-02-07 11:03:11,047 ?:? - Initializing system.peers
INFO  [main] 2017-02-07 11:03:11,063 ?:? - Initializing system.peer_events
INFO  [main] 2017-02-07 11:03:11,076 ?:? - Initializing system.range_xfers
INFO  [main] 2017-02-07 11:03:11,096 ?:? - Initializing system.compaction_history
INFO  [main] 2017-02-07 11:03:11,112 ?:? - Initializing system.sstable_activity
INFO  [main] 2017-02-07 11:03:11,168 ?:? - Initializing system.size_estimates
INFO  [main] 2017-02-07 11:03:11,182 ?:? - Initializing system.available_ranges
INFO  [main] 2017-02-07 11:03:11,213 ?:? - Initializing system.transferred_ranges
INFO  [main] 2017-02-07 11:03:11,241 ?:? - Initializing system.views_builds_in_progress
INFO  [main] 2017-02-07 11:03:11,260 ?:? - Initializing system.built_views
INFO  [main] 2017-02-07 11:03:11,277 ?:? - Initializing system.prepared_statements
INFO  [main] 2017-02-07 11:03:11,290 ?:? - Initializing system.repairs
INFO  [main] 2017-02-07 11:03:11,309 ?:? - Not submitting build tasks for views in keyspace system as storage service is not initialized
DEBUG [COMMIT-LOG-ALLOCATOR] 2017-02-07 11:03:11,418 ?:? - No segments in reserve; creating a fresh one
WARN  [COMMIT-LOG-WRITER] 2017-02-07 11:03:11,427 ?:? - Out of 1 commit log syncs over the past 0.00s with average duration of 6.62ms, 1 have exceeded the configured commit interval by an average of 5.62ms
INFO  [main] 2017-02-07 11:03:11,477 ?:? - Create new Keyspace: KeyspaceMetadata{name=hints_buffer_test, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[hints_buffer_test.""table""], views=[], functions=[], types=[]}
INFO  [main] 2017-02-07 11:03:11,603 ?:? - Initializing system_schema.keyspaces
INFO  [main] 2017-02-07 11:03:11,621 ?:? - Initializing system_schema.tables
INFO  [main] 2017-02-07 11:03:11,642 ?:? - Initializing system_schema.columns
INFO  [main] 2017-02-07 11:03:11,662 ?:? - Initializing system_schema.triggers
INFO  [main] 2017-02-07 11:03:11,680 ?:? - Initializing system_schema.dropped_columns
INFO  [main] 2017-02-07 11:03:11,704 ?:? - Initializing system_schema.views
INFO  [main] 2017-02-07 11:03:11,722 ?:? - Initializing system_schema.types
INFO  [main] 2017-02-07 11:03:11,739 ?:? - Initializing system_schema.functions
INFO  [main] 2017-02-07 11:03:11,766 ?:? - Initializing system_schema.aggregates
INFO  [main] 2017-02-07 11:03:11,791 ?:? - Initializing system_schema.indexes
INFO  [main] 2017-02-07 11:03:11,792 ?:? - Not submitting build tasks for views in keyspace system_schema as storage service is not initialized
DEBUG [main] 2017-02-07 11:03:11,824 ?:? - Enqueuing flush of keyspaces: 0.302KiB (0%) on-heap, 0.151KiB (0%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-02-07 11:03:12,229 ?:? - Writing Memtable-keyspaces@462779064(0.146KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (null, null]
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-02-07 11:03:12,235 ?:? - Completed flushing /home/automaton/cassandra/build/test/cassandra/data:222/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/md-1-big-Data.db (0.124KiB) for commitlog position CommitLogPosition(segmentId=1486465389194, position=1475)
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,366 ?:? - Initializing key cache with capacity of 12 MBs.
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,389 ?:? - Initializing row cache with capacity of 16 MBs
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,446 ?:? - OHC using Java8 Unsafe API
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,454 ?:? - OHC using JNA OS native malloc/free
DEBUG [MemtableFlushWriter:1] 2017-02-07 11:03:12,457 ?:? - OHC instance with 8 segments and capacity of 16777216 created.
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,458 ?:? - Initializing counter cache with capacity of 6 MBs
INFO  [MemtableFlushWriter:1] 2017-02-07 11:03:12,459 ?:? - Scheduling counter cache save to every 7200 seconds (going to save all keys).
DEBUG [MemtableFlushWriter:1] 2017-02-07 11:03:12,581 ?:? - Flushed to [BigTableReader(path='/home/automaton/cassandra/build/test/cassandra/data:222/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/md-1-big-Data.db')] (1 sstables, 5.093KiB), biggest 5.093KiB, smallest 5.093KiB
DEBUG [main] 2017-02-07 11:03:12,589 ?:? - Enqueuing flush of tables: 0.708KiB (0%) on-heap, 0.672KiB (0%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:2] 2017-02-07 11:03:13,698 ?:? - Writing Memtable-tables@1807045260(0.590KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (null, null]
DEBUG [PerDiskMemtableFlushWriter_0:2] 2017-02-07 11:03:13,699 ?:? - Completed flushing /home/automaton/cassandra/build/test/cassandra/data:222/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/md-1-big-Data.db (0.356KiB) for commitlog position CommitLogPosition(segmentId=1486465389194, position=1475)
DEBUG [MemtableFlushWriter:2] 2017-02-07 11:03:15,858 ?:? - Flushed to [BigTableReader(path='/home/automaton/cassandra/build/test/cassandra/data:222/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/md-1-big-Data.db')] (1 sstables, 6.642KiB), biggest 6.642KiB, smallest 6.642KiB
DEBUG [main] 2017-02-07 11:03:15,859 ?:? - Enqueuing flush of columns: 1.028KiB (0%) on-heap, 0.599KiB (0%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-02-07 11:03:15,918 ?:? - Writing Memtable-columns@703223389(0.535KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit), flushed range = (null, null]
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-02-07 11:03:15,920 ?:? - Completed flushing /home/automaton/cassandra/build/test/cassandra/data:222/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/md-1-big-Data.db (0.227KiB) for commitlog position CommitLogPosition(segmentId=1486465389194, position=1475)
DEBUG [MemtableFlushWriter:1] 2017-02-07 11:03:15,987 ?:? - Flushed to [BigTableReader(path='/home/automaton/cassandra/build/test/cassandra/data:222/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/md-1-big-Data.db')] (1 sstables, 5.260KiB), biggest 5.260KiB, smallest 5.260KiB
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,988 ?:? - forceFlush requested but everything is clean in dropped_columns
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,988 ?:? - forceFlush requested but everything is clean in triggers
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,989 ?:? - forceFlush requested but everything is clean in views
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,989 ?:? - forceFlush requested but everything is clean in types
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,989 ?:? - forceFlush requested but everything is clean in functions
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,989 ?:? - forceFlush requested but everything is clean in aggregates
DEBUG [MemtablePostFlush:1] 2017-02-07 11:03:15,989 ?:? - forceFlush requested but everything is clean in indexes
INFO  [main] 2017-02-07 11:03:16,070 ?:? - Global buffer pool is enabled, when pool is exhausted (max is 227.000MiB) it will allocate on heap
INFO  [main] 2017-02-07 11:03:16,118 ?:? - Scheduling approximate time-check task with a precision of 10 milliseconds
INFO  [main] 2017-02-07 11:03:16,210 ?:? - Initializing hints_buffer_test.table
INFO  [main] 2017-02-07 11:03:16,212 ?:? - Not submitting build tasks for views in keyspace hints_buffer_test as storage service is not initialized
INFO  [main] 2017-02-07 11:03:16,224 ?:? - byteman jar is /home/automaton/cassandra/build/lib/jars/byteman-3.0.3.jar
INFO  [Attach Listener] 2017-02-07 11:03:18,360 ?:? - Setting org.jboss.byteman.allow.config.update=true
INFO  [Attach Listener] 2017-02-07 11:03:18,370 ?:? - TransformListener() : unexpected exception opening server socket java.net.BindException: Address already in use
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - java.net.BindException: Address already in use
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.net.PlainSocketImpl.socketBind(Native Method)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:382)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.net.ServerSocket.bind(ServerSocket.java:375)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.net.ServerSocket.bind(ServerSocket.java:329)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at org.jboss.byteman.agent.TransformListener.initialize(TransformListener.java:80)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.lang.reflect.Method.invoke(Method.java:497)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at org.jboss.byteman.agent.Main.premain(Main.java:282)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at org.jboss.byteman.agent.Main.agentmain(Main.java:305)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at java.lang.reflect.Method.invoke(Method.java:497)
ERROR [Attach Listener] 2017-02-07 11:03:18,371 ?:? - 	at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:386)
ERROR [Attach Listener] 2017-02-07 11:03:18,372 ?:? - 	at sun.instrument.InstrumentationImpl.loadClassAndCallAgentmain(InstrumentationImpl.java:411)
{noformat}",,aholmber,jay.zhuang,mck,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16595,,,,,,CASSANDRA-16587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 20:25:13 UTC 2021,,,,,,,,,,"0|i39qhb:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"16/Apr/21 20:25;aholmber;Fixed by CASSANDRA-16595, which merged yesterday.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compaction-stress AssertionError from getMemtableFor(),CASSANDRA-13188,13040832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,07/Feb/17 02:49,07/Mar/23 11:52,14/Jul/23 05:56,25/Apr/17 16:04,3.11.0,4.0,4.0-alpha1,,,,Legacy/Testing,Legacy/Tools,,,,0,,,,,"Exception:
{noformat}
./compaction-stress compact -d /tmp/compaction -p https://gist.githubusercontent.com/tjake/8995058fed11d9921e31/raw/a9334d1090017bf546d003e271747351a40692ea/blogpost.yaml -t 4
WARN  18:45:04,854 JNA link failure, one or more native method will be unavailable.
java.lang.AssertionError: []
        at org.apache.cassandra.db.lifecycle.Tracker.getMemtableFor(Tracker.java:312)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1315)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:618)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:462)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:227)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:232)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:241)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:570)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:564)
        at org.apache.cassandra.cql3.QueryProcessor.executeOnceInternal(QueryProcessor.java:356)
        at org.apache.cassandra.schema.SchemaKeyspace.saveSystemKeyspacesSchema(SchemaKeyspace.java:265)
        at org.apache.cassandra.db.SystemKeyspace.finishStartup(SystemKeyspace.java:495)
        at org.apache.cassandra.stress.CompactionStress$Compaction.run(CompactionStress.java:209)
        at org.apache.cassandra.stress.CompactionStress.main(CompactionStress.java:349)
{noformat}",,jay.zhuang,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 25 16:04:35 UTC 2017,,,,,,,,,,"0|i39p4n:",9223372036854775807,3.11.x,5.0,,,,,tjake,,tjake,,,Normal,,,,,,,,,,,,,,,,,,,"08/Feb/17 01:24;jay.zhuang;The problem is because memtable [is not initialized for tool|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L410]
[The fix|https://github.com/cooldoger/cassandra/commit/d2ddeb75cbcbdd407aca33f56f4ab59552a13cdf] is attached.
[~tjake] would you please review.

An alternative fix would be changing [toolInitialization()|https://github.com/apache/cassandra/blob/cassandra-3.11/tools/stress/src/org/apache/cassandra/stress/CompactionStress.java#L77] to [daemonInitialization()|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L128]. (I prefer the first option, as compaction-stress is really a tool, not daemon).;;;","07/Apr/17 03:16;jay.zhuang;Rebased the code:
|[3.11| https://github.com/cooldoger/cassandra/commit/2de68a8ca3d25fa030d8e1a65de68198ea244de2] | [trunk|https://github.com/cooldoger/cassandra/commit/64912267433c304b1a736df65014f54f177b2e21] |
Passed utest locally. Please review.;;;","07/Apr/17 06:43;yukim;It is not initialized because Memtable is tied to commit log, so it creates commit log in commit log directory (CASSANDRA-8616).

Option here is to do daemonInitialization in tool.;;;","07/Apr/17 21:32;jay.zhuang;Make sense. Thanks [~yukim] for the review. Updated:
|[3.11|https://github.com/cooldoger/cassandra/commit/852647ce383aadc465bdf06e9c87d772574827c1] |[trunk|https://github.com/cooldoger/cassandra/commit/ce51690f3aef5241ca8dd852f83bd04ddb18e23d]|

[~yukim] would you please review the change?;;;","25/Apr/17 01:32;tjake;[~jay.zhuang] thank you for this patch and the test.  I'll take a tomorrow;;;","25/Apr/17 13:34;tjake;Changes look good, kicked off CI

||branch|testall|dtests||
|3.11|[testall|http://cassci.datastax.com/job/tjake-CASSANDRA-13188-testall]|[dtest|http://cassci.datastax.com/job/tjake-CASSANDRA-13188-dtest]|
|trunk|[testall|http://cassci.datastax.com/job/tjake-CASSANDRA-13188-trunk-testall]|[dtest|http://cassci.datastax.com/job/tjake-CASSANDRA-13188-trunk-dtest]|;;;","25/Apr/17 16:04;tjake;Committed {{2369faab7959d57c8f6bc1f324de47c5aeaf19b9}} thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh COPY doesn't support dates before 1900 or after 9999,CASSANDRA-13185,13040257,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefania,thobbs,thobbs,03/Feb/17 22:08,16/Apr/19 09:30,14/Jul/23 05:56,22/Feb/17 10:03,3.0.12,3.11.0,,,,,Legacy/Tools,,,,,0,,,,,"Although we fixed this problem for standard queries in CASSANDRA-10625, it still exists for COPY.  In CASSANDRA-10625, we replaced datetimes outside of the supported time range with a simple milliseconds-since-epoch long.  We may not want to use the same solution for COPY, because we wouldn't be able to load the same data back in through COPY.  Having consistency in the format of values and support for loading exported data seems more important for COPY.",,philipthompson,stefania,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 10:03:26 UTC 2017,,,,,,,,,,"0|i39lnj:",9223372036854775807,,,,,,,thobbs,,thobbs,,,Normal,,,,,,,,,,,,,,,,,,,"21/Feb/17 14:11;stefania;I think the only problem is that {{strftime()}} cannot format dates before 1900. I wrote a test [here|https://github.com/riptano/cassandra-dtest/pull/1445] and, dates after 9999 are already exported as milliseconds since the epoch. The patch attached does the same for dates before 1900, since they cannot be formatted by {{strftime()}}. 

Cqlsh COPY is already capable of importing dates as milliseconds from the epoch, this is the fallback in case the date cannot be parsed.

||3.0||3.11||trunk||
|[patch|https://github.com/stef1927/cassandra/tree/13185-cqlsh-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13185-cqlsh-3.11]|[patch|https://github.com/stef1927/cassandra/tree/13185-cqlsh]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13185-cqlsh-3.0-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13185-cqlsh-3.11-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13185-cqlsh-cqlsh-tests/]|

The same patch applies cleanly to all branches since 3.0.

[~thobbs] are you OK to review this?
;;;","21/Feb/17 21:24;thobbs;bq. Cqlsh COPY is already capable of importing dates as milliseconds from the epoch, this is the fallback in case the date cannot be parsed.

Okay, awesome.  The C* patch looks perfect to me, then.  For some reason, the 3.11 and trunk tests failed to run.  I've restarted them, and if they look good, then I think this is ready to commit.;;;","22/Feb/17 08:43;stefania;Thanks for the review. The trunk tests have completed, but the 3.11 tests without cython have timed out. I don't think the timeout is related, so I've just relaunched them. I will commit once they complete.;;;","22/Feb/17 10:03;stefania;Committed to 3.0 as f0502aa791897a30aea371a3032ea5ef679d25cc and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in sstableutil_test.SSTableUtilTest.compaction_test,CASSANDRA-13182,13040179,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Lerh Low,mshuler,mshuler,03/Feb/17 17:36,16/Apr/19 09:30,14/Jul/23 05:56,08/May/18 10:16,,,,,,,,,,,,0,dtest,test-failure,test-failure-fresh,,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/506/testReport/sstableutil_test/SSTableUtilTest/compaction_test

{noformat}
Error Message

Lists differ: ['/tmp/dtest-Rk_3Cs/test/node1... != ['/tmp/dtest-Rk_3Cs/test/node1...

First differing element 8:
'/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-CRC.db'
'/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-CRC.db'

First list contains 7 additional elements.
First extra element 24:
'/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Data.db'

  ['/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-CRC.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Data.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Digest.crc32',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Filter.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Index.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Statistics.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-Summary.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data0/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-4-big-TOC.txt',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-CRC.db',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-Digest.crc32',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-Filter.db',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-Index.db',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-Statistics.db',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-Summary.db',
-  '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-2-big-TOC.txt',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-CRC.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Data.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Digest.crc32',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Filter.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Index.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Statistics.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-Summary.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data1/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-6-big-TOC.txt',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-CRC.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Data.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Digest.crc32',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Filter.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Index.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Statistics.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-Summary.db',
   '/tmp/dtest-Rk_3Cs/test/node1/data2/keyspace1/standard1-11ee2450e8ab11e6b5a68de39eb517c4/mc-5-big-TOC.txt']
-------------------- >> begin captured logging << --------------------
{noformat}",,aweisberg,KurtG,Lerh Low,marcuse,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/17 17:36;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12850850/node1.log","03/Feb/17 17:36;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12850851/node1_debug.log","03/Feb/17 17:36;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12850852/node1_gc.log",,,,,,,,,,,,,,,,,3.0,Lerh Low,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 00:14:23 UTC 2017,,,,,,,,,,"0|i39l67:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"11/May/17 04:43;Lerh Low;I'll try and take a stab at this. ;;;","17/May/17 01:19;Lerh Low;The PR has been merged here: https://github.com/riptano/cassandra-dtest/pull/1473

As a side note, [~krummas] it seems that in this commit [d26187e5c2ab2cf08d8c874387b4674b860f5e4d|https://github.com/apache/cassandra/commit/d26187e5c2ab2cf08d8c874387b4674b860f5e4d] it looks like {{enable}} and {{disable}} have empty method bodies and they aren't overriden anywhere. Just wondering if they were intentionally left that way and if we should remove them? They confused me to mistakenly thinking disabling auto compaction wasn't working. It will be a really simple patch, like so (that's based on {{trunk}}, it's in both 3.0.X and 3.X): https://github.com/apache/cassandra/compare/trunk...juiceblender:13182-autocompaction

Let me know what you think - also happy to create another JIRA instead and close this one :);;;","19/May/17 10:21;marcuse;[~Lerh Low] yeah, we should probably at least add {{@Deprecated}} on the methods, but could you open a new ticket for that?

There were two reasons I kept it, first I wanted to avoid changing the public API for compaction strategies, second there might be some external compaction strategies that need the disabled/enabled notification. So deprecating in 4.0 and then eventually removing in 4.1/5.0 might be a way forward (I really should have deprecated the methods when I did the change..);;;","19/May/17 16:44;aweisberg;Thanks for the fix. I'll keep a look out for this test to see if it fails again.

One suggestion for this kind of flakey test bug where you are checking a condition asynchronously and you don't know when it's going to happen. Spin on the condition checking it once a second and then set a longish timeout (longer than 5 seconds, say 30) and only fail if the condition doesn't occur within the time limit. It's not unheard of to have crazy pauses on the infrastructure we run these tests on that last a few seconds. Trying to guess how long the magic number to wait is a source of flakey tests and setting the magic number longer ends up increasing test runtime when the condition has already become true.

You can factor out the spin until condition is true and error if it doesn't happen within the time limit to a dedicated method in a dtest utility class so all you have to provide is a boolean function object to check the condition.;;;","26/May/17 00:14;Lerh Low;Thanks for your feedback Ariel, makes sense :)

The ticket for marking those methods as deprecated is here: https://issues.apache.org/jira/browse/CASSANDRA-13541;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in paging_test.TestPagingData.test_select_in_clause_with_duplicate_keys,CASSANDRA-13181,13040157,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,mshuler,mshuler,03/Feb/17 16:23,16/Apr/19 09:30,14/Jul/23 05:56,16/Feb/17 15:51,,,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/352/testReport/paging_test/TestPagingData/test_select_in_clause_with_duplicate_keys

{noformat}
Error Message

'TestPagingData' object has no attribute 'create_ks'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-I7zCAw
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 1721, in test_select_in_clause_with_duplicate_keys
    self.create_ks(session, 'test_paging_static_cols', 2)
""'TestPagingData' object has no attribute 'create_ks'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-I7zCAw\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\n--------------------- >> end captured logging << ---------------------""
{noformat}",,jasobrown,mshuler,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 15:50:35 UTC 2017,,,,,,,,,,"0|i39l1b:",9223372036854775807,,,,,,,philipthompson,,philipthompson,,,Normal,,,,,,,,,,,,,,,,,,,"16/Feb/17 14:13;jasobrown;open PR on dtest repo: https://github.com/riptano/cassandra-dtest/pull/1443;;;","16/Feb/17 15:50;jasobrown;PR was merged. closing ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump doesn't handle non-empty partitions with a partition-level deletion correctly,CASSANDRA-13177,13039900,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,thobbs,thobbs,02/Feb/17 18:06,16/Apr/19 09:30,14/Jul/23 05:56,03/Feb/17 19:14,3.0.11,3.11.0,,,,,Legacy/Tools,,,,,0,,,,,"If a partition has a partition-level deletion, but still contains rows (with timestamps higher than the deletion), sstabledump will only show the deletion and not the rows.",,colinkuo,jeromatron,philipthompson,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 03 19:14:17 UTC 2017,,,,,,,,,,"0|i39jg7:",9223372036854775807,,,,,,,yukim,,yukim,,,Normal,,,,,,,,,,,,,,,,,,,"02/Feb/17 20:35;thobbs;Patch and pending test runs:

||branch||testall||dtest||
|[CASSANDRA-13177-3.0|https://github.com/thobbs/cassandra/tree/CASSANDRA-13177-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-3.0-dtest]|
|[CASSANDRA-13177-3.11|https://github.com/thobbs/cassandra/tree/CASSANDRA-13177-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-3.11-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-3.11-dtest]|
|[CASSANDRA-13177-trunk|https://github.com/thobbs/cassandra/tree/CASSANDRA-13177-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-trunk-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13177-trunk-dtest]|

dtest pull request: https://github.com/riptano/cassandra-dtest/pull/1435;;;","02/Feb/17 23:22;yukim;+1;;;","03/Feb/17 19:14;thobbs;Thank you, committed to {{cassandra-3.0}} as {{883c9f0f743139d78996f5faf191508a9be338b5}} and merged up to {{cassandra-3.11}} and {{trunk}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexing is allowed on Duration type when it should not be,CASSANDRA-13174,13039353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,kishkaru,kishkaru,01/Feb/17 01:17,15/May/20 08:00,14/Jul/23 05:56,23/Feb/17 13:50,3.11.0,4.0,4.0-alpha1,,,,Legacy/CQL,,,,,0,,,,,"Looks like secondary indexing is allowed on duration type columns. Since comparisons are not possible for the duration type, indexing on it also should be invalid.

1) 
{noformat}
CREATE TABLE duration_table (k int PRIMARY KEY, d duration);
INSERT INTO duration_table (k, d) VALUES (0, 1s);
SELECT * from duration_table WHERE d=1s ALLOW FILTERING;
{noformat}

The above throws an error: 
{noformat}
WARN  [ReadStage-2] 2017-01-31 17:09:57,821 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,10,main]: {}
java.lang.RuntimeException: java.lang.UnsupportedOperationException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2591) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.marshal.AbstractType.compareCustom(AbstractType.java:174) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:160) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compareForCQL(AbstractType.java:204) ~[main/:na]
	at org.apache.cassandra.cql3.Operator.isSatisfiedBy(Operator.java:201) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:719) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:324) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:120) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:110) ~[main/:na]
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:44) ~[main/:na]
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:174) ~[main/:na]
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:140) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:307) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:292) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:310) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:145) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:138) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:134) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[main/:na]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:333) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1884) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2587) ~[main/:na]
	... 5 common frames omitted
{noformat}

2)
Similarly, if an index is created on the duration column:
{noformat}
CREATE INDEX d_index ON simplex.duration_table (d);
SELECT * from duration_table WHERE d=1s;
{noformat}

results in:
{noformat}
WARN  [ReadStage-2] 2017-01-31 17:12:00,623 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,10,main]: {}
java.lang.RuntimeException: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'd_index' is not yet available
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2591) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'd_index' is not yet available
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:400) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1882) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2587) ~[main/:na]
	... 5 common frames omitted
{noformat}

3) 
Finally, no further updates can be made to the table once the index has been created. Attempting to modify or insert a new row with a non-null value for ""d"" results in an error:
{noformat}
ERROR [MutationStage-2] 2017-01-31 17:13:33,106 StorageProxy.java:1422 - Failed to apply mutation locally : {}
java.lang.RuntimeException: null for ks: simplex, table: duration_table.d_index for ks: simplex, table: duration_table
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1334) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:618) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:462) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1416) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2640) [main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.RuntimeException: null for ks: simplex, table: duration_table.d_index
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1334) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:388) ~[main/:na]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:914) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:333) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:139) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:121) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:178) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[main/:na]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:284) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[main/:na]
	... 12 common frames omitted
Caused by: java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.marshal.AbstractType.compareCustom(AbstractType.java:174) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:160) ~[main/:na]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:156) ~[main/:na]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:132) ~[main/:na]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[main/:na]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[main/:na]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:682) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:781) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_91]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:264) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[main/:na]
	... 25 common frames omitted
{noformat}

Similar errors/inconsistencies exist for materialized views.",C* 3.10,blerer,kishkaru,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11873,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 13:50:58 UTC 2017,,,,,,,,,,"0|i39g2n:",9223372036854775807,3.10,,,,,,thobbs,,thobbs,,,Normal,,,,,,,,,,,,,,,,,,,"08/Feb/17 09:00;blerer;||3.11|[branch|https://github.com/apache/cassandra/compare/trunk...blerer:13174-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13174-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13174-3.11-dtest/]|
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...blerer:13174-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13174-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13174-trunk-dtest/]|

The patch fix multiples problems:
# It disallow the creation of secondary indices on column using duration.
# To allow {{=}}, {{!=}}, {{IN}}, and {{CONTAINS}} operators on columns using duration, it makes the duration type binary comparable. The fact that the type was non comparable was preventing any comparison.
# It adds some checks to disallow slice comparisons in {{restrictions}} and {{conditions}} on columns using duration.

;;;","08/Feb/17 09:01;blerer;[~thobbs] could you review?;;;","23/Feb/17 00:03;thobbs;+1 on the patch.  Nice work on making thorough tests and good error messages!;;;","23/Feb/17 13:38;blerer;Thanks for the review :-);;;","23/Feb/17 13:50;blerer;Committed into 3.11 at 6487876dde14c46d5753f972909e5acec854cb53 and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reloading logback.xml does not work,CASSANDRA-13173,13039320,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,31/Jan/17 22:48,15/May/20 08:06,14/Jul/23 05:56,09/Feb/17 20:50,3.0.11,3.11.0,4.0,4.0-alpha1,,,,,,,,0,,,,,"Regression of CASSANDRA-12535

Reloading of logback.xml is broken by CASSANDRA-12535 because the delegate {{ReconfigureOnChangeFilter}} is not properly initialized.
(Broken in 3.0.11 + 3.10)",,jeromatron,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 20:50:14 UTC 2017,,,,,,,,,,"0|i39fvb:",9223372036854775807,,,,,,,yukim,,yukim,,,Low,,,,,,,,,,,,,,,,,,,"31/Jan/17 22:57;snazy;||cassandra-3.0|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...snazy:13173-logback-reload-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-3.0-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-3.0-dtest/lastSuccessfulBuild/]
||cassandra-3.X|[branch|https://github.com/apache/cassandra/compare/cassandra-3.X...snazy:13173-logback-reload-3.X]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-3.X-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-3.X-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13173-logback-reload-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13173-logback-reload-trunk-dtest/lastSuccessfulBuild/]

CI triggered;;;","08/Feb/17 19:26;yukim;Patch and tests look good to me. +1.;;;","09/Feb/17 20:50;snazy;Thanks!

Committed as [e885886d5c92bfd8d2fa1596bfa86d6a5a8d89bb|https://github.com/apache/cassandra/commit/e885886d5c92bfd8d2fa1596bfa86d6a5a8d89bb] to [cassandra-3.0|https://github.com/apache/cassandra/tree/cassandra-3.0] and merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compaction_large_partition_warning_threshold_mb not working properly when set to high value,CASSANDRA-13172,13039162,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,KurtG,vvavro,vvavro,31/Jan/17 11:46,15/May/20 08:00,14/Jul/23 05:56,19/Jun/17 00:50,3.0.14,3.11.0,4.0,4.0-alpha1,,,Local/Config,,,,,0,,,,,"compaction_large_partition_warning_threshold_mb has been set either by mistake or as an attempt to disable warnings completely to high value 512000
However system started to produce warning no matter what the partition size is:

 Compacting large partition system/compactions_in_progress:e631fe20-e488-11e6-bcd7-bf6151c7fa28 (32 bytes)

When looking into the code:
{code}
 public static int getCompactionLargePartitionWarningThreshold() { return conf.compaction_large_partition_warning_threshold_mb * 1024 * 1024; }
{code}
which is called in 
{code}
private void maybeLogLargePartitionWarning(DecoratedKey key, long rowSize)
    {
        if (rowSize > DatabaseDescriptor.getCompactionLargePartitionWarningThreshold())
        {
            String keyString = metadata().partitionKeyType.getString(key.getKey());
            logger.warn(""Writing large partition {}/{}:{} ({}) to sstable {}"", metadata.keyspace, metadata.name, keyString, FBUtilities.prettyPrintMemory(rowSize), getFilename());
        }
}
{code}
it looks like 512000 is multiplied by 1M and returned as int so being out of range... Maybe it would be better to use long  as it is used for rowSize


",,ferozshaik552@gmail.com,jeromatron,jjirsa,KurtG,vvavro,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/17 23:05;KurtG;13172.patch;https://issues.apache.org/jira/secure/attachment/12873188/13172.patch",,,,,,,,,,,,,,,,,,,1.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 19 00:50:01 UTC 2017,,,,,,,,,,"0|i39exb:",9223372036854775807,2.1.15,2.2.9,3.0.13,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"15/Jun/17 23:04;KurtG;Should be fine to just make the calculation as a long and return it. {{DatabaseDescriptor.getCompactionLargePartitionWarningThreshold()}} only gets called in one place where it won't matter if given a long.

On that note this isn't the first time I've seen this issue, and it seems there are a few other config properties that will suffer from the same problem. Not that it's ever a good idea to set these settings that high, however I have found use for some in the past, and I suspect others would too. Regardless, in my opinion if we are going to let users tune these things we should make them work as expected.

Patch is attached (against 3.0, but should be the same in all branches I believe);;;","18/Jun/17 23:14;KurtG;[~jjirsa] I didn't change Config.java to long as it doesn't make much of a difference. The setting is in MB, and MAX_INT in MB equates to 2047TB. Maybe it would make sense to print a warning if someone puts a ridiculous size in here? or maybe add something to the docs/defaul yaml docs.

Ran tests on circleci against 3.0. Are non-committers able to kick off dtests on ASF? If so do we have docs somewhere on how? Otherwise can you kick them off for me?
 circleci [testall|https://circleci.com/gh/kgreav/cassandra/4]
;;;","19/Jun/17 00:50;jjirsa;It's a pretty obvious fix, dont think we need dtests (there's not a dtest that would cause this to fail anyway). In theory we may want to at some point write a unit test to see if we handle this properly, but it's fairly obvious overflow, so it's fine as is. Committed as {{f3e38cb638113c2a23855a104d6082da5bc10ddb}} to 3.0 for 3.0.14, merged through to 3.11.0 and trunk (4.0)

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.compaction_throughput_test,CASSANDRA-13170,13038940,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Gerrrr,sean.mccarthy,sean.mccarthy,30/Jan/17 16:51,16/Apr/19 09:30,14/Jul/23 05:56,02/Feb/17 20:41,,,,,,,Test/dtest/python,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1479/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/compaction_throughput_test

{code}
Error Message

5.5 not greater than or equal to 6.175
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 280, in compaction_throughput_test
    self.assertGreaterEqual(float(threshold) + 0.5, float(avgthroughput))
  File ""/usr/lib/python2.7/unittest/case.py"", line 948, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}

Related failure:
http://cassci.datastax.com/job/trunk_dtest/1479/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/compaction_throughput_test/
http://cassci.datastax.com/job/trunk_dtest/1481/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/compaction_throughput_test/

Note: this test is failing for both Leveled Compaction, Size Tiered Compaction, and Date Tiered Compaction.",,Gerrrr,philipthompson,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13183,,,,,,,,,,,,,,,,,,,,,"30/Jan/17 16:51;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12850006/node1.log","30/Jan/17 16:51;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12850004/node1_debug.log","30/Jan/17 16:51;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12850005/node1_gc.log",,,,,,,,,,,,,,,,,3.0,Gerrrr,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 02 20:41:39 UTC 2017,,,,,,,,,,"0|i39djz:",9223372036854775807,,,,,,,philipthompson,,philipthompson,,,Normal,,,,,,,,,,,,,,,,,,,"02/Feb/17 20:20;Gerrrr;https://github.com/riptano/cassandra-dtest/pull/1434

The reason for the test to fail is that 5mb threshold is compared against the first matched avgthroughput without unit conversion. In the node_debug.log attached, the threshold was compared against 6.175KiB/s;;;","02/Feb/17 20:41;philipthompson;Looks great, a really nice patch, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StorageService.excise,CASSANDRA-13163,13038534,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,27/Jan/17 21:08,15/May/20 08:00,14/Jul/23 05:56,03/May/17 21:26,3.0.14,3.11.0,4.0,4.0-alpha1,,,Legacy/Core,,,,,0,,,,,"{code}
    private void excise(Collection<Token> tokens, InetAddress endpoint)
    {
        logger.info(""Removing tokens {} for {}"", tokens, endpoint);

        if (tokenMetadata.isMember(endpoint))
            HintsService.instance.excise(tokenMetadata.getHostId(endpoint));

{code}

The check for TMD.isMember() is not enough to guarantee that TMD.getHostId() will not return null. If HintsService.excise() is called with null you get an NPE in a map lookup.",,aweisberg,jasobrown,jjirsa,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 21:26:22 UTC 2017,,,,,,,,,,"0|i39bdz:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"30/Jan/17 18:22;aweisberg;||code|utests|dtests||
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...aweisberg:cassandra-13613-3.0?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-3.0-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-3.0-dtest/2]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13613-3.11?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-3.11-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-3.11-dtest/1]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13613-trunk?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-trunk-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13613-trunk-dtest/1]|

I looked at the test failures and none seemed related.;;;","03/May/17 13:00;jasobrown;all apologies for the delay, but +1;;;","03/May/17 21:26;aweisberg;Committed as [6ed9134336bb48d04284cefd303d8374ed901c0a|https://github.com/apache/cassandra/commit/6ed9134336bb48d04284cefd303d8374ed901c0a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Batchlog replay is throttled during bootstrap, creating conditions for incorrect query results on materialized views",CASSANDRA-13162,13038502,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,adelapena,weideng,weideng,27/Jan/17 18:52,16/Apr/19 09:30,14/Jul/23 05:56,11/Aug/17 04:43,,,,,,,Feature/Materialized Views,,,,,0,bootstrap,materializedviews,,,"I've tested this in a C* 3.0 cluster with a couple of Materialized Views defined (one base table and two MVs on that base table). The data volume is not very high per node (about 80GB of data per node total, and that particular base table has about 25GB of data uncompressed with one MV taking 18GB compressed and the other MV taking 3GB), and the cluster is using decent hardware (EC2 C4.8XL with 18 cores + 60GB RAM + 18K IOPS RAID0 from two 3TB gp2 EBS volumes). 

This is originally a 9-node cluster. It appears that after adding 3 more nodes to the DC, the system.batches table accumulated a lot of data on the 3 new nodes (each having around 20GB under system.batches directory), and in the subsequent week the batchlog on the 3 new nodes got slowly replayed back to the rest of the nodes in the cluster. The bottleneck seems to be the throttling defined in this cassandra.yaml setting: batchlog_replay_throttle_in_kb, which by default is set to 1MB/s.

Given that it is taking almost a week (and still hasn't finished) for the batchlog (from MV) to be replayed after the boostrap finishes, it seems only reasonable to unthrottle (or at least give it a much higher throttle rate) during the initial bootstrap, and hence I'd consider this a bug for our current MV implementation.

Also as far as I understand, the bootstrap logic won't wait for the backlogged batchlog to be fully replayed before changing the new bootstrapping node to ""UN"" state, and if batchlog for the MVs got stuck in this state for a long time, we basically will get wrong answers on the MVs during that whole duration (until batchlog is fully played to the cluster), which adds even more criticality to this bug.",,dikanggu,estevezsebastian@gmail.com,jasonstack,jay.zhuang,jeromatron,jjirsa,KurtG,pauloricardomg,sbtourist,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13069,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 11 04:43:11 UTC 2017,,,,,,,,,,"0|i39b6v:",9223372036854775807,,,,,,,,,,,,Critical,,,,,,,,,,,,,,,,,,,"11/Aug/17 04:43;pauloricardomg;Closing as this was superseded by CASSANDRA-13614 and CASSANDRA-13065.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testall failure in org.apache.cassandra.db.commitlog.CommitLogDescriptorTest.testVersions,CASSANDRA-13161,13038469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,sean.mccarthy,sean.mccarthy,27/Jan/17 15:27,15/May/20 08:02,14/Jul/23 05:56,06/Feb/17 12:23,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,testall,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_testall/1374/testReport/org.apache.cassandra.db.commitlog/CommitLogDescriptorTest/testVersions

{code}
Error Message

expected:<11> but was:<10>
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: expected:<11> but was:<10>
	at org.apache.cassandra.db.commitlog.CommitLogDescriptorTest.testVersions(CommitLogDescriptorTest.java:84)
{code}

Related Failures:
http://cassci.datastax.com/job/trunk_testall/1374/testReport/org.apache.cassandra.db.commitlog/CommitLogDescriptorTest/testVersions_compression/",,blerer,jkni,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/17 15:27;sean.mccarthy;TEST-org.apache.cassandra.db.commitlog.CommitLogDescriptorTest.log;https://issues.apache.org/jira/secure/attachment/12849707/TEST-org.apache.cassandra.db.commitlog.CommitLogDescriptorTest.log",,,,,,,,,,,,,,,,,,,1.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 12:23:46 UTC 2017,,,,,,,,,,"0|i39azj:",9223372036854775807,,,,,,,jkni,,jkni,,,Normal,,,,,,,,,,,,,,,,,,,"30/Jan/17 15:21;blerer;||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13161-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13161-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13161-trunk-dtest/]|

The patch increase the commit log version.

[~jkni] Could you review?;;;","01/Feb/17 18:31;jkni;+1 - thanks;;;","06/Feb/17 12:23;blerer;Committed in trunk at 21730666929886be5300c782319a852e46b22763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Coalescing strategy can enter infinite loop,CASSANDRA-13159,13038235,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,iksaif,iksaif,iksaif,26/Jan/17 19:32,16/Apr/19 09:30,14/Jul/23 05:56,10/Feb/17 23:33,2.1.17,2.2.9,3.0.11,3.11.0,,,Legacy/Streaming and Messaging,,,,,0,,,,,"{code}boolean maybeSleep(int messages, long averageGap, long maxCoalesceWindow, Parker parker){code} 

maybeSleep() can enter an infinite loop if messages or averageGap ends up being 0 because sleep will be 0 and the while loop will never exit. I've noticed that on one of my clusters twice this week.

This can happen if in averageGap() sum is bigger than MEASURED_INTERVAL, which should be pretty rare but apparently happen to me.

Even if the diagnostic is wrong (and I'm pretty sure that this thread was using 100% CPU doing nothing), the fix seems pretty safe to apply.

{code}
diff --git a/src/java/org/apache/cassandra/utils/CoalescingStrategies.java b/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
index 0aa980f..982d4a6 100644
--- a/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
+++ b/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
@@ -100,7 +100,7 @@ public class CoalescingStrategies
     {
         // only sleep if we can expect to double the number of messages we're sending in the time interval
         long sleep = messages * averageGap;
-        if (sleep > maxCoalesceWindow)
+        if (!sleep || sleep > maxCoalesceWindow)
             return false;
 
         // assume we receive as many messages as we expect; apply the same logic to the future batch:
{code}",,exabytes18,iksaif,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,iksaif,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 10 23:33:12 UTC 2017,,,,,,,,,,"0|i399if:",9223372036854775807,3.9,,,,,,jjirsa,,jjirsa,,,Normal,,2.1.0,,,,,,,,,,,,,,,,,"26/Jan/17 19:52;jjirsa;Good catch, pushing to CI 

|| branch || testall || dtest ||
| https://github.com/jeffjirsa/cassandra/tree/cassandra-13159-2.1 | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-2.1-testall/ | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-2.1-dtest/ | 
| https://github.com/jeffjirsa/cassandra/tree/cassandra-13159-2.2  | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-2.2-testall/ | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-2.2-dtest/ | 
| https://github.com/jeffjirsa/cassandra/tree/cassandra-13159-3.0  | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-3.0-testall/ | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-3.0-dtest/ | 
| https://github.com/jeffjirsa/cassandra/tree/cassandra-13159-3.11  | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-3.11-testall/ | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-3.11-dtest/ |
| https://github.com/jeffjirsa/cassandra/tree/cassandra-13159  | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-testall/ | http://cassci.datastax.com/job/jeffjirsa-cassandra-13159-dtest/ | 

;;;","04/Feb/17 10:38;exabytes18;+1, we've seen this too (3.0.10). This caused one of the outbound message threads to enter infinite loop ultimately causing the node to drop MUTATION and HINT messages.;;;","09/Feb/17 21:39;jjirsa;[~iksaif] - I made a very minor logic change to your patch. Can you confirm you're OK with it?  {{!sleep}} became {{sleep <= 0}} . Also kicked off CI again on a few of the runs that showed failures, just to see if jenkins was unhappy or if we've somehow introduced something real.;;;","10/Feb/17 09:45;iksaif;LGTM;;;","10/Feb/17 23:33;jjirsa;Committed as {{f6a7057815e66ba75253510b3ef6bad492381d94}} . Thanks for the patch, [~iksaif] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool cleanup throwing exception,CASSANDRA-13158,13038152,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eduard.tudenhoefner,trepik,trepik,26/Jan/17 15:11,15/May/20 08:00,14/Jul/23 05:56,19/Aug/17 18:56,4.0,4.0-alpha1,,,,,Tool/nodetool,,,,,0,,,,,"After running nodetool cleanup I get this exception:

error: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
-- StackTrace --
org.apache.cassandra.exceptions.ConfigurationException: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
	at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:383)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:125)
	at org.apache.cassandra.cql3.QueryProcessor.<clinit>(QueryProcessor.java:84)
	at org.apache.cassandra.config.CFMetaData.compile(CFMetaData.java:411)
	at org.apache.cassandra.schema.SchemaKeyspace.compile(SchemaKeyspace.java:240)
	at org.apache.cassandra.schema.SchemaKeyspace.<clinit>(SchemaKeyspace.java:88)
	at org.apache.cassandra.config.Schema.<init>(Schema.java:107)
	at org.apache.cassandra.config.Schema.<clinit>(Schema.java:55)
	at org.apache.cassandra.tools.nodetool.Cleanup.execute(Cleanup.java:50)
	at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:251)
	at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:165)",Fedora 25 x86,dcapwell,eduard.tudenhoefner,stefan.miklosovic,trepik,varung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/20 19:29;stefan.miklosovic;error.txt;https://issues.apache.org/jira/secure/attachment/12993806/error.txt",,,,,,,,,,,,,,,,,,,1.0,eduard.tudenhoefner,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 21:10:51 UTC 2020,,,,,,,,,,"0|i398zz:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"19/Aug/17 18:56;eduard.tudenhoefner;This is fixed by CASSANDRA-9054;;;","17/Feb/20 16:58;stefan.miklosovic;This is still happening in 4.0-alpha3;;;","18/Feb/20 01:43;dcapwell;Tested on alpha3 and don't see it

{code}
$ ./bin/nodetool cleanup
$ ./bin/nodetool cleanup system
$ ./bin/nodetool cleanup schema1
{code}

[~stefan.miklosovic] could you give more details?  What does your keyspace look like? Is it constant or only happens some times?;;;","18/Feb/20 08:48;stefan.miklosovic;[~dcapwell] I have to admit I am using some ""custom"" scripts for invoking nodetool but I was trying to be as close as possible to out-of-the-box setting and it is happening. I can try to give you the exact / raw command this issue occurs with once I get to it.

 

Why is this even happening? That error is very strange. What does murmur have in common with some cleanup? Dont you know _why_ is this a thing in the first place?;;;","18/Feb/20 17:25;dcapwell;bq. Why is this even happening? That error is very strange.

[~stefan.miklosovic] here is what I see. The stack trace points to this [line here|https://github.com/apache/cassandra/blob/cassandra-4.0-alpha3/src/java/org/apache/cassandra/tools/nodetool/Cleanup.java#L50]. That line calls [this function|https://github.com/apache/cassandra/blob/cassandra-4.0-alpha3/src/java/org/apache/cassandra/schema/SchemaConstants.java#L75] and the whole class does not depend on Schema (the stack trace says otherwise); if I look at 3.11 I see the same behavior (linked JIRA is marked 3.10).

Now, if I look at 3.0 I see the behavior in the stack trace; [this|https://github.com/apache/cassandra/blob/cassandra-3.0.20/src/java/org/apache/cassandra/tools/nodetool/Cleanup.java#L50] calls to the Schema class (matches stack trace).

bq. What does murmur have in common with some cleanup? Dont you know why is this a thing in the first place?

The main reason is because Schema class depends on the configuration for the node but nodetool shouldn't depend on the configuration (cassandra.yml).  This is also coupled with the fact the error message could actually mean -Dcassandra.partitioner is set in nodetool for some reason; if you look at https://github.com/apache/cassandra/blob/cassandra-3.0.20/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L380 you will see the error always prints what is in the yaml but -Dcassandra.partitioner=thiswillfail will cause that exception.

Now, the correct behavior is that nodetool shouldn't depend on the configuration and that is what I see in 4.0.  I don't see any dependency on the Schema class with regard to Cleanup (or on the configs), and only see it compare against constants class.  So without a new stack trace from 4.0 I can't say why the behavior is seen since I am unable to replicate and the reported stack trace has been fixed to not depend on Schema class.

[~stefan.miklosovic] if you have more details I would be glad to help.;;;","18/Feb/20 18:21;stefan.miklosovic;[~dcapwell] here you go

{code}
error: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
-- StackTrace --
org.apache.cassandra.exceptions.ConfigurationException: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
	at org.apache.cassandra.config.DatabaseDescriptor.applyPartitioner(DatabaseDescriptor.java:1108)
	at org.apache.cassandra.config.DatabaseDescriptor.toolInitialization(DatabaseDescriptor.java:218)
	at org.apache.cassandra.config.DatabaseDescriptor.toolInitialization(DatabaseDescriptor.java:185)
	at org.apache.cassandra.tools.NodeProbe.checkJobs(NodeProbe.java:291)
	at org.apache.cassandra.tools.NodeProbe.forceKeyspaceCleanup(NodeProbe.java:298)
	at org.apache.cassandra.tools.nodetool.Cleanup.execute(Cleanup.java:55)
	at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:341)
	at org.apache.cassandra.tools.NodeTool$NodeToolCmd.accept(NodeTool.java:326)
	at org.apache.cassandra.tools.NodeTool$NodeToolCmd.accept(NodeTool.java:300)
	at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:237)
	at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:84)

{code}
;;;","18/Feb/20 18:31;brandon.williams;This seems to indicate the partitioner has been changed (which is not something that should/can be done.);;;","18/Feb/20 18:39;stefan.miklosovic;I followed the code code and quickly ...

 
{code:java}
// definitely not safe for tools + clients - implicitly instantiates schema
public static void applyPartitioner()
{
    /* Hashing strategy */
    if (conf.partitioner == null)
    {
        throw new ConfigurationException(""Missing directive: partitioner"", false);
    }
    try
    {
        partitioner = FBUtilities.newPartitioner(System.getProperty(Config.PROPERTY_PREFIX + ""partitioner"", conf.partitioner));
    }
    catch (Exception e)
    {
        // IT THROWS THIS so try is bad
        throw new ConfigurationException(""Invalid partitioner class "" + conf.partitioner, false);
    }

    paritionerName = partitioner.getClass().getCanonicalName();
}
{code}
newPartitioner in FBUtilities does this:

 
{code:java}
static IPartitioner newPartitioner(String partitionerClassName, Optional<AbstractType<?>> comparator) throws ConfigurationException
{
    if (!partitionerClassName.contains("".""))
        partitionerClassName = ""org.apache.cassandra.dht."" + partitionerClassName;

    if (partitionerClassName.equals(""org.apache.cassandra.dht.LocalPartitioner""))
    {
        assert comparator.isPresent() : ""Expected a comparator for local partitioner"";
        return new LocalPartitioner(comparator.get());
    }
    return FBUtilities.instanceOrConstruct(partitionerClassName, ""partitioner"");
}
{code}
 

Hence finally, it constructs it like this:

 
{code:java}
public static <T> T instanceOrConstruct(String classname, String readable) throws ConfigurationException
{
    Class<T> cls = FBUtilities.classForName(classname, readable);
    try
    {
        Field instance = cls.getField(""instance"");
        return cls.cast(instance.get(null));
    }
    catch (NoSuchFieldException | SecurityException | IllegalArgumentException | IllegalAccessException e)
    {
        // Could not get instance field. Try instantiating.
        return construct(cls, classname, readable);
    }
}
{code}
The only place where it can throw is either in classForName, or in getField or in cast and finally in construct method. I would bet that it is not in ""cls.getField"" because that field exists on murmur instance and it points to object so it it can not call construct but why? Too bad that we are not propagaing underlying exception in that first ConfigurationException throw ... 

 ;;;","18/Feb/20 18:41;dcapwell;Thanks [~stefan.miklosovic] for the new stack trace!

Yeah, that looks weird.  I can only replicate it doing the following

{code}
$ ./bin/nodetool cleanup system_distributed
$ JAVA_TOOL_OPTIONS='-Dcassandra.partitioner=mwhahaha' ./bin/nodetool cleanup system_distributed
Picked up JAVA_TOOL_OPTIONS: -Dcassandra.partitioner=mwhahaha
error: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
-- StackTrace --
org.apache.cassandra.exceptions.ConfigurationException: Invalid partitioner class org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.config.DatabaseDescriptor.applyPartitioner(DatabaseDescriptor.java:1117)
        at org.apache.cassandra.config.DatabaseDescriptor.toolInitialization(DatabaseDescriptor.java:218)
        at org.apache.cassandra.config.DatabaseDescriptor.toolInitialization(DatabaseDescriptor.java:185)
        at org.apache.cassandra.tools.NodeProbe.checkJobs(NodeProbe.java:291)
        at org.apache.cassandra.tools.NodeProbe.forceKeyspaceCleanup(NodeProbe.java:298)
        at org.apache.cassandra.tools.nodetool.Cleanup.execute(Cleanup.java:55)
        at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:341)
        at org.apache.cassandra.tools.NodeTool$NodeToolCmd.accept(NodeTool.java:326)
        at org.apache.cassandra.tools.NodeTool$NodeToolCmd.accept(NodeTool.java:300)
        at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:237)
        at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:84)
{code}

The other bad part is that the root exception is dropped it looks very misleading.;;;","18/Feb/20 18:42;dcapwell;bq. Too bad that we are not propagaing underlying exception in that first ConfigurationException throw ... 

fully agree with that!;;;","18/Feb/20 18:54;dcapwell;[~stefan.miklosovic] could you run the following

{code}
JVM_OPTS='-XshowSettings:properties' ./bin/nodetool cleanup system_distributed
{code}

I can only replicate this if the system property is set; that will print out all system properties;;;","18/Feb/20 18:59;brandon.williams;You could also probably just change the partitioner in the yaml.  Can you also show us your yaml, [~stefan.miklosovic]?;;;","18/Feb/20 19:33;stefan.miklosovic;I uploaded error.txt file.

this cassandra.config.loader = com.instaclustr.cassandra.k8s.ConcatenatedYamlConfigurationLoader

is this: [https://github.com/instaclustr/cassandra-operator/blob/master/java/cassandra-4-k8s-addons/src/main/java/com/instaclustr/cassandra/k8s/ConcatenatedYamlConfigurationLoader.java]

 

It basically just concatenates yaml fragments from multiple files into one file. I do not want to go into this here (if it is not important).

From the console output I see it is using Murmur partitioner.;;;","18/Feb/20 21:06;dcapwell;thanks!  Looking at the file I don't see -Dcassandra.partitioner= showing up, so only makes sense if it throws.  We should improve the logic so we know what the root cause is;;;","19/Feb/20 18:26;dcapwell;I sent out a patch to attempt to improve the error message to help debug this; see CASSANDRA-15591.  That won't fix the issue, but it should no longer mask what the issue is.;;;","19/Feb/20 20:08;stefan.miklosovic;[~dcapwell] I am eager to try it but I am confused why you have closed that PR? ;;;","19/Feb/20 20:11;brandon.williams;Because it's committed to trunk.;;;","19/Feb/20 20:14;dcapwell;Sorry if there was confusion; PR is only for review, I close once its merged (merge doesn't happen in PR);;;","25/Mar/20 21:10;stefan.miklosovic;[~dcapwell] I tested this again and I was not able to reproduce it. I think I got my scripts wrong so I did it right way and it works. I still think that issue is still there burried in some complex ""this is not set and this is"" but user will not experience it if he does not do something really strange. Good there is cause of that exception given so we know what is going on if somebody ever hit it again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reappeared Data when Mixing Incremental and Full Repairs,CASSANDRA-13153,13037722,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,spod,Amanda.Debrot,Amanda.Debrot,25/Jan/17 15:13,15/May/20 08:03,14/Jul/23 05:56,20/Mar/17 19:29,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,Legacy/Tools,Local/Compaction,,,,0,Cassandra,,,,"This happens for both LeveledCompactionStrategy and SizeTieredCompactionStrategy.  I've only tested it on Cassandra version 2.2 but it most likely also affects all Cassandra versions after 2.2, if they have anticompaction with full repair.

When mixing incremental and full repairs, there are a few scenarios where the Data SSTable is marked as unrepaired and the Tombstone SSTable is marked as repaired.  Then if it is past gc_grace, and the tombstone and data has been compacted out on other replicas, the next incremental repair will push the Data to other replicas without the tombstone.

Simplified scenario:
3 node cluster with RF=3
Intial config:
	Node 1 has data and tombstone in separate SSTables.
	Node 2 has data and no tombstone.
	Node 3 has data and tombstone in separate SSTables.

Incremental repair (nodetool repair -pr) is run every day so now we have tombstone on each node.
Some minor compactions have happened since so data and tombstone get merged to 1 SSTable on Nodes 1 and 3.
	Node 1 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.
	Node 2 has data and tombstone in separate SSTables.
	Node 3 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.

Incremental repairs keep running every day.
Full repairs run weekly (nodetool repair -full -pr). 
Now there are 2 scenarios where the Data SSTable will get marked as ""Unrepaired"" while Tombstone SSTable will get marked as ""Repaired"".

Scenario 1:
        Since the Data and Tombstone SSTable have been marked as ""Repaired"" and anticompacted, they have had minor compactions with other SSTables containing keys from other ranges.  During full repair, if the last node to run it doesn't own this particular key in it's partitioner range, the Data and Tombstone SSTable will get anticompacted and marked as ""Unrepaired"".  Now in the next incremental repair, if the Data SSTable is involved in a minor compaction during the repair but the Tombstone SSTable is not, the resulting compacted SSTable will be marked ""Unrepaired"" and Tombstone SSTable is marked ""Repaired"".

Scenario 2:
        Only the Data SSTable had minor compaction with other SSTables containing keys from other ranges after being marked as ""Repaired"".  The Tombstone SSTable was never involved in a minor compaction so therefore all keys in that SSTable belong to 1 particular partitioner range. During full repair, if the last node to run it doesn't own this particular key in it's partitioner range, the Data SSTable will get anticompacted and marked as ""Unrepaired"".   The Tombstone SSTable stays marked as Repaired.

Then it’s past gc_grace.  Since Node’s #1 and #3 only have 1 SSTable for that key, the tombstone will get compacted out.
	Node 1 has nothing.
	Node 2 has data (in unrepaired SSTable) and tombstone (in repaired SSTable) in separate SSTables.
	Node 3 has nothing.

Now when the next incremental repair runs, it will only use the Data SSTable to build the merkle tree since the tombstone SSTable is flagged as repaired and data SSTable is marked as unrepaired.  And the data will get repaired against the other two nodes.
	Node 1 has data.
	Node 2 has data and tombstone in separate SSTables.
	Node 3 has data.
If a read request hits Node 1 and 3, it will return data.  If it hits 1 and 2, or 2 and 3, however, it would return no data.

Tested this with single range tokens for simplicity.
",Apache Cassandra 2.2,Amanda.Debrot,bdeggleston,christianmovi,eanujwa,ferozshaik552@gmail.com,jjirsa,kohlisankalp,marcuse,philipthompson,spod,vvavro,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13398,,,,,,,,,,,,,"25/Jan/17 15:13;Amanda.Debrot;Step-by-Step-Simulate-Reappeared-Data.txt;https://issues.apache.org/jira/secure/attachment/12849305/Step-by-Step-Simulate-Reappeared-Data.txt","25/Jan/17 15:13;Amanda.Debrot;log-Reappeared-Data.txt;https://issues.apache.org/jira/secure/attachment/12849306/log-Reappeared-Data.txt",,,,,,,,,,,,,,,,,,2.0,spod,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 19:29:52 UTC 2017,,,,,,,,,,"0|i396wf:",9223372036854775807,2.2.7,2.2.8,,,,,marcuse,,marcuse,,,Critical,,2.2.0 beta 1,,,,,,,,,,,,,,,,,"26/Jan/17 12:06;spod;Thanks reporting this, [~Amanda.Debrot]! Let me try to wrap-up again what's happending here..

I think the assumption was that anti-compaction will isolate repaired ranges into the repaired set of sstables, while parts of sstables not covered by the repair will stay in the unrepaired set. As described by Amanda, trouble starts when anti-compaction is taking place exclusively on already repaired sstables. Once we've finished repairing a certain range using full repair, anti-compaction will move unaffected ranges in overlapping sstables from the repaired into unrepaired set again, even if ranges have actually already been repaired before. As the overlap between ranges and sstables is non-deterministic, we could either see regular cells, tombstones or both being move to unrepaired, based on whether the sstable happens to overlap or not. 

Unfortunately this is not the only way that this could happen. As described in CASSANDRA-9143, compactions during the repairs can prevent anti-compaction for individual sstables and tombstones and data could end up in different sets in this case as well. 

bq.  I've only tested it on Cassandra version 2.2 but it most likely also affects all Cassandra versions with incremental repair - like 2.1 and 3.0.

I think 2.1 should not be affected, as we started doing anti-compactions for full repairs in 2.2.;;;","26/Jan/17 14:42;Amanda.Debrot;Hi Stefan,

Yes true, it should just affect Cassandra 2.2+ versions.  I forgot about that point with 2.1.  I'll update the ""since version"".  Thanks!;;;","31/Jan/17 10:09;spod;I can think of two options here. First of all we could simply prevent anti-compaction for full primary range repairs just as we did for CASSANDRA-9142 and CASSANDRA-10422, see [here|https://github.com/apache/cassandra/compare/trunk...spodkowinski:WIP-13153-global]. We could also try to adopt the anti-compaction process to fall back to the repairedAt min value of all compacting sstables, instead of using UNREPAIRED_SSTABLE, see [here|https://github.com/apache/cassandra/compare/trunk...spodkowinski:WIP-13153-repairedAt]. But that's a bit tricky as even a single unrepaired sstables can prevent this and we'd have to group sstables differently for that.

[~krummas], any other ideas or comments?;;;","14/Feb/17 13:50;spod;Getting back to this ticket and giving it some thoughts again, I'm pretty sure that it's not enough to disable anti-compaction for full PK repairs. This will only prevent the described issue for the repair initiator node, but not the involved other replicas. I'm afraid there's no way around disabling anti-compaction for full repairs completely to prevent this issue from happening.;;;","14/Feb/17 19:30;bdeggleston;I think this can also happen just by running incremental repair only, because of the way it leaks data into the unrepaired sstable bucket. This has been fixed in CASSANDRA-9143… but that was only committed to trunk, since it’s not a trivial change. Unfortunately, the only way to avoid this in pre-4.0 clusters is to just not run incremental repair.

This may not be as bad as it sounds though, since what pre CASSANDRA-9143 incremental repair gained in validation time, it likely lost in redundant re-streaming of otherwise repaired data. If you compacted a large sstable that was also involved in a repair, the entire contents of that sstable would end up getting streamed to every other replica on the next incremental repair.;;;","14/Feb/17 19:57;spod;CASSANDRA-13153 is not just about redundant re-streaming. It's about streaming only _partial_ data for partitions or cells based on the circumstance if an individual sstable has been affected or not. If it did, you may end up leaking data that is covered by a tombstone back to unrepaired, while the tombstone in the unaffected sstable stays in repaired, and have the data streamed from there to all other nodes (which may already compacted the data and tombstone away). Or am I missing something here?

With CASSANDRA-9143 it's not _that_ bad, since you start on unrepaired, recent data and the next incremental run will indeed fix the data that has been left in unrepaired before, given it's run within gc_grace. But with CASSANDRA-13153 you might leak arbitrary old data into unrepaired, which should never happen.;;;","14/Feb/17 22:15;bdeggleston;bq. CASSANDRA-13153 is not just about redundant re-streaming. It's about streaming only partial data for partitions or cells 

Right, agreed. My point was that not using incremental repair should fix [~Amanda.Debrot]'s problem. The part about redundant streaming just meant that as a workaround, it might not actually be as bad as it sounds.

bq. With CASSANDRA-9143 it's not that bad, since you start on unrepaired, recent data and the next incremental run will indeed fix the data that has been left in unrepaired before, given it's run within gc_grace. But with CASSANDRA-13153 you might leak arbitrary old data into unrepaired, which should never happen.

I'm not sure what you mean here. The goal of CASSANDRA-9143 was to prevent repaired data from ever leaking back into unrepaired, for both correctness and performance reasons. Do you mean that leaking data is still possible after CASSANDRA-9143, or that the point of this ticket is different?;;;","15/Feb/17 09:39;spod;Taking a closer look at CASSANDRA-9143 again, I'm certain that your work there would indeed fix the issue described in this ticket, as we no longer do anti-compaction for full repairs. So that brings me back to the question: shouldn't we get rid of anti-compactions for full repairs in 2.2+ as well?;;;","15/Feb/17 09:54;marcuse;[~spodxx@gmail.com] are you saying that sstables marked as repaired are getting moved to unrepaired? I don't see how that could happen, with -full repairs, if a repaired sstable gets compacted away, it will (should?) stay in repaired, not get moved to unrepaired. The already repaired sstables will just not get anticompacted with the new repairedAt time;;;","15/Feb/17 13:20;marcuse;Ok, so the problem is actually if we run a -full repair and some of the ranges fail, we might anticompact an sstable containing the data to unrepaired, but the repaired tombstone stays in repaired because that sstable was compacted away. The fix would be that we anticompact to the previous value of repairedAt.

Or that we, as suggested, don't anticompact on full repairs at all.;;;","15/Feb/17 16:44;bdeggleston;bq. shouldn't we get rid of anti-compactions for full repairs in 2.2+ as well?

I think we're ok leaving them as is. Using pre 4.0 incremental repairs is root cause of this. If operators stop using incremental repairs, there's no harm in doing an anticompaction after a full repair. The only scenario it would cause problems is when using incremental repair for the first time after upgrading to 4.0, when the repaired datasets are very likely inconsistent. This could be addressed by just running a final full repair on the upgraded cluster. As part of CASSANDRA-9143, full repairs no longer perform anticompaction, and streamed sstables include the repairedAt time, which would bring the repaired and unrepaired datasets in sync.

So having said all that, it seems like we should recommend that users who delete data:
1. Stop using incremental repair (pre-4.0)
2. Run a full repair after upgrading to 4.0 before using incremental repair again

We should also recommend that even if users don't delete data, they should take a look at the amount of streaming their incremental repair is doing, and decide if it might be less expensive to just do full repairs instead.

Thoughts?;;;","16/Feb/17 12:28;spod;bq.  If operators stop using incremental repairs, there's no harm in doing an anticompaction after a full repair. 

Even if there's no harm when it comes to consistency, it's still causing fragmentation of existing sstables. All repaired ranges will cause all replicas to go through all local, intersecting sstables and rewrite them segregated by affected and unaffected token ranges. This will cause unnecessary load and is probably pretty bad for LCS or STCS, as we constantly break up bigger sstables by doing so.

One option to avoid this would be just to never run anti-compaction on repaired sstables. See [here|https://github.com/spodkowinski/cassandra/commit/684d1c72cda58fecea15b46f928a451df38d87cb] for a simple approach. I don't think anti-compaction was ever meant to work on already repaired sstables, so that's probably the most non-intrusive fix to avoid most of the known issues around incremental repairs discussed here.

Btw, I'm also a bit confused by looking at [createWriterForAntiCompaction|https://github.com/apache/cassandra/blob/98d74ed998706e9e047dc0f7886a1e9b18df3ce9/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1285]. Each sstable's level will be streamed as well, doesn't it? Can we really throw them into the same level locally, just because they have been at level X on other nodes? Won't this potentially break the ""non-overlapping sstables"" guarantee by dropping them blindly to level X?
;;;","16/Feb/17 12:35;marcuse;bq. Can we really throw them into the same level locally, just because they have been at level X on other nodes?
no, we check if it would create overlap before adding it to the manifest:
https://github.com/apache/cassandra/blob/98d74ed998706e9e047dc0f7886a1e9b18df3ce9/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java#L149;;;","17/Feb/17 15:29;marcuse;[~spodxx@gmail.com] the patch LGTM, could you run CI on it?;;;","22/Feb/17 12:46;spod;Patch has now been finished based on my last suggestion to simply skip already repaired sstables during anti-compaction. I've also made the repairedAt timestamps for both the containing and not containing ranges an explicit parameter. This should help to avoid overlooking the fact that we have to deal with repairedAt for the not containing part as well.

PR for corresponding dtest can be found [here|https://github.com/riptano/cassandra-dtest/pull/1447].

Test results (just started new dtest run with PR branch):

||2.2||3.0||3.11||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-2.2]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-3.0]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-3.11]|
|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-2.2-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.11-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-2.2-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.11-testall/]|



;;;","28/Feb/17 15:35;marcuse;Not sure I agree that adding the parameter helps, could we just add an assert in {{anticompactGroup}} that all sstables are unrepaired instead?;;;","28/Feb/17 15:59;spod;I'm not sure I really understand what the additional {{repairedAtNotContainedInRange}} parameter has to do with adding an assert for making sure ""all sstables are unrepaired"". Even if all sstables are, we still need to apply a repairedAt value for those ranges not successfully repaired.;;;","28/Feb/17 16:15;marcuse;My thinking was that the only time anyone could expect the sstable with the non-repaired ranges to be something other than UNREPAIRED would be if they passed in repaired sstables, so having the assert shows that this is not expected;;;","28/Feb/17 17:52;spod;Yes, that could be an option as well. But as we already discussed the possibility of actually doing anti-compaction on repaired sstables for the sake of tracking repairedAt more accurately, I was hoping someone someday would be able to make use of the method as is by providing a reasonable repairedAt value for both anti-compaction outputs. But I'm open to add an assert instead, if you think I'm a bit to optimistic here.;;;","01/Mar/17 07:12;marcuse;Makes sense, but lets add this if/when we do that change?;;;","02/Mar/17 16:13;spod;I've changed my branches to the bare minimum of what needs to be done for filtering already repaired sstables and re-run tests. See above for links.;;;","09/Mar/17 09:38;spod;[~krummas], any feedback on the latest, simplified patch version?;;;","09/Mar/17 09:41;marcuse;oops, sorry for the delay, +1;;;","20/Mar/17 19:29;spod;Merged as 06316df549c0096bd774893a405d1d32512e97bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UPDATE on counter columns with empty list as argument in IN disables cluster,CASSANDRA-13152,13037694,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,blerer,george C,george C,25/Jan/17 13:14,15/May/20 08:01,14/Jul/23 05:56,07/Feb/17 09:58,3.0.11,3.11.0,4.0,4.0-alpha1,,,CQL/Interpreter,Local/SSTable,,,,0,,,,,"On a 3 node cluster
with this table (replication factor of 2):

{code}
CREATE TABLE tracking.item_items_rec_history (
	reference_id bigint,
	country text,
	portal text,
	app_name text,
	recommended_id bigint,
	counter counter,
	PRIMARY KEY (reference_id, country, portal, app_name, recommended_id)
);
{code}
If I execute 

{code}
UPDATE user_items_rec_history 
SET counter = counter + 1 
WHERE reference_id = 1 AND country = '' AND portal = '' AND app_name = '' AND recommended_id IN ();
{code}

Take note that the IN is empty

The cluster starts to malfunction and responds a lot of timeouts to any query.

After resetting some of the nodes, the cluster starts to function normally again.
","Linux Ubuntu 16
3 Virtual machines",blerer,george C,ifesdjeen,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,Availability -> Process Crash,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 09:58:46 UTC 2017,,,,,,,,,,"0|i396q7:",9223372036854775807,,,,,,,ifesdjeen,,ifesdjeen,,,Critical,,,,,,,,,,,,,,,,,,,"01/Feb/17 09:06;blerer;An {{UPDATE}} or a {{DELETE}} query with an empty {{IN}} restriction will result in an invalid mutation that we be send to the replicas.
The patch makes sure that if an {{UPDATE}} or {{DELETE}} query contains an empty {{IN}} restriction no mutation will be created.

||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13152-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13152-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13152-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13152-trunk-dtest/]|;;;","03/Feb/17 15:01;ifesdjeen;+1 

Thanks for the patch!;;;","07/Feb/17 09:57;blerer;Thanks for the review.;;;","07/Feb/17 09:58;blerer;Committed into 3.0 at fb606dd41c9f14324749efc1344421237c36a6db and merged into 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unicode unittest fail,CASSANDRA-13151,13037574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,jay.zhuang,jay.zhuang,25/Jan/17 04:26,16/Apr/19 09:30,14/Jul/23 05:56,07/Apr/17 09:32,,,,,,,,,,,,0,,,,,"Following unittests are failed in 3.11 and trunk(4.0) branch
{noformat}
SASIIndexTest.testUnicodeSupport
StandardAnalyzerTest.testTokenizationJaJp1
StandardAnalyzerTest.testTokenizationJaJp2
StandardAnalyzerTest.testTokenizationRuRu1
StandardAnalyzerTest.testTokenizationZnTw1
{noformat}
It works fine on my local mac, but not linux server. I guess it's related to Unicode setting. Does anyone have any idea on that? (could it be related to CASSANDRA-11077, CASSANDRA-11431?)

Here are the failure details
{noformat}
$ ant testsome -Dtest.name=org.apache.cassandra.index.sasi.SASIIndexTest -Dtest.methods=testUnicodeSupport
...
    [junit] Testcase: testUnicodeSupport(org.apache.cassandra.index.sasi.SASIIndexTest):        FAILED
    [junit] []
    [junit] junit.framework.AssertionFailedError: []
    [junit]     at org.apache.cassandra.index.sasi.SASIIndexTest.testUnicodeSupport(SASIIndexTest.java:1159)
    [junit]     at org.apache.cassandra.index.sasi.SASIIndexTest.testUnicodeSupport(SASIIndexTest.java:1122)
{noformat}
{noformat}
$ ant test -Dtest.name=StandardAnalyzerTest
...
    [junit] Testcase: testTokenizationJaJp1(org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest):     FAILED
    [junit] expected:<210> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<210> but was:<0>
    [junit]     at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest.testTokenizationJaJp1(StandardAnalyzerTest.java:85)
    [junit]
    [junit]
    [junit] Testcase: testTokenizationJaJp2(org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest):     FAILED
    [junit] expected:<57> but was:<9>
    [junit] junit.framework.AssertionFailedError: expected:<57> but was:<9>
    [junit]     at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest.testTokenizationJaJp2(StandardAnalyzerTest.java:104)
    [junit]
    [junit]
    [junit] Testcase: testTokenizationRuRu1(org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest):     FAILED
    [junit] expected:<456> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<456> but was:<0>
    [junit]     at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest.testTokenizationRuRu1(StandardAnalyzerTest.java:120)
    [junit]
    [junit]
    [junit] Testcase: testTokenizationZnTw1(org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest):     FAILED
    [junit] expected:<963> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<963> but was:<0>
    [junit]     at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest.testTokenizationZnTw1(StandardAnalyzerTest.java:136)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.index.sasi.analyzer.StandardAnalyzerTest FAILED
{noformat}",,ifesdjeen,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 09:32:18 UTC 2017,,,,,,,,,,"0|i395zj:",9223372036854775807,3.11.0,4.0,,,,,jay.zhuang,,jay.zhuang,,,Normal,,,,,,,,,,,,,,,,,,,"04/Apr/17 14:20;ifesdjeen;Input reader charset wasn't being set correctly. 

Steps to reproduce: 

{code}
LC_ALL=ru_RU ant test -Dtest.name=StandardAnalyzerTest
{code}

And a trivial patch:

|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13151-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13151-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13151-trunk-dtest/]|;;;","04/Apr/17 21:15;jay.zhuang;Thanks Alex for the fix. Verified the change locally and it works fine. +1

A few minor comments, or further improvement questions:
1. How about change {{Charset.forName(""UTF-8"")}} to {{StandardCharsets.UTF_8}}? at least to consistency with [StopWordFactory.java|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/sasi/analyzer/filter/StopWordFactory.java#L79].
2. There're other {{InputStreamReader}} without set the coding, will they also be problems?
3. Maybe add an unittest to set the Locale and test this?
4. Nit: remove unused import {{java.util.Locale}};;;","06/Apr/17 13:17;ifesdjeen;Thanks for the review [~jay.zhuang], I've addressed your comments. 

I hope you don't mind that I put you as a reviewer.;;;","06/Apr/17 22:03;jay.zhuang;Looks good to me. +1

Could you please also push the change to 3.11?;;;","07/Apr/17 09:32;ifesdjeen;Committed to 3.11 as [5f54d42107b0e0ece0ebb94a285cd7957b608523|https://github.com/apache/cassandra/commit/5f54d42107b0e0ece0ebb94a285cd7957b608523] and merged up to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError prepending to a list,CASSANDRA-13149,13037464,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,swarren,swarren,24/Jan/17 20:45,15/May/20 08:03,14/Jul/23 05:56,29/Sep/17 12:28,3.0.15,3.11.1,4.0,4.0-alpha1,,,Legacy/CQL,,,,,0,,,,,"Prepending to a list produces the following AssertionError randomly. Changing the update to append (and sort in the client) works around the issue.

{code}
java.lang.AssertionError: null
	at org.apache.cassandra.cql3.Lists$PrecisionTime.getNext(Lists.java:275) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.Lists$Prepender.execute(Lists.java:430) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:94) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:682) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:613) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:420) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:408) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:487) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:464) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [apache-cassandra-3.0.8.jar:3.0.8]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.8.jar:3.0.8]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{code}",3.0.8,blerer,ifesdjeen,jasobrown,samt,slebresne,swarren,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 12:28:26 UTC 2017,,,,,,,,,,"0|i395b3:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"08/Feb/17 14:08;ifesdjeen;Do you have NTP on that server? ({{System#currentTimeMillis}} is not monotonic, so that _might_ be causing that, just need to understand how to reproduce that);;;","09/Feb/17 01:37;swarren;That is what I initially suspected and so confirmed ntp on all servers (and the client machine as well). This bit of code seems to be sensitive down to the millisecond so not sure ntp would help with that in any case.;;;","29/May/17 12:07;blerer;I had a quick look at the code and it seems to me that the code can trigger the assertion even with  {{System#currentTimeMillis}} being monotonic.
As the time is being retrieved before the start of the loop adding the values [here| https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/cql3/Lists.java#L425] another query might end up being faster and change the last time before all the value got added.
Even if the {{PrecisionTime#getNext}} method was used only once per update we could have a race condition and trigger the assertion.

The best solution would be in my opinion to avoid sharing {{PrecisionTime}} and create one instance per operation.

[~slebresne] does my analysis and my proposed solution make sense to you?;;;","31/May/17 09:39;slebresne;Your analysis does make sense. The concern with creating one instance per operation however is that one of the goal here is to avoid ever generating the same UUID twice, especially among competing operations. Basically, if someone do 2 concurrent prepend on a given list, we can't guarantee which one will end up first in the list, but we do want to ensure both elements ends up in the list, not that one gets thrown away because both operation happen to generate the same UUID.

Overall, there appears to be 2 problems: 1) the broken reliance on {{System#currentTimeMillis}} being monotonic and 2) the fact that the way we use the {{time}} is not thread-safe between operations. For 1), we can do something like we do in {{UUIDGen}}, basically by having {{getNext}} handle the case of {{millis > current.millis}} by ignoring {{millis}} and using {{current.millis}} instead (so if {{System#currentTimeMillis}} goes back in time, rely on the last time saved until the time catches up to that last saved value; we probably want to change how we handle the case where {{current.nanos == 0}} though, by decreasing {{millis}} rather than bailing out and generating the same time as that's wrong). For 2), I suspect the simplest approach could be to change {{getNext()}} to be able to general {{n}} values atomically instead of one. Each operation would then grab as many value it needs upfront and use them. A minor benefit being that if 2 concurrent prepend are done concurrently, all of the value from each update would end up ""together"", which feels nicer (gives some sense of isolation; we may even want to make this work for append too).;;;","13/Sep/17 13:03;jasobrown;Patch here:

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13149-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13149-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13149-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/305/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/309/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/308/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13149-3.0]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13149-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13149-trunk]|

It mostly follows [~slebresne]'s proposed changes as they are pretty reasonable.;;;","19/Sep/17 12:29;samt;Mostly the changes look good. I do think though that we need to handle the case where the list of elements to prepend > {{MAX_NANOS}} better than just throwing an assertion error (obviously not a good idea to do it, but it is allowed). 

This is probably more contentious, but there's also some potential for misuse here as nothing currently stops us overflowing the range of the PT instance except the construction of the for loop. We could make PT.nanos private and add a {{getNanosPlusDelta(int)}} instance method that  asserts that we don't try to use more that were originally allocated.

Nits: 
* typo Lists line #290 {{miilisToUse}}
* typo ListsTest line #65 {{_Mulitple}}
* in other fixtures where we've used the snake case name format, the names tend to have the {{test}} prefix ({{BufferPoolTest/BTreeTest/SegmentedFileTest/etc}}, only {{RepariMessageSerializationsTest}} deviates from that pattern currently).
;;;","20/Sep/17 23:05;jasobrown;Pushed a few updates to the branches.

bq. handle the case where the list of elements to prepend > {{MAX_NANOS}}

Done, even though a client would be rather bananas to prepare 10,000 elements to the beginning of list! Also, added test for that.

bq. nothing currently stops us overflowing the range of the PT instance except the construction of the for loop

I tried turning the ""range"" represented by the instance returned by {{PrecisionTime#getNext()}} into something like an interator, as wel basically want to iterate/walk the value represented by the range. It quickly started to feel like it was over-engineered. While there is this ""tied-at-the-hip"" relationship between {{Prepender}} and {{PrecisionTime}}, I'm not sure that additional guards won't feel similiarly out of place. wdyt?

Tests and other nits have been addressed,as well.
;;;","22/Sep/17 09:12;samt;The batching logic in {{Prepender}} isn't quite right, in the huge list test for e.g. it winds up creating only 1 PT and the nanos used for the UUID end up going negative. I've pushed an alternative version [here|https://github.com/beobal/cassandra/commit/3e96a523d5c7e4692e7f743c65d725e2f893ab5f], lmk what you think. 

re: the over-engineering, yes I think you're probably right. I was thinking ahead, based on [~slebresne]'s suggestion that we could start using PT more widely, but let's leave it until that happens, perhaps YAGNI.

;;;","25/Sep/17 17:38;jasobrown;[~beobal] yeah, I like your change - correct, and simpler! I've added that to all three branches, and they are queued up for testing on circleci.;;;","29/Sep/17 07:49;samt;LGTM. Doesn't look like there are any new dtest regressions, but CircleCI barfed on 2 of the runs and there are a couple of failures on the 3.0 branch (which I'm sure are unrelated), so IDK what you want to do there. +1 from me though;;;","29/Sep/17 12:28;jasobrown;Committed as sha {{ab0adf9f9bc72074a02025bdecd9479f790d6463}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index query on partition key columns might not return all the rows.,CASSANDRA-13147,13037335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,blerer,blerer,24/Jan/17 11:15,15/May/20 08:01,14/Jul/23 05:56,07/Apr/17 10:02,2.1.18,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,Feature/2i Index,,,,,0,,,,,"A secondary index query on a partition key column will, apparently, not return the empty partitions with static data.

The following unit test can be used to reproduce the problem.

{code}
    public void testIndexOnPartitionKeyWithStaticColumnAndNoRows() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk1 int, pk2 int, c int, s int static, v int, PRIMARY KEY((pk1, pk2), c))"");
        createIndex(""CREATE INDEX ON %s (pk2)"");
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 2, 9, 2);
        execute(""INSERT INTO %s (pk1, pk2, s) VALUES (?, ?, ?)"", 2, 1, 9);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 3, 1, 1, 9, 1);

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(2, 1, null, 9, null), <-- is not returned
                   row(3, 1, 1, 9, 1));
    }
{code}",,adelapena,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 10:02:19 UTC 2017,,,,,,,,,,"0|i394in:",9223372036854775807,3.9,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"30/Mar/17 12:08;adelapena;It seems that it affects to 2.1, 2.2, 3.0, 3.x and trunk. There are two causes for the problem:
* For 2.1, 2.2 and 3.0, the partition containing only static columns is found but [it is not returned at CQL level|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L720].
* For 3.0, 3.x and trunk, the partition containing only static columns [is never indexed|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/index/internal/CassandraIndex.java#L382].
;;;","30/Mar/17 13:15;adelapena;There are initial versions of the patch for 2.1 and 2.2:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13147-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13147-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.2-dtest/]|;;;","04/Apr/17 10:40;adelapena;And there are the patch versions for all the involved branches:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13147-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13147-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-2.2-dtest/]|
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13147-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13147-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13147-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13147-trunk-dtest/]|;;;","06/Apr/17 12:22;blerer;It took me a bit of time to understand why the patches for 2.1 and 2.2 were not breaking the following unit test:
{code}
    @Test
    public void testIndexOnRegularColumnWithPartitionWithoutRows() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk int, c int, s int static, v int, PRIMARY KEY(pk, c))"");
        createIndex(""CREATE INDEX ON %s (v)"");
        execute(""INSERT INTO %s (pk, c, s, v) VALUES (?, ?, ?, ?)"", 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk, c, s, v) VALUES (?, ?, ?, ?)"", 1, 2, 9, 2);
        execute(""INSERT INTO %s (pk, s) VALUES (?, ?)"", 2, 9);
        execute(""INSERT INTO %s (pk, c, s, v) VALUES (?, ?, ?, ?)"", 3, 1, 9, 1);
        flush();
        execute(""DELETE FROM %s WHERE pk = ? and c = ?"", 3, 1);
        assertRows(execute(""SELECT * FROM %s WHERE v = ?"", 1),
                   row(1, 1, 9, 1));
    }
{code}

Basically, I would have expected the empty partition to arrive at {{SelectStatement:processColumnFamily}}.
This does not happen because the empty partition is removed on the replica side [here|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L284].
This behavior seems wrong as I believe that it can result in some stale data being returned.
If one of the node has not yet received the deletion, C* might end up returning invalid data, due to that.

The correct solution to that problem is in my opinion to remove rows from the index only when they have been garbage collected from the indexed table. Now, I guess that it should probably be done in a separate ticket.

Regarding the 2.1 and 2.2 pach, I think that the solution should be similar to the 3.0 one. Meaning that  {{usesSecondaryIndexing}} should be replaced by {{hasClusteringColumnsRestriction()}}.
;;;","06/Apr/17 19:30;adelapena;I see, thanks for the comments. I have just updated the 2.1 and 2.2 patches to use {{hasNoRegularColumnsRestriction()}}/{{!hasRegularColumnsRestriction()}} (I guess you meant this instead of {{hasClusteringColumnsRestriction()}}). 

Do you think we should add the aforementioned {{testIndexOnRegularColumnWithPartitionWithoutRows}}?;;;","07/Apr/17 07:30;blerer;{quote}
I see, thanks for the comments. I have just updated the 2.1 and 2.2 patches to use {{hasNoRegularColumnsRestriction()}}/{{!hasRegularColumnsRestriction()}} (I guess you meant this instead of {{hasClusteringColumnsRestriction()}}).
{quote}

Yes, sorry for the wrong copy past.

{quote}Do you think we should add the aforementioned {{testIndexOnRegularColumnWithPartitionWithoutRows}}?{quote}

This test already exist in the 3.x branches. I will add it has part of the commit in the 2.1 and 2.2 branches.



;;;","07/Apr/17 07:41;blerer;Thanks for the patch, +1. ;;;","07/Apr/17 10:02;blerer;Committed into 2.1 at 64d8a1d9fdacf3e7396cdf2f5c61171c1c9bccf2 and merged into 2.2, 3.0 and 3.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra can accept invalid durations,CASSANDRA-13143,13036680,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,20/Jan/17 20:28,16/Apr/19 09:30,14/Jul/23 05:56,27/Jan/17 10:05,3.10,,,,,,Legacy/CQL,,,,,0,,,,,"A duration can be positive or negative. If the duration is positive the months, days and nanoseconds must be greater or equals to zero. If the duration is negative the months, days and nanoseconds must be smaller or equals to zero.
Currently, it is possible to send to C* a duration which does not respect that rule and the data will not be reject.",,aboudreault,blerer,stamhankar999,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 02 13:58:23 UTC 2017,,,,,,,,,,"0|i390h3:",9223372036854775807,,,,,,,thobbs,,thobbs,,,Normal,,,,,,,,,,,,,,,,,,,"20/Jan/17 20:36;blerer;||3.11|[branch|https://github.com/apache/cassandra/compare/trunk...blerer:13143-3.11]|[utest|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13143-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13143-3.11-dtest/]|

The patch fix the validation and add some tests for it.;;;","20/Jan/17 20:37;blerer;[~thobbs] Could you also review that ticket?;;;","26/Jan/17 19:07;thobbs;+1;;;","27/Jan/17 10:05;blerer;Thanks for the review.;;;","27/Jan/17 10:05;blerer;Committed into 3.11 at 91564abaddce59b06c024e8959c46ec1e4d19eda and merged into trunk;;;","02/Feb/17 13:52;stamhankar999;The fix-version on this ticket says 3.10. Will this fix be back-ported to 3.10, or should the fix-version of the ticket be updated?;;;","02/Feb/17 13:58;blerer;The 3.11 branch is the one being used for the 3.10 release. The fix will be in 3.10.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgradesstables cancels compactions unnecessarily,CASSANDRA-13142,13036610,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,20/Jan/17 16:41,16/Apr/19 09:30,14/Jul/23 05:56,10/May/18 11:07,,,,,,,,,,,,0,,,,,"Since at least 1.2 upgradesstables will cancel any compactions bar validations when run. This was originally determined as a non-issue in CASSANDRA-3430 however can be quite annoying (especially with STCS) as a compaction will output the new version anyway. Furthermore, as per CASSANDRA-12243 it also stops things like view builds and I assume secondary index builds as well which is not ideal.

We should avoid cancelling compactions unnecessarily.",,colinkuo,githubbot,jasonstack,jjirsa,KurtG,marcuse,tvdw,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12243,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14197,,,"09/May/17 06:25;KurtG;13142-v1.patch;https://issues.apache.org/jira/secure/attachment/12867048/13142-v1.patch",,,,,,,,,,,,,,,,,,,1.0,KurtG,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 11:07:05 UTC 2018,,,,,,,,,,"0|i3901j:",9223372036854775807,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"20/Jan/17 16:47;KurtG;I've already done all the necessary reconnaissance so will write a patch soon, I think best solution would be to provide a more modular way to specify compaction types to ""ignore"" in {{CompactionManager.interruptCompactionsForCFs()}}. ;;;","20/Jan/17 16:48;tvdw;Cancelling view/index builds is not a ""minor"" issue. :) Granted, a restart should resume the view build, but that's not obvious to operators at all.;;;","25/Jan/17 03:35;KurtG;+1000, I didn't consider the case where builds/compactions are not re-triggered automatically. I think that's sufficient to bump it up to major.;;;","09/May/17 06:20;githubbot;GitHub user kgreav opened a pull request:

    https://github.com/apache/cassandra/pull/110

    Don't stop compactions when running upgradesstables (CASSANDRA-13142)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/instaclustr/cassandra 13142

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/110.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #110
    
----
commit 18570092324f6ab6bace9d3ce3673f59e7d10d7b
Author: kurt <kurt@instaclustr.com>
Date:   2017-05-09T06:16:49Z

    Don't stop compactions when running upgradesstables

----
;;;","09/May/17 06:24;KurtG;First go at https://github.com/apache/cassandra/compare/trunk...kgreav:13142?expand=1

I've attached the patch here as well.
Patch is against 2.2. doesn't apply cleanly to >=3.0 but happy to fix that once ready for commit.

I just wrote a unit test that seems to work reliably, however it only tests the interrupt method. It could be made more extensive if deemed necessary but wanted to see if anyone had any better ideas on testing first.;;;","16/May/17 06:48;marcuse;this idea makes sense to me, a few comments;
* We should probably interrupt all compactions if the user runs ""upgradesstables -a""
* Maybe we should block until the running compactions are finished? An operator might expect all sstables to be upgraded after running upgradesstables but that might not be true after this patch. We would probably need to keep track of the compaction-futures and get a 'snapshot' of the interesting ones (for the current table) when upgradesstables start, and then, when upgradesstables is finished, we wait for the futures in the snapshot to finish.
* [this|https://github.com/instaclustr/cassandra/blob/18570092324f6ab6bace9d3ce3673f59e7d10d7b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L2872-L2878] will always return null if we don't cancel the ongoing compactions ({{getCompacting()}} will not be empty) - I guess we need to compare with which compactions we expect to be cancelled?
* {{cfs.markAllCompacting()}} will fail if we don't actually cancel all ongoing compactions
* [this|https://github.com/instaclustr/cassandra/blob/18570092324f6ab6bace9d3ce3673f59e7d10d7b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L2900-L2907] looks like it will only cancel validation compaction (if that parameter is set), nothing else.
* [this|https://github.com/instaclustr/cassandra/blob/18570092324f6ab6bace9d3ce3673f59e7d10d7b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1748-L1754] will also only ever cancel validation compaction (ie, it calls {{interruptCompactionFor}} with an empty list)
* [this|https://github.com/instaclustr/cassandra/blob/18570092324f6ab6bace9d3ce3673f59e7d10d7b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1751] creates a new list and calls {{.add}} on it - this returns {{true}}, not the new list, so it will recurse forever
;;;","24/May/17 09:22;KurtG;Thanks Marcus - I'll get to this in the next few weeks hopefully.;;;","26/Jun/17 11:55;githubbot;Github user kgreav closed the pull request at:

    https://github.com/apache/cassandra/pull/110
;;;","26/Jun/17 12:10;KurtG;[~krummas] I've updated the branch although haven't addressed your second point yet. I agree we should block until all compactions for a given CF are finished, however I'm having trouble identifying a way to get currently running compaction futures. Is there some novel way to achieve this or should I just go the route of using {{CompactionInfo.Holder}}?

Also regarding {{cfs.markAllCompacting()}}, I've changed it to take operations to interrupt, however the main change being that it now uses {{getUncompactingSSTables()}} rather than {{getPermittedToCompactSSTables()}}. I believe this is OK as at the moment it's only ever called after all compactions are stopped, so these should return the same assuming all types are interruptible, otherwise it will return the subset of SSTables that are not compacting and didn't get forcibly stopped. Worth checking my logic here though.

And yeah those last three points were broken because I changed tract from ""uninterruptibles"" to ""interruptibles"" and confused myself... heh. Should be addressed now.
[github|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:cassandra-2.2]
;;;","27/Jun/17 12:46;KurtG;Well, I've seriously broken SSTableRewriterTest. Haven't found out why yet though.
edit: seems it was actually already broken
edit2: seems it wasn't actually ever broken so ignore this completely. :going_slightly_mad:;;;","20/Jul/17 04:41;KurtG;[~krummas] Did you see my question above? Also, updated link to [branch|https://github.com/apache/cassandra/compare/trunk...kgreav:13142?expand=1] (didn't mean to PR the first time - just dumb)

If this path even works, tests will need some serious thought, but rather make sense of the code before jumping into that.;;;","20/Jul/17 05:00;jjirsa;{quote} tests will need some serious thought{quote}

Byteman is your friend here - you can create a byteman rule pause the compaction after starting it, and then test how upgradesstables interacts with that (running but not proceeding) compaction task.
;;;","20/Jul/17 06:22;KurtG;I was afraid someone would say that;;;","20/Jul/17 15:14;marcuse;bq.  I'm having trouble identifying a way to get currently running compaction futures.
Yeah we don't keep the futures around, and it becomes quite a hack to actually do that

maybe we should just only not cancel view/index builds and cancel regular compactions like we do today?

;;;","21/Jul/17 02:05;KurtG;TBH leaving out normal compactions kind of sucks (was the original reason i created this ticket), however as an alternative, what if we make the fix for views/index builds part of the default behaviour and add a new option to upgradesstables that has the new behaviour in that it won't wait for compactions to complete before finishing (new option might could even let you filter what to interrupt?).;;;","09/May/18 06:27;marcuse;[~KurtG] you think we can close this now that we have CASSANDRA-14197 ?;;;","10/May/18 11:07;KurtG;Yeah I think so. It's a suitable alternative and tbh I didn't even really like the idea of a separate command as I previously suggested.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in cql_tests.MiscellaneousCQLTester.prepared_statement_invalidation_test,CASSANDRA-13141,13036592,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,sean.mccarthy,sean.mccarthy,20/Jan/17 15:24,16/Apr/19 09:30,14/Jul/23 05:56,27/Mar/17 11:27,,,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1471/testReport/cql_tests/MiscellaneousCQLTester/prepared_statement_invalidation_test

{code}
Error Message

Error from server: code=2200 [Invalid query] message=""Altering of types is not allowed""
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tests.py"", line 633, in prepared_statement_invalidation_test
    session.execute(""ALTER TABLE test ALTER d TYPE blob"")
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
{code}",,adelapena,sean.mccarthy,vishnuprasadh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/17 15:24;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12848580/node1.log","20/Jan/17 15:24;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12848578/node1_debug.log","20/Jan/17 15:24;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12848579/node1_gc.log",,,,,,,,,,,,,,,,,3.0,adelapena,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 11:27:16 UTC 2017,,,,,,,,,,"0|i38zxj:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"27/Feb/17 10:33;vishnuprasadh;I am getting error when i try to alter a column from time type to timeuuid as follows:
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Altering of types is not allowed""\

existing structure:
CREATE TABLE products.streaminginfo (
    userid text,
    transtime time,
    appid text,
    clientip inet,
    gatewayip inet,
    geolocation map<text, text>,
    processtime timeuuid,
    serverip inet,
    sessionid text,
    url text,
    PRIMARY KEY (userid, transtime)
) ;

Command issued is :
alter table products.streaminginfo ALTER transtime TYPE timeuuid;;;;","27/Mar/17 11:27;adelapena;This has been already solved by [~blerer] in [this commit|https://github.com/riptano/cassandra-dtest/commit/21ba1f7ea785fe2cb6f84e1b595b8f608650ceb2].

[~vishnuprasadh], [CASSANDRA-12443|https://issues.apache.org/jira/browse/CASSANDRA-12443] removed ALTER TYPE support [from 3.0.11|https://github.com/apache/cassandra/commit/5f66d48b38d32d3768c78f16753ed4f2095bbede].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in hintedhandoff_test.TestHintedHandoff.hintedhandoff_decom_test,CASSANDRA-13139,13036588,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,sean.mccarthy,sean.mccarthy,20/Jan/17 15:16,16/Apr/19 09:30,14/Jul/23 05:56,26/Jun/17 20:49,,,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/503/testReport/hintedhandoff_test/TestHintedHandoff/hintedhandoff_decom_test

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', ['decommission']] exited with non-zero status; exit status: 1; 
stdout: nodetool: Unsupported operation: Not enough live nodes to maintain replication factor in keyspace system_distributed (RF = 3, N = 3). Perform a forceful decommission to ignore.
See 'nodetool help' or 'nodetool help <command>'.
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/hintedhandoff_test.py"", line 169, in hintedhandoff_decom_test
    node2.decommission()
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1314, in decommission
    self.nodetool(""decommission"")
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 783, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1993, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",,jkni,sean.mccarthy,spod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/17 15:16;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12848565/node1.log","20/Jan/17 15:16;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12848563/node1_debug.log","20/Jan/17 15:16;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12848564/node1_gc.log","20/Jan/17 15:16;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12848568/node2.log","20/Jan/17 15:16;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12848566/node2_debug.log","20/Jan/17 15:16;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12848567/node2_gc.log","20/Jan/17 15:16;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12848571/node3.log","20/Jan/17 15:16;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12848569/node3_debug.log","20/Jan/17 15:16;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12848570/node3_gc.log","20/Jan/17 15:16;sean.mccarthy;node4.log;https://issues.apache.org/jira/secure/attachment/12848574/node4.log","20/Jan/17 15:16;sean.mccarthy;node4_debug.log;https://issues.apache.org/jira/secure/attachment/12848572/node4_debug.log","20/Jan/17 15:16;sean.mccarthy;node4_gc.log;https://issues.apache.org/jira/secure/attachment/12848573/node4_gc.log",,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 26 20:49:58 UTC 2017,,,,,,,,,,"0|i38zwn:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"23/Jan/17 08:43;spod;I think there's already a [PR|https://github.com/riptano/cassandra-dtest/pull/1423] out there by [~pauloricardomg] that should fix the test.;;;","26/Jun/17 20:49;jkni;The linked PR resolved this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unclosed file descriptors when querying SnapshotsSize metric,CASSANDRA-13133,13035922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,nrushton,nrushton,18/Jan/17 16:54,16/Apr/19 09:30,14/Jul/23 05:56,19/Jan/17 19:21,3.10,,,,,,Legacy/Local Write-Read Paths,,,,,1,jmx,lhf,metrics,newbie,"Started to notice many open file descriptors (100k+) per node, growing at a rate of about 30 per minute in our cluster. After turning off our JMX exporting server(https://github.com/prometheus/jmx_exporter), which gets queried every 30 seconds, the number of file descriptors remained static. 

Digging a bit further I ran a jmx dump tool over all the cassandra metrics and tracked the number of file descriptors after each query, boiling it down to a single metric causing the number of file descriptors to increase:

org.apache.cassandra.metrics:keyspace=tpsv1,name=SnapshotsSize,scope=events_by_engagement_id,type=Table

running a query a few times against this metric shows the file descriptors increasing after each query:

{code}
for _ in {0..3} 
do 
   java -jar jmx-dump-0.4.2-standalone.jar --port 7199 --dump org.apache.cassandra.metrics:keyspace=tpsv1,name=SnapshotsSize,scope=events_by_engagement_id,type=Table > /dev/null; 
   sudo lsof -p `pgrep -f CassandraDaemon` | fgrep ""DIR"" | awk '{a[$(NF)]+=1}END{for(k in a){print k, a[k]}}' | grep ""events_by"" 
done


> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33176
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33177
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33178
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33179
{code}

it should be noted that the file descriptor is open on a directory, not an actual file",CentOS 7,jjirsa,nrushton,rustyrazorblade,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 06:39:05 UTC 2017,,,,,,,,,,"0|i38vsf:",9223372036854775807,3.9,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"18/Jan/17 23:43;nrushton;only place I see directory file descriptors being explicitly opened is here: https://github.com/apache/cassandra/blob/cassandra-3.X/src/java/org/apache/cassandra/db/lifecycle/LogAwareFileLister.java#L83-L95

but, there may be more indirect ways they are being created.;;;","19/Jan/17 06:29;stefania;CASSANDRA-11594 fixed a leak in file descriptors for directories when calculating the snapshot size. TLDR is that whenever the snapshot size is queried with an ongoing transaction (e.g. sstable flushing or compaction) then the file descriptor would be leaked. I'm 99% sure this is the same problem. 

[~nrushton], the fix is in 3.10, which is due for release soon. If you want to apply the patch manually you find it [here|https://github.com/stef1927/cassandra/commit/7b940acc00fe1a907a40bd6b7dca7f2ea80fdddc].;;;","19/Jan/17 16:39;nrushton;[~Stefania] thank you, will take a look;;;","19/Jan/17 19:19;nrushton;patch worked, closing this ticket as fixed by CASSANDRA-11594;;;","19/Jan/17 19:21;nrushton;fixed by CASSANDRA-11594;;;","20/Jan/17 06:39;stefania;Thanks for confirming!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange result of several list updates in a single request,CASSANDRA-13130,13035608,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,mkrupits_jb,mkrupits_jb,17/Jan/17 17:41,15/May/20 08:02,14/Jul/23 05:56,10/Mar/17 20:45,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,,,,,,1,,,,,"Let's assume that we have a row with the 'listColumn' column and value \{1,2,3,4\}.
For me it looks logical to expect that the following two pieces of code will ends up with the same result but it isn't so.
Code1:
{code}
UPDATE t SET listColumn[2] = 7, listColumn[2] = 8  WHERE id = 1;
{code}
Expected result: listColumn=\{1,2,8,4\} 
Actual result: listColumn=\{1,2,7,8,4\}

Code2:
{code}
UPDATE t SET listColumn[2] = 7  WHERE id = 1;
UPDATE t SET listColumn[2] = 8  WHERE id = 1;
{code}
Expected result: listColumn=\{1,2,8,4\} 
Actual result: listColumn=\{1,2,8,4\}

So the question is why Code1 and Code2 give different results?
Looks like Code1 should give the same result as Code2.",,blerer,mkrupits_jb,slebresne,vvsh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 17:27:09 UTC 2017,,,,,,,,,,"0|i38tuv:",9223372036854775807,3.5,,,,,,slebresne,,slebresne,,,Low,,,,,,,,,,,,,,,,,,,"16/Feb/17 16:59;blerer;There is clearly a bug. The {{7}} should not appear.
At the same time you should be carefull with those type of queries. When you issue a single query with two updates, the two updates are  actually having the same timestamp which means that the update with the higher value will win. As if you had send 2 {{UPDATE}} queries with exactly the same timestamp.

So, {code}UPDATE t SET listColumn[2] = 8, listColumn[2] = 7  WHERE id = 1;{code} should also return {{listColumn=(1,2,8,4)}}.;;;","16/Feb/17 19:45;mkrupits_jb;{quote}When you issue a single query with two updates, the two updates are actually having the same timestamp which means that the update with the higher value will win.{quote}
Could you please clarify what does ""the higher value will win"" mean?
Does it mean that it's not defined which of two updates will be actually applied?;;;","17/Feb/17 12:08;blerer;bq. Could you please clarify what does ""the higher value will win"" mean?

Sorry, I meant the {{greater}} value win.

bq. Does it mean that it's not defined which of two updates will be actually applied?

It is clearly defined which value will win. It might not just be the one that you expect.

If you look at the following query: {{UPDATE t SET listColumn\[2\] = 8, listColumn\[2\] = 7  WHERE id = 1;}}
You might expect that the second value will win because it comes last.

In reality C* will consider that it has received 2 updates with exactly the same {{timestamp}}: {{UPDATE t SET listColumn\[2\] = 8  WHERE id = 1;}} and {{UPDATE t SET listColumn\[2\] = 7  WHERE id = 1;}} and will reconcile the data.

The data will be reconciled as follow:
# if one of the modifications is a deletion (tomstone), it wins and the data is marked as deleted
# if none of the modifications is a deletion, the update with the greater value win. If {{A > B}} then A win. If {{B > A}} then B win.

You will face the same issue with batches:
{code}
BEGIN BATCH
UPDATE t SET listColumn[2] = 8  WHERE id = 1;
UPDATE t SET listColumn[2] = 7  WHERE id = 1;
APPLY BATCH;
{code}
will also result in {{listColumn=\{1,2,8,4\}}}.
 
I you want the second update to always be the winner then you should send to separate updates.
;;;","17/Feb/17 15:27;mkrupits_jb;Thank you for the clarification!
Such behaviour looks like not intuitive - will be very useful to have the description somewhere in Cassandra's docs. 

1) Are there any plans to make the behaviour more intuitive like ""the second value will win because it comes last""?
2) How the following query will work (does an order matter?)?
{code}UPDATE t SET listColumn[2] = 8, listColumn = 7 + listColumn WHERE id = 1;{code}
3) Is there a general recommendation not to combine several updates to one column in the single query?;;;","20/Feb/17 09:38;blerer;bq. 1) Are there any plans to make the behaviour more intuitive like ""the second value will win because it comes last""?

Not for the moment. Feel free to open an improvement ticket if you want to (but make sure that you have looked at my answer to question 3))

bq.  How the following query will work (does an order matter?)?

I had to fix the behavior for this type of queries as part of this ticket patch.
As long as the 2 operations do not affect the same column, the results should be the same as if they were done in 2 separate statements.
If they affect the same column then the data will be reconciled using the rules that I previously mentioned .

For examples you can look [here|https://github.com/blerer/cassandra/blob/13130-3.0/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java#L911]. 

bq. 3) Is there a general recommendation not to combine several updates to one column in the single query?

Our recommendation is: *Do not do it*.
The output is difficult to predict and it is inefficient from the performance point of view.
Even if the output was predictable, the update would require unecessary transfert of data between the client and the server and unecessary computing. Due to that our advise would still be: *Do not do it* :-)  
     

 ;;;","20/Feb/17 09:50;blerer;||[2.2|https://github.com/apache/cassandra/compare/trunk...blerer:13130-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-2.2-dtest/]|
||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13130-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13130-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13130-trunk-dtest/]|

The patches fix 2 problems:
# For lists, the previous operations were not taken into account as the code was only looking at the prefetched list.
# In 3.0 and after, the reconciliation of the Cells was not performed correctly for complex columns;;;","09/Mar/17 15:51;slebresne;Sorry, it appears I missed that one and so it may require rebase, but +1 on the patches otherwise.;;;","10/Mar/17 20:45;blerer;Committed into 2.2 at 5ef8a8b408d4c492f7f2ffbbbe6fce237140c7cb and merged into 3.0, 3.11 and trunk;;;","13/Mar/17 17:27;mkrupits_jb;Benjamin, thank you for the clarifications.
Will try to follow your recommendations.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate rows after upgrading from 2.1.16 to 3.0.10/3.9,CASSANDRA-13125,13035206,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,zzheng,zzheng,16/Jan/17 12:29,16/Apr/19 09:30,14/Jul/23 05:56,08/Feb/17 10:43,3.0.11,3.11.0,,,,,,,,,,2,,,,,"I found that rows are splitting and duplicated after upgrading the cluster from 2.1.x to 3.0.x.

I found the way to reproduce the problem as below.
{code}
$ ccm create test -v 2.1.16 -n 3 -s                                                                               
Current cluster is now: test
$ ccm node1 cqlsh  -e ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':3}""
$ ccm node1 cqlsh -e ""CREATE TABLE test.test (id text PRIMARY KEY, value1 set<text>, value2 set<text>);""

# Upgrade node1
$ for i in 1; do ccm node${i} stop; ccm node${i} setdir -v3.0.10; ccm node${i} start;ccm node${i} nodetool upgradesstables; done

# Insert a row through node1(3.0.10)
$ ccm node1 cqlsh -e ""INSERT INTO test.test (id, value1, value2) values ('aaa', {'aaa', 'bbb'}, {'ccc', 'ddd'});""                       

# Insert a row through node2(2.1.16)
$ ccm node2 cqlsh -e ""INSERT INTO test.test (id, value1, value2) values ('bbb', {'aaa', 'bbb'}, {'ccc', 'ddd'});"" 

# The row inserted from node1 is splitting
$ ccm node1 cqlsh -e ""SELECT * FROM test.test ;""

 id  | value1         | value2
-----+----------------+----------------
 aaa |           null |           null
 aaa | {'aaa', 'bbb'} | {'ccc', 'ddd'}
 bbb | {'aaa', 'bbb'} | {'ccc', 'ddd'}

$ for i in 1 2; do ccm node${i} nodetool flush; done

# Results of sstable2json of node2. The row inserted from node1(3.0.10) is different from the row inserted from node2(2.1.16).
$ ccm node2 json -k test -c test
running
['/home/zzheng/.ccm/test/node2/data0/test/test-5406ee80dbdb11e6a175f57c4c7c85f3/test-test-ka-1-Data.db']
-- test-test-ka-1-Data.db -----
[
{""key"": ""aaa"",
 ""cells"": [["""","""",1484564624769577],
           [""value1"",""value2:!"",1484564624769576,""t"",1484564624],
           [""value1:616161"","""",1484564624769577],
           [""value1:626262"","""",1484564624769577],
           [""value2:636363"","""",1484564624769577],
           [""value2:646464"","""",1484564624769577]]},
{""key"": ""bbb"",
 ""cells"": [["""","""",1484564634508029],
           [""value1:_"",""value1:!"",1484564634508028,""t"",1484564634],
           [""value1:616161"","""",1484564634508029],
           [""value1:626262"","""",1484564634508029],
           [""value2:_"",""value2:!"",1484564634508028,""t"",1484564634],
           [""value2:636363"","""",1484564634508029],
           [""value2:646464"","""",1484564634508029]]}
]

# Upgrade node2,3
$ for i in `seq 2 3`; do ccm node${i} stop; ccm node${i} setdir -v3.0.10; ccm node${i} start;ccm node${i} nodetool upgradesstables; done

# After upgrade node2,3, the row inserted from node1 is splitting in node2,3
$ ccm node2 cqlsh -e ""SELECT * FROM test.test ;""                                                                                        

 id  | value1         | value2
-----+----------------+----------------
 aaa |           null |           null
 aaa | {'aaa', 'bbb'} | {'ccc', 'ddd'}
 bbb | {'aaa', 'bbb'} | {'ccc', 'ddd'}

(3 rows)

# Results of sstabledump
# node1
[
  {
    ""partition"" : {
      ""key"" : [ ""aaa"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 17,
        ""liveness_info"" : { ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" },
        ""cells"" : [
          { ""name"" : ""value1"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:44.769576Z"", ""local_delete_time"" : ""2017-01-16T11:03:44Z"" } },
          { ""name"" : ""value1"", ""path"" : [ ""aaa"" ], ""value"" : """" },
          { ""name"" : ""value1"", ""path"" : [ ""bbb"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:44.769576Z"", ""local_delete_time"" : ""2017-01-16T11:03:44Z"" } },
          { ""name"" : ""value2"", ""path"" : [ ""ccc"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""path"" : [ ""ddd"" ], ""value"" : """" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""bbb"" ],
      ""position"" : 48
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 65,
        ""liveness_info"" : { ""tstamp"" : ""2017-01-16T11:03:54.508029Z"" },
        ""cells"" : [
          { ""name"" : ""value1"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:54.508028Z"", ""local_delete_time"" : ""2017-01-16T11:03:54Z"" } },
          { ""name"" : ""value1"", ""path"" : [ ""aaa"" ], ""value"" : """" },
          { ""name"" : ""value1"", ""path"" : [ ""bbb"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:54.508028Z"", ""local_delete_time"" : ""2017-01-16T11:03:54Z"" } },
          { ""name"" : ""value2"", ""path"" : [ ""ccc"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""path"" : [ ""ddd"" ], ""value"" : """" }
        ]
      }
    ]
  }
]                                                                                                                                                    

# node2
[
  {
    ""partition"" : {
      ""key"" : [ ""aaa"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 17,
        ""liveness_info"" : { ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" },
        ""cells"" : [ ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 22,
        ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:44.769576Z"", ""local_delete_time"" : ""2017-01-16T11:03:44Z"" },
        ""cells"" : [
          { ""name"" : ""value1"", ""path"" : [ ""aaa"" ], ""value"" : """", ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" },
          { ""name"" : ""value1"", ""path"" : [ ""bbb"" ], ""value"" : """", ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" },
          { ""name"" : ""value2"", ""path"" : [ ""ccc"" ], ""value"" : """", ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" },
          { ""name"" : ""value2"", ""path"" : [ ""ddd"" ], ""value"" : """", ""tstamp"" : ""2017-01-16T11:03:44.769577Z"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""bbb"" ],
      ""position"" : 57
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 74,
        ""liveness_info"" : { ""tstamp"" : ""2017-01-16T11:03:54.508029Z"" },
        ""cells"" : [
          { ""name"" : ""value1"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:54.508028Z"", ""local_delete_time"" : ""2017-01-16T11:03:54Z"" } },
          { ""name"" : ""value1"", ""path"" : [ ""aaa"" ], ""value"" : """" },
          { ""name"" : ""value1"", ""path"" : [ ""bbb"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""deletion_info"" : { ""marked_deleted"" : ""2017-01-16T11:03:54.508028Z"", ""local_delete_time"" : ""2017-01-16T11:03:54Z"" } },
          { ""name"" : ""value2"", ""path"" : [ ""ccc"" ], ""value"" : """" },
          { ""name"" : ""value2"", ""path"" : [ ""ddd"" ], ""value"" : """" }
        ]
      }
    ]
  }
]
{code}

Another example of row splitting is as follows.
{code}
$ ccm create test2 -v 2.1.16 -n 3 -s                                                                                                    
Current cluster is now: test2
$ ccm node1 cqlsh  -e ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':3}""                      
$ ccm node1 cqlsh -e ""CREATE TABLE test.text_set_set (id text PRIMARY KEY, value1 text, value2 set<text>, value3 set<text>);""           
$ for i in `seq 1`; do ccm node${i} stop; ccm node${i} setdir -v3.0.10; ccm node${i} start;ccm node${i} nodetool upgradesstables; done  
$ ccm node1 cqlsh -e ""INSERT INTO test.text_set_set (id, value1, value2, value3) values ('aaa', 'aaa', {'aaa', 'bbb'}, {'ccc', 'ddd'});""
$ ccm node1 cqlsh -e ""SELECT * FROM test.text_set_set;""                                                                                 

 id  | value1 | value2         | value3
-----+--------+----------------+----------------
 aaa |    aaa |           null |           null
 aaa |   null | {'aaa', 'bbb'} | {'ccc', 'ddd'}

(2 rows)
{code}

As far as I investigated, the occurrence conditions are as follows.
* Table schema contains multiple collections.
* Insert a row, which values of the collection column are not null through 3.x node while both 2.1 and 3.x nodes exist in a cluster.
* Rows in sstables of node which version was 2.1 at the time the row was inserted is splitting after upgrading to 3.x.

Thanks.",,christianmovi,jeromatron,jjirsa,KurtG,mnantern,mshuler,sashley,slebresne,t2y,thobbs,Yasuharu,ytakata,zzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12144,CASSANDRA-11887,,,,,,,,,"19/Jan/17 01:55;Yasuharu;diff-a.patch;https://issues.apache.org/jira/secure/attachment/12848197/diff-a.patch","19/Jan/17 01:56;Yasuharu;diff-b.patch;https://issues.apache.org/jira/secure/attachment/12848198/diff-b.patch",,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 03:16:06 UTC 2017,,,,,,,,,,"0|i38rdz:",9223372036854775807,3.0.10,3.9,,,,,thobbs,,thobbs,,,Critical,,,,,,,,,,,,,,,,,,,"19/Jan/17 01:55;Yasuharu;h2. Investigations...

After some debugging, I found interesting difference in serialized RangeTombstoneLists between 2.1.16 and 3.0.10.

- I ran 3 Cassandra nodes with some debug prints.
-- 127.0.0.1 (C* 3.0.10)
-- 127.0.0.2 (C* 2.1.16)
-- 127.0.0.3 (C* 2.1.16)
- They have a keyspace and a table already created.
-- CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}
-- CREATE TABLE test.test ( a int PRIMARY KEY, b int, c set<int>, d set<int>, e int )
- And I query a same INSERT (which mutation is sent to 127.0.0.2) query from 127.0.0.1(C*3.0) and 127.0.0.3(C*2.1) and see the difference.

Insert a row from 127.0.0.1 and scan. ( inserted (a=14) row is broken)
{code:sql}
cqlsh> insert into test.test(a,b,c,d,e) values(14,1,{2,3},{4,5},6);
cqlsh> select * from test.test;

 a  | b    | c      | d      | e
----+------+--------+--------+------
 14 |    1 |   null |   null | null
 14 | null | {2, 3} | {4, 5} |    6

(2 rows)
{code}

And then, I insert from 127.0.0.3 and scan.  (neither a=5 nor a=14 are broken)
{code:sql}
cqlsh> insert into test.test(a,b,c,d,e)values(5,1,{2,3},{4,5},6);
cqlsh> select * from test.test;

 a  | b | c      | d      | e
----+---+--------+--------+---
  5 | 1 | {2, 3} | {4, 5} | 6
 14 | 1 | {2, 3} | {4, 5} | 6
{code}

And back to 127.0.0.1 and scan the table. a=14 is broken but a=5 is not.
{code:sql}
cqlsh> select * from test.test;

 a  | b    | c      | d      | e
----+------+--------+--------+------
  5 |    1 | {2, 3} | {4, 5} |    6
 14 |    1 |   null |   null | null
 14 | null | {2, 3} | {4, 5} |    6
{code}

Therefore,It looks like that ""C*3 can't scan properly rows that is stored in C*2 but inserted from C*3."";

Next, I observed some incoming MUTATIONs in 127.0.0.2 like below. I saw that C*3.0 sent RangeTombstones like {{[c-c],[c-d]}}, but C*2.1 sent {{[c:_-c],[d:_-d]}}.

{noformat}
> insert into test.test(a,b,c,d,e) values(14,1,{2,3},{4,5},6); from 127.0.0.1

DeletionInfo:{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484710273390930, localDeletion=1484710273][c-d:!, deletedAt=1484710273390930, localDeletion=1484710273]}
from:/127.0.0.1, payload:Mutation(keyspace='test', key='0000000e', modifications=[ColumnFamily(test -{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484710273390930, localDeletion=1484710273][c-d:!, deletedAt=1484710273390930, localDeletion=1484710273]}- [:false:0@1484710273390931,b:false:4@1484710273390931,c:00000002:false:0@1484710273390931,c:00000003:false:0@1484710273390931,d:00000004:false:0@1484710273390931,d:00000005:false:0@1484710273390931,e:false:4@1484710273390931,])]), verb:MUTATION, version:8

> insert into test.test(a,b,c,d,e) values(14,1,{2,3},{4,5},6); from 127.0.0.3
DeletionInfo:{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c:_-c:!, deletedAt=1484710277987556, localDeletion=1484710277][d:_-d:!, deletedAt=1484710277987556, localDeletion=1484710277]}
from:/127.0.0.3, payload:Mutation(keyspace='test', key='0000000e', modifications=[ColumnFamily(test -{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c:_-c:!, deletedAt=1484710277987556, localDeletion=1484710277][d:_-d:!, deletedAt=1484710277987556, localDeletion=1484710277]}- [:false:0@1484710277987557,b:false:4@1484710277987557,c:00000002:false:0@1484710277987557,c:00000003:false:0@1484710277987557,d:00000004:false:0@1484710277987557,d:00000005:false:0@1484710277987557,e:false:4@1484710277987557,])]), verb:MUTATION, version:8
{noformat}

h2. Workaround Plan-A

But, LegacyRangeTombstone remove {{collectionName}} from RangeTombStone which start.bound != end.bound like {{[c-d]}}
https://github.com/apache/cassandra/blob/cassandra-3.0.10/src/java/org/apache/cassandra/db/LegacyLayout.java#L1592-L1599
It seems like that this deletions of collectionName corrupt the unmarshal of legacy tombstone. After I commentized these else-if block, I could scan the table correctly.


{code:java}
            if ((start.collectionName == null) != (stop.collectionName == null))
            {
                if (start.collectionName == null)
                    stop = new LegacyBound(stop.bound, stop.isStatic, null);
                else
                    start = new LegacyBound(start.bound, start.isStatic, null);
            }
            /*else if (!Objects.equals(start.collectionName, stop.collectionName))
            {
                // We're in the similar but slightly more complex case where on top of the big tombstone
                // A, we have 2 (or more) collection tombstones B and C within A. So we also end up with
                // a tombstone that goes between the end of B and the start of C.
                start = new LegacyBound(start.bound, start.isStatic, null);
                stop = new LegacyBound(stop.bound, stop.isStatic, null);
            }
            */
{code}

{noformat}
cqlsh> select * from test.test;

 a  | b | c      | d      | e
----+---+--------+--------+---
  5 | 1 | {2, 3} | {4, 5} | 6
 14 | 1 | {2, 3} | {4, 5} | 6
{noformat}

see patch diff-a.patch.

h2. Workaround Plan-B
Instead of modify the LegacyLayout unmarshal code, commentizing the following line fixed the problem too. It changes the TombStoneRange which is serialized  by LegacyLayout from {{[c-c][c-d]}} to {{[c-c][d-d]}}.

https://github.com/apache/cassandra/blob/cassandra-3.0.10/src/java/org/apache/cassandra/db/LegacyLayout.java#L2099
{code:java}
//                     start = ends[i];
{code}


{noformat}

DeletionInfo:{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484715120458008, localDeletion=1484715120][d-d:!, deletedAt=1484715120458008, localDeletion=1484715120]}
from:/127.0.0.1, payload:Mutation(keyspace='test', key='0000000e', modifications=[ColumnFamily(test -{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484715120458008, localDeletion=1484715120][d-d:!, deletedAt=1484715120458008, localDeletion=1484715120]}- [:false:0@1484715120458009,b:false:4@1484715120458009,c:00000002:false:0@1484715120458009,c:00000003:false:0@1484715120458009,d:00000004:false:0@1484715120458009,d:00000005:false:0@1484715120458009,e:false:4@1484715120458009,])]), verb:MUTATION, version:8
{noformat}

see patch diff-b.patch.

I'm not sure if my solution cause any unexpected effects. But I attach my patches for reference.
Could anyboody please review my patch?
;;;","19/Jan/17 01:55;Yasuharu;A patch for Plan-A on Cassandra 3.0.10;;;","19/Jan/17 01:56;Yasuharu;A patch for Plan-B on Cassandra-3.0.10.;;;","19/Jan/17 01:57;Yasuharu;I've submitted a brief patch for reference.;;;","20/Jan/17 14:31;slebresne;Thanks a lot for the very detailed reproduction case and analysis. You are absolutely right that it's abnormal for 3.0 to transform a range like {{\[c-c!\]\[d-d!\]}} into {{\[c-c!\]\[c-d!\]}}. And having duplicate row entries is only one of the problem this triggers, as with more collections you could this ending up to delete data that it shouldn't. In any case, the problem is definitively in 3.0 generating this in the first place, so that's what we should fix, so fixing this in LegacyRangeTombstone constructor (Plan-A above) is too late.

So the problem is when building the {{LegacyRangeTombstonList}} on 3.0, though it's not in the {{insertFrom}} method and the Plan-B patch would likely create other problems. The problem is in the {{LegacyBoundComparator}}: it delegates to {{clusteringComparator.compare()}} before it even checks if the bounds it compares have a {{collectionName}}, but that's incorrect.

Anyway, attaching below a fix for this as well as a simple unit test showing
where the problem lies:
| [13125-3.0|https://github.com/pcmanus/cassandra/commits/13125-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13125-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13125-3.0-dtest] |
| [13125-3.11|https://github.com/pcmanus/cassandra/commits/13125-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13125-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13125-3.11-dtest] |

[~Yasuharu], if you could check if this patch does properly work in your case, that would be much appreciated. [~thobbs], since you're familiar with 3.0 compatibility code, would you also mind having a quick look at the patch?;;;","20/Jan/17 17:48;Yasuharu;Thank you [~slebresne]! I checked that your patches worked properly in my reproduce procedure. The result is below. Now I could see that C*3.0 generated {{[c-c!][d-d!]}} style range tombstones and the rows are not broken!

h4. On 13125-3.0
{noformat}
cqlsh> insert into test.test(a,b,c,d,e) values(14,1,{2,3},{4,5},6);
cqlsh> select * from test.test;

 a  | b | c      | d      | e
----+---+--------+--------+---
 14 | 1 | {2, 3} | {4, 5} | 6

RangeTombstone(0), start:org.apache.cassandra.db.composites.CompoundSparseCellName@78e3b54a, end:org.apache.cassandra.db.composites.BoundedComposite@6b517b57, markedAt:1484931972134334, delTime:1484931972
RangeTombstone(1), start:org.apache.cassandra.db.composites.CompoundSparseCellName@78e3b54b, end:org.apache.cassandra.db.composites.BoundedComposite@6b517b58, markedAt:1484931972134334, delTime:1484931972
DeletionInfo:{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484931972134334, localDeletion=1484931972][d-d:!, deletedAt=1484931972134334, localDeletion=1484931972]}
from:/127.0.0.1, payload:Mutation(keyspace='test', key='0000000e', modifications=[ColumnFamily(test -{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484931972134334, localDeletion=1484931972][d-d:!, deletedAt=1484931972134334, localDeletion=1484931972]}- [:false:0@1484931972134335,b:false:4@1484931972134335,c:00000002:false:0@1484931972134335,c:00000003:false:0@1484931972134335,d:00000004:false:0@1484931972134335,d:00000005:false:0@1484931972134335,e:false:4@1484931972134335,])]), verb:MUTATION, version:8
{noformat}

h4. On 13125-3.11
{noformat}
cqlsh> insert into test.test(a,b,c,d,e) values(14,1,{2,3},{4,5},6);
cqlsh> select * from test.test;                               
 a  | b | c      | d      | e
----+---+--------+--------+---
 14 | 1 | {2, 3} | {4, 5} | 6

Mutation.deserialize() size==1
RangeTombstone(0), start:org.apache.cassandra.db.composites.CompoundSparseCellName@4316af5d, end:org.apache.cassandra.db.composites.BoundedComposite@256e93b0, markedAt:1484933162431359, delTime:1484933162
RangeTombstone(1), start:org.apache.cassandra.db.composites.CompoundSparseCellName@4316af5e, end:org.apache.cassandra.db.composites.BoundedComposite@256e93b1, markedAt:1484933162431359, delTime:1484933162
DeletionInfo:{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484933162431359, localDeletion=1484933162][d-d:!, deletedAt=1484933162431359, localDeletion=1484933162]}
from:/127.0.0.1, payload:Mutation(keyspace='test', key='0000000e', modifications=[ColumnFamily(test -{deletedAt=-9223372036854775808, localDeletion=2147483647, ranges=[c-c:!, deletedAt=1484933162431359, localDeletion=1484933162][d-d:!, deletedAt=1484933162431359, localDeletion=1484933162]}- [:false:0@1484933162431360,b:false:4@1484933162431360,c:00000002:false:0@1484933162431360,c:00000003:false:0@1484933162431360,d:00000004:false:0@1484933162431360,d:00000005:false:0@1484933162431360,e:false:4@1484933162431360,])]), verb:MUTATION, version:8
{noformat}

;;;","20/Jan/17 20:51;thobbs;[~slebresne] the patch and new test look good to me.  For some reason related to sigar your new unit test is erroring on Jenkins in the 3.11 version -- maybe [~mshuler] knows what's up with that?;;;","20/Jan/17 21:12;mshuler;Would this have something to do with it?
{noformat}
(cassandra-3.11)mshuler@hana:~/git/cassandra$ grep -r sigar-bin conf/
conf/cassandra-env.ps1:    $env:JVM_OPTS = ""$env:JVM_OPTS -Djava.library.path=""""$env:CASSANDRA_HOME\lib\sigar-bin""""""
conf/cassandra-env.sh:JVM_OPTS=""$JVM_OPTS -Djava.library.path=$CASSANDRA_HOME/lib/sigar-bin""

(cassandra-3.11)mshuler@hana:~/git/cassandra$ grep -r sigar-bin test/conf/
(cassandra-3.11)mshuler@hana:~/git/cassandra$
{noformat};;;","23/Jan/17 10:06;slebresne;bq. For some reason related to sigar your new unit test is erroring on Jenkins in the 3.11 version

The sigar stack is unrelated noise: it's not being properly set by Jenkins, but that shouldn't impact any test, and while it could be nice to fix it, let's please leave that to some other venue.

The failure seems to be due to the fact that the test don't initialize {{DatabaseDescriptor}} and so {{CFMetaData.Builder.build()}} complains about not knowing the partitioner. Pushed a trivial fix that forces a partitioner (we could also force daemon initialization I suppose but no point is making the test more heavy than it has to be). Restarted the unit test on 3.11 to check it fixes it.
;;;","24/Jan/17 17:22;thobbs;The test fix and latest test run look good, so +1 on committing.;;;","08/Feb/17 10:43;slebresne;Committed, thanks.;;;","08/Feb/17 10:53;Yasuharu;Thank you guys! :);;;","09/Feb/17 01:03;zzheng;Thank you!;;;","21/Feb/17 16:28;mnantern;Thank you for the fix !

Just a small question, how can I repair my table now ? I have rows with null because of that issue and I cannot delete them:

(primary_id and secondary_id are my primary key)

{noformat}
select primary_id,secondary_id, access_rights, date_modification from access WHERE primary_id = 92a9a568-e585-45cd-a1f6-96376809098c;

 primary_id                           | secondary_id                         | access_rights   | date_modification
--------------------------------------+--------------------------------------+-----------------+--------------------------
 92a9a568-e585-45cd-a3f6-96376819098c | 078ae733-5be5-402e-b9b2-5f989550787e | ['b', 'u', 'r'] | 2017-02-20 12:35:02+0000
 92a9a568-e585-45cd-a3f6-96376819098c | 078ae733-5be5-402e-b9b2-5f989550787e |            null | 2017-02-20 12:35:02+0000
{noformat};;;","22/Feb/17 03:16;Yasuharu;[~mnantern] In our case, {{nodetool scrub}} (C* 3.0.9 or later) fixed our broken sstables.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abort or retry on failed hints delivery,CASSANDRA-13124,13035067,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,spod,pauloricardomg,pauloricardomg,16/Jan/17 01:03,16/Apr/19 09:30,14/Jul/23 05:56,06/Feb/17 20:04,3.0.11,,,,,,,,,,,0,hintedhandoff,,,,"CASSANDRA-12905 changed the hints path to be asynchronous in the normal case, but when the hint is non-local and should be stored (ie. on decommission) an ack is not sent so the operation does not complete successfully.

CASSANDRA-13058 fixed this for 3.0+, but for 3.11+ but there is an additional concern which on 3.0 is that non-acked hints are being completed successfully so we should probably look into this as well here.",,colinkuo,jeromatron,pauloricardomg,spod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 20:04:46 UTC 2017,,,,,,,,,,"0|i38qj3:",9223372036854775807,,,,,,,pauloricardomg,,pauloricardomg,,,Low,,,,,,,,,,,,,,,,,,,"16/Jan/17 01:24;pauloricardomg;continue discussion from CASSANDRA-13058 (cc [~spodxx@gmail.com]):

bq. and terminate with a successful return value afterwards

Does this mean retry is broken on 3.0? Should we file a ticket for this? From what I understood CASSANDRA-11960 only affected 3.X so I guess retry should be working on 3.0?

If my understanding above is correct we should probably also fix this on 3.0 to reinstate the same behavior before CASSANDRA-12905? Can you create a 3.0 patch and submit CI?

bq. That being said, it's less than ideal that dtests annotated with ""@no_vnodes"" are simply skipped during ad-hoc job scheduling during reviews.

Agreed, created the following [issue|https://github.com/riptano/cassandra-dtest/issues/1422] on cassandra-dtest to address this, feel welcome to give feedback there.;;;","17/Jan/17 10:48;spod;

bq. Does this mean retry is broken on 3.0? Should we file a ticket for this? From what I understood CASSANDRA-11960 only affected 3.X so I guess retry should be working on 3.0?

You can just test it by commenting out all code in {{HintVerbHandler}}. Afterwards the {{hintedhandoff_decom_test}} will fail due to data loss (that was supposed to be prevented by the transferred hints). Although no hints have been handled at all, decom will happily proceed and there won't be any exceptions or errors in the logs.

bq. If my understanding above is correct we should probably also fix this on 3.0 to reinstate the same behavior before CASSANDRA-12905? Can you create a 3.0 patch and submit CI?

This plus removing the retry loop in the dispatcher, due to the broken behaviour described in [my comment|https://issues.apache.org/jira/browse/CASSANDRA-13058?focusedCommentId=15818371&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15818371], which is what has been done for CASSANDRA-11960 as well. Afterwards retrying hints on page basis should happen just once as initiated by {{TransferHintsTask}} and periodically for regular hinting, executed by the HintsDispatcher executor. 

||3.0||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13124-3.0]|
|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13124-3.0-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13124-3.0-testall/]|

;;;","06/Feb/17 20:04;pauloricardomg;LGTM, thanks! Committed as dab0e31ba0294ccc5e8af35cbbe0a6733f35794e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Draining a node might fail to delete all inactive commitlogs,CASSANDRA-13123,13034973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,wulczer,wulczer,wulczer,14/Jan/17 21:28,15/May/20 08:05,14/Jul/23 05:56,26/Oct/17 20:43,3.0.16,3.11.2,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,,,,,0,,,,,"After issuing a drain command, it's possible that not all of the inactive commitlogs are removed.

The drain command shuts down the CommitLog instance, which in turn shuts down the CommitLogSegmentManager. This has the effect of discarding any pending management tasks it might have, like the removal of inactive commitlogs.

This in turn leads to an excessive amount of commitlogs being left behind after a drain and a lengthy recovery after a restart. With a fleet of dozens of nodes, each of them leaving several GB of commitlogs after a drain and taking up to two minutes to recover them on restart, the additional time required to restart the entire fleet becomes noticeable.

This problem is not present in 3.x or trunk because of the CLSM rewrite done in CASSANDRA-8844.",,aweisberg,bdeggleston,jasobrown,jeromatron,jjirsa,jjordan,JoshuaMcKenzie,wulczer,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/17 21:32;wulczer;13123-2.2.8.txt;https://issues.apache.org/jira/secure/attachment/12847511/13123-2.2.8.txt","14/Jan/17 21:32;wulczer;13123-3.0.10.txt;https://issues.apache.org/jira/secure/attachment/12847512/13123-3.0.10.txt","14/Jan/17 21:32;wulczer;13123-3.9.txt;https://issues.apache.org/jira/secure/attachment/12847513/13123-3.9.txt","14/Jan/17 21:32;wulczer;13123-trunk.txt;https://issues.apache.org/jira/secure/attachment/12847514/13123-trunk.txt",,,,,,,,,,,,,,,,4.0,wulczer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 27 07:49:48 UTC 2017,,,,,,,,,,"0|i38py7:",9223372036854775807,2.2.8,3.0.10,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"14/Jan/17 21:32;wulczer;Patches for 2.2.8 and 3.0.10 and test-only patches for 3.9 and trunk;;;","19/Jan/17 12:56;jasobrown;[~wulczer] Thanks for the patch. We are at the critical bug fix stage with 2.2, so I'll only look at the patch for 3.0 and up. I've taken a quick look and things seem legit (need to think about it a bit more), but can you comment on any startup improvement time you've observed, if you've deployed this?

Also, when you are issuing a drain? On normal node restarts, or only at ""special"" events, like upgrading a node?;;;","19/Jan/17 17:03;wulczer;[~jasobrown] I haven't had the chance to try this out in production yet, I'll try to do that tomorrow. The initial commitlog replay takes up to two minutes for each of our nodes right now and if I understand correctly, after a drain all commitlogs except for at most two would be deleted, so the initial replay phase would be reduced to essentially zero. The shutdown phase might take a bit longer, because it'll have to wait for those commitlogs to be deleted, of course.

The exact improvement depends on the number of CLs left behind after a drain - on machines with heavily contended disks it can be a lot, on lightly loaded ones it might be 0.

As to when we're doing drains, it's on every restart (it's part of the restart procedure that we have).;;;","20/Jan/17 15:05;jasobrown;Pushed code for cassci to run tests. I think the change to {{CommitLogSegmentManager}} is probably legit, but I want to look at the test a little more before +1'ing it

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13123-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13123-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13123-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-3.11-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-13123-trunk-testall/]|
;;;","24/Jan/17 12:46;wulczer;I tested this patch on a 3.7 test cluster I have running with the following procedure:

 * check initial commitlog dir size and file count
 * run nodetool disablegossip
 * run nodetool drain, measure the time it took
 * send SIGTERM to Cassandra, wait for it to exit
 * check post-stop commitlog dir size and file count
 * start Cassandra
 * read logs to get time it took to go through commitlog replay

With vanilla 3.7 the drain took around 25 seconds and when starting back up commitlog replay took around 40 seconds. Pre-shutdown CL dir size was 3.5G and 110 files. Post-shutdown CL dir size was almost identical.

With patched 3.7 the drain also too 25 seconds, but commitlog replay took essentially 0 seconds. Pre-shutdown CL dir size was similar, but post-shutdown there was just one file left.;;;","10/Apr/17 02:06;zznate;Ping [~jasobrown] [~wulczer] Are we good to commit on this then? ;;;","10/Apr/17 08:14;wulczer;We've been running it for weeks with no problems, so +1 from me.;;;","12/Sep/17 20:09;jasobrown;Sorry this fell off my review radar (more than) a few months ago. For the last month, however, I've been trying to run this patch, rebased on 3.0/3.11/trunk, on circleci and the results have almost always been broken (in ways seemingly unrelated to this ticket). I've run it locally and everything seemed legit, and I've now run the utests on the apache jenkins server, and things were good (a few completely unrelated things failed);

||3.0||3.11||trunk||
|[apache utest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/9/]|[apache utest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/10/]|[apache utest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/11/]|

Running the [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/304/] now (only for 3.0), and if it looks good I'll commit.;;;","13/Sep/17 13:37;jasobrown;committed to 3.0 and up as sha {{d826c81874e5d13a30a418f8b982531cb7e5d158}}.

Thanks for the patch, and sorry for the delay!;;;","25/Sep/17 10:19;wulczer;No worries, thanks for the commit!;;;","29/Sep/17 23:50;jjirsa;Hi folks,

Pretty sure this commit breaks {{CommitLogSegmentManagerTest}} - have seen a pretty sharp rise in failures, and reverting this commit seems to solve them.

{code}
    [junit] INFO  23:34:52 Initializing CommitLogTest.Standard1
    [junit] INFO  23:34:52 Initializing CommitLogTest.Standard2
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testShutdownWithPendingTasks(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:105)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerTest.testShutdownWithPendingTasks(CommitLogSegmentManagerTest.java:147)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$10.evaluate(BMUnitRunner.java:371)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$6.evaluate(BMUnitRunner.java:241)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$1.evaluate(BMUnitRunner.java:75)
    [junit]
    [junit]
    [junit] Testcase: testCompressedCommitLogBackpressure(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerTest):	FAILED
    [junit] expected:<3> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<3> but was:<1>
    [junit] 	at org.apache.cassandra.Util.spinAssertEquals(Util.java:535)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerTest.testCompressedCommitLogBackpressure(CommitLogSegmentManagerTest.java:112)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$9.evaluate(BMUnitRunner.java:342)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$6.evaluate(BMUnitRunner.java:241)
    [junit] 	at org.jboss.byteman.contrib.bmunit.BMUnitRunner$1.evaluate(BMUnitRunner.java:75)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.commitlog.CommitLogSegmentManagerTest FAILED
   [delete] Deleting directory /Users/jjirsa/Desktop/Dev/cassandra/build/test/cassandra/commitlog:0
   [delete] Deleting directory /Users/jjirsa/Desktop/Dev/cassandra/build/test/cassandra/data:0
   [delete] Deleting directory /Users/jjirsa/Desktop/Dev/cassandra/build/test/cassandra/saved_caches:0
[junitreport] Processing /Users/jjirsa/Desktop/Dev/cassandra/build/test/TESTS-TestSuites.xml to /var/folders/nq/4w83hn7s3h13dc5wxmvcdn9w0000gn/T/null1048031913
[junitreport] Loading stylesheet jar:file:/usr/local/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 256ms
[junitreport] Deleting: /var/folders/nq/4w83hn7s3h13dc5wxmvcdn9w0000gn/T/null1048031913
{code}
;;;","30/Sep/17 10:49;wulczer;Ugh, I'll take a look as soon as I can, thanks for the heads up. That's on master, right?;;;","30/Sep/17 15:59;jjirsa;I noticed it on 3.0 branch, I haven't had time to investigate but I suspect it may be a test ordering issue (if the two tests are run in one order they pass, in the other they fail, so probably setup/teardown conditions).

The first failure I see in cassci (datastax's CI environment, which I don't have access to other than the public read-only view) is http://cassci.datastax.com/job/cassandra-3.0_testall/954/ , which is the build after this change was committed ( http://cassci.datastax.com/job/cassandra-3.0_testall/953/ ) .

It also fails in:
http://cassci.datastax.com/job/cassandra-3.0_testall/964/
http://cassci.datastax.com/job/cassandra-3.0_testall/963/
http://cassci.datastax.com/job/cassandra-3.0_testall/956/

So % wise, it seems like 4 failures in the 15 builds since introduction.



;;;","03/Oct/17 10:29;JoshuaMcKenzie;bq.  suspect it may be a test ordering issue (if the two tests are run in one order they pass, in the other they fail, so probably setup/teardown conditions).
The brittleness of CL startup/teardown in unit testing was a pretty significant pain in the ass when I was working on CDC. Stupp and I have both bumped up against that in the memorable recent past and tidied things up a bit, but I suspect it will require a more invasive re-arch of the segment allocation and CL startup/shutdown to get it really ironed out.;;;","20/Oct/17 22:17;aweisberg;The new test that was added doesn't set up it's schema at all. It relies on the schema created during the previous test. Easy fix there.

The original test (before the new one was added) configures DatabaseDescriptor before loading the commit log (after which it can't be reloaded). The new test loads the commit log before this configuration occurs causing the original test to fail because the commit log configuration is wrong.

If you move the configuration into @BeforeClass the backpressure that is supposed to be in the original test doesn't occur and the test fails. No fragile test goes unpunished I guess. I think the might be an interaction with Byteman that I don't understand.;;;","26/Oct/17 18:06;bdeggleston;I don't think these 2 tests can be in the same test class without being run in a specific order. {{testCompressedCommitLogBackpressure}} needs it's byteman rules setup before the commit log is started. So if {{testShutdownWithPendingTasks}} sets up it's schema and successfully runs first, the other will hang.

I have a branch where each test is in it's own class [here|https://github.com/bdeggleston/cassandra/tree/13123-fix-3.0], let me know if there are any objections;;;","26/Oct/17 18:10;jjirsa;+1 (on splitting, and on your patch)


;;;","26/Oct/17 20:43;bdeggleston;committed as {{8fad4cd59ea5ef6d111cfe67d9a4c0345c4d7fd7}};;;","27/Oct/17 07:49;wulczer;I just wanted to say thanks for fixing that test (aka cleaning up my mess). I was squinting at this for a while, but could not figure out the byteman/commitlog init interaction...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair progress message breaks legacy JMX support,CASSANDRA-13121,13034761,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ptbannister,scottbale,scottbale,13/Jan/17 18:03,01/Aug/21 12:26,14/Jul/23 05:56,14/May/18 09:21,3.0.17,3.11.5,,,,,Legacy/Streaming and Messaging,,,,,0,jmx,lhf,notifications,repair,"The error progress message in {{RepairRunnable}} is not compliant with the {{LegacyJMXProgressSupport}} class, which uses a regex to match on the text of a progress event. Therefore, actual failures slip through as successes if using legacy JMX for repairs.

In {{RepairRunnable}}
{code}
    protected void fireErrorAndComplete(String tag, int progressCount, int totalProgress, String message)
    {
        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, message));
        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, String.format(""Repair command #%d finished with error"", cmd)));
    }
{code}
Note the {{""Repair command #%d finished with error""}}
See https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/RepairRunnable.java#L109

In {{LegacyJMXProgressSupport}}:
{code}
    protected static final Pattern SESSION_FAILED_MATCHER = Pattern.compile(""Repair session .* for range .* failed with error .*"");
    protected static final Pattern SESSION_SUCCESS_MATCHER = Pattern.compile(""Repair session .* for range .* finished"");
{code}
See https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/progress/jmx/LegacyJMXProgressSupport.java#L38

Legacy JMX support was introduced for CASSANDRA-11430 (version 2.2.6) and the bug was introduced as part of CASSANDRA-12279 (version 2.2.8).",,githubbot,mck,mr_pathak,pauloricardomg,ptbannister,scottbale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11530,,,CASSANDRA-11430,CASSANDRA-8901,,CASSANDRA-12279,,,,,,,"21/Mar/18 03:08;ptbannister;13121-3.0.txt;https://issues.apache.org/jira/secure/attachment/12915425/13121-3.0.txt",,,,,,,,,,,,,,,,,,,1.0,ptbannister,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 09:21:13 UTC 2018,,,,,,,,,,"0|i38onb:",9223372036854775807,,,,,,,mck,,mck,,,Low,,2.2.8,,,,,,,,,,,,,,,,,"13/Jan/17 18:08;scottbale;Note also the comment elsewhere in {{RepairRunnable}}:
{code}
                public void onFailure(Throwable t)
                {
                    /**
                     * If the failure message below is modified, it must also be updated on
                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}
                     * for backward-compatibility support.
                     */
                    String message = String.format(""Repair session %s for range %s failed with error %s"",
                                                   session.getId(), session.getRange().toString(), t.getMessage());

{code}
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/RepairRunnable.java#L269;;;","09/Mar/18 03:42;ptbannister;Would you please assign this issue to me? I'm finishing up a fix and a dtest for this issue, and I should have a patch soon for 3.11.2.;;;","18/Mar/18 01:44;ptbannister;I posted a pull request to cassandra-dtest for a new dtest (TestDeprecatedRepairNotifications in repair_tests/deprecated_repair_test.py) to test this issue:

[https://github.com/apache/cassandra-dtest/pull/22|http://example.com/];;;","21/Mar/18 03:13;ptbannister;I've developed a patch for 3.0. The same changes should also work for 3.11.

To effectively test the patch, I recommend using the TestDeprecatedRepairNotifications dtest available in the CASSANDRA-13121 branch of https://github.com/ptbannister/cassandra-dtest. There's a pull request in for this dtest, but right now it's only available in my fork.;;;","13/May/18 01:58;ptbannister;Thanks for reviewing this ticket!;;;","13/May/18 02:20;mck;[~ptbannister], i've put in your patches for testing (including your dtest change).
I'll take a closer look at the code and can commit if all looks ok.

|| Branch || uTest || aTest || dTest ||
| [cassandra-3.0_13121|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.0_13121]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.0_13121.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.0_13121] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/29/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/29] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/556/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/556] |
| [cassandra-3.11_13121|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13121] |[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13121.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13121 ]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/30/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/30] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/557/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/557] |
| [dtest_13121|https://github.com/thelastpickle/cassandra-dtest/tree/mck/CASSANDRA-13121] |[!https://travis-ci.org/thelastpickle/cassandra-dtest.svg?branch=mck%2FCASSANDRA-13121!|https://travis-ci.org/thelastpickle/cassandra-dtest/builds/378499306]|  | |;;;","14/May/18 09:01;mck;*aTest results*:
 - 3.0 & 3.11
 -- Tests in {{CompressedInputStreamTest}} failed. Known flakey, bc timeouts. Tested ok locally.

*dTest results*:
 - 3.0: 13 failures vs 15 failures on the 3.0 branch. No new failures from patch.
 - 3.11: 10 failures vs 6 failures on trunk. 
 -- failures in {{repair_test}}. Known flakey. Tested ok locally.

;;;","14/May/18 09:20;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/22
;;;","14/May/18 09:21;mck;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trace and Histogram output misleading,CASSANDRA-13120,13034648,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,ahattrell,ahattrell,13/Jan/17 13:41,15/May/20 08:00,14/Jul/23 05:56,01/Jun/17 08:26,3.0.14,3.11.0,4.0,4.0-alpha1,,,Legacy/Core,,,,,2,,,,,"If we look at the following output:

{noformat}
[centos@cassandra-c-3]$ nodetool getsstables -- keyspace table 60ea4399-6b9f-4419-9ccb-ff2e6742de10
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647146-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647147-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647145-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647152-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647157-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-648137-big-Data.db
{noformat}

We can see that this key value appears in just 6 sstables.  However, when we run a select against the table and key we get:

{noformat}
Tracing session: a6c81330-d670-11e6-b00b-c1d403fd6e84

 activity                                                                                                          | timestamp                  | source         | source_elapsed
-------------------------------------------------------------------------------------------------------------------+----------------------------+----------------+----------------
                                                                                                Execute CQL3 query | 2017-01-09 13:36:40.419000 | 10.200.254.141 |              0
 Parsing SELECT * FROM keyspace.table WHERE id = 60ea4399-6b9f-4419-9ccb-ff2e6742de10; [SharedPool-Worker-2]       | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            104
                                                                         Preparing statement [SharedPool-Worker-2] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            220
                                        Executing single-partition query on table [SharedPool-Worker-1]            | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            450
                                                                Acquiring sstable references [SharedPool-Worker-1] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            477
                                                 Bloom filter allows skipping sstable 648146 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            496
                                                 Bloom filter allows skipping sstable 648145 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            503
                                                            Key cache hit for sstable 648140 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            513
                                                 Bloom filter allows skipping sstable 648135 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            520
                                                 Bloom filter allows skipping sstable 648130 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            526
                                                 Bloom filter allows skipping sstable 648048 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            530
                                                 Bloom filter allows skipping sstable 647749 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            535
                                                 Bloom filter allows skipping sstable 647404 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            540
                                                            Key cache hit for sstable 647145 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            548
                                                            Key cache hit for sstable 647146 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            556
                                                            Key cache hit for sstable 647147 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            564
                                                 Bloom filter allows skipping sstable 647148 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            570
                                                 Bloom filter allows skipping sstable 647149 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            575
                                                 Bloom filter allows skipping sstable 647150 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            580
                                                 Bloom filter allows skipping sstable 647151 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            585
                                                            Key cache hit for sstable 647152 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            591
                                                 Bloom filter allows skipping sstable 647153 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            597
                                                 Bloom filter allows skipping sstable 647154 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            601
                                                 Bloom filter allows skipping sstable 647155 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            606
                                                 Bloom filter allows skipping sstable 647156 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            611
                                                            Key cache hit for sstable 647157 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            617
                                                 Bloom filter allows skipping sstable 647158 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            623
                  Skipped 0/22 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            644
                                                 Merging data from memtables and 22 sstables [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            654
                                                           Read 9 live and 0 tombstone cells [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            732
                                                                                                  Request complete | 2017-01-09 13:36:40.419808 | 10.200.254.141 |            808
{noformat}

You'll note we claim not to have skipped any files due to bloom filters - even though we know the data is only in 6 files.

CFHistograms also report that we're hitting every sstables:

{noformat}
Percentile SSTables Write Latency Read Latency Partition Size Cell Count 
(micros) (micros) (bytes) 
50% 24.00 14.24 182.79 103 1 
75% 24.00 17.08 315.85 149 2 
95% 24.00 20.50 7007.51 372 7 
98% 24.00 24.60 10090.81 642 12 
99% 24.00 29.52 12108.97 770 14 
Min 21.00 3.31 29.52 43 0 
Max 29.00 1358.10 62479.63 1597 35
{noformat}

Code for the read is here:

https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L561

We seem to iterate over all the sstables and increment the metric as part of that iteration.

Either the reporting is incorrect - or we should maybe check the bloom filters first and then iterate the tombstones after?  

In this particular case we were using TWCS which makes the problem more apparent.  TWCS guarantees that we'll keep more sstables in an un-merged state.  With STCS we have to search them all, but most of them should be merged together if Compaction is keeping up.  LCS the read path is restricted which will disguise the impact.
",,ahattrell,blerer,jeromatron,jjirsa,johnny15676,kohlisankalp,nbozicns,stefania,urandom,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 01 08:26:09 UTC 2017,,,,,,,,,,"0|i38ny7:",9223372036854775807,3.0.9,,,,,,stefania,,stefania,,,Low,,,,,,,,,,,,,,,,,,,"26/Jan/17 11:58;nbozicns;I wanted to share some more details from digging and testing. First thing that I tried is LCS on clean node, to rule out TWCS as a problem. After some data loaded via *cassandra-stress* and some manually I managed to have same behavior.

{code:title=getsstables|borderStyle=solid}
vagrant@cassandra:/var/lib/cassandra/data/rts/lcs_test-2be45680e3b111e6b03e5fb4fc6cee63$ nodetool getsstables rts lcs_test 1eea2a12-e3ad-11e6-bf01-fe55135034f3;
/var/lib/cassandra/data/rts/lcs_test-2be45680e3b111e6b03e5fb4fc6cee63/mb-993-big-Data.db
{code}

So UUID is in single SSTable. Than I went to *cqlsh* and tried tracing and select and here is outcome:
{code:title=tracing|borderStyle=solid}
cqlsh> SELECT * FROM rts.lcs_test WHERE id = 1eea2a12-e3ad-11e6-bf01-fe55135034f3;

 id                                   | tp_id | ex_uuid
--------------------------------------+-------+----------
 1eea2a12-e3ad-11e6-bf01-fe55135034f3 |     3 | asddddad

(1 rows)

Tracing session: aad45de0-e3bc-11e6-8de0-89a4196d20ae

 activity                                                                                                  | timestamp                  | source        | source_elapsed
-----------------------------------------------------------------------------------------------------------+----------------------------+---------------+----------------
                                                                                        Execute CQL3 query | 2017-01-26 11:43:34.078000 | 192.168.34.20 |              0
 Parsing SELECT * FROM rts.lcs_test WHERE id = 1eea2a12-e3ad-11e6-bf01-fe55135034f3; [SharedPool-Worker-1] | 2017-01-26 11:43:34.078000 | 192.168.34.20 |            199
                                                                 Preparing statement [SharedPool-Worker-1] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            361
                                        Executing single-partition query on lcs_test [SharedPool-Worker-2] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            660
                                                        Acquiring sstable references [SharedPool-Worker-2] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            684
                                           Bloom filter allows skipping sstable 1406 [SharedPool-Worker-2] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            732
                                                       Key cache hit for sstable 993 [SharedPool-Worker-2] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            755
           Skipped 0/2 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] | 2017-01-26 11:43:34.079000 | 192.168.34.20 |            771
                                          Merging data from memtables and 2 sstables [SharedPool-Worker-2] | 2017-01-26 11:43:34.079001 | 192.168.34.20 |            785
                                                   Read 1 live and 0 tombstone cells [SharedPool-Worker-2] | 2017-01-26 11:43:34.079001 | 192.168.34.20 |            849
                                                                                          Request complete | 2017-01-26 11:43:34.082327 | 192.168.34.20 |           4327

{code}

So again 2 sstables are merged while bloom filter says one can be skipped.

Next think that I checked is access time of files in linux system with *ls -l --time=atime* command:
{code:title=accesstime|borderStyle=solid}
vagrant@cassandra:/var/lib/cassandra/data/rts/lcs_test-2be45680e3b111e6b03e5fb4fc6cee63$ ls -l --time=atime mb-993-big-*
-rw-r--r-- 1 cassandra cassandra      76 Jan 26 11:42 mb-993-big-CRC.db
-rw-r--r-- 1 cassandra cassandra 1048687 Jan 26 11:42 mb-993-big-Data.db
-rw-r--r-- 1 cassandra cassandra       9 Jan 26 11:42 mb-993-big-Digest.crc32
-rw-r--r-- 1 cassandra cassandra    5744 Jan 26 11:42 mb-993-big-Filter.db
-rw-r--r-- 1 cassandra cassandra  100769 Jan 26 11:42 mb-993-big-Index.db
-rw-r--r-- 1 cassandra cassandra   17703 Jan 26 11:42 mb-993-big-Statistics.db
-rw-r--r-- 1 cassandra cassandra    1072 Jan 26 11:42 mb-993-big-Summary.db
-rw-r--r-- 1 cassandra cassandra      80 Jan 26 11:42 mb-993-big-TOC.txt
{code}

This timestamp of 11:42 is not changing when I do select on CQLSH and it is timestamp when Cassandra probably read file first time and placed it in memory. This actually proves that Cassandra is not accessing SSTables each time query is done, it is using memory so probably logging is misleading in this case. I would expect if bloom filter allowed skipping of some sstable to log ""Merging data from memtables and 1 sstables"" instead of 2.

Why [this line in SinglePartitionReadCommand|https://github.com/apache/cassandra/blob/554d6beb0920cba9bcc0124fa9f33c580012b761/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L509] returns 2 sstables when it applies PK filter instead of 1?;;;","07/Feb/17 10:35;blerer;The trace and histogram output are effectively wrong. The problem is caused by the fact that the code within {{SinglePartitionReadCommand}} is not aware of which SSTables are skipped due to the Bloom filters.
I will try to find a way to correct that problem.;;;","07/Feb/17 12:02;nbozicns;Basically counter is incremented (_sstablesIterated_) on each loop no matter if Bloom Filter says it should be skipped or not. I was waiting for someone to check if that is intended or not.

I have suggestion how we can fix that (I can create a patch as well). Out of all sstables candidates for a file, only sstables which return non null from [BigTableReader| https://github.com/apache/cassandra/blob/af3fe39dcabd9ef77a00309ce6741268423206df/src/java/org/apache/cassandra/io/sstable/format/big/BigTableReader.java] will add up to result. Read path and this code is executed from [this line|https://github.com/apache/cassandra/blob/cassandra-3.0.10/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L584].

We can introduce counter _sstablesWithData_ if _sstablesIterated_ is used somewhere else, and increment _sstablesWithData_ only when _iter_ has non-null result returned from BigTableReader.

What do you think? That would give correct number of sstables based on read path including Bloom Filter.



;;;","07/Feb/17 12:36;blerer;bq. Out of all sstables candidates for a file, only sstables which return non null from BigTableReader will add up to result.

I would prefer to not use that approach as it could break quite easily without anybody noticing. If somebody was replacing the reader by an {{Optional<BigTableReader>}} for example.
To be honest, I am not sure yet how to fix it in the best way which is why I want to think a bit about it.;;;","09/May/17 09:41;blerer;I pushed a patch to solve the problem [here|https://github.com/blerer/cassandra/tree/13120-3.0]. The problem does not affect the {{3.11}} branch.

The patch passed CI without problems.

[~nbozicns] I ended up relying on checking the {{RowIndexEntry}} value. I do not like this approach much but as it is properly fixed in {{3.11}} I think it is acceptable.

[~Stefania] Could you review?;;;","10/May/17 01:06;stefania;The 3.0 patch LGTM, as you said it's not the cleanest, but it is safe and a good choice for 3.0.

I am not quite sure why you think this is _properly fixed in 3.11_? {{UnfilteredRowIteratorWithLowerBound}} relies on cached RIEs only when we cannot use the metadata bounds and by then the iterator is initialized, it could return a lower bound from metadata which is before the key and in this case the iterator would be initialized. Even when the lower bound is null, merge iterator will initialize the iterator by calling {{hasNext}}. Iterator initialized means that sstables iterated is incremented ({{SPRC.withSSTablesIterated}}). Besides, {{SPRC.queryMemtableAndSSTablesInTimestampOrder}} doesn't use this iterator at all. I don't see any other check on the BF in 3.11 so is it just that it cannot be reproduced and or am I missing something?
;;;","10/May/17 15:08;blerer;Sorry, my mistake. I did not checked the code correctly. I will make a new patch for 3.11;;;","19/May/17 10:35;blerer;Right now, what {{CFHistograms}} expose is the number of SSTables on which we do a partition lookup.
The partition look up can lead to skipping the SSTable (BF, min max, partition index lookup or index entry not found) or not.
Nevertheless, partition lookups are not cheap. Especially when an index lookup has to be done. Due to that, [~tjake] suggested to me, in an offline discussion, to keep the metric as it is and to add a new one {{mergedSSTable}} to track how many SSTables have been actually merged.

The number of actually merged SSTables should also be the one used for the {{Trace}} message and for determining if the {{SSTables}} must be compacted.;;;","22/May/17 00:59;stefania;bq. Right now, what CFHistograms expose is the number of SSTables on which we do a partition lookup.

Except it has never been documented, as far as I can see neither the [ASF docs|http://cassandra.apache.org/doc/latest/tools/nodetool/tablehistograms.html] nor the [DS docs|http://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsTablehisto.html] say what the number of sstables actually is  and [people|https://www.smartcat.io/blog/2017/where-is-my-data-debugging-sstables-in-cassandra/] tend to think this is the number of sstables it touches on each read, so it is very misleading. Our own comment says {{/** Histogram of the number of sstable data files accessed per read */}}, which to me would indicate that if the BF excludes a table then it should not be counted. We should at a minimum improve our own comments.

bq. keep the metric as it is and to add a new one mergedSSTable to track how many SSTables have been actually merged.

Are we thinking of a new metrics histogram? I'm not opposed, as long as we document {{nodetool \[cf|table\]histograms}} accordingly. My only concern is that adding a new histogram on each read may have a performance impact - but I do understand if we don't want to change the existing behavior, especially in 3.0.

;;;","22/May/17 06:09;stefania;Just to clarify what the current metric excludes: sstables that are not live, not selected by the view interval tree (which relies on the first and last partition key in the sstable), not selected by the filters (which rely on the clustering min and max values) unless they may have tombstones, not selected because of a max timestamp older than the latest tombstone. Basically anything not selected by {{queryMemtableAndDisk}}. Also, for 3.11+, sstables that were not accessed because the query was already satisfied ({{UnfilteredRowIteratorWithLowerBound}}).

However, sstables that are discarded by {{SSTableReader.getPosition()}}, so BF, min and max keys (again) and partition not found in the sstable (BF false positive) end up in the current metric. The partition lookup in the index is the only expensive part, the first two are not.

IMO the current metric is not at all consistent, so OK for two metrics if we must, but let's clarify the boundaries in a way meaningful to the users, not dependent on how we organized the code.;;;","22/May/17 06:18;jjirsa;What did the 2.x metric include/exclude? Did we diverge with 8099, or has it always been like this?
;;;","22/May/17 06:32;stefania;bq. What did the 2.x metric include/exclude? Did we diverge with 8099, or has it always been like this?

From a very quick look, the 2.x code looks similar to 3.x, so I think it has always been like this. The entry point is {{CFS.getTopLevelColumns()}}.;;;","22/May/17 10:01;blerer;We double checked the {{2.2}} code with [~Stefania] and the behavior was actually changed by CASSANDRA-8099.

After some discussion, we agreed that it is probably best to fix the metric and leave the addition of a new metric to a followup ticket (if people feel the need for a new metric).

Regarding, {{SSTableReader:readCount}} as it is used by {{SizedTierd}} compaction to determine the SSTable hotness, it makes sense to also take into account parition range queries.   ;;;","22/May/17 10:09;stefania;To clarify further, the code before 8099 did not increment the number of sstables iterated if the sstable iterator had a [null column family|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/CollationController.java#L131], which is the equivalent of {{SSTableReader.getPosition()}} returning null. Therefore, sstables excluded by the BF or a failed partition lookup were not considered. This patch will restore this behavior.;;;","24/May/17 08:40;blerer;I made an initial patch for 3.11 [here|https://github.com/apache/cassandra/compare/trunk...blerer:13120-3.11].
[~Stefania] could you check the patch and tell me if you are fine with it before I rewrite it for {{3.0}} and {{trunk}}?
;;;","26/May/17 03:01;stefania;The approach looks good. 

Some nits [here|https://github.com/stef1927/cassandra/commit/4ff0ccf5e290749b60204509b7d4b7d5d469de59]. 

I have two suggestions:

* Pass a boolean or a new enum to the {{SSTableReadMetricsCollector}} constructor that indicates the query type (single or range). This way we know for sure if we need to increment {{mergedSSTables}}. At the moment we rely on knowing which methods get called where, which is a bit brittle.

* Make the listener symmetric w.r.t. sstables skipped and selected. At the moment we have {{skippingSSTable}} with a reason to indicate that an sstable was skipped and different methods to indicate that an sstable was selected. I would personally prefer to only have two methods, something like {{onSSTableSkipped}} and {{onSSTableSelected}}, with two parameters: the sstable and the reason. The reason enums should probably be two distinct enums. We loose the RIE parameter but it is not really used at the moment. WDYT?

Regarding the unit tests:
* The javadoc of {{SSTablesIteratedTest}} needs updating, since it is no longer only limited to CASSANDRA-8180.
* I'm not sure the new test covers {{queryMemtableAndSSTablesInTimestampOrder()}}.
* It may be useful to also have a range query with the test checking that the metric is not updated and with a comment explaining why that it the case.
;;;","31/May/17 15:54;blerer;I took the feedbacks into account and made some new patches for [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:13120-3.0], [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13120-3.11] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13120-trunk]. I ran the patch on our internal CI. There are not failures for the unit tests and the failing dtests are not related to the change.   ;;;","01/Jun/17 03:36;stefania;+1, latest approach looks pretty good, great job!;;;","01/Jun/17 08:25;blerer;Thanks for the review :-);;;","01/Jun/17 08:26;blerer;Committed into 3.0 at e22cb278b63a6ee5f03c7213071d07fd3b198659 and merged into 3.11 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure upgrade_tests.upgrade_supercolumns_test.TestSCUpgrade.upgrade_super_columns_through_all_versions_test,CASSANDRA-13119,13033986,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aweisberg,aweisberg,aweisberg,11/Jan/17 21:57,16/Apr/19 09:30,14/Jul/23 05:56,03/May/17 22:19,2.2.10,,,,,,Legacy/Core,Test/dtest/python,,,,0,,,,,"The test complains about unreadable sstables version ka and lb during upgrade which is 2.1 and 2.2. These tables look like system tables not user tables.

I looked and I can't find any place where system tables are upgraded on upgrade. You can specify them explicitly by name with nodetool, but nodetool defaults to only upgrading user tables and doesn't have a flag to upgrade all tables.

These tables probably need to be removed if unused or upgraded if in use.",,aweisberg,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 22:19:37 UTC 2017,,,,,,,,,,"0|i38kvz:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Critical,,,,,,,,,,,,,,,,,,,"30/Jan/17 18:34;aweisberg;||code|utests|dtests||
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:cassandra-13119-2.2?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-2.2-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-2.2-dtest/1]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...aweisberg:cassandra-13119-3.0?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-3.0-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-3.0-dtest/1]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13119-3.11?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-3.11-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-3.11-dtest/1]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13119-trunk?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-trunk-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13119-3.11-dtest/1]|;;;","03/May/17 13:31;jasobrown;due to my delay in reviewing, [~jjirsa] has already applied the same fix in CASSANDRA-13410, to 3.0, 3.11, and trunk.

I'm +1 if you want to commit to 2.2 (I think it makes sense to do so).;;;","03/May/17 22:19;aweisberg;Committed as [6c5ea192c75072ba3f7369dfc23592d6ed0c319f|https://github.com/apache/cassandra/commit/6c5ea192c75072ba3f7369dfc23592d6ed0c319f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dump threads when unit test times out,CASSANDRA-13117,13033736,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,11/Jan/17 14:26,16/Apr/19 09:30,14/Jul/23 05:56,01/Feb/17 14:03,3.0.11,3.11.0,,,,,Legacy/Testing,,,,,0,,,,,It would be nice to get a thread dump when unit tests time out,,jasobrown,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 14:03:58 UTC 2017,,,,,,,,,,"0|i38k3j:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"11/Jan/17 14:28;marcuse;https://github.com/krummas/cassandra/commits/marcuse/dumpthreadsjunit

uses https://github.com/krummas/jstackjunit to execute {{jstack -l}} on timeout.

Example output (Thread.sleep-induced timeout):
https://gist.github.com/krummas/af6af79da739837f5509ae3dbb82758c;;;","20/Jan/17 00:46;jasobrown;+1. I like this idea!;;;","01/Feb/17 14:03;marcuse;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair is not blocking repair to finish in foreground repair,CASSANDRA-13115,13033546,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,xiaolong302@gmail.com,xiaolong302@gmail.com,10/Jan/17 23:00,16/Apr/19 09:30,14/Jul/23 05:56,19/Jan/17 15:55,3.0.11,3.10,,,,,,,,,,0,,,,,"The code trying to wait(block) for repair result to come back in 3.X is below:
{code:title= DataResolver.java|borderStyle=solid}
public void close()
        {
            try
            {
                FBUtilities.waitOnFutures(repairResults, DatabaseDescriptor.getWriteRpcTimeout());
            }
            catch (TimeoutException ex)
            {
                // We got all responses, but timed out while repairing
                int blockFor = consistency.blockFor(keyspace);
                if (Tracing.isTracing())
                    Tracing.trace(""Timed out while read-repairing after receiving all {} data and digest responses"", blockFor);
                else
                    logger.debug(""Timeout while read-repairing after receiving all {} data and digest responses"", blockFor);

                throw new ReadTimeoutException(consistency, blockFor-1, blockFor, true);
            }
        }
{code}
in DataResolver class, but this close method is never called and it's also not auto close(RepairMergeListener is not extending from AutoCloseable/CloseableIterator) which means we never wait for repair to finish before returning final result. 

The steps to reproduce:
1. create some keyspace/table with RF = 2
2. start 2 nodes using ccm
3. stop node2
4. disable node1 hinted hand off
5. write some data to node1 with consistency level one
6. start node2
7. query some data from node1 
This should trigger read repair. I put some log in above close method, and can not see log print put.

So this bug will basically violate ""monotonic quorum reads "" guarantee. 
",ccm on OSX ,jasobrown,jay.zhuang,jeromatron,jjirsa,kohlisankalp,rha,slebresne,xiaolong302@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 19 15:55:59 UTC 2017,,,,,,,,,,"0|i38ixj:",9223372036854775807,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"11/Jan/17 10:00;rha;I don't understand why you say it violates quorum reads, with RF=2 {{QUORUM == 2}} i.e. like {{ALL}}. 
But you read with {{ONE / LOCAL_ONE}} when doing {{7. query some data from node1}}, don't you? 

bq. This should trigger read repair. 

Background read repair because of CL {{ONE}}, right? 
Did you set {{dclocal_read_repair_chance/read_repair_chance = 1.0}}?
;;;","12/Jan/17 01:32;xiaolong302@gmail.com;[~rha]Please check this https://issues.apache.org/jira/browse/CASSANDRA-10726 for more context and also see Jonathan's comments.;;;","12/Jan/17 09:34;slebresne;You're absolutely right, this is clearly wrong, thanks for noticing.

I'm attaching the pretty trivial fix. Now, while looking at this, I realized we were also not handling ""asynchronous read repairs"" properly as we were not consuming the result of {{resolve()}} in that case. So a 2nd commit fixes that part (also fairly trivial).
| [13115-3.0|https://github.com/pcmanus/cassandra/commits/13115-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13115-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13115-3.0-dtest] |
| [13115-3.X|https://github.com/pcmanus/cassandra/commits/13115-3.X] | [utests|http://cassci.datastax.com/job/pcmanus-13115-3.X-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13115-3.X-dtest] |
;;;","17/Jan/17 22:12;xiaolong302@gmail.com;[~slebresne]Thanks for fixing this. I went through both changes and it looks good to me. Can you please merge both patches? Thanks!!;;;","19/Jan/17 13:12;jasobrown;+1;;;","19/Jan/17 15:55;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade netty to 4.0.44 to fix memory leak with client encryption,CASSANDRA-13114,13033102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,spod,tvdw,tvdw,09/Jan/17 16:35,15/May/20 08:05,14/Jul/23 05:56,09/Feb/17 20:16,2.1.17,2.2.9,3.0.11,3.11.0,4.0,4.0-alpha1,,,,,,0,,,,,"https://issues.apache.org/jira/browse/CASSANDRA-12032 updated netty for Cassandra 3.8, but this wasn't backported. Netty 4.0.23, which ships with Cassandra 3.0.x, has some serious bugs around memory handling for SSL connections.

It would be nice if both were updated to 4.0.42, a version released this year.

4.0.23 makes it impossible for me to run SSL, because nodes run out of memory every ~30 minutes. This was fixed in 4.0.27.",,jay.zhuang,jjirsa,jjordan,mshuler,norman,rha,seb.arzt,snazy,spod,tvdw,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13126,CASSANDRA-12032,CASSANDRA-12928,CASSANDRA-14460,,,,,,,,,,"01/Feb/17 14:17;spod;13114_netty-4.0.44_2.x-3.0.patch;https://issues.apache.org/jira/secure/attachment/12850429/13114_netty-4.0.44_2.x-3.0.patch","01/Feb/17 14:17;spod;13114_netty-4.0.44_3.11.patch;https://issues.apache.org/jira/secure/attachment/12850428/13114_netty-4.0.44_3.11.patch",,,,,,,,,,,,,,,,,,2.0,spod,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 20:35:55 UTC 2017,,,,,,,,,,"0|i38g73:",9223372036854775807,3.0.10,,,,,,snazy,,snazy,,,Critical,,,,,,,,,,,,,,,,,,,"10/Jan/17 09:13;spod;Netty 4.0.23 has been introduced in CASSANDRA-7761 and is used since 2.1.1. Looking at the [4.0.27 release notes|http://netty.io/news/2015/04/02/4-0-27-Final.html] I assume that [#3567|https://github.com/netty/netty/issues/3567] fixes the memory issue you're referring to? If I read the ticket correctly, the bug does not manifest in 4.0.23, but only after updating to 4.0.25/26. Can you link the actual issue that makes you believe 4.0.23 is affected in case there's something I'm missing here?;;;","10/Jan/17 09:17;tvdw;Sorry, the actual fixes are in 4.0.24 (http://netty.io/news/2014/10/29/4-0-24-Final.html / https://github.com/netty/netty/pull/3058 / https://github.com/netty/netty/issues/3057).;;;","25/Jan/17 11:54;spod;[~snazy], since you've been working on a couple of related issues (CASSANDRA-12032, CASSANDRA-12033, CASSANDRA-12034, CASSANDRA-11937, CASSANDRA-11818).. what would be your suggestion when it comes to back-porting  some of these to further stabilize memory management in 2.x and 3.0? Do you see any issues updating netty to 4.0.42 on all branches, regardless of any back-porting decisions for the listed tickets?;;;","25/Jan/17 13:08;snazy;No objections regarding a _minor_ netty upgrade, if that solves real SSL issues / an existing bug.  I'd just be very careful with dependency changes since 4.0.37 introduced a couple of issues.
Would setting {{-XX:MaxDirectMemorySize=SomeBigValue}} help in this case?
However, I do object introducing the new stuff from CASSANDRA-12032 in 2.2 or 3.0, because it is a new thing, which is disabled by default in 3.x.
;;;","25/Jan/17 13:47;tvdw;4.0.39 has been tested pretty well with 3.x by now, so it should be relatively safe (much safer than, say, 3.0.36, which not only hasn't been tested, but also won't get the attention that 3.0.39 would get if it is used by both 3.0 and 3.x). Definitely don't pick 4.1!

As for setting MaxDirectMemorySize: it was set to 20GB when I filed this ticket. No luck :)

Side-note: CASSANDRA-13126 is a related issue, on clusters with SSL enabled even non SSL clients may be impacted due to this netty issue.;;;","25/Jan/17 14:55;spod;I admit that I don't really have an opinion on using any particular version. Any reasons not to go with the latest stable 4.0 release? 

Let's try a CI run using 4.0.43:

||2.1||2.2||3.0||3.11||trunk||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13114-2.1]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13114-2.2]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13114-3.0]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13114-3.11]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13114-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-2.1-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-2.2-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-3.11-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-2.1-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-2.2-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-3.11-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-trunk-testall/]|


[~jasobrown], you have any strong preference for netty releases and gossip 2.0?

[~norman], anything to double check or look out for from our side while updating from 4.0.23 -> 4.0.43?
;;;","25/Jan/17 19:55;norman;[~spodxx@gmail.com] nope just upgrade to 4.0.43. FTW ;) That said we will release 4.0.44.Final end of this week or next week latest.;;;","27/Jan/17 15:09;spod;Some insights on dtest results:

* {{local_quorum_bootstrap_test}}
See CASSANDRA-12437, [PR|https://github.com/riptano/cassandra-dtest/pull/1429] created
* {{test_tombstone_failure_v3}}
Seems to be [failing on constant basis|http://cassci.datastax.com/view/cassandra-2.1/job/cassandra-2.1_dtest/lastCompletedBuild/testReport/read_failures_test/TestReadFailures/test_tombstone_failure_v3/], I've opened a [PR| https://github.com/riptano/cassandra-dtest/pull/1430]
* {{cqlsh_tests.cqlsh_tests.CqlshSmokeTest.test_alter_table}} and
* {{cql_tests.MiscellaneousCQLTester.prepared_statement_invalidation_test}}
Fallout from CASSANDRA-12443 and already addressed in [PR|https://github.com/riptano/cassandra-dtest/pull/1427]

Other tests seem to be flaky or have known issues.;;;","30/Jan/17 19:12;norman;[~spodxx@gmail.com] FYI netty 4.0.44.Final was released earlier today. http://netty.io/news/2017/01/30/4-0-44-Final-4-1-8-Final.html;;;","01/Feb/17 14:21;spod;Updated the patch to 4.0.44 and latest CI results already reflect the version change.

[~snazy], would you review the patch?;;;","01/Feb/17 17:45;jjordan;You probably want to run some QUORUM stress smoke tests to make sure there are no regressions there.;;;","02/Feb/17 13:57;snazy;I can, but would need some time.

EDIT: [~jjordan]'s suggestion makes sense. We already had an issue with a regression in Netty - just want to prevent a second upgrade-revert-upgrade-ticket ;);;;","02/Feb/17 20:24;mshuler;I built SNAPSHOT artifacts on top of current {{cassandra-3.0}} for Tom to test: http://people.apache.org/~mshuler/13114_netty-4.0.44_3.0/

{noformat}
commit cd9a6590dd950d456278461919d7e6a079531ee4 (HEAD -> 13114_netty-4.0.44_3.0, github/13114_netty-4.0.44_3.0)
Author: Stefan Podkowinski <s.podkowinski@gmail.com>
Date:   Wed Jan 25 15:16:25 2017 +0100

    Upgrade to netty 4.0.44
    
    patch by Stefan Podkowinski; reviewed by X for CASSANDRA-13114

 CHANGES.txt                                                             |   1 +
 build.xml                                                               |   2 +-
 lib/licenses/{netty-all-4.0.23.Final.txt => netty-all-4.0.44.Final.txt} |   0
 lib/netty-all-4.0.23.Final.jar                                          | Bin 1779991 -> 0 bytes
 lib/netty-all-4.0.44.Final.jar                                          | Bin 0 -> 2342652 bytes
 src/java/org/apache/cassandra/transport/Message.java                    |  12 ++++++++----
 6 files changed, 10 insertions(+), 5 deletions(-)

commit 6caf3c4f13bb29e3fd732365210be6b3c954f04a (origin/cassandra-3.0, cassandra-3.0)
Author: Michael Shuler <michael@pbandjelly.org>
Date:   Wed Feb 1 14:39:35 2017 -0600

    Remove 'no upgrades' note from NEWS.txt

{noformat};;;","03/Feb/17 11:19;snazy;[~spodxx@gmail.com] can you explain the code change in {{Message}}? It looks like it doesn't change anything.;;;","03/Feb/17 12:36;spod;Any exceptions caught by Netty will be logged by {{UnexpectedChannelExceptionHandler}}. Before the changes to {{Message}}, this would only happen when the channel would still be open. In Netty 4.0.23 this seems to be the case, in 4.0.44 the channel is now closed (at least for handshake errors, which makes sense) when {{exceptionCaught}} is called. What the patch does is simply to log the exception in both cases, while leaving the error reply sending case as it was (keep sending reply only in case channel is still open). I've noticed the changed behaviour through a dtest, [see results|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13114-3.11-dtest/1/testReport/junit/native_transport_ssl_test/NativeTransportSSL/connect_to_ssl_test/].

As for the mentioned performance regression tests, I've only have a single physical host available for cstar_perf, so I can't really stress test multi-node setups. A single node tests with and without ssl didn't show any significant changes, but the results are little meaningful. Besides I also can't find any way to test the actual memory leak issue at hand by simulating lots of connecting and deconnecting clients.

;;;","03/Feb/17 12:43;snazy;Ah - logging takes place in the {{apply}} method. OK, makes sense.;;;","03/Feb/17 14:07;tvdw;I've deployed [~mshuler]'s snapshot build to most nodes in the cluster that previously experienced this issue, and re-enabled TLS on a few hundred clients. The nodes still on 3.0.10 quickly showed memory issues, while the nodes on 3.0.11-SNAPSHOT were happy.

So I upgraded the remaining nodes and enabled TLS on another ~50000 clients, many many more than I originally needed to cause problems.

Nothing broke.

So basically I can confirm that this patch fixes the problem, and at least for my test cases (one involves fairly heavy use of QUORUM reads and writes) doesn't appear to introduce new ones. I wouldn't take that over good dtests/utests, but it's a useful data point to have.;;;","09/Feb/17 16:26;mshuler;I set this ticket as a blocker for 3.0.11.

Is there any additional testing required to merge this to the cassandra-3.0 branch, so we can release 3.0.11?  Thanks!;;;","09/Feb/17 18:58;snazy;Patch LGTM - however, is this going into 3.0...trunk or 2.1...trunk? Fix version (and summary) say 3.0, but there are patches for 2.1 and 2.2, too.;;;","09/Feb/17 19:30;spod;I don't think this is critical enough for 2.1, as you have to have large enough direct allocations through netty that will exhaust the memory between regular CMS runs. So you don't necessarily have to run into this problem and there's an easy workaround by not enabling disableexplicitgc (as described in CASSANDRA-13126) or by simply replacing the netty jar with a new version. 
But 2.2 and 3.0 - why not? Both are supported and it should be possible to do a library updated there.;;;","09/Feb/17 20:16;snazy;Alright, committed as eb0f443705425bac93f92a5e7e04ea5def77b1d8 to 2.1 and merged up to trunk.;;;","13/Feb/17 22:04;jjirsa;Was this dire enough to go into 2.1 ?  Seems like a pretty high bar for a .17 release. 

;;;","14/Feb/17 20:35;snazy;Well, the new netty version _should_ not hurt. But if you run into this SSL/netty issue, it hurts a lot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in auth_test.TestAuth.system_auth_ks_is_alterable_test,CASSANDRA-13113,13033095,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,sean.mccarthy,sean.mccarthy,09/Jan/17 15:39,15/May/20 08:01,14/Jul/23 05:56,12/May/17 07:14,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1466/testReport/auth_test/TestAuth/system_auth_ks_is_alterable_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 582, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
{code}{code}
Standard Output

Unexpected error in node2 log, error: 
ERROR [Native-Transport-Requests-1] 2017-01-08 21:10:55,056 Message.java:623 - Unexpected exception during request; channel = [id: 0xf39c6dae, L:/127.0.0.2:9042 - R:/127.0.0.1:43640]
java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:503) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:310) ~[main/:na]
	at org.apache.cassandra.service.ClientState.login(ClientState.java:271) ~[main/:na]
	at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:80) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[main/:na]
	at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1734) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1696) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1642) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1557) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:964) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:282) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:252) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:511) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:493) ~[main/:na]
	... 13 common frames omitted
{code}",,eperott,ifesdjeen,samt,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13367,,,,,,,,,,,,CASSANDRA-10855,,,,,,,"09/Jan/17 15:39;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12846358/node1.log","09/Jan/17 15:39;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12846356/node1_debug.log","09/Jan/17 15:39;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12846357/node1_gc.log","09/Jan/17 15:39;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12846361/node2.log","09/Jan/17 15:39;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12846359/node2_debug.log","09/Jan/17 15:39;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12846360/node2_gc.log","09/Jan/17 15:39;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12846364/node3.log","09/Jan/17 15:39;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12846362/node3_debug.log","09/Jan/17 15:39;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12846363/node3_gc.log",,,,,,,,,,,9.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 07:14:11 UTC 2017,,,,,,,,,,"0|i38g5j:",9223372036854775807,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"10/Jan/17 18:10;samt;From a first look at the logs, this seems to be caused by the client session attempting to (re)connect while the cluster is in the process of shutting down (after the RF of system_auth has been altered, hence the error). It's a bit strange as this is pretty much exactly the issue that [this dtest commit|https://github.com/riptano/cassandra-dtest/commit/7c6a11c8885c59aa550ccf70df1ae61afd1b2416] was supposed to obviate. 

I'll dig further, but just FTR I don't believe this is a serious problem/regression.;;;","23/Mar/17 08:02;ifesdjeen;I've investigated a bit deeper. Although in my opinion it's kind of a regression, even if it's not super-serious, but it has some user-facing implications. I've ran {{bisect}} and narrowed it down to [this commit|https://github.com/apache/cassandra/commit/c607d76413be81a0e125c5780e068d7ab7594612] 

Checking logs reveals that before this commit, we had error messages in the form of:

{code}
Error from server: code=0100 [Bad credentials] message=""Error during authentication of user cassandra : org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM""
{code}

After, it's changed to 

{code}
Error from server: code=0000 [Server error] message=""java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM""
{code}

I've checked underlying code and it looks like Guava was doing some unwrapping in case of runtime exceptions on [cache loading|http://grepcode.com/file/repo1.maven.org/maven2/com.google.guava/guava/11.0/com/google/common/cache/LocalCache.java#2234] (might be a wrong guava version but you get the idea). Previously, we had to unwrap the {{UncheckedExecutionException}} in order to extract cause and [turn it into authentication exception|https://github.com/ifesdjeen/cassandra/commit/c607d76413be81a0e125c5780e068d7ab7594612#diff-ef1e335e8d51911f09bcc735b0632c5cL97], in order to trigger a correct error code. Now, we don't have to since exception isn't un/rewrapped. 

The stack trace of the other exception that was happening and causing {{Server error}} instead of {{Bad Credentials}} was

{code}
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:487) [main/:na]
        at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:310) [main/:na]
        at org.apache.cassandra.service.ClientState.login(ClientState.java:271) [main/:na]
        at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:80) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}

Consequently, I have removed guava-specific exception rewrapping. The other places (JMX permissions cache, Credentials cache, Passwords cache and Permissions cache) look fine, with an exception with Permission cache where we do re-wrap an exception but that doesn't change bubbling/error code. 

|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13367-trunk]|[dtest|https://cassci.datastax.com/job/ifesdjeen-13367-trunk-dtest/]|[testall|https://cassci.datastax.com/job/ifesdjeen-13367-trunk-testall/]|;;;","23/Mar/17 20:27;ifesdjeen;As [~beobal] noted, we do not need wrapping exceptions at all now, and can get rid of {{ExecutionException}} in {{AuthCache#get}}. I've changed the patch according to his suggestions.;;;","08/May/17 08:25;ifesdjeen;Hey [~beobal] will you get some time to review this one any time soon? It'd be great to get it committed, as it'll make our build much greener..;;;","08/May/17 12:24;samt;Sorry [~ifesdjeen], this slipped by me. Your patch LGTM, though I think there's on other place where the wrapping & rethrowing can be removed - I pushed a commit [here|https://github.com/beobal/cassandra/commit/4d9525de6709ab3887c6af48785f64a82b92e40d];;;","12/May/17 07:14;ifesdjeen;Thank you a lot for the review! As Ariel's email mentioned this is one of the top contenders contributing to our unstable build, so we're getting one step closer to having it fixed. 

Committed to trunk as [d8871bd5b41038849c77ddd9950bd1e4dcf77a78|https://github.com/apache/cassandra/commit/d8871bd5b41038849c77ddd9950bd1e4dcf77a78].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in snitch_test.TestDynamicEndpointSnitch.test_multidatacenter_local_quorum,CASSANDRA-13112,13033093,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jkni,sean.mccarthy,sean.mccarthy,09/Jan/17 15:31,16/Apr/19 09:30,14/Jul/23 05:56,07/Apr/17 16:11,,,,,,,,,,,,0,dtest,test-failure,,,"example failure:

http://cassci.datastax.com/job/trunk_large_dtest/48/testReport/snitch_test/TestDynamicEndpointSnitch/test_multidatacenter_local_quorum

{code}
Error Message

75 != 76
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/snitch_test.py"", line 168, in test_multidatacenter_local_quorum
    bad_jmx.read_attribute(read_stage, 'Value'))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
{code}",,jkni,sean.mccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 15:31;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12846340/node1.log","09/Jan/17 15:31;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12846338/node1_debug.log","09/Jan/17 15:31;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12846339/node1_gc.log","09/Jan/17 15:31;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12846343/node2.log","09/Jan/17 15:31;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12846341/node2_debug.log","09/Jan/17 15:31;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12846342/node2_gc.log","09/Jan/17 15:31;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12846346/node3.log","09/Jan/17 15:31;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12846344/node3_debug.log","09/Jan/17 15:31;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12846345/node3_gc.log","09/Jan/17 15:31;sean.mccarthy;node4.log;https://issues.apache.org/jira/secure/attachment/12846349/node4.log","09/Jan/17 15:31;sean.mccarthy;node4_debug.log;https://issues.apache.org/jira/secure/attachment/12846347/node4_debug.log","09/Jan/17 15:31;sean.mccarthy;node4_gc.log;https://issues.apache.org/jira/secure/attachment/12846348/node4_gc.log","09/Jan/17 15:31;sean.mccarthy;node5.log;https://issues.apache.org/jira/secure/attachment/12846352/node5.log","09/Jan/17 15:31;sean.mccarthy;node5_debug.log;https://issues.apache.org/jira/secure/attachment/12846350/node5_debug.log","09/Jan/17 15:31;sean.mccarthy;node5_gc.log;https://issues.apache.org/jira/secure/attachment/12846351/node5_gc.log","09/Jan/17 15:31;sean.mccarthy;node6.log;https://issues.apache.org/jira/secure/attachment/12846355/node6.log","09/Jan/17 15:31;sean.mccarthy;node6_debug.log;https://issues.apache.org/jira/secure/attachment/12846353/node6_debug.log","09/Jan/17 15:31;sean.mccarthy;node6_gc.log;https://issues.apache.org/jira/secure/attachment/12846354/node6_gc.log",,18.0,jkni,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 16:11:25 UTC 2017,,,,,,,,,,"0|i38g53:",9223372036854775807,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"18/Jan/17 03:43;jkni;This should be a dtest only fix. I've PRed a fix at [https://github.com/riptano/cassandra-dtest/pull/1425].;;;","07/Apr/17 16:11;jkni;The dtest PR was merged and this failure has not reoccurred.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lightweight transactions temporarily fail after upgrade from 2.1 to 3.0,CASSANDRA-13109,13032647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sklock,sklock,sklock,06/Jan/17 19:50,25/Oct/19 13:10,14/Jul/23 05:56,09/Feb/17 09:29,3.0.11,3.11.0,,,,,Feature/Lightweight Transactions,,,,,1,LWT,,,,"We've observed this upgrading from 2.1.15 to 3.0.8 and from 2.1.16 to 3.0.10: some lightweight transactions executed on upgraded nodes fail with a read failure.  The following conditions seem relevant to this occurring:

* The transaction must be conditioned on the current value of at least one column, e.g., {{IF NOT EXISTS}} transactions don't seem to be affected.
* There should be a collection column (in our case, a map) defined on the table on which the transaction is executed.
* The transaction should be executed before sstables on the node are upgraded.  The failure does not occur after the sstables have been upgraded (whether via {{nodetool upgradesstables}} or effectively via compaction).
* Upgraded nodes seem to be able to participate in lightweight transactions as long as they're not the coordinator.
* The values in the row being manipulated by the transaction must have been consistently manipulated by lightweight transactions (perhaps the existence of Paxos state for the partition is somehow relevant?).
* In 3.0.10, it _seems_ to be necessary to have the partition split across multiple legacy sstables.  This was not necessary to reproduce the bug in 3.0.8 or .9.

For applications affected by this bug, a possible workaround is to prevent nodes being upgraded from coordinating requests until sstables have been upgraded.

We're able to reproduce this when upgrading from 2.1.16 to 3.0.10 with the following steps on a single-node cluster using a mostly pristine {{cassandra.yaml}} from the source distribution.

# Start Cassandra-2.1.16 on the node.
# Create a table with a collection column and insert some data into it.
{code:sql}
CREATE KEYSPACE test WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE test.test (key TEXT PRIMARY KEY, cas_target TEXT, some_collection MAP<TEXT, TEXT>);
INSERT INTO test.test (key, cas_target, some_collection) VALUES ('key', 'value', {}) IF NOT EXISTS;
{code}
# Flush the row to an sstable: {{nodetool flush}}.
# Update the row:
{code:sql}
UPDATE test.test SET cas_target = 'newvalue', some_collection = {} WHERE key = 'key' IF cas_target = 'value';
{code}
# Drain the node: {{nodetool drain}}
# Stop the node, upgrade to 3.0.10, and start the node.
# Attempt to update the row again:
{code:sql}
UPDATE test.test SET cas_target = 'lastvalue' WHERE key = 'key' IF cas_target = 'newvalue';
{code}
Using {{cqlsh}}, if the error is reproduced, the following output will be returned:
{code:sql}
$ ./cqlsh <<< ""UPDATE test.test SET cas_target = 'newvalue', some_collection = {} WHERE key = 'key' IF cas_target = 'value';""
(start: 2016-12-22 10:14:27 EST)
<stdin>:2:ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'QUORUM'}
{code}
and the following stack trace will be present in the system log:
{noformat}
WARN  15:14:28 Uncaught exception on thread Thread[SharedPool-Worker-10,10,main]: {}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2476) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer.getReduced(Row.java:617) ~[main/:na]
	at org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer.getReduced(Row.java:569) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:220) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:159) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.rows.Row$Merger.merge(Row.java:546) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:563) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:527) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:220) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:159) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.partitions.AbstractBTreePartition.build(AbstractBTreePartition.java:334) ~[main/:na]
	at org.apache.cassandra.db.partitions.ImmutableBTreePartition.create(ImmutableBTreePartition.java:111) ~[main/:na]
	at org.apache.cassandra.db.partitions.ImmutableBTreePartition.create(ImmutableBTreePartition.java:94) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.add(SinglePartitionReadCommand.java:810) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndSSTablesInTimestampOrder(SinglePartitionReadCommand.java:760) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:519) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:496) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) ~[main/:na]
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:394) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1794) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2472) ~[main/:na]
	... 5 common frames omitted
{noformat}

Under both 3.0.8 and .9, the {{nodetool flush}} and additional {{UPDATE}} statement before upgrading to 3.0 are not necessary to reproduce this.  In that case (when Cassandra only has to read the data from one sstable?), a different stack trace appears in the log.  Here's a sample from 3.0.8:

{noformat}
 WARN [SharedPool-Worker-3] 2016-12-13 15:19:48,863 AbstractLocalAwareExecutorService.java (line 169) Uncaught exception on thread Thread[SharedPool-Worker-3,5,main]: {}
java.lang.RuntimeException: java.lang.IllegalStateException: [ColumnDefinition{name=REDACTED, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, position=-1}, ColumnDefinition{name=REDACTED2, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, position=-1}] is not a subset of [REDACTED]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2453) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.IllegalStateException: [ColumnDefinition{name=REDACTED, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, position=-1}, ColumnDefinition{name=REDACTED2, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, position=-1}] is not a subset of [REDACTED]
        at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:531) ~[main/:na]
        at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:465) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:178) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:108) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:96) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:132) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:77) ~[main/:na]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:300) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:134) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:127) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:123) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:65) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:289) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1796) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2449) ~[main/:na]
        ... 5 common frames omitted
 WARN [SharedPool-Worker-1] 2016-12-13 15:19:48,943 AbstractLocalAwareExecutorService.java (line 169) Uncaught exception on thread Thread[SharedPool-Worker-1,5,main]: {}
java.lang.IllegalStateException: [ColumnDefinition{name=REDACTED, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, position=-1}, ColumnDefinition{name=REDACTED2, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, position=-1}] is not a subset of [REDACTED]
        at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:531) ~[main/:na]
        at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:465) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:178) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:108) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:96) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:132) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[main/:na]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:77) ~[main/:na]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:300) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:134) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:127) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:123) ~[main/:na]
        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:65) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:289) ~[main/:na]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:47) ~[main/:na]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{noformat}

It's not clear to us what changed in 3.0.10 to make this behavior somewhat more difficult to reproduce.

We spent some time trying to track down the cause in 3.0.8, and we've identified a very small patch (which I will attach to this issue) that _seems_ to fix it.  The problem appears to be that the logic that reads data from legacy sstables can pull range tombstones covering collection columns that weren't requested, which then breaks downstream logic that doesn't expect those tombstones to be present in the data.  The patch attempts to include those tombstones only if they're explicitly requested.  However, there's enough going on in that logic that it's not clear to us whether the change is safe, so it is definitely in need of review from someone knowledgable about what that area of the code is intended to do.",,jeromatron,kohlisankalp,mnantern,sklock,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 19:53;sklock;13109-3.0.txt;https://issues.apache.org/jira/secure/attachment/12846070/13109-3.0.txt",,,,,,,,,,,,,,,,,,,1.0,sklock,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 09:29:20 UTC 2017,,,,,,,,,,"0|i38ddz:",9223372036854775807,3.0.10,3.0.8,3.0.9,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"06/Jan/17 19:53;sklock;Attaching the patch.;;;","08/Feb/17 10:30;slebresne;Thanks for the report and reproduction steps. I _was_ able to reproduce this consistently with 3.0.10 but this actually happens to be fixed on the current 3.0 branch (future 3.0.11).

And the reason this is not a problem anymore is CASSANDRA-12694 and that's because it basically made us fetch all columns for CAS (much like we do for any other CQL query really), thus side-stepping the ""the code to read legacy sstable can return stuffs for non fetched columns"".

That said, that problem in {{LegacyLayout}} is kind of legit: we should include something that is not fetched and while we correctly skip non-fetched column for ""cells"", we did miss doing the same for collection tombsones. But I say ""kind of"" because thruth is, after CASSANDRA-12694, I'm not sure we can really run into this problem anymore: only thrift queries uses the mode where we don't fetch all columns, but thrift don't support collections so actually cannot create a query that would be problematic for this case.

Still, the code is definitively not doing what it should and the fix is trivial enough that it's not worth risking running into problems in the future, or if I happen to miss a genuine case where this could happen, so I've pushed the patch for CI below and will commit if tests are clear. I did modify said patch a bit because if the column for which we have a collection tombstone is not fetched, we also want to skip the few lines that were before the condition your patch added as otherwise we might end up returning an empty row which would break assumptions of the code. Still, mostly the same thing, just moving the condition a bit up.
| [13109-3.0|https://github.com/pcmanus/cassandra/commits/13109-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13109-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13109-3.0-dtest] |
| [13109-3.11|https://github.com/pcmanus/cassandra/commits/13109-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13109-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13109-3.11-dtest] |
;;;","09/Feb/17 09:29;slebresne;CI was clean so committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exeption stack traces not logged,CASSANDRA-13108,13032559,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,chbatey,chbatey,chbatey,06/Jan/17 14:29,16/Apr/19 09:30,14/Jul/23 05:56,22/Feb/17 16:21,2.1.18,2.2.10,3.0.12,3.11.0,,,Legacy/Core,,,,,0,,,,,"In a refactoring to parameterized logging we lost the stack traces of uncaught exceptions. This means, apart from the thread, I have no idea where they come from e.g.

{code}
ERROR [OptionalTasks:1] 2017-01-06 12:53:14,204 CassandraDaemon.java:231 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException: null
{code}",,chbatey,jjirsa,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,chbatey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 16:21:36 UTC 2017,,,,,,,,,,"0|i38cuf:",9223372036854775807,,,,,,,thobbs,,thobbs,,,Low,,,,,,,,,,,,,,,,,,,"06/Jan/17 14:34;chbatey;Trivial patch here: https://github.com/chbatey/cassandra-1/commit/fc81ff82fc96288336836d066d249b35edf44994.patch;;;","21/Feb/17 22:27;thobbs;Pending test runs on 2.1 (skipping tests on 2.2+ since the change is quite trivial):

||branch||testall||dtest||
|[CASSANDRA-13108|https://github.com/thobbs/cassandra/tree/CASSANDRA-13108]|[testall|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13108-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/thobbs/job/thobbs-CASSANDRA-13108-dtest]|;;;","22/Feb/17 16:21;thobbs;Test results look good, so I've committed this to 2.1 as {{3c2f87610de0f11071f3d5c005c1d14c06c832f8}} and merged up to 2.2, 3.0, 3.11, and trunk.  Thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect jvm metric names,CASSANDRA-13103,13032357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,arastoul,arastoul,arastoul,05/Jan/17 19:30,15/May/20 08:02,14/Jul/23 05:56,03/Apr/17 18:08,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,,Legacy/Observability,,,,,2,lhf,,,,"Some jvm metrics have a double dot in name like:
jvm.memory..total.max , jvm.memory..total.init (etc).
it seems that an extra dot is added at the end of the name in CassandraDaemon.java, around line 367 (in 3.0.10):
...
                // enable metrics provided by metrics-jvm.jar
                CassandraMetricsRegistry.Metrics.register(""jvm.buffers."", new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));
                CassandraMetricsRegistry.Metrics.register(""jvm.gc."", new GarbageCollectorMetricSet());
                CassandraMetricsRegistry.Metrics.register(""jvm.memory."", new MemoryUsageGaugeSet());

and also added in append method of MetricRegistry.
Call stack is:
MetricRegistry>>registerAll(String prefix, MetricSet metrics)
MetricRegistry>>static String name(String name, String... names)
MetricRegistry>>static void append(StringBuilder builder, String part)

and in append the dot is also added:
...
            if(builder.length() > 0) {
                builder.append('.');
            }
            builder.append(part);
...

The codahale MetricRegistry class seems to have no recent modification of name or append methods, so it look like a small bug.
May be the fix could be to simply not to add  the final dot in the metric name, ie  ""jvm.buffers""  instead of ""jvm.buffers.""
",,jjirsa,johnny15676,lvkr10,ostefano,spod,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/17 10:41;arastoul;Fix-incorrect-jvm-metric-names-CASSANDRA-13103.patch;https://issues.apache.org/jira/secure/attachment/12846220/Fix-incorrect-jvm-metric-names-CASSANDRA-13103.patch",,,,,,,,,,,,,,,,,,,1.0,arastoul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 03 18:08:26 UTC 2017,,,,,,,,,,"0|i38blj:",9223372036854775807,3.0.10,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"05/Jan/17 21:30;zznate;From ""weird jvm metrics"" thread on user ML, from OP this appeared somewhere between 3.5 and 3.9. ML Thread: 

https://lists.apache.org/thread.html/558bd7c1f9eff0be60939bae620e13dbbef3a819846e2c4735aa2cfa@%3Cuser.cassandra.apache.org%3E

;;;","08/Jan/17 10:53;arastoul;The problem has been introduced in 3.9, CASSANDRA-12312 : Add JVM metrics for custom metrics reporter, 2016-07-26.

It should be present in cassandra-2.2, cassandra-3.0, cassandra-3.11, cassandra-3.X, trunk

I have attached a patch for 3.9 (and 3.X) which removes the unnecessary '.' and added a unit test for jvm metrics registration.

I followed the 'How to contribute' wiki notes but was unable to assign myself the issue. 
May be an ASF member can tell me what to do or add me as assignable, in case I could work on other LHF?
TIA


;;;","29/Jan/17 16:29;lvkr10;Sorry I accidentally committed it.
The issue is still unresolved;;;","30/Jan/17 06:06;jjirsa;[~arastoul] - Ticket's assigned to you (was a bit ago), and I've reopened it after the accidental closing. We'll set it as patch-available, and hopefully a reviewer finds time to check it.

Thanks for the help!
;;;","06/Feb/17 13:03;spod;Thanks for the patch, [~arastoul]! This is a little strange. I remember that I had to add that single dot character while working on CASSANDRA-12312, or else the dot would have been absent between the two names. But I can't really reproduce this and the code and netcat clearly indicates otherwise. So I'd suggest to go with the proposed patch (2.2 upwards), as the code is clearly not working as intended now.

||2.2||3.0||trunk||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13103-2.2]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13103-3.0]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13103-trunk]|
|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-2.2-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-trunk-dtest/]|
|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-2.2-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13103-trunk-testall/]|

(please note that trunk contains extra code for unit test)
;;;","07/Feb/17 20:27;arastoul;Thank you for the review, Stefan. 
I checked the codahale metrics registry and the append method who adds the dot has been added
in mars 2013 : https://github.com/dropwizard/metrics/commit/0ae1fc0292fa2f7509ab7f237f6a752e316a0d86
so IMHO this patch should be useful for any recent codahale library version use.
;;;","03/Apr/17 18:08;spod;Merged as a79cc9840d8c5a400e9838e0a366bb371ff5d918;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian package does not install the hostpot_compiler command file,CASSANDRA-13092,13031690,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mshuler,wulczer,wulczer,03/Jan/17 16:29,15/May/20 08:06,14/Jul/23 05:56,10/Apr/17 14:15,2.1.18,2.2.10,3.0.13,3.11.0,4.0,4.0-alpha1,Packaging,,,,,0,debian,patch,,,"The default {{cassandra-env.sh}} file sets a JIT compiler commands file via {{-XX:CompileCommandFile=$CASSANDRA_CONF/hotspot_compiler}} but the Debian package does not install that file, even though it's generated during the build process.

Trivial patch against trunk attached.",,mshuler,wulczer,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/17 16:29;wulczer;install-jit-compiler-command-file-in-Debian-package.patch;https://issues.apache.org/jira/secure/attachment/12845403/install-jit-compiler-command-file-in-Debian-package.patch",,,,,,,,,,,,,,,,,,,1.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Apr 10 14:35:13 UTC 2017,,,,,,,,,,"0|i387hr:",9223372036854775807,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"10/Apr/17 02:09;zznate;This indeed looks like just an oversight ([~mshuler] batsignal for verification of that though). ;;;","10/Apr/17 14:15;mshuler;Committed {{3dfc78449a402c984d3aa43b1b4fc43d07f92b7e}} to cassandra-2.1 branch and merged up.

Thanks Jan!;;;","10/Apr/17 14:35;wulczer;Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Coalescing strategy sleeps too much,CASSANDRA-13090,13031613,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,iksaif,iksaif,iksaif,03/Jan/17 09:13,15/May/20 08:01,14/Jul/23 05:56,28/Feb/17 23:14,2.2.10,3.0.12,3.11.0,4.0,4.0-alpha1,,Legacy/Streaming and Messaging,,,,,0,,,,,"With the current code maybeSleep is called even if we managed to take maxItems out of the backlog. In this case we should really avoid sleeping because it means that backlog is building up.

I'll send a patch shortly.",,aweisberg,iksaif,jay.zhuang,jjordan,mshuler,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 16:19;iksaif;0001-Fix-wait-time-coalescing-CASSANDRA-13090-2.patch;https://issues.apache.org/jira/secure/attachment/12846370/0001-Fix-wait-time-coalescing-CASSANDRA-13090-2.patch","05/Jan/17 08:30;iksaif;0001-Fix-wait-time-coalescing-CASSANDRA-13090.patch;https://issues.apache.org/jira/secure/attachment/12845745/0001-Fix-wait-time-coalescing-CASSANDRA-13090.patch",,,,,,,,,,,,,,,,,,2.0,iksaif,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 23:13:53 UTC 2017,,,,,,,,,,"0|i3870n:",9223372036854775807,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"03/Jan/17 15:45;aweisberg;Nice find.

I don't think comparing now and limit is valid because limit is a duration and now is a relative time.

Can you add a unit test for this condition?

Does it even make sense to try and coalesce more than some constant number of messages? At some point we should just let the network handle packets. I think waiting for backlog full is the wrong condition to avoid these extra sleeps. Maybe coalesce at most 4 or 8 messages? Definitely make it tunable.;;;","03/Jan/17 16:09;iksaif;I didn't intend to change the behavior of parkLoop(), in theory it should do exactly the same as before. ""limit"" is just something that is almost like ""timer"" but good enough if we reach it. ""timer - (now = System.nanoTime()) > nanos / 16"" should be the same as ""now < limit"".

Network will not coalesce anything/much because we set TCP_NODELAY to 1.

;;;","03/Jan/17 16:22;aweisberg;Right but we don't need the network to coalesce for us since we are already doing it. But... there isn't a motivation to coalesce more than some number of messages. All it's going to do is add latency with minimal reduction in overhead.

I see I understood limit incorrectly. It might be clearer as now + nano / 16 and some parens for those of us a bit weaker at math.;;;","03/Jan/17 16:39;iksaif;Would you prefer count > 1 to start and then another patch to make it configurable (if necessary, I'll run benchmarks).

limit isn't now + nanos / 16, it's now + nanos - nanos / 16, (or ~= now + nanos * 0.9, maybe this last form is better ?);;;","03/Jan/17 16:46;aweisberg;Ouch. I am just not getting this right today.

I would be happy to have it as one patch. I think if we set it to 8 it will still be better than 128 in a heavy backlog , but it's also still safe in that it's way more coalescing than we could possibly benefit from.

I think tweaking the number has diminishing returns because you only hit this when the backlog grows and in that case you are already not looking at coalescing as the problem or the solution.;;;","05/Jan/17 08:30;iksaif;New version of the patch with discussed settings and unit tests.;;;","06/Jan/17 02:52;zznate;Since we are dropping some settings into {{cassandra.yaml}} for coalescing, it would be great if we can add some additional details in the comments about common variations (when you would do what and why). [~aweisberg] A couple of tidbits from your initial blog post would be cool. ;;;","06/Jan/17 08:34;iksaif;Sure I'll do that. I'll wait for Ariel answer's and additional comments in order to batch the changes :);;;","06/Jan/17 17:06;aweisberg;* I don't think {{otc_coalescing_max_coalesced_messages}} is useful although I'm interested in hearing the argument for it. When is pulling more than 128 items out of the LBQ going to improve performance?
* I think rather for the growing backlog scenario we should skip coalescing entirely and rely on back pressure from the socket to block the thread. So right now if coalescing says ""write stuff"" we should skip coalescing until the backlog has been drained into the socket. The strategy will need to know about the messages so it can log the arrival rate.
* Granted since the batch size is 128 it will still only add the undesired coalescing every 128 messages and only if the backlog is < {{otc_coaslescing_enough_coalesced_messages}}
* {{otc_coaslescing_enough_coalesced_messages}} in the yaml has a typo
* {{otc_coalescing_window_us}} has a typo in the comment
* Is there an extra drain to in {{coalesceInternal}}? One in the condition and another immediately after before checking for enough?;;;","09/Jan/17 16:19;iksaif;otc_coalescing_max_coalesced_messages was introduced because it could give more priority to the reader due to the way drainTo() is implemented (lock, iterate, unlock). But since the backlog is currently unbounded and all this code generally burst into flames under sustain load if backlog.size() > 100Mk, I agree that this may not be necessary for now. *Removed*

I think I also fixed the typo I found.

I harmonized movingaverage and timehorizon strategies to have a coalesceInternal() that look similar.;;;","09/Jan/17 16:50;aweisberg;* {{otc_coaslescing_enough_coalesced_messages}} is still misspelled in the YAML. This causes the server not to start.
* The get/setter in DatabaseDescriptor are also misspelled

I'll fix it while I run it through CassCI and star.

;;;","10/Jan/17 08:32;iksaif;Wow, I was sure I got rid of that one, thanks for the hotfix.;;;","27/Jan/17 22:41;aweisberg;Tests pass but I had to rebase because the merge order changed. Should be ready to commit soon.;;;","30/Jan/17 18:39;aweisberg;||code|utests|dtests||
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:cassandra-13090-2.2?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-2.2-testall/1]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-2.2-dtest/1]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...aweisberg:cassandra-13090-3.0?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-3.0-testall/5]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-3.0-dtest/6]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13090-3.11?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-3.11-testall/6]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-3.11-dtest/6]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13090-trunk?expand=1]|[utest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-trunk-testall/6]|[dtest|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-trunk-dtest/10]|

Seem to be a few test failures in 3.11 and on. Maybe I messed up the merge forward? [~iksaif] can you take a look?;;;","31/Jan/17 11:31;iksaif;All the unit tests failures seem really unrelated.
Also looks like there is a significant diff between your trunk and the original trunk.;;;","31/Jan/17 18:02;aweisberg;Can you be more specific, maybe link to what you are talking about? I looked at trunk and 3.11 and they look like the patch to me. I also check and they are based off recent trunk (last commit Jan 27).

I am talking about the dtests where 500 tests fails.

There is this in the logs
{noformat}
ERROR [MessagingService-Outgoing-/127.0.0.1-Gossip] 2017-01-27 22:19:28,638 OutboundTcpConnection.java:334 - error writing to /127.0.0.1
java.lang.NullPointerException: null
	at org.apache.cassandra.io.util.UnbufferedDataOutputStreamPlus.writeUTF(UnbufferedDataOutputStreamPlus.java:256) ~[main/:na]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUTF(BufferedDataOutputStreamPlus.java:304) ~[main/:na]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.serialize(GossipDigestSyn.java:86) ~[main/:na]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.serialize(GossipDigestSyn.java:81) ~[main/:na]
	at org.apache.cassandra.net.MessageOut.serialize(MessageOut.java:120) ~[main/:na]
	at org.apache.cassandra.net.OutboundTcpConnection.writeInternal(OutboundTcpConnection.java:351) [main/:na]
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:303) [main/:na]
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:237) [main/:na]
{noformat};;;","31/Jan/17 19:58;iksaif;What I mean by ""significant diff between your trunk and the original trunk.""

{code}
$ git fetch origin 
$ git fetch aweisberg 
$ git diff origin/trunk aweisberg/cassandra-13090-trunk --stat
...
585 files changed, 11085 insertions(+), 10474 deletions(-)
{code}

Am I missing anything that could explain this diff ? 
I'll also try to run the dtests on my workstation tomorrow based on your branch.;;;","31/Jan/17 20:20;aweisberg;I'm not sure why diff stat reports that, but if you actually do the merge you don't get the extra stuff.

I can reproduce what you are talking about though.

If you need help getting going with the dtests ping me on IRC. I find them to be non-trivial to get working.;;;","01/Feb/17 08:54;iksaif;I cloned both repositories, set the branch correctly and made a manual diff and I get the same thing. 
767d973bea8d1baaa8c0ec89316bbda147599bbd seems to be pretty big.

Maybe try to rebuild the 3.11 and 3.X branches ? I can also create them if necessary.;;;","06/Feb/17 17:15;aweisberg;Yes can you please try creating them. I did the merge and it doesn't do anything wonky and the compare view in GitHub shows what I expect. I'm not sure why git diff shows additional stuff that doesn't end up in the merge.

Generally the assignee is supposed to produce the branches ready to merge with the tests passing in Cassci. Talk to ptnapoleon in IRC to see if you can get set up to run your branches in cassci.;;;","08/Feb/17 16:30;iksaif;Created the branches, they look good. I'll look into cassci tomorrow.

FYI:
https://github.com/iksaif/cassandra/tree/cassandra-13090-3.0
https://github.com/iksaif/cassandra/tree/cassandra-13090-3.11
https://github.com/iksaif/cassandra/tree/cassandra-13090-trunk;;;","09/Feb/17 17:53;aweisberg;The trunk version didn't build. Your branches don't do the diff stat weirdness which is good.
https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13090-trunk-dtest/8/consoleText
{noformat}
build-test:
[javac] Compiling 502 source files to /home/automaton/cassandra/build/test/classes
[javac] /home/automaton/cassandra/test/unit/org/apache/cassandra/utils/CoalescingStrategiesTest.java:109: error: cannot find symbol
[javac]         DatabaseDescriptor.forceStaticInitialization();
[javac]                           ^
[javac]   symbol:   method forceStaticInitialization()
[javac]   location: class DatabaseDescriptor
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 1 error

BUILD FAILED
/home/automaton/cassandra/build.xml:1047: Compile failed; see the compiler error output for details.
{noformat};;;","09/Feb/17 18:47;iksaif;Sorry, did not build the tests for this branch apparently. Fixed. Pushed to the branch linked above.

FYI:
{code}
$ git diff
diff --git a/test/unit/org/apache/cassandra/utils/CoalescingStrategiesTest.java b/test/unit/org/apache/cassandra/utils/CoalescingStrategiesTest.java
index 26b6b3a..b10d70b 100644
--- a/test/unit/org/apache/cassandra/utils/CoalescingStrategiesTest.java
+++ b/test/unit/org/apache/cassandra/utils/CoalescingStrategiesTest.java
@@ -106,7 +106,7 @@ public class CoalescingStrategiesTest
     @BeforeClass
     public static void initDD()
     {
-        DatabaseDescriptor.forceStaticInitialization();
+        DatabaseDescriptor.daemonInitialization();
     }
 
     @SuppressWarnings({ ""serial"" })
{code};;;","10/Feb/17 03:12;aweisberg;Get set up with cassci anyways, but I rolled it again because I realized we should do this back to 2.2. 2.1 is critical fixes only and I don't think this counts. It's off by default in 2.1.;;;","10/Feb/17 09:46;iksaif;Yep, I'm setting up cassci :) Thanks for the help !;;;","10/Feb/17 22:26;aweisberg;+1;;;","13/Feb/17 20:06;aweisberg;Committed as [5725e2c422d21d8efe5ae3bc4389842939553650|https://github.com/apache/cassandra/commit/5725e2c422d21d8efe5ae3bc4389842939553650];;;","20/Feb/17 20:16;jjordan;The changes.txt here says ""shouldn't be enabled by default"", but I don't see a code change switching the default to DISABLED? And cassandra.yaml comments still say timehorizon is the default. So which should it be?;;;","20/Feb/17 20:18;aweisberg;It's a mistake. I'll need to update changes.txt. Originally I was going to disable it, but after discussion it seems like we didn't want to change out of the box performance in older releases.;;;","21/Feb/17 22:27;aweisberg;Fixed CHANGES.txt in [c6462d793b5736b7728c6f188b224dd9d3760bc6|https://github.com/apache/cassandra/commit/c6462d793b5736b7728c6f188b224dd9d3760bc6];;;","27/Feb/17 16:20;mshuler;The CHANGE.txt entry for this ticket is under the incorrect releases in the cassandra-2.2, cassandra-3.0, cassandra-3.11, and trunk branches. This was committed during release votes were -tentative tagged.  The JIRA fix versions for 2.2.x and 3.0.x should be corrected, as well, since this commit should be listed in CHANGES.txt under:
- 2.2.10 (currently under 2.2.9)
- 3.0.12 (currently under 3.0.11)
- 3.11.0 (currently under 3.10 in both cassandra-3.11 and trunk branches CHANGES.txt);;;","28/Feb/17 23:13;aweisberg;CHANGES.txt fixed in [3748bf7c2a135baab33ccd3b79db5f3fb9132995|https://github.com/apache/cassandra/tree/3748bf7c2a135baab33ccd3b79db5f3fb9132995];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
