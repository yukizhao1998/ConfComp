Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Negative value for 'Preferred Replica Imbalance' metric,KAFKA-13572,13419945,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,sahuja,sahuja,30/Dec/21 01:03,18/Jul/22 06:22,13/Jul/23 09:17,18/Jul/22 06:22,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,,,,,,,0,,,,,"A negative value (-822) for the metric - {{kafka_controller_kafkacontroller_preferredreplicaimbalancecount}} has been observed - please see the attached screenshot and the output below:

{code:java}
$ curl -s http://localhost:9101/metrics | fgrep 'kafka_controller_kafkacontroller_preferredreplicaimbalancecount'
# HELP kafka_controller_kafkacontroller_preferredreplicaimbalancecount Attribute exposed for management (kafka.controller<type=KafkaController, name=PreferredReplicaImbalanceCount><>Value)
# TYPE kafka_controller_kafkacontroller_preferredreplicaimbalancecount gauge
kafka_controller_kafkacontroller_preferredreplicaimbalancecount -822.0
{code}

The issue has appeared after an operation where the number of partitions for some topics were increased, and some topics were deleted/created in order to decrease the number of their partitions.

Ran the following command to check if there is/are any instance/s where the preferred leader (1st broker in the Replica list) is not the current Leader:
 
{code:java}
% grep "".*Topic:.*Partition:.*Leader:.*Replicas:.*Isr:.*Offline:.*"" kafka-topics_describe.out | awk '{print $6 "" "" $8}' | cut -d "","" -f1 | awk '{print $0, ($1==$2?_:""NOT"") ""MATCHED""}'|grep NOT | wc -l
     0
{code}

but could not find any such instances.

{{leader.imbalance.per.broker.percentage=2}} is set for all the brokers in the cluster which means that we are allowed to have an imbalance of up to 2% for preferred leaders. This seems to be a valid value, as such, this setting should not contribute towards a negative metric.

The metric seems to be getting subtracted in the code [here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerContext.scala#L474-L503] , however it is not clear when it can become -ve (i.e. subtracted more than added) in absence of any comments or debug/trace level logs in the code. However, one thing is for sure, you either have no imbalance (0) or have imbalance (> 0), it doesn’t make sense for the metric to be < 0. 

FWIW, no other anomalies besides this have been detected.

Considering these metrics get actively monitored, we should look at adding DEBUG/TRACE logging around the addition/subtraction of these metrics (and elsewhere where appropriate) to identify any potential issues.",,ocadaruma,sahuja,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/21 01:04;sahuja;kafka_negative_preferred-replica-imbalance-count_jmx_2.JPG;https://issues.apache.org/jira/secure/attachment/13038047/kafka_negative_preferred-replica-imbalance-count_jmx_2.JPG",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 14:47:23 UTC 2022,,,,,,,,,,"0|z0y4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 14:47;ocadaruma;We experienced similar phenomenon in our Kafka cluster and we found that following scenario can cause negative metric.

Let's say there are topic-A, topic-B.

 
 # Initiate topic deletion of topic-A
 ** TopicDeletionManager#enqueueTopicsForDeletion is called with argument Set(topic-A)
 *** [https://github.com/apache/kafka/blob/3.2.0/core/src/main/scala/kafka/controller/KafkaController.scala#L1771]
 # During topic-A's deletion procedure, topic-A's all partitions are marked as Offline (Leader = -1)
 ** [https://github.com/apache/kafka/blob/3.2.0/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala#L368]
 # Before topic-A's deletion procedure completes, initiate topic deletion of topic-B
 ** Since topic-A's ZK delete-topic node still exists, TopicDeletionManager#enqueueTopicsForDeletion is called with argument Set(topic-A, topic-B)
 ** ControllerContext#cleanPreferredReplicaImbalanceMetric is called for both topic-A, topic-B
 *** [https://github.com/apache/kafka/blob/3.2.0/core/src/main/scala/kafka/controller/ControllerContext.scala#L496]
 *** Since topic-A is now NoLeader, `!hasPreferredLeader(replicaAssignment, leadershipInfo)` evaluates to true, then `preferredReplicaImbalanceCount` is decremented unexpectedly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindCoordinatorFuture never get cleared in non-group mode( consumer#assign),KAFKA-13563,13419017,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,22/Dec/21 13:31,12/Sep/22 06:08,13/Jul/23 09:17,06/Feb/22 23:10,2.6.2,2.7.1,3.0.0,,,,,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,clients,,,,,0,new-consumer-threading-should-fix,,,,"In KAFKA-10793, we fix the race condition when lookup coordinator by clearing the _findCoordinatorFuture_ when handling the result, rather than in the listener callbacks. It works well under consumer group mode (i.e. Consumer#subscribe), but we found when user is using non consumer group mode (i.e. Consumer#assign) with group id provided (for offset commitment, so that there will be consumerCoordinator created), the _findCoordinatorFuture_ will never be cleared in some situations, and cause the offset committing keeps getting NOT_COORDINATOR error.

 

After KAFKA-10793, we clear the _findCoordinatorFuture_ in 2 places:
 # heartbeat thread
 # AbstractCoordinator#ensureCoordinatorReady

But in non consumer group mode with group id provided, there will be no (1)heartbeat thread , and it only call (2)AbstractCoordinator#ensureCoordinatorReady when 1st time consumer wants to fetch committed offset position. That is, after 2nd lookupCoordinator call, we have no chance to clear the _findCoordinatorFuture_ .

 

To avoid the race condition as KAFKA-10793 mentioned, it's not safe to clear the _findCoordinatorFuture_ in the future listener. So, I think we can fix this issue by calling AbstractCoordinator#ensureCoordinatorReady when coordinator unknown in non consumer group case, under each Consumer#poll.

 

Reproduce steps:
 
1. Start a 3 Broker cluster with a Topic having Replicas=3.
2. Start a Client with Producer and Consumer (with Consumer#assign(), not subscribe, and provide a group id) communicating over the Topic.
3. Stop the Broker that is acting as the Group Coordinator.
4. Observe successful Rediscovery of new Group Coordinator.
5. Restart the stopped Broker.
6. Stop the Broker that became the new Group Coordinator at step 4.
7. Observe ""Rediscovery will be attempted"" message but no ""Discovered group coordinator"" message.
 
 

 ",,guozhang,jaebinyo,jim_b_o,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10793,,,,,,,,,,,,,,"26/Dec/21 21:32;jim_b_o;kafka.zip;https://issues.apache.org/jira/secure/attachment/13037946/kafka.zip",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 12 04:42:05 UTC 2022,,,,,,,,,,"0|z0xymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/21 21:36;jim_b_o;[~showuon] I've attached a reproducer ({{{}kafka.zip{}}}).  It includes a {{docker-compose.yml}} that brings up a 3 node cluster and a {{Main}} class with Producer and Consumer.

P.S. The easiest way to find the current coordinator is to search the logs for `discovered`.;;;","27/Dec/21 02:33;showuon;Thank you [~jim_b_o] ! I'll check it today. Thanks.;;;","28/Dec/21 08:13;showuon;[~jim_b_o] , this issue is because of using `consumer#assign()`. It will be fine by using `consumer#subscribe`. I'll work on fix for this issue. Thank you for reporting and the reproduce steps.;;;","05/Jan/22 03:33;showuon;[~guozhang] [~ableegoldman], you might want to check this issue, and the WIP PR:  [https://github.com/apache/kafka/pull/11631] . Thank you.;;;","05/Jan/22 05:09;guozhang;Hi [~showuon] thanks for the report, I've reviewed the PR and left some comment.;;;","12/Sep/22 04:42;jaebinyo;Affected versions should include 2.6.2, which included the KAFKA-10793.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NioEchoServer fails to close resources,KAFKA-13558,13418680,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,20/Dec/21 18:44,08/Feb/22 21:44,13/Jul/23 09:17,08/Feb/22 21:44,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,"{{NioEchoServer}} does not close the selectors that it opens. The result of this can be manifested in flaky tests because the JVM/OS can run out of available file descriptors if the {{NioEchoServer}} is used in a lot of tests. Each test then leaks a handful of descriptors, and eventually the 'too many open files' error is thrown.

This was compounded because the {{NioEchoServer}} intentionally doesn't output stack traces, so the underlying issue was hidden and was manifest in odd (flaky) ways in tests.",,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-20 18:44:32.0,,,,,,,,,,"0|z0xwk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to dynamically change broker log levels on KRaft,KAFKA-13552,13418111,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,rndgstn,rndgstn,16/Dec/21 23:08,04/Feb/22 18:27,13/Jul/23 09:17,22/Jan/22 00:01,3.0.0,3.1.0,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,kraft,,,,,0,,,,,"It is currently not possible to dynamically change the log level in KRaft.  For example:

kafka-configs.sh --bootstrap-server <address> --alter --add-config ""kafka.server.ReplicaManager=DEBUG"" --entity-type broker-loggers --entity-name 0

Results in:

org.apache.kafka.common.errors.InvalidRequestException: Unexpected resource type BROKER_LOGGER.

The code to process this request is in ZkAdminManager.alterLogLevelConfigs().  This needs to be moved out of there, and the functionality has to be processed locally on the broker instead of being forwarded to the KRaft controller.

It is also an open question as to how we can dynamically alter log levels for a remote KRaft controller.  Connecting directly to it is one possible solution, but that may not be desirable since generally connecting directly to the controller is not necessary.  The ticket for this particular spect of the issue is https://issues.apache.org/jira/browse/KAFKA-13502",,dengziming,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 17 14:26:12 UTC 2021,,,,,,,,,,"0|z0xt1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/21 00:40;dengziming;hello [~rndgstn] , It seems this is a duplicated of KAFKA-13502.;;;","17/Dec/21 14:26;rndgstn;[~dengziming] Thanks for pointing that out.  Although there is not much in that ticket, it appears to address controller-only nodes, whereas this ticket indicates that no KRaft node (broker-only, controller-only, or combined broker+controller) supports dynamic changes to the log levels .  I updated the description of this ticket to point to that one since it is just one aspect of the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explicitly specifying default topic creation groups should not make connector fail,KAFKA-13546,13417517,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,venkatteki,venkatteki,venkatteki,14/Dec/21 18:00,18/Oct/22 21:29,13/Jul/23 09:17,18/Oct/22 21:29,2.6.0,2.6.1,2.6.2,2.6.3,2.7.0,2.7.1,2.7.2,2.8.0,2.8.1,,,,,,,,,,,,,,3.4.0,,,,,,,,KafkaConnect,,,,,0,,,,,"[KIP-158|https://cwiki.apache.org/confluence/display/KAFKA/KIP-158%3A+Kafka+Connect+should+allow+source+connectors+to+set+topic-specific+settings+for+new+topics] introduced support for Connect worker to allow source connector configurations to define topic creation settings.

A new source connector configuration {{topic.creation.groups}} was introduced, which takes a list of groups. 

*Expected behavior*

According to KIP-158, specifying value ""default"" in {{topic.creation.groups}} configration should throw a warning, but not let connector fail.

*Actual behavior*

Specifying ""default"" as one of the topic creation groups will make a connector fail",,ChrisEgerton,venkatteki,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 19 09:28:22 UTC 2021,,,,,,,,,,"0|z0xpds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/21 09:28;venkatteki;PR is at https://github.com/apache/kafka/pull/11615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock during shutting down kafka broker because of connectivity problem with zookeeper ,KAFKA-13544,13417321,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,joecqupt,andrei-vlg,andrei-vlg,14/Dec/21 08:47,23/Dec/21 20:17,13/Jul/23 09:17,18/Dec/21 00:35,2.8.1,,,,,,,,,,,,,,,,,,,,,,2.8.2,3.0.1,3.1.0,,,,,,core,,,,,1,,,,,"Hi team,

*Kafka version:* 2.8.1
*Configuration:* 3 kafka brokers in different availability zones and 3 zookeeper brokers in different availability zones.

I faced with deadlock in kafka. I've attached stack dump of the kafka state to this ticket. The locked threads are ""feature-zk-node-event-process-thread"" and ""kafka-shutdown-hook"".

*Context:*
My kafka cluster had connectivity problems with zookeeper and in the logs I saw the next exception:

The stacktrace:
{code:java}
[2021-12-06 18:31:14,629] WARN Unable to reconnect to ZooKeeper service, session 0x10000039563000f has expired (org.apache.zookeeper.ClientCnxn)
[2021-12-06 18:31:14,629] INFO Unable to reconnect to ZooKeeper service, session 0x10000039563000f has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2021-12-06 18:31:14,629] INFO EventThread shut down for session: 0x10000039563000f (org.apache.zookeeper.ClientCnxn)
[2021-12-06 18:31:14,631] INFO [ZooKeeperClient Kafka server] Session expired. (kafka.zookeeper.ZooKeeperClient)
[2021-12-06 18:31:14,632] ERROR [feature-zk-node-event-process-thread]: Failed to process feature ZK node change event. The broker will eventually exit. (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka.zookeeper.ZooKeeperClientExpiredException: Session expired either before or while waiting for connection
    at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:279)
    at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$1(ZooKeeperClient.scala:261)
    at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:261)
    at kafka.zk.KafkaZkClient.retryRequestsUntilConnected(KafkaZkClient.scala:1797)
    at kafka.zk.KafkaZkClient.retryRequestsUntilConnected(KafkaZkClient.scala:1767)
    at kafka.zk.KafkaZkClient.retryRequestUntilConnected(KafkaZkClient.scala:1762)
    at kafka.zk.KafkaZkClient.getDataAndStat(KafkaZkClient.scala:771)
    at kafka.zk.KafkaZkClient.getDataAndVersion(KafkaZkClient.scala:755)
    at kafka.server.FinalizedFeatureChangeListener$FeatureCacheUpdater.updateLatestOrThrow(FinalizedFeatureChangeListener.scala:74)
    at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.doWork(FinalizedFeatureChangeListener.scala:147)
    at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96) {code}
The exception is thrown in feature-zk-node-event-process-thread thread and it is catched in method FinalizedFeatureChangeListener.ChangeNotificationProcessorThread.doWork and then doWork method throws FatalExitError(1).
The FatalExitError catched in ShutdownableThread.run method and call Exit.exit(e.statusCode()) which calls System.exit under the hood.

The stackdump of ""feature-zk-node-event-process-thread"" thread:
{code:java}
""feature-zk-node-event-process-thread"" #23 prio=5 os_prio=0 cpu=163.19ms elapsed=1563046.32s tid=0x00007fd0dcdec800 nid=0x2088 in Object.wait()  [0x00007fd07e2c1000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(java.base@11.0.11/Native Method)
    - waiting on <no object reference available>
    at java.lang.Thread.join(java.base@11.0.11/Thread.java:1300)
    - waiting to re-lock in wait() <0x0000000088b9d3c8> (a org.apache.kafka.common.utils.KafkaThread)
    at java.lang.Thread.join(java.base@11.0.11/Thread.java:1375)
    at java.lang.ApplicationShutdownHooks.runHooks(java.base@11.0.11/ApplicationShutdownHooks.java:107)
    at java.lang.ApplicationShutdownHooks$1.run(java.base@11.0.11/ApplicationShutdownHooks.java:46)
    at java.lang.Shutdown.runHooks(java.base@11.0.11/Shutdown.java:130)
    at java.lang.Shutdown.exit(java.base@11.0.11/Shutdown.java:174)
    - locked <0x00000000806872f8> (a java.lang.Class for java.lang.Shutdown)
    at java.lang.Runtime.exit(java.base@11.0.11/Runtime.java:116)
    at java.lang.System.exit(java.base@11.0.11/System.java:1752)
    at org.apache.kafka.common.utils.Exit$2.execute(Exit.java:43)
    at org.apache.kafka.common.utils.Exit.exit(Exit.java:66)
    at kafka.utils.Exit$.exit(Exit.scala:28)
    at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:102) {code}
System.exit method before shutting down virtual machine calls shutdown hooks and spawns the thread ""kafka-shutdown-hook"".
The thread ""kafka-shutdown-hook"" calls kafka.server.KafkaServer.shutdown which during work calls
{code:java}
if (featureChangeListener != null)
        CoreUtils.swallow(featureChangeListener.close(), this) {code}

Where featureChangeListener.close is
{code:java}
def close(): Unit = {
    zkClient.unregisterStateChangeHandler(ZkStateChangeHandler.name)
    zkClient.unregisterZNodeChangeHandler(FeatureZNodeChangeHandler.path)
    queue.clear()
    thread.shutdown()
    thread.join()
  } {code}

It enteres in the deadlock on the line ""thread.join()"" because it waits that the thread ""feature-zk-node-event-process-thread"" should die but this thread spawned the thead ""kafka-shutdown-hook"" and can't die until the thread ""kafka-shutdown-hook"" is finished.

The stack dump of the thread ""kafka-shutdown-hook"":
{code:java}
""kafka-shutdown-hook"" #18 prio=5 os_prio=0 cpu=134.10ms elapsed=50906.28s tid=0x00007fd0ac15c800 nid=0x6d3bb in Object.wait()  [0x00007fd02dcba000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(java.base@11.0.11/Native Method)
    - waiting on <no object reference available>
    at java.lang.Thread.join(java.base@11.0.11/Thread.java:1300)
    - waiting to re-lock in wait() <0x0000000088973e98> (a kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
    at java.lang.Thread.join(java.base@11.0.11/Thread.java:1375)
    at kafka.server.FinalizedFeatureChangeListener.close(FinalizedFeatureChangeListener.scala:245)
    at kafka.server.KafkaServer.$anonfun$shutdown$23(KafkaServer.scala:717)
    at kafka.server.KafkaServer$$Lambda$1681/0x000000084082b040.apply$mcV$sp(Unknown Source)
    at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:68)
    at kafka.server.KafkaServer.shutdown(KafkaServer.scala:717)
    at kafka.Kafka$.$anonfun$main$3(Kafka.scala:100)
    at kafka.Kafka$$$Lambda$205/0x0000000840233840.apply$mcV$sp(Unknown Source)
    at kafka.utils.Exit$.$anonfun$addShutdownHook$1(Exit.scala:38)
    at kafka.Kafka$$$Lambda$206/0x0000000840233c40.run(Unknown Source)
    at java.lang.Thread.run(java.base@11.0.11/Thread.java:829){code}",,andrei-vlg,epikhinm,guozhang,joecqupt,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/21 08:41;andrei-vlg;kafka_broker_logs.log;https://issues.apache.org/jira/secure/attachment/13037454/kafka_broker_logs.log","14/Dec/21 08:41;andrei-vlg;kafka_broker_stackdump.txt;https://issues.apache.org/jira/secure/attachment/13037453/kafka_broker_stackdump.txt",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 23 08:53:05 UTC 2021,,,,,,,,,,"0|z0xo68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/21 18:20;joecqupt;i make a pull request to fix it. 

[#11607|https://github.com/apache/kafka/pull/11607]

 

it thank should remove operation: `thread.join()`  

ChangeNotificationProcessorThread extends ShutdownableThread, so when {{thread.shutdown()}} return it means that this thread has shutdownCompleted, it is unnecessary to call {{thread.join()}};;;","18/Dec/21 00:35;junrao;Merged the PR to trunk.;;;","21/Dec/21 06:46;andrei-vlg;Hello [~joecqupt] , [~junrao] 

Thank you for the quick fixing the issue.

Could you please also include this fix to version 2.8 ?

Automatic restarts of kafka broker don't work without this fix and I have to investigate every time the reason of freezing brokers and then, in case of this bug, kill the process of broker manually.;;;","22/Dec/21 19:55;junrao;[~andrei-vlg] : I merged the PR to 2.8 branch too.;;;","22/Dec/21 21:49;guozhang;Hi @joecqupt thanks for pinging me on slack, I've added you to the contributor list too so that you can assign yourself to tickets in the future. Thanks for your contribution!;;;","23/Dec/21 08:53;andrei-vlg;[~junrao] thank you very much!

Could you please clarify a bit when will be the release 2.8.2 ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve propagation and processing of SSL handshake failures,KAFKA-13539,13416998,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,13/Dec/21 10:52,14/Dec/21 10:02,13/Jul/23 09:17,14/Dec/21 10:02,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,security,,,,,0,,,,,"{color:#172b4d}When server fails SSL handshake and closes its connection, we attempt to report this to clients on a best-effort basis. However, our tests assume that peer always detects the failure. This may not be the case when there are delays. It will be good to improve reliability of handshake failure reporting. {color}",,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-13 10:52:15.0,,,,,,,,,,"0|z0xm6g:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft RegisterBroker should validate that the cluster ID matches,KAFKA-13528,13416403,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,cmccabe,cmccabe,09/Dec/21 22:21,08/Jan/22 03:27,13/Jul/23 09:17,08/Jan/22 03:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,cmccabe,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-09 22:21:16.0,,,,,,,,,,"0|z0xizk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add top-level error code field to DescribeLogDirsResponse,KAFKA-13527,13416263,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,09/Dec/21 10:29,01/Feb/22 17:57,13/Jul/23 09:17,01/Feb/22 17:57,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,Ticket for KIP-784: https://cwiki.apache.org/confluence/display/KAFKA/KIP-784%3A+Add+top-level+error+code+field+to+DescribeLogDirsResponse,,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-09 10:29:40.0,,,,,,,,,,"0|z0xi4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update gson dependency,KAFKA-13518,13416001,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,pavel-sbor,pavel-sbor,08/Dec/21 10:13,24/Oct/22 17:44,13/Jul/23 09:17,24/Oct/22 17:44,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.4.0,,,,,,,,core,,,,,0,security,,,,"*Describe the bug*
I checked kafka_2.13-3.0.0.tgz distribution with WhiteSource and find out that some libraries have vulnerabilities.
Here they are:
* gson-2.8.6.jar has WS-2021-0419 vulnerability. The way to fix it is to upgrade to com.google.code.gson:gson:2.8.9
* netty-codec-4.1.65.Final.jar has CVE-2021-37136 and CVE-2021-37137 vulnerabilities. The way to fix it is to upgrade to io.netty:netty-codec:4.1.68.Final

*To Reproduce*
Download kafka_2.13-3.0.0.tgz and find jars, listed above.
Check that these jars with corresponding versions are mentioned in corresponding vulnerability description.

*Expected behavior*

* gson upgraded to 2.8.9 or higher
* netty-codec upgraded to 4.1.68.Final or higher

*Actual behaviour*

* gson is 2.8.6
* netty-codec is 4.1.65.Final",,dongjin,pavel-sbor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 08 11:50:01 UTC 2021,,,,,,,,,,"0|z0xgig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/21 11:50;dongjin;The security problems on netty-codec were already fixed with KAFKA-13294. It will be shipped with AK 3.1.0 and 3.0.1.

In the case of gson, this problem is introduced by spotbugs 4.2.2. [spotbugs 4.5.0|https://mvnrepository.com/artifact/com.github.spotbugs/spotbugs/4.5.0] uses [gson 2.8.9|https://github.com/google/gson/releases/tag/gson-parent-2.8.9], which [resolves|https://github.com/google/gson/pull/1991] WS-2021-0419 vulnerability.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
topicIdsToNames and topicNamesToIds allocate unnecessary maps,KAFKA-13512,13415874,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jolshan,jolshan,jolshan,07/Dec/21 17:24,09/Dec/21 21:28,13/Jul/23 09:17,09/Dec/21 21:28,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"Currently we write the methods as follows:

{{def topicNamesToIds(): util.Map[String, Uuid] = {}}
{{    new util.HashMap(metadataSnapshot.topicIds.asJava)}}
{{}}}

We do not need to allocate a new map however, we can simply use

{{Collections.unmodifiableMap(metadataSnapshot.topicIds.asJava)}}

We can do something similar for the topicIdsToNames implementation.",,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-07 17:24:54.0,,,,,,,,,,"0|z0xfq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-769: Connect APIs to list all connector plugins and retrieve their configuration definitions,KAFKA-13510,13415790,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,07/Dec/21 10:35,23/Jan/23 15:58,13/Jul/23 09:17,03/Mar/22 13:29,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,,,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13442,,,,,,,,,,KAFKA-14645,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-12-07 10:35:05.0,,,,,,,,,,"0|z0xf7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlobalProcessor ignores user specified names,KAFKA-13507,13415353,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tamara_skokova,mjsax,mjsax,03/Dec/21 23:00,09/Dec/21 14:48,13/Jul/23 09:17,09/Dec/21 14:48,2.5.0,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,streams,,,,,0,beginner,newbie,,,"Using `StreamsBuilder.addGlobalStore` users can specify a name via `Consumed` parameter. However, the specified name is ignored and the created source and global processor get generated names assigned.",,bbejeck,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 09 14:48:18 UTC 2021,,,,,,,,,,"0|z0xciw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/21 14:48;bbejeck;resolved via https://github.com/apache/kafka/pull/11573;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some problems with KRaft createTopics and incrementalAlterConfigs,KAFKA-13490,13414601,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,30/Nov/21 19:10,07/Dec/21 00:54,13/Jul/23 09:17,06/Dec/21 00:34,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,kip-500,,,,"For CreateTopics, fix a bug where if one createTopics in a batch failed, they would all fail with the same error code. Make the error message for TOPIC_ALREADY_EXISTS consistent with the ZK-based code by including the topic name.

For IncrementalAlterConfigs, before we allow topic configurations to be set, we should check that they are valid. (This also applies to newly created topics.) IncrementalAlterConfigs should ignore non-null payloads for DELETE operations. Previously we would return an error in these cases. However, this is not compatible with the old ZK-based code, which ignores the payload in these cases.",,cmccabe,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-11-30 19:10:15.0,,,,,,,,,,"0|z0x7wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer fails to recover if topic gets deleted (and gets auto-created),KAFKA-13488,13414349,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,prat0318,prat0318,prat0318,29/Nov/21 16:18,16/Dec/21 15:58,13/Jul/23 09:17,16/Dec/21 15:01,2.2.2,2.3.1,2.4.1,2.5.1,2.6.3,2.7.2,2.8.1,,,,,,,,,,,,,,,,2.8.2,3.0.1,3.1.0,,,,,,producer ,,,,,0,,,,,"Producer currently fails to produce messages to a topic if the topic is deleted and gets auto-created OR is created manually during the lifetime of the producer (and certain other conditions are met - leaderEpochs of deleted topic > 0).

 

To reproduce, these are the steps which can be carried out:

0) A cluster with 2 brokers 0 and 1 with auto.topic.create=true.

1) Create a topic T with 2 partitions P0-> (0,1), P1-> (0,1)

2) Reassign the partitions such that P0-> (1,0), P1-> (1,0).

2) Create a producer P and send few messages which land on all the TPs of topic T.

3) Delete the topic T

4) Immediately, send a new message from producer P, this message will be failed to send and eventually timed out.

A test-case which fails with the above steps is added at the end as well as a patch file.

 

This happens after leaderEpoch (KIP-320) was introduced in the MetadataResponse KAFKA-7738. There is a solution attempted to fix this issue in KAFKA-12257, but the solution has a bug due to which the above use-case still fails.

 

*Issue in the solution of KAFKA-12257:*
{code:java}
// org.apache.kafka.clients.Metadata.handleMetadataResponse():
       ...
        Map<String, Uuid> topicIds = new HashMap<>();
        Map<String, Uuid> oldTopicIds = cache.topicIds();
        for (MetadataResponse.TopicMetadata metadata : metadataResponse.topicMetadata()) {
            String topicName = metadata.topic();
            Uuid topicId = metadata.topicId();
            topics.add(topicName);
            // We can only reason about topic ID changes when both IDs are valid, so keep oldId null unless the new metadata contains a topic ID
            Uuid oldTopicId = null;
            if (!Uuid.ZERO_UUID.equals(topicId)) {
                topicIds.put(topicName, topicId);
                oldTopicId = oldTopicIds.get(topicName);
            } else {
                 topicId = null;
            }
    ...
} {code}
With every new call to {{{}handleMetadataResponse{}}}(), {{cache.topicIds()}} gets created afresh. When a topic is deleted and created immediately soon afterwards (because of auto.create being true), producer's call to {{MetadataRequest}} for the deleted topic T will result in a {{UNKNOWN_TOPIC_OR_PARTITION}} or {{LEADER_NOT_AVAILABLE}} error {{MetadataResponse}} depending on which point of topic recreation metadata is being asked at. In the case of errors, TopicId returned back in the response is {{{}Uuid.ZERO_UUID{}}}. As seen in the above logic, if the topicId received is ZERO, the method removes the earlier topicId entry from the cache.

Now, when a non-Error Metadata Response does come back for the newly created topic T, it will have a non-ZERO topicId now but the leaderEpoch for the partitions will mostly be ZERO. This situation will lead to rejection of the new MetadataResponse if the older LeaderEpoch was >0 (for more details, refer to KAFKA-12257). Because of the rejection of the metadata, producer will never get to know the new Leader of the TPs of the newly created topic.

 

{{*}} 1. Solution / Fix (Preferred){*}:
Client's metadata should keep on remembering the old topicId till:
1) response for the TP has ERRORs
2) topicId entry was already present in the cache earlier
3) retain time is not expired
{code:java}
--- a/clients/src/main/java/org/apache/kafka/clients/Metadata.java
+++ b/clients/src/main/java/org/apache/kafka/clients/Metadata.java
@@ -336,6 +336,10 @@ public class Metadata implements Closeable {
                 topicIds.put(topicName, topicId);
                 oldTopicId = oldTopicIds.get(topicName);
             } else {
+                // Retain the old topicId for comparison with newer TopicId created later. This is only needed till retainMs
+                if (metadata.error() != Errors.NONE && oldTopicIds.get(topicName) != null && retainTopic(topicName, false, nowMs))
+                    topicIds.put(topicName, oldTopicIds.get(topicName));
+                else
                     topicId = null;
             }

{code}
{{*}} 2. Alternative Solution / Fix {{*}}:
To allow updates to LeaderEpoch when originalTopicId was {{{}null{}}}. This is less desirable as when cluster moves from no topic IDs to using topic IDs, we will count this topic as new and update LeaderEpoch irrespective of whether newEpoch was greater than current or not.
{code:java}
@@ -394,7 +398,7 @@ public class Metadata implements Closeable {
         if (hasReliableLeaderEpoch && partitionMetadata.leaderEpoch.isPresent()) {
             int newEpoch = partitionMetadata.leaderEpoch.get();
             Integer currentEpoch = lastSeenLeaderEpochs.get(tp);
-            if (topicId != null && oldTopicId != null && !topicId.equals(oldTopicId)) {
+            if (topicId != null && !topicId.equals(oldTopicId)) {
                 // If both topic IDs were valid and the topic ID changed, update the metadata
                 log.info(""Resetting the last seen epoch of partition {} to {} since the associated topicId changed from {} to {}"",
                          tp, newEpoch, oldTopicId, topicId);
{code}
From the above discussion, i think Solution 1 would be a better solution.

–
Testcase to repro the issue:
{code:java}
  @Test
  def testSendWithTopicDeletionMidWay(): Unit = {
    val numRecords = 10

    // create topic with leader as 0 for the 2 partitions.
    createTopic(topic, Map(0 -> Seq(0, 1), 1 -> Seq(0, 1)))

    val reassignment = Map(
      new TopicPartition(topic, 0) -> Seq(1, 0),
      new TopicPartition(topic, 1) -> Seq(1, 0)
    )

    // Change leader to 1 for both the partitions to increase leader Epoch from 0 -> 1
    zkClient.createPartitionReassignment(reassignment)
    TestUtils.waitUntilTrue(() => !zkClient.reassignPartitionsInProgress,
      ""failed to remove reassign partitions path after completion"")

    val producer = createProducer(brokerList, maxBlockMs = 5 * 1000L, deliveryTimeoutMs = 20 * 1000)

    (1 to numRecords).map { i =>
      val resp = producer.send(new ProducerRecord(topic, null, (""value"" + i).getBytes(StandardCharsets.UTF_8))).get
      assertEquals(topic, resp.topic())
    }

    // start topic deletion
    adminZkClient.deleteTopic(topic)

    // Verify that the topic is deleted when no metadata request comes in
    TestUtils.verifyTopicDeletion(zkClient, topic, 2, servers)
    
    // Producer would timeout and not self-recover after topic deletion.
    val e = assertThrows(classOf[ExecutionException], () => producer.send(new ProducerRecord(topic, null, (""value"").getBytes(StandardCharsets.UTF_8))).get)
    assertEquals(classOf[TimeoutException], e.getCause.getClass)
  }
{code}
Attaching the solution proposal and test repro as a patch file.",,dengziming,hachikuji,jolshan,prat0318,satish.duggana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 16:19;prat0318;KAFKA-13488.patch;https://issues.apache.org/jira/secure/attachment/13036767/KAFKA-13488.patch",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 16 15:01:21 UTC 2021,,,,,,,,,,"0|z0x6cg:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"29/Nov/21 16:22;prat0318;[~jolshan] [~hachikuji] Let me know your thoughts on the solution proposal (as you've worked on the prior [PR|https://github.com/apache/kafka/pull/10952] to this). I will create the PR with more test cases with the fix.;;;","29/Nov/21 17:29;jolshan;To clarify, did KAFKA-12257 make this issue worse, or just didn't fix it all the way? ;;;","29/Nov/21 17:56;prat0318;[~jolshan] KAFKA-12257 doesn't fix it all the way.;;;","29/Nov/21 20:33;hachikuji;[~prat0318] Thanks for the investigation and the patch. Would you mind opening a pull request to https://github.com/apache/kafka? In general, I think we probably need to be looser in the epoch check to allow for these kinds of cases. The consequence of taking stale metadata is much less severe than the potential for the client to get stuck. It is tempting to go as far as saying that we _only_ rely on the leader epoch check when the topicId matches.;;;","30/Nov/21 04:09;prat0318;For sake of simplicity, i would go with Solution 2 i.e. accept the new epochs in case of old topicId is null.;;;","30/Nov/21 07:19;prat0318;[~hachikuji] I have raised [https://github.com/apache/kafka/pull/11552] to fix this.;;;","09/Dec/21 18:49;prat0318;Hi [~hachikuji] [~jolshan] I have incorporated the review suggestions to the PR: [https://github.com/apache/kafka/pull/11552]. PTAL.;;;","16/Dec/21 15:01;prat0318;Fixed in https://github.com/apache/kafka/pull/11552;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stale Missing ISR on a partition after a zookeeper timeout,KAFKA-13483,13413905,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,fmethot,fmethot,25/Nov/21 18:53,17/Jun/22 03:06,13/Jul/23 09:17,17/Jun/22 03:06,2.8.1,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,replication,,,,,0,,,,,"We hit a situation where we had a Stale Missing ISR on a single partition on an output changelog topic after a ""broker to zookeeper"" connection timed out in our production system, This ticket shows the logs of what happened and a workaround that got us out of this situation.

 

*Cluster config*

7 Kafka Brokers v2.8.1  (k8s bitnami)

3 Zookeeper v3.6.2 (k8s bitnami)

kubernetes v1.20.6

 

*Processing pattern:*
{code:java}
source topic 
     -> KStream application: update 40 stores backed by 
           -> data-##-changelog topics {code}
 

All topics have {*}10 partitions{*}, {*}3 replicas{*}, *min.isr 2*

After a broker to zookeeper connection timeed out (see logs below) , lots of topic's partitions ISR went missing.
Almost all partition recovered a few milliseconds later, as the reconnection to zk re-established.

Except for partition number 3 of *one* of the 40 data-##-changelog topics
It stayed overnight under-replicated, preventing any progress to be done from the source topic's partition 3 of the kstream app. At the same time halting production of data for the 39 other changelog topic on partition 3 (because they also reply on partition 3 of the input topic)

+*Successfull Workaround*+
We ran kafka-reassign-partitions.sh on that partition, with the exact same replicas config, and the ISR came back normal, in a matter of milliseconds.
{code:java}
kafka-reassign-partitions.sh --bootstrap-server kafka.kafka.svc.cluster.local:9092 --reassignment-json-file ./replicas-data-23-changlog.json {code}
where replicas-data-23-changlog.json  contains that original ISR config
{code:java}
{""version"":1,""partitions"":[{""topic"":""data-23-changelog"",""partition"":3,""replicas"":[1007,1005,1003]}]} {code}
 

+*Questions:*+
Would you be able to provide an explanation why that specific partition did not recover like the others after the zk timeout  failure?

Or could it be a bug?

We are glad the workaround worked, but is there an explanation why it did?

Otherwise what should have been done to address this issue?

 

+*Observed summary of the logs*+

 
{code:java}
[2021-11-20 20:21:42,577] WARN Client session timed out, have not heard from server in 26677ms for sessionid 0x2000086f5260006 (org.apache.zookeeper.ClientCnxn)
[2021-11-20 20:21:42,582] INFO Client session timed out, have not heard from server in 26677ms for sessionid 0x2000086f5260006, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2021-11-20 20:21:44,644] INFO Opening socket connection to server zookeeper.kafka.svc.cluster.local
[2021-11-20 20:21:44,646] INFO Socket connection established, initiating session, client: , server: zookeeper.kafka.svc.cluster.local (org.apache.zookeeper.ClientCnxn)
[2021-11-20 20:21:44,649] INFO Session establishment complete on server zookeeper.kafka.svc.cluster.local, sessionid = 0x2000086f5260006, negotiated timeout = 40000 (org.apache.zookeeper.ClientCnxn)
 
[2021-11-20 20:21:57,133] INFO [ReplicaFetcher replicaId=1007, leaderId=1001, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:21:57,137] INFO [ReplicaFetcher replicaId=1007, leaderId=1001, fetcherId=0] Error sending fetch request (sessionId=1896541533, epoch=50199) to node 1001: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Client was shutdown before response was read
        at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
        at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
        at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:217)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:325)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:141)
        at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:140)
        at scala.Option.foreach(Option.scala:407)
        at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:140)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:123)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2021-11-20 20:21:57,141] INFO [ReplicaFetcher replicaId=1007, leaderId=1001, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:21:57,141] INFO [ReplicaFetcher replicaId=1007, leaderId=1001, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:21:57,145] INFO [ReplicaFetcher replicaId=1007, leaderId=1005, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:21:57,145] INFO [ReplicaFetcher replicaId=1007, leaderId=1005, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:21:57,146] INFO [ReplicaFetcher replicaId=1007, leaderId=1005, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)
[2021-11-20 20:22:24,577] WARN [Partition data-23-changelog-3 broker=1007] Failed to update ISR to PendingExpandIsr(isr=Set(1007), newInSyncReplicaId=1003) due to unexpected UNKNOWN_SERVER_ERROR. Retrying. (kafka.cluster.Partition)
[2021-11-20 20:22:24,578] ERROR [broker-1007-to-controller] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient)
java.lang.IllegalStateException: Failed to enqueue ISR change state LeaderAndIsr(leader=1007, leaderEpoch=2, isr=List(1007, 1003), zkVersion=2) for partition data-23-changelog-3
        at kafka.cluster.Partition.sendAlterIsrRequest(Partition.scala:1379)
        at kafka.cluster.Partition.$anonfun$handleAlterIsrResponse$1(Partition.scala:1413)
        at kafka.cluster.Partition.handleAlterIsrResponse(Partition.scala:1392)
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1(Partition.scala:1370)
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1$adapted(Partition.scala:1370)
        at kafka.server.DefaultAlterIsrManager.$anonfun$handleAlterIsrResponse$8(AlterIsrManager.scala:262)
        at kafka.server.DefaultAlterIsrManager.$anonfun$handleAlterIsrResponse$8$adapted(AlterIsrManager.scala:259)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at kafka.server.DefaultAlterIsrManager.handleAlterIsrResponse(AlterIsrManager.scala:259)
        at kafka.server.DefaultAlterIsrManager$$anon$1.onComplete(AlterIsrManager.scala:179)
        at kafka.server.BrokerToControllerRequestThread.handleResponse(BrokerToControllerChannelManager.scala:362)
        at kafka.server.BrokerToControllerRequestThread.$anonfun$generateRequests$1(BrokerToControllerChannelManager.scala:333)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:584)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:576)
        at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:74)
        at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManager.scala:368)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96){code}
 At this point we ran kafka-reassign-partitions.sh on that partition, with the exact same config, and it fixed it.
{code:java}
kafka-reassign-partitions.sh --bootstrap-server kafka.kafka.svc.cluster.local:9092 --reassignment-json-file ./replicas-data-23-changlog.json 

where replicas-data-23-changlog.json  contains

{""version"":1,""partitions"":[{""topic"":""data-23-changelog"",""partition"":3,""replicas"":[1007,1005,1003]}]} {code}
 

 

Log showing ISR is back to normal:
{code:java}
[2021-11-21 12:36:36,727] INFO [Admin Manager on Broker 1007]: Updating broker 1007 with new configuration :  (kafka.server.ZkAdminManager)
[2021-11-21 12:36:36,747] INFO Processing notification(s) to /config/changes (kafka.common.ZkNodeChangeNotificationListener)
[2021-11-21 12:36:36,749] INFO Processing override for entityPath: brokers/1005 with config: Map() (kafka.server.DynamicConfigManager)
[2021-11-21 12:36:36,752] INFO Processing override for entityPath: brokers/1007 with config: Map() (kafka.server.DynamicConfigManager)
...
[2021-11-21 12:37:19,435] INFO [Partition data-23-changelog-3 broker=1007] ISR updated to 1007,1005 and version updated to [4] (kafka.cluster.Partition)
[2021-11-21 12:37:19,928] INFO [Partition data-23-changelog-3 broker=1007] ISR updated to 1007,1005,1003 and version updated to [5] (kafka.cluster.Partition) {code}
 

 

Thank you kindly for any comments and suggestions!

Francois",,alivshits,epikhinm,fmethot,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13720,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 17 03:06:03 UTC 2022,,,,,,,,,,"0|z0x3ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/21 22:19;alivshits;Looks like a bug: the handleAlterIsrResponse tries to re-send the update before removing from unsentUpdates and that throws an exception because the update is already present in unsentUpdates.  The broker may not retry the operation for a while.

The workaround you've found should work well.

The bug seems to be introduced as part of the original implementation of KIP-497.;;;","17/Jun/22 03:06;showuon;Fixed indirectly in v3.1.0 and later. Check KAFKA-13720 for more info. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams crashes when non Base64 Offset Metadata is found,KAFKA-13476,13413655,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,RBosch81,RBosch81,RBosch81,24/Nov/21 14:45,06/Jan/22 05:38,13/Jul/23 09:17,06/Jan/22 05:38,2.4.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,streams,,,,,0,,,,,"Kafka Streams applications use the metadata stored with the committed offsets from previous running instances to extract timestamps.

But when the metadata field contains other data the Base64 decoder will throw an exception causing the Streams application to fail.
A new Offset commit is then required to stop this failure.

I've included the part of the log when we started a Kafka Streams app after setting the offsets using a third party tool. This tool adds some tracing metadata so developers and operators could debug who performed this custom offset commit.

 
{noformat}
2021-11-16 12:56:36.020  INFO 25 --- [-StreamThread-2] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=example-app-3, groupId=axual-demo-example-example-app] Unsubscribed all topics or patterns and assigned partitions
	at java.base/java.util.Base64$Decoder.decode(Unknown Source) ~[na:na]
	at org.apache.kafka.streams.processor.internals.StreamTask.decodeTimestamp(StreamTask.java:1039) ~[kafka-streams-2.7.0.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:553) ~[kafka-streams-2.7.0.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamTask.initializeTaskTime(StreamTask.java:837) ~[kafka-streams-2.7.0.jar:na]
java.lang.IllegalArgumentException: Illegal base64 character 7b
	at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:728) ~[kafka-streams-2.7.0.jar:na]
	at org.apache.kafka.streams.processor.internals.StreamTask.initializeMetadata(StreamTask.java:818) ~[kafka-streams-2.7.0.jar:na]
2021-11-16 12:56:36.127 ERROR 25 --- [-StreamThread-1] org.apache.kafka.streams.KafkaStreams    : stream-client [streams-example-app-1] All stream threads have died. The instance will be in error state and should be closed.
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:553) ~[kafka-streams-2.7.0.jar:na]
java.lang.IllegalArgumentException: Illegal base64 character 7b
{noformat}
I recommend adding a Try Catch block around the Base64 decode in the StreamTask.decodeTimestamp method and return the Unknown value when this occurs.
This is pure for resilience when bad data is encountered.
After the Streams application performs a new offset commit the error should not occur again, limiting the change of frequently occurring warnings in the logs

I've already made the changes and added a test for this issue, as I would like to contribute to Kafka.",,guozhang,mjsax,RBosch81,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 17 02:03:06 UTC 2021,,,,,,,,,,"0|z0x228:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/21 18:10;mjsax;[~RBosch81] Thanks for reporting this issue. What I am wondering: where could the corrupted metadata come from? Only Kafka Streams should commit offsets for the use group (ie, `application.id`)?;;;","06/Dec/21 19:28;guozhang;Hi [~RBosch81] thanks for reporting. I'm also wondering what's the case when non base64 metadata was written in the first place, do you have any ideas?;;;","13/Dec/21 10:23;RBosch81;Hello [~mjsax] [~guozhang] , This is triggered by a tool that can be used to set offsets for a consumer group to make sure that applications can start somewhere else than beginning or end of a topic.

Tracing data was added to metadata because previous releases of the Kafka Client and Streams did not use the metadata part of the OffsetAndMetadata structure.;;;","13/Dec/21 21:26;guozhang;Thanks [~RBosch81], that clears my questions then. I saw [~mjsax] is already on your PR so I'll leave it to him to review and merge it.;;;","17/Dec/21 02:03;mjsax;{quote}Tracing data was added to metadata because previous releases of the Kafka Client and Streams did not use the metadata part of the OffsetAndMetadata structure.
{quote}
Thanks for the details. I'll follow up on the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression in dynamic update of broker certificate,KAFKA-13474,13413510,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,divijvaidya,Shipenkov,Shipenkov,24/Nov/21 03:32,12/Jul/22 06:33,13/Jul/23 09:17,09/Jul/22 10:12,2.7.0,2.7.2,2.8.1,3.0.0,3.1.0,3.1.1,3.2.0,3.2.1,,,,,,,,,,,,,,,3.2.1,3.3.0,,,,,,,core,,,,,0,,,,,"h1. Problem

It seems, after updating listener SSL certificate with dynamic broker configuration update, old certificate is somehow still used for broker client SSL factory. Because of this broker fails to create new connection to controller after old certificate expires.
h1. History

Back in KAFKA-8336 there was an issue, when client-side SSL factory wasn't updating certificate, when it was changed with dynamic configuration. That bug have been fixed in version 2.3 and I can confirm, that dynamic update worked for us with kafka 2.4. But now we have updated clusters to 2.7 and see this (or at least similar) problem again.
h1. Affected versions

First we've seen this on confluent 6.1.2, which (I think) based on kafka 2.7.0. Then I tried vanilla versions 2.7.0 and 2.7.2 and can reproduce problem on them just fine
h1. How to reproduce
 * Have zookeeper somewhere (in my example it will be ""10.88.0.21:2181"").
 * Get vanilla version 2.7.2 (or 2.7.0) from [https://kafka.apache.org/downloads] .
 * Make basic broker config like this (don't forget to actually create log.dirs):
{code:none}
broker.id=1

listeners=SSL://:9092
advertised.listeners=SSL://localhost:9092

log.dirs=/tmp/broker1/data

zookeeper.connect=10.88.0.21:2181

security.inter.broker.protocol=SSL
ssl.protocol=TLSv1.2
ssl.client.auth=required
ssl.endpoint.identification.algorithm=
ssl.keystore.type=PKCS12
ssl.keystore.location=/tmp/broker1/secrets/broker1.keystore.p12
ssl.keystore.password=changeme1
ssl.key.password=changeme1
ssl.truststore.type=PKCS12
ssl.truststore.location=/tmp/broker1/secrets/truststore.p12
ssl.truststore.password=changeme
{code}
(I use here TLS 1.2 just so I can see client certificate in TLS handshake in traffic dump, you will get same error with default TLS 1.3 too)
 ** Repeat this config for another 2 brokers, changing id, listener port and certificate accordingly.
 * Make basic client config (I use for it one of brokers' certificates):
{code:none}
security.protocol=SSL
ssl.key.password=changeme1
ssl.keystore.type=PKCS12
ssl.keystore.location=/tmp/broker1/secrets/broker1.keystore.p12
ssl.keystore.password=changeme1
ssl.truststore.type=PKCS12
ssl.truststore.location=/tmp/broker1/secrets/truststore.p12
ssl.truststore.password=changeme
ssl.endpoint.identification.algorithm=
{code}
 * Create usual local self-signed PKI for test
 ** generate self-signed CA certificate and private key. Place certificate in truststore.
 ** create keys for broker certificates and create requests from them as usual (I'll use here same subject for all brokers)
 ** create 2 certificates as usual
{code:bash}
openssl x509 \
       -req -CAcreateserial -days 1 \
       -CA ca/ca-cert.pem -CAkey ca/ca-key.pem \
       -in broker1.csr -out broker1.crt
{code}
 ** Use ""faketime"" utility to make third certificate expire soon:
{code:bash}
# date here is some point yesterday, so certificate will expire like 10-15 minutes from now
faketime ""2021-11-23 10:15"" openssl x509 \
       -req -CAcreateserial -days 1 \
       -CA ca/ca-cert.pem -CAkey ca/ca-key.pem \
       -in broker2.csr -out broker2.crt
{code}
 ** create keystores from certificates and place them according to broker configs from earlier
 * Run 3 brokers with your configs like
{code:bash}
./bin/kafka-server-start.sh server2.properties
{code}
(I start it here without daemon mode to see logs right on terminal - just use ""tmux"" or something to run 3 brokers simultaneously)
 ** you can check that one broker certificate will expire soon with
{code:bash}
openssl s_client -connect localhost:9093 </dev/null | openssl x509 -noout -text | grep -A2 Valid
{code}
 * Issue new certificate to replace one, which will expire soon, place it in keystore and replace old keystore with it.
 * Use dynamic configuration to make broker re-read keystore:
{code:bash}
./bin/kafka-configs --command-config ssl.properties --bootstrap-server localhost:9092 --entity-type brokers --entity-name ""2"" --alter --add-config ""listener.name.SSL.ssl.keystore.location=/tmp/broker2/secrets/broker2.keystore.p12""
{code}
 ** You can check that broker now has new certificate on its listener with same command
{code:bash}
openssl s_client -connect localhost:9093 </dev/null | openssl x509 -noout -text | grep -A2 Valid
{code}
 * Wait until that old certificate expires and make some changes, which provoke broker to make new controller connection. For example if I have controller on broker ""1"" and expired certificate was on broker ""2"", then I restart broker ""3"".
 * On broker with expired certificate you will see in log something like
{code:none}
INFO [broker-2-to-controller-send-thread]: Recorded new controller, from now on will use broker 1 (kafka.server.BrokerToControllerRequestThread)
INFO [broker-2-to-controller] Failed authentication with localhost/127.0.0.1 (SSL handshake failed) (org.apache.kafka.common.network.Selector)
ERROR [broker-2-to-controller] Connection to node 1 (localhost/127.0.0.1:9092) failed authentication due to: SSL handshake failed (org.apache.kafka.clients.NetworkClient)
ERROR [broker-2-to-controller-send-thread]: Failed to send the following request due to authentication error: ClientRequest(expectResponse=true, callback=kafka.server.BrokerToControllerRequestThread$$Lambda$996/0x0000000801724c40@4d3e77ce, destination=1, correlationId=626, clientId=2, createdTimeMs=1637718291682, requestBuilder=AlterIsrRequestData(brokerId=2, brokerEpoch=293, topics=<some topic topology> kafka.server.BrokerToControllerRequestThread)
{code}
and controller log will show something like
{code:none}
INFO [SocketServer brokerId=1] Failed authentication with /127.0.0.1 (SSL handshake failed) (org.apache.kafka.common.network.Selector)
{code}
and if broker with expired and changed certificate was controller itself, then it even could not connect to itself.
 * If you make traffic dump (and you use TLS 1.2 or less) then you will see that broker client connection tries to use old certificate in TLS handshake.

Here is example of traffic dump, when broker with expired and dynamically changed certificate is current controller, so it can't connect to itself: [^failed-controller-single-session-20211119.pcap.gz] 
In this example you will see that ""Server"" use new certificate and ""Client"" use old certificate, but it's same broker!",,cadonna,divijvaidya,ijuma,Shipenkov,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/21 03:32;Shipenkov;failed-controller-single-session-20211119.pcap.gz;https://issues.apache.org/jira/secure/attachment/13036511/failed-controller-single-session-20211119.pcap.gz",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 18:12:11 UTC 2022,,,,,,,,,,"0|z0x15s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/21 03:28;Shipenkov;Tried to reproduce problem on kafka 2.8.1. And problem is still there: I've updated certificate, I can see that listener use new certificate, but it still uses old certificate for client connections and when certificate expires, this broker can't connect to others, for example connection to controller get errors like
{code:none}
INFO [broker-2-to-controller] Failed authentication with localhost/127.0.0.1 (SSL handshake failed) (org.apache.kafka.common.network.Selector)

ERROR [broker-2-to-controller] Connection to node 1 (localhost/127.0.0.1:9092) failed authentication due to: SSL handshake failed (org.apache.kafka.clients.NetworkClient)

ERROR [broker-2-to-controller-send-thread]: Failed to send the following request due to authentication error: ClientRequest(expectResponse=true, callback=kafka.server.BrokerToControllerRequestThread$$Lambda$1289/0x00000008017d8440@28eb9d3c, destination=1, correlationId=7078, clientId=2, createdTimeMs=1638414992079, requestBuilder=AlterIsrRequestData(brokerId=2, brokerEpoch=1025, topics=[TopicData(name='amadeus-pnr', partitions=[long list of partitions])]) failed due to authentication error with controller (kafka.server.BrokerToControllerRequestThread)
org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failed
Caused by: javax.net.ssl.SSLProtocolException: Unexpected handshake message: server_hello
	at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:129)
	at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:307)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:254)
	at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:437)
	at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1074)
	at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1061)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:689)
	at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1008)
	at org.apache.kafka.common.network.SslTransportLayer.runDelegatedTasks(SslTransportLayer.java:430)
	at org.apache.kafka.common.network.SslTransportLayer.handshakeUnwrap(SslTransportLayer.java:514)
	at org.apache.kafka.common.network.SslTransportLayer.doHandshake(SslTransportLayer.java:368)
	at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:291)
	at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:178)
	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:543)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
	at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:74)
	at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManager.scala:368)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{code}
and packet capture shows old client certificate in TLS handshake.
I will add 2.8.1 to list of affected versions.;;;","02/Dec/21 06:43;Shipenkov;Well, I tried version 3.0.0 and I can reproduce this problem on it just fine. Same ""SSL handshake failed"" error, still can see old client certificate in traffic dump.
Guess I'll just add this to affected version too.;;;","13/Feb/22 09:26;ijuma;Thanks for the report. We should triage this before the next release.;;;","04/Apr/22 17:52;cadonna;Moving to the next release since code freeze for 3.2.0 has passed.;;;","30/Jun/22 16:54;divijvaidya;I have been able to reproduce this via a test in Kafka code. I assigned this ticket to myself and am working on filing a PR with the fix.;;;","05/Jul/22 18:12;divijvaidya;PR for fixing this problem is ready for review: [https://github.com/apache/kafka/pull/12381]

[~ijuma] [~showuon] , perhaps you folks would be interested to review this? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect can lose track of last committed offsets for topic partitions after partial consumer revocation,KAFKA-13472,13413254,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,22/Nov/21 21:37,29/Nov/21 20:18,13/Jul/23 09:17,29/Nov/21 19:45,3.0.1,3.1.0,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,KafkaConnect,,,,,0,,,,,"The Connect framework tracks the last successfully-committed offsets for each topic partition that is currently assigned to the consumer of each sink task. If a sink task throws an exception from {{{}SinkTask::preCommit{}}}, the consumer is ""rewound"" by seeking to those last successfully-committed offsets for each topic partition, so that the same records can be redelivered to the task again.

With the changes from KAFKA-12487, we failed to correctly update the logic for tracking these last-committed offsets which can cause topic partitions to be missing from them after partial revocation of topic partitions from the consumer. Specifically, we make the assumption that, whenever an offset commit succeeds, the offsets that were successfully committed constitute the entirely of the last-committed offsets for the task; when a partial consumer revocation takes place, we only commit offsets for some of the topic partitions assigned to the task's producer, and this assumption fails.",,ChrisEgerton,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12487,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 29 19:45:17 UTC 2021,,,,,,,,,,"0|z0wzkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/21 14:46;ChrisEgerton;Approved as a 3.1 blocker on the [3.1 release thread|https://mail-archives.apache.org/mod_mbox/kafka-dev/202111.mbox/%3CCAHn4u3tPba%2BOQuyawfg%2BmrfyAMnZYFBOrmzruZNGsdJy%2BrBJXg%40mail.gmail.com%3E].;;;","29/Nov/21 19:45;kkonstantine;Merged to `trunk` and cherry-picked to 3.1 and 3.0 branches. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
End-of-life offset commit for source task can take place before all records are flushed,KAFKA-13469,13413206,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,22/Nov/21 16:02,30/Nov/21 17:54,13/Jul/23 09:17,30/Nov/21 17:54,3.0.1,3.1.0,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,3.2.0,,,,,,KafkaConnect,,,,,0,,,,,"When we fixed KAFKA-12226, we made offset commits for source tasks take place without blocking for any in-flight records to be acknowledged. While a task is running, this change should yield significant benefits in some cases and allow us to continue to commit offsets even when a topic partition on the broker is unavailable or the producer is unable to send records to Kafka as quickly as they are produced by the task.

However, this becomes problematic when a task is scheduled for shutdown with in-flight records. During shutdown, the latest committable offsets are calculated, and then flushed to the offset backing store (in distributed mode, this is the offsets topic). During that flush, the task's producer may continue to send records to Kafka, but their offsets will not be committed, which causes these records to be redelivered if/when the task is restarted.

Essentially, duplicate records are now possible even in healthy source tasks.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12226,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 30 17:54:51 UTC 2021,,,,,,,,,,"0|z0wza8:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"23/Nov/21 14:46;ChrisEgerton;Approved as a 3.1 blocker on the [3.1 release thread|https://mail-archives.apache.org/mod_mbox/kafka-dev/202111.mbox/%3CCAHn4u3tPba%2BOQuyawfg%2BmrfyAMnZYFBOrmzruZNGsdJy%2BrBJXg%40mail.gmail.com%3E].;;;","30/Nov/21 17:54;rhauch;Merged to the following branches:
* `trunk` for the next 3.2 release
* `3.1` for the upcoming 3.1.0 release
* `3.0` for the next 3.0.1 patch release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
delete unused config batch.size in kafka-console-producer.sh,KAFKA-13466,13412767,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,youngping,youngping,19/Nov/21 12:09,07/Mar/22 06:32,13/Jul/23 09:17,05/Mar/22 08:13,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,core,,,,,0,,,,," 

official docs:

!image-2021-11-19-04-05-15-754.png!

 

shell scripts is:

!image-2021-11-19-04-09-28-299.png!


in fact, the batch-size config is already not used everywhere in the code anymore, so delete this and may not have a misunderstanding in the future
",,youngping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12392,,,,,,,,,,,,,,,"19/Nov/21 12:05;youngping;image-2021-11-19-04-05-15-754.png;https://issues.apache.org/jira/secure/attachment/13036349/image-2021-11-19-04-05-15-754.png","19/Nov/21 12:09;youngping;image-2021-11-19-04-09-28-299.png;https://issues.apache.org/jira/secure/attachment/13036348/image-2021-11-19-04-09-28-299.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-11-19 12:09:53.0,,,,,,,,,,"0|z0wwkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaController stops functioning as active controller after ZooKeeperClient auth failure,KAFKA-13461,13412353,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,vincent81jiang,vincent81jiang,17/Nov/21 20:25,24/Feb/22 22:13,13/Jul/23 09:17,02/Dec/21 22:27,,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,zkclient,,,,,0,,,,,"When java.security.auth.login.config is present, but there is no ""Client"" section,  ZookeeperSaslClient creation fails and raises LoginExcpetion, result in warning log:
{code:java}
WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '***'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.{code}
When this happens after initial startup, ClientCnxn enqueues an AuthFailed event which will trigger following sequence:
 # zkclient reinitialization is triggered
 # the controller resigns.
 # Before the controller's ZK session expires, the controller successfully connect to ZK and maintains the current session
 # In KafkaController.elect(), the controller sets activeControllerId to itself and short-circuits the rest of the elect. Since the controller resigned earlier and also skips the call to onControllerFailover(), the controller is not actually functioning as the active controller (e.g. the necessary ZK watchers haven't been registered).

 ",,fvaleri,junrao,vincent81jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 24 22:13:51 UTC 2022,,,,,,,,,,"0|z0wu0w:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"24/Feb/22 22:13;junrao;Basically, when there is no JAAS configured for ZK client and the ZK client tries to establish a new connection, the client will first receive an AUTH_FAIL event. However, this doesn't mean that the ZK client's session is gone since the client will retry the connection without auth, which typically succeeds. Previously, we mistakenly try to reinitialize the controller with the AUTH_FAIL event, which causes the controller to resign but not regain the controllership since the underlying session is still valid.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SocketChannel in Acceptor#accept is not closed upon IOException,KAFKA-13457,13411927,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,functioner,functioner,16/Nov/21 01:14,22/Mar/22 12:47,13/Jul/23 09:17,24/Nov/21 08:31,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,network,,,,,0,,,,,"When the kafka.network.Acceptor in SocketServer.scala accepts a new connection in the `accept` function, it handles the `TooManyConnectionsException` and `ConnectionThrottledException`. However, the socketChannel operations (line 720 or 721 or 722) within the try block may potentially throw an IOException as well, which is not handled.

 
{code:java}
//core/src/main/scala/kafka/network/SocketServer.scala
// Acceptor class
  private def accept(key: SelectionKey): Option[SocketChannel] = {
    val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]
    val socketChannel = serverSocketChannel.accept()     // line 717
    try {
      connectionQuotas.inc(endPoint.listenerName, socketChannel.socket.getInetAddress, blockedPercentMeter)
      socketChannel.configureBlocking(false)             // line 720
      socketChannel.socket().setTcpNoDelay(true)         // line 721
      socketChannel.socket().setKeepAlive(true)          // line 722
      if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
        socketChannel.socket().setSendBufferSize(sendBufferSize)
      Some(socketChannel)
    } catch {
      case e: TooManyConnectionsException =>       
        info(s""Rejected connection from ${e.ip}, address already has the configured maximum of ${e.count} connections."")
        close(endPoint.listenerName, socketChannel)
        None
      case e: ConnectionThrottledException => 
        val ip = socketChannel.socket.getInetAddress
        debug(s""Delaying closing of connection from $ip for ${e.throttleTimeMs} ms"")
        val endThrottleTimeMs = e.startThrottleTimeMs + e.throttleTimeMs
        throttledSockets += DelayedCloseSocket(socketChannel, endThrottleTimeMs)
        None
    }
  }
{code}
This thrown IOException is caught in the caller `acceptNewConnections` in line 706, which only prints an error message. The socketChannel that throws this IOException is not closed.

 
{code:java}
//core/src/main/scala/kafka/network/SocketServer.scala
  private def acceptNewConnections(): Unit = {
    val ready = nioSelector.select(500)
    if (ready > 0) {
      val keys = nioSelector.selectedKeys()
      val iter = keys.iterator()
      while (iter.hasNext && isRunning) {
        try {
          val key = iter.next
          iter.remove()          if (key.isAcceptable) {
            accept(key).foreach { socketChannel => 
                ...
              } while (!assignNewConnection(socketChannel, processor, retriesLeft == 0))
            }
          } else
            throw new IllegalStateException(""Unrecognized key state for acceptor thread."")
        } catch {
          case e: Throwable => error(""Error while accepting connection"", e)   // line 706
        }
      }
    }
  }
{code}
We found during testing this would cause our Kafka clients to experience errors (InvalidReplicationFactorException) for 40+ seconds when creating new topics. After 40 seconds, the clients would be able to create new topics successfully.

We check that after adding the socketChannel.close() upon IOException, the symptoms will disappear, so the clients do not need to wait for 40s to be working again.

 

 ",,dajac,functioner,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-18024,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 22 12:47:34 UTC 2022,,,,,,,,,,"0|z0wrew:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"22/Mar/22 11:54;mimaison;[~dajac] I see you merged https://github.com/apache/kafka/pull/11504 a while back. Can we now close this issue? or is there more work to do?;;;","22/Mar/22 12:34;dajac;[~mimaison] Done.;;;","22/Mar/22 12:47;mimaison;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tighten KRaft config checks/constraints,KAFKA-13456,13411846,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,15/Nov/21 16:33,12/Dec/21 06:05,13/Jul/23 09:17,10/Dec/21 21:29,2.8.0,3.0.0,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,kraft,,,,,0,,,,,"We need to tighten the configuration constraints/checks related to KRaft configs because the current checks do not eliminate illegal configuration combinations.  Specifically, we need to add the following constraints:

* controller.listener.names is required to be empty for the non-KRaft (i.e. ZooKeeper) case. A ZooKeeper-based cluster that sets this config will fail to restart until this config is removed.  This generally should not be occurring -- nobody should be setting KRaft-specific configs in a ZooKeeper-based cluster -- but we currently do not prevent it from happening.
* There must be no advertised listeners when running just a KRaft controller (i.e. when process.roles=controller). This means neither listeners nor advertised.listeners (if the latter is explicitly defined) can contain a listener that does not also appear in controller.listener.names.
* When running a KRaft broker (i.e. when process.roles=broker or process.roles=broker,controller), advertised listeners must not include any listeners appearing in controller.listener.names.
* When running a KRaft controller (i.e. when process.roles=controller or process.roles=broker,controller) controller.listener.names must be non-empty and every one must appear in listeners
* When running just a KRaft broker (i.e. when process.roles=broker) controller.listener.names must be non-empty and none of them can appear in listeners. This is currently checked indirectly, but the indirect checks do not catch all cases.  We will check directly.
* When running just a KRaft broker we log a warning if more than one entry appears in controller.listener.names because only the first entry is used.

In addition to the above additional constraints, we should also map the CONTROLLER listener name to the PLAINTEXT security protocol by default when using KRaft -- this would be a very helpful convenience.
",,iekpo,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-11-15 16:33:53.0,,,,,,,,,,"0|z0wqww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Producer Client Callback behaviour does not align with Javadoc,KAFKA-13448,13411145,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,pnee,seamus.oceanainn,seamus.oceanainn,10/Nov/21 22:19,26/Apr/22 18:05,13/Jul/23 09:17,26/Apr/22 18:05,1.1.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,clients,,,,,0,,,,,"In PR [#4188|https://github.com/apache/kafka/pull/4188] as part of KAFKA-6180, a breaking change was accidentally introduced in the behaviour of Callbacks for the producer client.

Previously, whenever an exception was thrown when producing an event, the value for 'metadata' passed to the Callback.onCompletion method was always null. In PR [#4188|https://github.com/apache/kafka/pull/4188], in one part of the code where Callback.onCompletion is called, the behaviour was changed so that instead of passing a null value for metadata, a 'placeholder' value was provided instead (see [here|https://github.com/apache/kafka/pull/4188/files#diff-42d8f5166459ee28f201ff9cec0080fc7845544a0089ac9e8f3e16864cc1193eR1196] and [here|https://github.com/apache/kafka/pull/4188/files#diff-42d8f5166459ee28f201ff9cec0080fc7845544a0089ac9e8f3e16864cc1193eR1199]).  This placeholder contained only topic and partition information, and with all other fields set to '-1'.

This change only impacted a subset of exceptions, so that in the case of ApiExceptions metadata would still be null (see [here|https://github.com/apache/kafka/commit/aa42a11dfd99ee9ab24d2e9a7521ef1c97ae1ff4#diff-42d8f5166459ee28f201ff9cec0080fc7845544a0089ac9e8f3e16864cc1193eR843]), but for all other exceptions the placeholder value would be used. The behaviour at the time of writing remains the same.

This issue was first reported in KAFKA-7412 when a user first noticed the difference between the documented behaviour of Callback.onCompletion and the implemented behaviour.

At the time it was assumed that the behaviour when errors occur was to always provide a placeholder metadata value to Callback.onCompletion, and the documentation was updated at that time to reflect this assumption in [PR #5798|https://github.com/apache/kafka/pull/5798]. The documentation now states that when an exception occurs that the method will be called with an empty metadata value (see [here|https://github.com/apache/kafka/blob/3.1/clients/src/main/java/org/apache/kafka/clients/producer/Callback.java#L30-L31]). However, there is still one case where Callback.onCompletion is called with a null value for metadata (see [here|https://github.com/apache/kafka/blob/3.1/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1002]), so there is still a discrepancy between the documented behaviour and the implementation of Callback.onCompletion.",,junrao,seamus.oceanainn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12841,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 26 18:05:55 UTC 2022,,,,,,,,,,"0|z0wml4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/22 18:05;junrao;merged [https://github.com/apache/kafka/pull/11689] and a followup fix [https://github.com/apache/kafka/pull/12064] to trunk and 3.2 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JWT access token from logs,KAFKA-13446,13411129,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kirktrue,kirktrue,kirktrue,10/Nov/21 19:19,29/Jun/23 21:07,13/Jul/23 09:17,15/Nov/21 09:10,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,logging,security,,,,0,OAuth,,,,"The OAuth code logs the access token on both the client and the server, potentially exposing service account details. Remove all logging entries to prevent this from leaking.",,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-11-10 19:19:01.0,,,,,,,,,,"0|z0wmhk:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka broker exits when OAuth enabled and certain configuration not specified,KAFKA-13443,13411124,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,kirktrue,kirktrue,kirktrue,10/Nov/21 19:06,29/Jun/23 21:07,13/Jul/23 09:17,17/Nov/21 10:19,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,config,security,,,,0,beginner,OAuth,,,"The {{sasl.oauthbearer.jwks.endpoint.retry.backoff.ms}} and {{sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms}} configuration options were added to the {{SaslConfig}} class but their default values were not added to {{{}KafkaConfig{}}}. As a result, when the OAuth validation feature is enabled in the broker and those two configuration values aren't explicitly provided by the user, the broker exits.

The fix is to define them in the {{KafkaConfig}} class.",,kirktrue,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-11-10 19:06:12.0,,,,,,,,,,"0|z0wmgg:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Omitted BrokerTopicMetrics metrics in the documentation,KAFKA-13436,13410448,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dongjin,dongjin,dongjin,07/Nov/21 06:57,11/Jul/22 15:41,13/Jul/23 09:17,13/Jun/22 14:46,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,documentation,,,,,0,,,,,"As of present, there are 18 'kafka.server:type=BrokerTopicMetrics' but, only 13 of them are described in the documentation.

The omitted metrics are:
 * kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec
 * kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec
 * kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec
 * kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec
 * kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec",,dongjin,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 11 15:41:57 UTC 2022,,,,,,,,,,"0|z0wiao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/22 09:12;githubbot;dongjinleekr opened a new pull request, #417:
URL: https://github.com/apache/kafka-site/pull/417

   Do we need to apply this into 3.1, 3.0, or even 2.8 documentations? :thinking:


;;;","10/Jul/22 06:59;githubbot;dongjinleekr commented on PR #417:
URL: https://github.com/apache/kafka-site/pull/417#issuecomment-1179669841

   @mimaison @ableegoldman @dajac Could you kindly have a look? This is continuation of [this PR](https://github.com/apache/kafka/pull/11473).


;;;","11/Jul/22 15:41;githubbot;mimaison merged PR #417:
URL: https://github.com/apache/kafka-site/pull/417


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static membership protocol should let the leader skip assignment (KIP-814),KAFKA-13435,13410313,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dajac,rleslie,rleslie,05/Nov/21 18:59,20/Oct/22 00:20,13/Jul/23 09:17,14/Feb/22 10:56,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,consumer,,,,,0,new-rebalance-should-fix,,,,"When using consumer groups with static membership, if the consumer marked as leader has restarted, then metadata changes such as partition increase are not triggering expected rebalances.

To reproduce this issue, simply:
 # Create a static consumer subscribed to a single topic
 # Close the consumer and create a new one with the same group instance id
 # Increase partitions for the topic
 # Observe that no rebalance occurs and the new partitions are not assigned

I have only tested this in 2.7, but it may apply to newer versions as well.
h3. Analysis

In {_}ConsumerCoordinator{_}, one responsibility of the leader consumer is to track metadata and trigger a rebalance if there are changes such as new partitions added:

[https://github.com/apache/kafka/blob/43bcc5682da82a602a4c0a000dc7433d0507b450/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L793]
{code:java}
if (assignmentSnapshot != null && !assignmentSnapshot.matches(metadataSnapshot)) {
    ...
    requestRejoinIfNecessary(reason);
    return true;
}
{code}
Note that _assignmentSnapshot_ is currently only set if the consumer is the leader:

[https://github.com/apache/kafka/blob/43bcc5682da82a602a4c0a000dc7433d0507b450/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L353]
{code:java}
// Only the leader is responsible for monitoring for metadata changes (i.e. partition changes)
if (!isLeader)
    assignmentSnapshot = null;
{code}
And _isLeader_ is only true after an assignment is performed during a rebalance:

[https://github.com/apache/kafka/blob/43bcc5682da82a602a4c0a000dc7433d0507b450/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L634]

That is, when a consumer group forms, exactly one consumer in the group should have _isLeader == True_ and be responsible for triggering rebalances on metadata changes.

However, in the case of static membership, if the leader has been restarted and rejoined the group, the group essentially no longer has a current leader. Even though the metadata changes are fetched, no rebalance will be triggered. That is, _isLeader_ will be false for all members.

This issue does not resolve until after an actual group change that causes a proper rebalance. In order to safely make a partition increase when using static membership, consumers must be stopped and have timed out, or forcibly removed with {_}AdminClient.removeMembersFromConsumerGroup(){_}.

Correcting this in the client probably also requires help from the broker. Currently, when a static consumer that is leader is restarted, the coordinator does recognize the change:

e.g. leader _bbfcb930-61a3-4d21-945c-85f4576490ff_ was restarted
{noformat}
[2021-11-04 13:53:13,487] INFO [GroupCoordinator 4]: Static member Some(1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0) of group ryan_test with unknown member id rejoins, assigning new member id 1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0-af88ecf2-
6ebf-47da-95ef-c54fef17ab74, while old member id 1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0-bbfcb930-61a3-4d21-945c-85f4576490ff will be removed. (
kafka.coordinator.group.GroupCoordinator){noformat}
However, it does not attempt to update the leader id since this isn't a new rebalance, and JOIN_GROUP will continue returning the now stale member id as leader:
{noformat}
2021-11-04 13:53:13,490 DEBUG o.a.k.c.c.i.AbstractCoordinator [Consumer instanceId=1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0, clientId=1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0, groupId=ryan_test] Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=40, protocolType='consumer', protocolName='range', leader='1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0-bbfcb930-61a3-4d21-945c-85f4576490ff', memberId='1GK7DRJPHZ0LRV91Y4D3SYHS5928XHXJQ6263GT26V5P70QX0-af88ecf2-6ebf-47da-95ef-c54fef17ab74', members=[]){noformat}
This means that it's not easy for any particular restarted member to identify that it should consider itself leader and handle metadata changes.

There is reference to the difficulty of leader restarts in KAFKA-7728 but the focus seemed mainly on avoiding needless rebalances for static members. That goal was accomplished, but this issue seems to be a side effect of both not rebalancing AND not having the rejoined member reclaim its leadership status.

Also, I have not verified if it's strictly related or valid, but noticed this ticket has been opened too: KAFKA-12759.",,dajac,guozhang,hachikuji,rleslie,twmb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12759,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 20 00:20:15 UTC 2022,,,,,,,,,,"0|z0whl4:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"08/Nov/21 23:41;guozhang;Hello [~rleslie], thanks for your reported issue. I looked through the code and I agree with you that this is a bug on the broker side --- unfortunately, since it is broker-side we need to fix it and upgrade the broker version in order to avoid hitting that again --- would you like to submit a PR that fixes on the broker? I think the fix should be inside {{updateStaticMemberAndRebalance}}, where we update the {{group.leader}} based on the oldMemberId.;;;","08/Nov/21 23:42;guozhang;Updating the affected version since this bug still exists in trunk today.;;;","11/Nov/21 01:06;rleslie;[~guozhang] Thanks for taking a look at this. You mentioned the broker change, but is any strategy also needed on the client side? I think with static membership we probably still don't want to trigger a full rebalance even if the leader rejoins. If I'm interpreting it correctly, it looks like {{ConsumerCoordinator}} current only sets {{isLeader / assignmentSnapshot}} if there is an actual assignment performed. And without that, metadata changes won't trigger a subsequent rebalance. With static membership, if the broker is now changed to return the new {{leaderId}}, is it still safe or useful for {{performAssignment()}} to actually occur?

[https://github.com/apache/kafka/blob/43bcc5682da82a602a4c0a000dc7433d0507b450/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L597]

[https://github.com/apache/kafka/blob/43bcc5682da82a602a4c0a000dc7433d0507b450/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L694]

That is, could there be any side effects if a static member simply rejoining invokes its assignor and sends back a new assignment in {{SyncGroup}}, one that also possibly differs from the existing assignment not meant to be updated?;;;","17/Nov/21 08:06;dajac;[~rleslie] Thanks for reporting this bug. I have assigned the Jira to me and I plan to fix this in the coming weeks.;;;","18/Nov/21 19:18;guozhang;[~rleslie] today only the brokers can determine if a join-group request would trigger a rebalance or not. In this specific case, if a join-group request is sent with a known instance id and its corresponding old member id is considered the leader, the broker's coordinator can still assign a new member id to it, set the leader to the new member id; but whether or not it would set the Leader ID in the response depends on whether there are any metadata change that really do need a rebalance. If there's no need for a rebalance, it would not set the leader id at all --- i.e. only the broker knows who's the current leader, but members do not know. I think this is okay since members do not try to remember who's the current leader anyways.

[~dajac] please let me know if you've have a PR ready.;;;","18/Nov/21 23:17;rleslie;[~dajac] Thank you for picking this up quickly.

[~guozhang] Thanks for reviewing again. I think as {{ConsumerCoordinator}} is currently written, at least one member needs to know its the leader and record the assignment. Otherwise {{assignmentSnapshot}} is not set and rebalance won't trigger when the metadata is refreshed with new partitions added. But I understand there may be another way to address this. Let's wait for David and see how the PR ends up looking.;;;","18/Nov/21 23:35;guozhang;[~rleslie] Sorry I misunderstood your scenario, I thought that the member was not newly created and hence would still have its {{assignmentSnapshot}} locally, now that I re-read the first section of the JIRA description I think I get you now.

In this case, at the moment I feel we cannot avoid doing an unnecessary full rebalance which triggers perform-assignment.. but in the near future when we improve the rebalance protocol, I think it would be better to not having the client to remember whether itself is the leader or not, instead, anyone with the full subscription information should be able to do the assignment, and for anyone (including the leader) to re-join the group, it should be the broker's responsibility to decide if a rebalance needs to be triggered or not.;;;","18/Nov/21 23:54;rleslie;[~guozhang] Awesome, glad we are more in sync now. :)

I think it would be tough for most users of static membership to pick up a new Kafka version that suddenly triggers rebalance on leader rejoin, since this was avoided in the original implementation. This ticket is an edge case, after all. My thought was that if {{JoinResponse}} can at least inform the client that it is the returning leader, then it can still have opportunity to check metadata for changes compared to its subscription and trigger the rebalance. It may not be a perfect solution, and it's still some careful refactoring. Hoping David has some ideas around this too. I agree with you it would be ideal if the broker controlled these decisions, but that does sound like an even bigger change.;;;","30/Nov/21 13:55;dajac;I have looked into this issue and I do agree with [~guozhang]. It seems that the best way to proceed is to trigger a full rebalance when the leader rejoins the group. [~rleslie] The issue with your proposal is that the returning leader does not know the so called `assignmentSnapshot` anymore therefore it cannot decide if a rebalance is required or not. `assignmentSnapshot` is assigned when the leader run the assignment locally. Moreover, comparing the subscription is not sufficient in this particular case. The subscription only contains the topic and it does not change here.

I do agree that it is a bit unfortunate that we have to do a full rebalance in this case but I don't see any other solutions with the current protocol. I will prepare a PR going in this direction and we can discuss further in the PR if necessary.;;;","30/Nov/21 15:56;rleslie;[~dajac] Triggering a rebalance whenever the leader rejoins is a backward incompatible change to the documented behavior of static membership. I maintain a system that strongly depends on the current behavior, and it would be impacted negatively by this. Would much prefer to have to manually restart consumers when adding partitions only than have that rebalance occur regularly just in case. If we are going to trigger a rebalance at all times it should at least be optional behavior based on a property so that users can opt out. I certainly would. And in that case this behavior change should require a KIP. Let's not rush the decision here.;;;","01/Dec/21 09:36;dajac;[~rleslie] I understand your point. Manually restarting consumers when adding partitions is not even enough because the returning leader is not able to detect new partitions until a rebalance has taken place. This is required for two reasons: 1) to let it know that it is leader and 2) to let it rebuilt its internal `assignmentSnapshot` state. The leader needs both in order to be able to trigger a rebalance when the metadata has changed. However, you could force a rebalance with `Consumer#enforceRebalance` when you add partitions.

I would like to better understand your case. Could you elaborate a bit more on why having a single rebalance is such an issue? I suppose that the members would retain their assignments so the disruption should be minimal (I am not sure if this is true for all assignors though).

Alternatively, we could extend the group coordinator (on the broker side) to monitor the new partitions and trigger a rebalance when required. This is a bit tricky in the current implementation because the group coordinator does not parse the assignments. I am not sure if we want to go down that route at the moment.
;;;","01/Dec/21 15:45;dajac;I have put a bit more thoughts into this and it seems that we could (partially) resolve this issue with the following PR: https://github.com/apache/kafka/pull/11559. The main idea is to return to the re-joining leaders all the subscriptions such that it can re-compute the assignment and initialize its state. As the group is Stable, the new assignment is ignored by the group coordinator. This allow the leader to detect future metadata changes and trigger a rebalance if needed.

However, it does not work if the new partitions were created while the leader was down because the new leader will directly have the newest metadata with the new partitions when it starts up. To circumvent this, the group coordinator could compare the new compute assignment with the current one and trigger a rebalance if they differ. This would rely on the assumption that the assignor is deterministic to avoid triggering an unnecessary rebalance. I am not sure if this is always true.

What do you guys think?;;;","01/Dec/21 16:57;rleslie;[~dajac] Yes, I was going to suggest something similar to this. The broker knows that the re-joining member is the leader based on its instance id, and can inform it of such in the JoinGroupResponse. And SyncGroup is also already responding with assignment data. So putting that together can possibly allow the client to rebuild enough state to continue monitoring metadata changes. You are completely right that this won't handle partitions being added in between the leader exiting and coming back. However, that is usually not meant to be a long time period or else it will time out or stall progress in the consumer group.

Though it is a very clever idea, I'm not 100% about taking it further to have the assignment actually recomputed instead of trying to have the broker return it. Users might define their own assignors and we can't be sure if they are side-effect free or not, regardless of whether or not they are deterministic. Rebuilding state in the consumer without invoking the assignor may require a separate code path in the client-side coordinator, however. Perhaps others can weigh in on this trade-off too.

This bug is already quite rare, and AFAIK only reported once since the inception of static membership / internal.leave.group.on.close. It may be better to start with a 95% solution than a perfect one involving more frequent rebalancing or much more invasive changes that could lead to newer bugs.

And sorry, when I said restart consumers above, I meant not only to close them but call _AdminClient.removeMembersFromConsumerGroup()_ as well so they are new members when coming back up.

To answer your other question, the main purpose of static membership is obviously to avoid rebalances when restarting, and we have to assume those who explicitly enable are very interested in this for one reason or another. Any solution that changes that guarantee and involves rebalancing every time a particular member restarts is a pretty serious negative and goes against the documented behavior. I feel it would have to be a very serious bug to make that decision, not an edge case like this where we can be patient and explore workarounds.

You are right that triggering the rebalance should never break any application, but it can result in the kind of slow down that users aim to avoid. In our case, we happen to have some processing decoupled from the actual Kafka consumer loop and rebalances can occur asynchronous of processing. Because of this we are not always able to wait and commit all processed offsets prior to a rebalance which leads to duplicate messages being processed afterwards. That is why we use static membership, to help reduce that and limit duplicates to only certain, and more rare, situations. I haven't gone into full detail, but I hope that clarifies the concern a bit more.;;;","02/Dec/21 16:24;dajac;Yeah, I do agree that this solution does not fly if we can't assume that the assignor is deterministic and idempotent. I think that it is OK for internal assignors but as you said we don't know about external ones.

I have been playing with an alternative approach. If we summarise, the issue is that the re-joining leader does not know that it is the leader because the group coordinator does not inform it on purpose to avoid doing an assignment. As a side effect, the leader is not longer able to trigger a rebalance when the metadata is updated (e.g. partitions added) and only the leader is able to do this at the moment. What if we relax this constraint?

For the subscription, every member of the group does the check locally but only the leader known by the group coordinator can trigger a rebalance in the end. We can argue that we are wasting a bit of resources because all the non-leaders are doing it for nothing in the end. I wonder if we could do the same in our case for the metadata. All the members would check metadata changes and would try to trigger a rebalance. However, only the leader known by the group coordinator would succeed.

Looking at all the options, that seems to be a reasonable approach which is not too disruptive. ;;;","03/Dec/21 01:00;rleslie;That's an interesting idea. Each member of the group is already refreshing its metadata, anyway. It's probably not much additional overhead given that metadata changes are rare. However, the current check relies on comparing the cached assignment for the group which only the leader has after the assignor is invoked. Is the plan to change this so that each member just compares the last two values of metadataSnapshot instead?;;;","03/Dec/21 13:01;dajac;I don't have the implementation details yet but the idea would be to keep the metadata snapshot used to join the group and use it to trigger a rebalance when the metadata is updated. I need to workout the details. Before I do so, I'd like to get a consensus on the approach with [~guozhang] and [~hachikuji].;;;","06/Dec/21 16:21;dajac;Let me summarise the options that we have discussed so far:

1. The group coordinator could trigger a rebalance when the static leader re-joins the group. The major advantage of this solution is that it would fix the issue for all client versions. The downside is that it would trigger a rebalance every time that the leader re-joins the group and that goes a bit against the initial goal of the static members.

2. The group coordinator could parse the assignments, listen to partition changes and trigger rebalances when required. That is very likely the cleanest option. However, as the group coordinator does not really parse the assignments yet, it would require a significant effort to be implemented. Given that this is an edge case, I am not sure that it is worth the effort. It might be better to address this when we redesign the protocol.

3. At the moment, the re-joining leader can't detect metadata changes because it does not know that it is the leader when re-joining the group. This is something that we did on purpose otherwise the leader would compute assignment but they would not be propagated to other members. I am not sure if they is a real reason besides reducing noise and confusion on the client side. Here we could do this differently. For instance, we could tell the leader that it is the leader of the group when rejoining it by setting the correct leader in the JoinGroup response. Based on this, the leader would assign partitions and send a SyncGroup request. When the SyncGroup request is received from the leader and the group is Stable, we could check if the assignments have changed and trigger a rebalance if they did.

4. As explained in pt. 3, the issue is that the re-joining member does not know that it is the leader when re-joining the group and this prevents the leader from monitoring any metadata changes based on the so called ""assignment snapshot"". Unlike the subscriptions which are monitored by all members of the group, we have restricted this case to the leader only. It seems that we could relax this constraint and extend the mechanism to be executed by all members of the group. Note that only a JoinGroup request coming from the leader of group will trigger a rebalance. The downside is that it would increase the number of JoinGroup requests as all members would try to send them.;;;","07/Dec/21 00:49;guozhang;Hi [~dajac] [~rleslie] Thanks for the great discussions here, and for the summary. Personally I'm leaning towards option 4) because:

a) It's usually better to fix on client side only, than having to change both brokers and/or clients, since the latter requires such fixes to be only in after a broker is upgraded, which is much more infrequent than client upgrades.

b) Today we are splitting the decision on ""when should a rebalance be triggered"" between the client and brokers, e.g. for metadata change the client have some logic that ""only leaders need to re-join"" and brokers have some logic that ""only leader's join will trigger"". Given our long term plan is to move such triggering logic to brokers (i.e. option 2), I feel its okay to make client's side logic ""dummier"" since it aligns with the long term design a.k.a. dummy client and sophisticated brokers.

The downside of course is that, in this interim period, we would have more entires flooding the rebalance logs since there would be more clients sending the join group request. AND that with a large group, these non-leader join-group requests may become a network request pressure on brokers --- in the even older days it's exactly as the proposed fix, where everyone can send join-group request, and brokers would treat them differently, I think we added this split-brain logic on the client side exactly for reducing the request rate with large groups --- but my preference is based on the assumption if we would soon remove this logic on the client completely and moves on to option 2). It might be a big IF though :P;;;","05/Jan/22 23:22;hachikuji;[~dajac] [~rleslie] [~guozhang] Thanks for the discussion. The protocol has us a bit boxed in. One problem I can think of with option 4) is that it doesn't handle the general case when subscriptions are not consistent among members. The leader needs a way to learn the subscriptions of all members. That is probably a rare case, so maybe it is not worth worrying about too much it, but it would be nice to have a complete solution. Another issue is that it requires a client update. 

I favor option 3) a bit more than I thought I would when I first heard about it from David. Our basic intent for static groups is to make the rebalance protocol idempotent so that returning members can rejoin without the need for a new rebalance (so long as the subscription hasn't changed). Our current approach cheats the protocol because we skip the leader's role in the rebalance. In other words, it is not actually idempotent because the end state is a group without a leader. It seems more consistent to let the leader recompute the assignment. Provided there are no changes, then the other members don't need to know about it. The big question is whether any of the standard assignors that AK ships have any non-deterministic behavior. 
;;;","07/Jan/22 22:44;guozhang;Sounds good, thanks @Jason. Let's do option 3) for now.;;;","10/Jan/22 14:57;dajac;Thanks, [~hachikuji]. I think that AK assignors are usually deterministic given the exact same input. One issue that I can see is that the assignor interface takes a `Map<String, Subscription> subscriptions` (member id to subscription) as input. This means that we don't guarantee the ordering while iterating over the map to compute the assignment. The issue is that the new joining member will get a new member id so the input is not exactly the same and the ordering will change very likely. I tried this quickly with the RangeAssignor and the assignment changes depending on the new member id.;;;","11/Jan/22 04:48;rleslie;Not ideal, but as a first step, is a partial solution achievable for sticky and cooperative-sticky? I believe the latter is slated to be the new default, at least. With these assignors I think the broker is sent UserData that includes the current assignments, and this is available in SyncGroupResponse to the returning leader. Then the client would need to store this and resume checking for metadata changes.;;;","19/Jan/22 15:28;dajac;[~rleslie] I have published a small KIP to address this issue: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-814%3A+Static+membership+protocol+should+let+the+leader+skip+assignment.] I have discussed all the options with [~hachikuji] and [~guozhang] and we feel like this is the best approach overall.;;;","20/Oct/22 00:20;twmb;KIP-814 doesn't have a solution to KAFKA-12759 mentioned above. If the leader is turned off, has its assignment switched, and rejoins, the leader will receive its old assignment. KIP-814 says that a rebalance should not occur.

If a leader is assigned ""foo"", turns off, changes its configuration to be assigned ""bar"", and turns back on, the broker will tell the leader that its current assignment is ""foo"". Worse, the JoinGroupResponse will indicate that ""bar"" is the member assignment, and then the SyncGroupResponse will say ""you own foo"" even though the leader is not interested.

Rather than returning leader's new protocol metadata, the broker should return the old leader protocol metadata. This would allow the leader to detect if its metadata has changed and if it should continue with a full balance and assignment.

I'll open a new issue for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove broker-wide quota properties from the documentation,KAFKA-13430,13409889,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,dongjin,dongjin,03/Nov/21 14:47,04/Nov/21 09:08,13/Jul/23 09:17,04/Nov/21 09:08,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,3.1.0,,,,,,,documentation,,,,,0,,,,,"I found this problem while working on [KAFKA-13341|https://issues.apache.org/jira/browse/KAFKA-13341].

Broker-wide quota properties ({{quota.producer.default}}, {{quota.consumer.default}}) are [removed in 3.0|https://issues.apache.org/jira/browse/KAFKA-12591], but it is not applied to the documentation yet.",,dongjin,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 04 09:06:51 UTC 2021,,,,,,,,,,"0|z0wez4:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"03/Nov/21 14:53;dongjin;[~dajac] Could you kindly have a look? It seems like We should also address this issue in 3.1.0.;;;","04/Nov/21 04:54;githubbot;dongjinleekr opened a new pull request #380:
URL: https://github.com/apache/kafka-site/pull/380


   A counterpart of [KAFKA-13430: Remove broker-wide quota properties from the documentation](https://github.com/apache/kafka/pull/11463).
   
   cc/ @dajac


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Nov/21 09:06;githubbot;dajac merged pull request #380:
URL: https://github.com/apache/kafka-site/pull/380


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error falsely reported on Kafka Streams app when GlobalKTables are used ,KAFKA-13423,13409287,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,michal.brylka,michal.brylka,30/Oct/21 15:13,06/Feb/22 19:56,13/Jul/23 09:17,06/Feb/22 19:56,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,streams,,,,,0,,,,,"It seems that error of this form:

_ERROR streams.KafkaStreams: stream-client [testAppId-56832986-6ff0-4583-8aaf-85fafd7b4fe4] Global thread has died. The streams application or client will now close to ERROR._

are being reported when Streams are closed gracefully. This seems to be not right. See attached files for repro case 

 ",,michal.brylka,mjsax,twbecker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/21 15:13;michal.brylka;GlobalKtableTest.java;https://issues.apache.org/jira/secure/attachment/13035507/GlobalKtableTest.java","30/Oct/21 15:13;michal.brylka;log4j.properties;https://issues.apache.org/jira/secure/attachment/13035508/log4j.properties","30/Oct/21 15:12;michal.brylka;pom.xml;https://issues.apache.org/jira/secure/attachment/13035509/pom.xml",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-10-30 15:13:25.0,,,,,,,,,,"0|z0wb9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sync group failed with rebalanceInProgress error might cause out-of-date ownedPartition in Cooperative protocol,KAFKA-13419,13409059,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,29/Oct/21 07:32,28/Oct/22 09:14,13/Jul/23 09:17,23/Feb/22 01:36,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,clients,,,,,0,,,,,"In KAFKA-13406, we found there's user got stuck when in rebalancing with cooperative sticky assignor. The reason is the ""ownedPartition"" is out-of-date, and it failed the cooperative assignment validation.

Investigate deeper, I found the root cause is we didn't reset generation and state after sync group fail. In KAFKA-12983, we fixed the issue that the onJoinPrepare is not called in resetStateAndRejoin method. And it causes the ownedPartition not get cleared. But there's another case that the ownedPartition will be out-of-date. Here's the example:
 # consumer A joined and synced group successfully with generation 1
 # New rebalance started with generation 2, consumer A joined successfully, but somehow, consumer A doesn't send out sync group immediately
 # other consumer completed sync group successfully in generation 2, except consumer A.
 # After consumer A send out sync group, the new rebalance start, with generation 3. So consumer A got REBALANCE_IN_PROGRESS error with sync group response
 # When receiving REBALANCE_IN_PROGRESS, we re-join the group, with generation 3, with the assignment (ownedPartition) in generation 1.
 # So, now, we have out-of-date ownedPartition sent, with unexpected results happened

 

We might want to do *resetStateAndRejoin* when *RebalanceInProgressException* errors happend in *sync group*. Because when we got sync group error, it means, join group passed, and other consumers (and the leader) might already completed this round of rebalance. The assignment distribution this consumer have is already out-of-date.

 ",,ableegoldman,aiquestion,Andy_Dufresne,calohmn,dajac,kirktrue,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13406,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 09:14:02 UTC 2022,,,,,,,,,,"0|z0w9uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 23:39;kirktrue;[~showuon] - this Jira is marked as in progress, yet I see that the corresponding PR has been merged already. Is this still in progress? If not, please update the fixed version and mark as fixed.

Thanks!;;;","23/Feb/22 01:36;showuon;[~kirktrue] , thanks for the reminder. Updated.;;;","17/Jun/22 06:11;aiquestion;Hi [~showuon] 

After i applied this fix and my previous change to make this fix work[https://github.com/apache/kafka/pull/12140, |https://github.com/apache/kafka/pull/12140]what we are seeing is that: sometimes consumer will revoker almost all partitions with cooperative enabled.

detail:
 * we have more than 1000 consumers, coopeartive rebalance. 
 * Just the same as the example in this JIRA:  in cooperative rebalance some consumer will do a very quick re-join after get SyncGroupResponse. if there are some consumer that didn't send SyncGroupRequest yet, it will do a revoke-all and re-join operation.
 * after applied this change, it will solve the rebalance many rounds problem https://issues.apache.org/jira/browse/KAFKA-13891
 * but it will result in many partitions revoked if there is a very fast re-join consumer, and make cooperative almost the same as eager rebalance.

So instead of ""{*}resetStateAndRejoin{*} when *RebalanceInProgressException* errors happend in {*}sync group{*}"", can we just treat the ownedPartition in previous generation legal if there are no same partition claimed by other member? 

 

What do you think?

Thanks a lot!;;;","27/Oct/22 05:42;ableegoldman;{quote}  can we just treat the ownedPartition in previous generation legal if there are no same partition claimed by other member? 
{quote}
Huh, I thought that's already what the cooperative assignor does? Maybe we intentionally left/took it out of the constrained case algorithm for some reason? Or possibly we had just discussed doing this and never did, either way I definitely remember this specific handling logic coming up during the recent(ish) optimizations that I worked on with [~showuon] 

I'll check out the code I guess;;;","27/Oct/22 06:48;showuon;[~ableegoldman] , you are right. We have the logic to handle multiple consumers owned the same partitions case.

 ;;;","27/Oct/22 07:54;ableegoldman;Cool, thanks for confirming this is already handled in the assignor. Ok then, that said – or actually, _because_ of this, I do agree that we should revert https://issues.apache.org/jira/browse/KAFKA-13891. 

 

Maybe we can look into the logs to better understand the original problem you were experiencing [~aiquestion] – or were you not personally hitting the issue ""fixed"" by https://issues.apache.org/jira/browse/KAFKA-13891, just vigilant about submitting a patch when you noticed we had mentioned making this change in [https://github.com/apache/kafka/pull/11451] but then apparently left it out?;;;","28/Oct/22 09:00;dajac;[~ableegoldman] [~showuon] Looking at the code in AbstractStickyPartitioner, it seems that we discard all previous generations when we see a more recent one. So we have the code to handle multiple generations but we don't try to reuse previous generations. So if a partition is claimed by a member at an older generation and no one else, it seems to me that we won't reuse it. I may be reading it wrong as well. This is the first time I read this code.;;;","28/Oct/22 09:11;showuon;[~dajac] , you're right. We don't use owned partitions in old generation. We treat is as stale owned partitions and ignore them. But you're right, if there are no other members claiming owning the partitions, we can keep letting the member own them.

 ;;;","28/Oct/22 09:14;dajac;[~showuon] Thanks for confirming.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brokers disconnect intermittently with TLS1.3,KAFKA-13418,13409007,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,skokoori,skokoori,skokoori,29/Oct/21 00:17,31/Mar/22 16:57,13/Jul/23 09:17,29/Mar/22 22:08,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,,,,,clients,,,,,0,,,,,"Using TLS1.3 (with JDK11) is causing a regression and an increase in inter-broker p99 latency, as mentioned by Yiming in [Kafka-9320|https://issues.apache.org/jira/browse/KAFKA-9320?focusedCommentId=17401818&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17401818]. We tested this with Kafka 2.8.
The issue seems to be because of a renegotiation exception being thrown by 
{code:java}
read(ByteBuffer dst)
{code}
 & 
{code:java}
write(ByteBuffer src)
{code}
 in 
_clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java_

This exception is causing the connection to close between the brokers before read/write is completed. In our internal experiments we have seen the p99 latency stabilize when we remove this exception.

Given that TLS1.3 does not support renegotiation, I would like to make it applicable just for TLS1.2.",,ijuma,showuon,skokoori,yzang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/21 00:29;skokoori;tls1_3.patch;https://issues.apache.org/jira/secure/attachment/13037856/tls1_3.patch",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 31 16:57:43 UTC 2022,,,,,,,,,,"0|z0w9jc:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"23/Dec/21 00:30;skokoori;After enabling SSL logging (javax.net.debug=ssl,handshake),
I see that unwrap call in the SslTransportLayer.read function returns handshakeStatus=NEED_WRAP when ssl key_update takes place. (log snippet below)

Based on documentation provided in [https://datatracker.ietf.org/doc/html/rfc8446]
key_updates normally happen during a read/write and connection has to be closed when it happens during handshake. 
Given that here key_updates are happening after handshaking is done, will something like attached patch work? I am new to Kafka and any feedback would be helpful.

Kafka log:
{code:java}
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.574 UTC|KeyUpdate.java:192|Consuming KeyUpdate post-handshake message (
""KeyUpdate"": {
  ""request_update"": update_requested
}
)
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.575 UTC|SSLCipher.java:1866|KeyLimit read side: algorithm = AES/GCM/NOPADDING:KEYUPDATE
countdown value = 137438953472
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.575 UTC|KeyUpdate.java:236|KeyUpdate: read key updated
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.575 UTC|KeyUpdate.java:271|Produced KeyUpdate post-handshake message (
""KeyUpdate"": {
  ""request_update"": update_not_requested
}
)
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.575 UTC|SSLCipher.java:2020|KeyLimit write side: algorithm = AES/GCM/NOPADDING:KEYUPDATE
countdown value = 137438953472
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.575 UTC|KeyUpdate.java:323|KeyUpdate: write key updated
[2021-12-21 06:14:09,575] ERROR [SslTransportLayer channelId=2 key=channel=java.nio.channels.SocketChannel[connection-pending remote=/192.168.24.11:9093], selector=sun.nio.ch.EPollSelectorImpl@2eb1a872, interestOps=8, readyOps=0] Renegotiation requested, but it is not supported, channelId 2, appReadBuffer pos 0, netReadBuffer pos 0, netWriteBuffer pos 147 handshakeStatus NEED_WRAP State READY (org.apache.kafka.common.network.SslTransportLayer)
javax.net.ssl|DEBUG|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.578 UTC|Alert.java:238|Received alert message (
""Alert"": {
  ""level""      : ""warning"",
  ""description"": ""close_notify""
}
)
javax.net.ssl|ALL|8D|ReplicaFetcherThread-0-2|2021-12-21 06:14:09.580 UTC|SSLEngineImpl.java:752|Closing outbound of SSLEngine{code};;;","13/Feb/22 10:14;ijuma;[~skokoori] Thanks for the report. Can you please submit a pull request? See https://kafka.apache.org/contributing .;;;","14/Feb/22 13:41;ijuma;cc [~rajinisivaram@gmail.com] ;;;","18/Feb/22 21:55;skokoori;[~ijuma] Thank you. Let me test with the latest trunk and will submit a pull request;;;","28/Feb/22 19:52;yzang;Thanks [~skokoori] for creating this issue, and [~ijuma] for reviewing the pull request. This will solve the TLS issue Twitter has been seeing since upgrading to 2.7 with TLS 1.3;;;","12/Mar/22 15:14;ijuma;Looks like the property that controls when the key update happens is a security property with a default of 2^37.
{quote}There are cryptographic limits some algorithms have on the amount of plaintext which can be safely encrypted under a given set of keys. A new Security Property, ""jdk.tls.keyLimits"" has been added for TLS 1.3. When the amount of encrypted data by the algorithm has been reached a post-handshake Key and IV Update is triggered to derive new keys. This value is configurable so administrators can control their own security policies.
{quote}
https://bugs.openjdk.java.net/browse/JDK-8234226;;;","29/Mar/22 23:13;skokoori;Thank you for addressing this [~ijuma]. I was running into an issue with one of the unit tests. Looks like you addressed it;;;","29/Mar/22 23:22;ijuma;Thanks for your contribution [~skokoori] ! Tricky bug this one. :);;;","31/Mar/22 16:57;yzang;Thanks a lot! [~ijuma]  [~skokoori] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic thread pool re-configurations may not get processed,KAFKA-13417,13408994,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,28/Oct/21 20:59,09/Nov/21 21:45,13/Jul/23 09:17,09/Nov/21 21:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"`DynamicBrokerConfig.updateCurrentConfig` includes the following logic to update the current configuration and to let each `Reconfigurable` process the update:
{code}
    val oldConfig = currentConfig
    val (newConfig, brokerReconfigurablesToUpdate) = processReconfiguration(newProps, validateOnly = false)
    if (newConfig ne currentConfig) {
      currentConfig = newConfig
      kafkaConfig.updateCurrentConfig(newConfig)

      // Process BrokerReconfigurable updates after current config is updated
      brokerReconfigurablesToUpdate.foreach(_.reconfigure(oldConfig, newConfig))
    }
{code}

The problem here is that `currentConfig` gets initialized as `kafkaConfig` which means that the first call to `kafkaConfig.updateCurrentConfig(newConfig)` ends up mutating `currentConfig` and consequently `oldConfig`. The problem with this is that some of the `reconfigure` implementations will only apply a new configuration if the value in `oldConfig` does not match the value in `newConfig`. For example, here is the logic to update thread pools dynamically:

{code}
  override def reconfigure(oldConfig: KafkaConfig, newConfig: KafkaConfig): Unit = {
    if (newConfig.numIoThreads != oldConfig.numIoThreads)
      server.dataPlaneRequestHandlerPool.resizeThreadPool(newConfig.numIoThreads)
    if (newConfig.numNetworkThreads != oldConfig.numNetworkThreads)
      server.socketServer.resizeThreadPool(oldConfig.numNetworkThreads, newConfig.numNetworkThreads)
    if (newConfig.numReplicaFetchers != oldConfig.numReplicaFetchers)
      server.replicaManager.replicaFetcherManager.resizeThreadPool(newConfig.numReplicaFetchers)
    if (newConfig.numRecoveryThreadsPerDataDir != oldConfig.numRecoveryThreadsPerDataDir)
      server.logManager.resizeRecoveryThreadPool(newConfig.numRecoveryThreadsPerDataDir)
    if (newConfig.backgroundThreads != oldConfig.backgroundThreads)
      server.kafkaScheduler.resizeThreadPool(newConfig.backgroundThreads)
  }
{code}

Because of this, the dynamic update will not get applied the first time it is made. I believe subsequent updates would work correctly though because we would have lost the indirect reference to `kafkaConfig`. Other than the `DynamicThreadPool` configurations, it looks like the config to update unclean leader election may also be affected by this bug.

NOTE: This bug only affects kraft, which is missing the call to `DynamicBrokerConfig.initialize()`. ",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-10-28 20:59:32.0,,,,,,,,,,"0|z0w9gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retry of initTransactions after timeout may cause invalid transition,KAFKA-13412,13408788,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,27/Oct/21 23:14,21/Jan/22 17:37,13/Jul/23 09:17,21/Jan/22 17:37,,,,,,,,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,producer ,,,,,0,,,,,"If `initTransactions()` cannot be completed before the timeout defined by `max.block.ms`, then the call will raise a `TimeoutException`. The user is expected to retry this, which is what Kafka Streams does. However, the producer will keep retrying the `InitProducerId` request in the background and it is possible for it to return before the retry call to `initTransaction()`. This leads to the following exception:
{code}
org.apache.kafka.common.KafkaException: TransactionalId blah: Invalid transition attempted from state READY to state INITIALIZING

	at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1077)
	at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1070)
	at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$initializeTransactions$1(TransactionManager.java:336)
	at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1198)
	at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:333)
	at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:328)
	at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:597)
{code}

",,ableegoldman,dengziming,hachikuji,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-10-27 23:14:10.0,,,,,,,,,,"0|z0w86o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka controller out of service after ZK leader restart,KAFKA-13407,13408650,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,Olsson,Olsson,27/Oct/21 09:41,03/May/22 21:03,13/Jul/23 09:17,24/Feb/22 22:11,2.8.0,2.8.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,"When the Zookeeper leader disappears, a new instance becomes the leader, the instances need to reconnect to Zookeeper, but the Kafka ""Controller"" gets lost in limbo state after re-establishing connection.

See below for how I manage to reproduce this over and over.

*Prerequisites*

Have a Kafka cluster with 3 instances running version 2.8.1. Figure out which one is the Controller. I'm using Kafkacat 1.5.0 and get this info using the `-L` flag.

Zookeeper runs with 3 instances on version 3.5.9. Figure out which one is leader by checking

 
{code:java}
echo stat | nc -v localhost 2181
{code}
 

 

*Reproduce*

1. Stop the leader Zookeeper service.

2. Watch the logs of the Kafka Controller and ensure that it reconnects and registers again.

 
{code:java}
Oct 27 09:13:08 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:08,882] INFO Unable to read additional data from server sessionid 0x1f2a12870003, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
Oct 27 09:13:10 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:10,548] WARN SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/opt/kafka/config/kafka_server_jaas.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. (org.apache.zookeeper.ClientCnxn)
Oct 27 09:13:10 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:10,548] INFO Opening socket connection to server zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.10.84.12:2181 (org.apache.zookeeper.ClientCnxn)
Oct 27 09:13:10 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:10,548] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient)
Oct 27 09:13:10 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:10,549] INFO Socket connection established, initiating session, client: /10.10.85.215:39338, server: zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.10.84.12:2181 (org.apache.zookeeper.ClientCnxn)
Oct 27 09:13:10 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:10,569] INFO Session establishment complete on server zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.10.84.12:2181, sessionid = 0x1f2a12870003, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,548] INFO [ZooKeeperClient Kafka server] Reinitializing due to auth failure. (kafka.zookeeper.ZooKeeperClient)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,550] INFO [PartitionStateMachine controllerId=1003] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,550] INFO [ReplicaStateMachine controllerId=1003] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,551] INFO [RequestSendThread controllerId=1003] Shutting down (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,551] INFO [RequestSendThread controllerId=1003] Stopped (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,551] INFO [RequestSendThread controllerId=1003] Shutdown completed (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,552] INFO [RequestSendThread controllerId=1003] Shutting down (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,552] INFO [RequestSendThread controllerId=1003] Stopped (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,552] INFO [RequestSendThread controllerId=1003] Shutdown completed (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,554] INFO [RequestSendThread controllerId=1003] Shutting down (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,554] INFO [RequestSendThread controllerId=1003] Stopped (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,554] INFO [RequestSendThread controllerId=1003] Shutdown completed (kafka.controller.RequestSendThread)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,556] INFO Processing notification(s) to /config/changes (kafka.common.ZkNodeChangeNotificationListener)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,557] INFO [Controller id=1003] Resigned (kafka.controller.KafkaController)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,557] INFO Creating /brokers/ids/1003 (is it secure? false) (kafka.zk.KafkaZkClient)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,560] INFO Updated cache from existing FinalizedFeaturesAndEpoch(features=Features{}, epoch=1) to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=1). (kafka.server.FinalizedFeatureCache)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,572] INFO Stat of the created znode at /brokers/ids/1003 is: 128849019044,128849019044,1635323000996,1635323000996,1,0,0,34265559924739,389,0,128849019044
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]:  (kafka.zk.KafkaZkClient)
Oct 27 09:13:11 ip-10-10-85-215 kafka[62961]: [2021-10-27 09:13:11,572] INFO Registered broker 1003 at path /brokers/ids/1003 with addresses: PLAINTEXT://kafka-tr-2.node.consul.lab.aws.blue.example.net:9093,SASL_SSL://kafka-tr-2.node.consul.lab.aws.blue.example.net:9092, czxid (broker epoch): 128849019044 (kafka.zk.KafkaZkClient)
{code}
 

 

Now it is in this limbo state but to notice the problem create a topic for example.

 
{code:java}
./kafka-topics.sh --zookeeper zookeeper-kafka.service.consul.lab.aws.blue.example.net:2181/kafka_tr --create --topic danols-ts-4 --partitions 10 --replication-factor 3
Created topic danols-ts-4.
{code}
 

 

Then when describing the topic it looks like this:

 
{code:java}
./kafka-topics.sh --zookeeper zookeeper-kafka.service.consul.lab.aws.blue.example.net:2181/kafka_tr --describe --topic danols-ts-4
Topic: danols-ts-4	PartitionCount: 10	ReplicationFactor: 3	Configs: 
	Topic: danols-ts-4	Partition: 0	Leader: none	Replicas: 1003,1001,1002	Isr: 
	Topic: danols-ts-4	Partition: 1	Leader: none	Replicas: 1001,1002,1003	Isr: 
	Topic: danols-ts-4	Partition: 2	Leader: none	Replicas: 1002,1003,1001	Isr: 
	Topic: danols-ts-4	Partition: 3	Leader: none	Replicas: 1003,1002,1001	Isr: 
	Topic: danols-ts-4	Partition: 4	Leader: none	Replicas: 1001,1003,1002	Isr: 
	Topic: danols-ts-4	Partition: 5	Leader: none	Replicas: 1002,1001,1003	Isr: 
	Topic: danols-ts-4	Partition: 6	Leader: none	Replicas: 1003,1001,1002	Isr: 
	Topic: danols-ts-4	Partition: 7	Leader: none	Replicas: 1001,1002,1003	Isr: 
	Topic: danols-ts-4	Partition: 8	Leader: none	Replicas: 1002,1003,1001	Isr: 
	Topic: danols-ts-4	Partition: 9	Leader: none	Replicas: 1003,1002,1001	Isr: 

{code}
 

 

If I describe using the --bootstrap-server flag instead it doesn't exist
{code:java}
./kafka-topics.sh --bootstrap-server localhost:9093 --describe --topic danols-ts-4
Error while executing topic command : Topic 'danols-ts-4' does not exist as expected
[2021-10-27 09:24:15,691] ERROR java.lang.IllegalArgumentException: Topic 'danols-ts-4' does not exist as expected
	at kafka.admin.TopicCommand$.kafka$admin$TopicCommand$$ensureTopicExists(TopicCommand.scala:542)
	at kafka.admin.TopicCommand$AdminClientTopicService.describeTopic(TopicCommand.scala:317)
	at kafka.admin.TopicCommand$.main(TopicCommand.scala:69)
	at kafka.admin.TopicCommand.main(TopicCommand.scala)
 (kafka.admin.TopicCommand$)
{code}
If I restart the Kafka service on a broker that is *not* ""Controller"" it doesn't come up again. It just spams the following in the log for all different partitions and it's not possible to connect to it.
{code:java}
Oct 27 09:27:48 ip-10-226-69-122 kafka[67342]: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 0.
Oct 27 09:27:48 ip-10-226-69-122 kafka[67342]: [2021-10-27 09:27:48,984] INFO [Admin Manager on Broker 1001]: Error processing create topic request CreatableTopic(name='example-message-consumer', numPartitions=10, replicationFactor=3, assignments=[], configs=[]) (kafka.server.ZkAdminManager)
Oct 27 09:27:48 ip-10-226-69-122 kafka[67342]: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 0.
Oct 27 09:27:49 ip-10-226-69-122 kafka[67342]: [2021-10-27 09:27:49,021] INFO [Admin Manager on Broker 1001]: Error processing create topic request CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600')]) (kafka.server.ZkAdminManager)
{code}
To get out of this state I simply restart the Kafka service on the ""Controller"". It will make another become ""Controller"" and all brokers becomes OK again and the newly created topic gets all expected ISRs.",Ubuntu 20.04,ecomar,fabiovh,fvaleri,fxbing,junrao,loicmonney,Olsson,vinsonZhang,,,,,,,,,,,,,,,,,,,,,,KAFKA-13461,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 24 22:10:25 UTC 2022,,,,,,,,,,"0|z0w7c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/21 17:01;ecomar;Hi we're experiencing a similar issue. We think the following patch to 2.8 may solve it

[https://github.com/apache/kafka/pull/11476]

we have however not been able to replicate the issue yet using the ducktape system tests

Would you be able to test our patch in your setup [~Olsson] ?;;;","14/Nov/21 14:26;vinsonZhang;[~Olsson] 

It looks like ZK auth failure triggered the Controller resign.
You can try to remove the environment variable 'java.security.auth.login.config' and declare the JAAS file content in Kafka server.properties.

It works on my local.
Thanks.;;;","14/Nov/21 14:32;vinsonZhang;[~ecomar] 
The PR attached is trying to reinitialize the Controller Context.
But for this case, maybe the controller should never resign himself after ZK leader restart.

Thanks.;;;","15/Nov/21 11:44;ecomar;Hi [~vinsonZhang] 
the zookeeper ""Auth failed"" error in the logs appears every time authentication is not configured for ZK ""SASL configuration failed"" then the zookeeper clients falls back to unauthenticated connection 
I don't see it as the cause of resignation;;;","15/Nov/21 11:46;ecomar;[~vinsonZhang] are you suggesting loss of connection with ZK should not cause a controller to resign? ;;;","15/Nov/21 18:08;Olsson;Thanks, [~vinsonZhang] when declaring what we had in the JAAS file in server.properties seems to solve the problem for us! I can no longer reproduce the issue and it looks much cleaner in the log when it reconnects.
{code:java}
Nov 15 17:46:28 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:28,458] INFO Unable to read additional data from server sessionid 0x4d7d0049, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zoo>
Nov 15 17:46:29 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:29,673] INFO Opening socket connection to server zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.226.84.12:2181. Will not attempt to authenticate using SASL (unknown >
Nov 15 17:46:29 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:29,675] INFO Socket error occurred: zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.226.84.12:2181: Connection refused (org.apache.zookeeper.ClientCnxn)
Nov 15 17:46:31 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:31,510] INFO Opening socket connection to server zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.226.76.12:2181. Will not attempt to authenticate using SASL (unknown >
Nov 15 17:46:31 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:31,512] INFO Socket connection established, initiating session, client: /10.10.69.184:45330, server: zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.226.76.12:2181 (>
Nov 15 17:46:31 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:46:31,513] INFO Session establishment complete on server zookeeper-kafka.service.consul.lab.aws.blue.example.net/10.226.76.12:2181, sessionid = 0x4d7d0049, negotiated timeout = 180>
Nov 15 17:48:05 ip-10-226-69-184 kafka[6489]: [2021-11-15 17:48:05,988] INFO [Controller id=1001] Processing automatic preferred replica leader election (kafka.controller.KafkaController) {code}
This is what I added to my server.properties in my lab environment:
{code:java}
listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username=""admin"" \
  password=""admin-secret"";
listener.name.sasl_ssl.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username=""admin"" \
  password=""admin-secret"" \
  user_demo=""demo-secret"";
{code}
 ;;;","24/Feb/22 22:10;junrao;[~Olsson] : Thanks for the reply. I think we fixed this issue in https://issues.apache.org/jira/browse/KAFKA-13461. Basically, when there is no JAAS configured for ZK client and the ZK client tries to establish a new connection, the client will first receive an AUTH_FAIL event. However, this doesn't mean that the ZK client's session is gone since the client will retry the connection without auth, which typically succeeds. Previously, we mistakenly try to reinitialize the controller with the AUTH_FAIL event, which causes the controller to resign but not regain the controllership since the underlying session is still valid. https://issues.apache.org/jira/browse/KAFKA-1346 fixes that issue.

 

I am closing the issue for now. Feel free to reopen it if that doesn't fix the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cooperative sticky assignor got stuck due to assignment validation failed,KAFKA-13406,13408621,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,27/Oct/21 07:32,23/Feb/22 01:41,13/Jul/23 09:17,16/Nov/21 03:09,3.0.0,,,,,,,,,,,,,,,,,,,,,,2.8.2,3.0.1,3.1.0,,,,,,clients,,,,,0,,,,,"We'll do validateCooperativeAssignment for cooperative assignor, where we validate if there are previously owned partitions directly transfer to other consumers without ""revoke"" step. However, the ""ownedPartition"" in subscription might contain out-of-dated data, which might cause the validation always failure.

We should consider the short-term fix it by disabling validateCooperationAssignment for built-in cooperativeStickyAssignor because we've already consider the generation in the assignor, and discard the old generation ownedPartition if any.",,ableegoldman,Andy_Dufresne,asittler,calohmn,dajac,Juhyeon Kim,kami,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13419,,,,,,,,KAFKA-12984,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 23 01:41:35 UTC 2022,,,,,,,,,,"0|z0w75k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/21 09:28;showuon;[~Andy_Dufresne], I've opened a WIP PR with the fix here: [https://github.com/apache/kafka/pull/11439|https://github.com/apache/kafka/pull/11439.] Could you please help check if this change fixes your issue? Thank you.;;;","27/Oct/21 14:38;Andy_Dufresne;[~showuon] since the possible fix is in the 
{code:java}
internals/ConsumerCoordinator
{code}
and not in publicly accessible  API of CooperativeStickyAssignor and AbstractStickyAssignor which we can easily pull and customly change and then use 
{code:java}
partition.assignment.strategy
{code} 
mapping to changed class , it's becoming a bit complicated to test on the same environment I did before.
Is there any chance to test changes in CooperativeStickyAssignor or AbstractStickyAssignor and then port them in internals ?
Or the only way for me is to test on internal classes, so I manually have to  build snapshot from wip branch and then pointing the rest of the app to use this build.
;;;","27/Oct/21 17:54;ableegoldman;Setting the Fix Version as 3.1 – we should try to get a fix in to 3.1 at least for the CooperativeStickyAssignor. Maybe we can have a separate ticket for fixing this more generally for custom assignors as well (which will almost definitely require a KIP, I think);;;","28/Oct/21 08:16;showuon;[~Andy_Dufresne], I understand, but I'm afraid we have to fix it in the `internals/ConsumerCoordinator` file, because this issue is happened after the assignor completed its job. And we had already fixed the issue in assignor in  KAFKA-12984 , and the `internals/ConsumerCoordinator` is what we missed before. We have to make sure when this issue (out-of-date `ownedPartition`) happened, there's no other places missing and cause the rebalance stuck issue happen again.

I created another PR based on V2.8: [https://github.com/apache/kafka/pull/11444] . I think it should be easier for you if you are running on v2.8.1. Or, please let me know how I can help you with it.

Thanks.

 

[~ableegoldman], I agree with you for fixing it in V3.1.0, and have another ticket for long-term fix.;;;","28/Oct/21 08:26;Andy_Dufresne;[~showuon] Thank you, I'll try to re-test it in a couple of days on the same test environment and stand and get back (y);;;","28/Oct/21 08:32;showuon;Awesome! Thank you!;;;","15/Nov/21 10:05;dajac;Moving it to the next release as we are part the code freeze for 3.1.0.;;;","16/Nov/21 02:29;showuon;[~ableegoldman] , I think this issue should fix into v3.1.0. Otherwise, user might get stuck during rebalance again. What do you think?;;;","16/Nov/21 02:59;ableegoldman;Yes, we should definitely get this fix into 3.1. I'll also try to port it back to earlier branches if the cherrypick is smooth;;;","16/Nov/21 03:06;showuon;[~ableegoldman] , thank you very much! :);;;","19/Nov/21 15:37;Andy_Dufresne;[~showuon] Seems the issue is fixed. Thank you for backporting it to 2.8 as well (y);;;","28/Dec/21 06:06;Juhyeon Kim;Hi, I faced same issue described above with kafka-client 3.0.0.

So I think it'd be nice if the fix-version (will be kafka-client:3.0.1) was provided in maven repository. 

Can I know the expected release date for kafka-client:3.0.1? Or It would be good if a snapshot-version of 3.0.1 (to resolve this issue) was provided. 

Thank you! ;;;","28/Dec/21 06:35;showuon;[~Juhyeon Kim] , the release date for v3.0.1 is not planned, yet. But v3.1.0 is going to be released, which includes the fix. FYI.;;;","28/Dec/21 08:15;Juhyeon Kim;[~showuon], thank you very much for your quick answer!

Then.. could you please clarify a bit when will be the release v3.1.0?;;;","28/Dec/21 12:34;showuon;v3.1.0 was planned to be released in Dec. 2021 based on the plan: [https://cwiki.apache.org/confluence/display/KAFKA/Future+release+plan]

We're tackling some blockers, and should be released in Jan. 2022 ( hope so)

 

Thanks.;;;","29/Dec/21 02:07;Juhyeon Kim;[~showuon], Thank you for sharing it :);;;","25/Jan/22 11:34;kami;First thank you for fixing this bug.

 

I see that v3.1.0 which includes this bug fix has just been released.

 

The fix version field seems to indicate this bug fix will aksi be included in the v2.8.x release series. Do we happen to have an eta for v2.8.2 (we are also affected by this bug, but we still run 2.8.x series so we can't use v3.1.0 just yet).

 

Thanks!;;;","22/Feb/22 16:42;asittler;I second Tomaz. 

Is there an ETA for 2.8.2 ?;;;","23/Feb/22 01:41;showuon;[~asittler] [~kami] , so far, I can only make sure the v3.0.1 will be released in late Feb. or March 2022. (ref: [https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.0.1).] However, there is no release plan for 2.8.2, yet. That's all I know. 

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker2 shouldn't mirror Connect's internal topics,KAFKA-13397,13408099,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dongjin,dongjin,dongjin,24/Oct/21 11:21,24/Nov/21 10:35,13/Jul/23 09:17,17/Nov/21 17:22,,,,,,,,,,,,,,,,,,,,,,,3.1.0,3.2.0,,,,,,,mirrormaker,,,,,0,,,,,"This issue is a follow-up of KAFKA-10777 ([KIP-690|https://cwiki.apache.org/confluence/display/KAFKA/KIP-690%3A+Add+additional+configuration+to+control+MirrorMaker+2+internal+topics+naming+convention]).

As of present, a user can set custom 'replication.policy.separator' configuration in MirrorMaker 2. It determines the topic name of internal topics like heartbeats, checkpoints, and offset-syncs ([KIP-690|https://cwiki.apache.org/confluence/display/KAFKA/KIP-690%3A+Add+additional+configuration+to+control+MirrorMaker+2+internal+topics+naming+convention]).

However, there are some glitches here:
 # MirrorMaker2 creates internal topics to track the offsets, configs, and status of the MM2 tasks. But, these topics are not affected by a custom 'replication.policy.separator' settings - that is, these topics may be replicated against the user`s intention.
 # The internal topic names include a dash in their name (e.g., 'mm2-offsets.\{source}.internal') so, a single '-' should be disallowed when configuring 'replication.policy.separator'.",,dongjin,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 17 17:22:14 UTC 2021,,,,,,,,,,"0|z0w3xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/21 17:22;mimaison;We decided to update Replication.isInternalTopic() to keep the pre KIP-690 behaviour that marked topics with the "".internal"" suffix as internal regardless of the configured separator.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-topics.sh --create should not require the partitions/replication-factor argument,KAFKA-13396,13408057,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,showuon,showuon,23/Oct/21 13:11,05/Nov/21 20:36,13/Jul/23 09:17,05/Nov/21 08:11,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,,,,,,0,,,,,"When removing the support of --zookeeper for kafka-topics.sh in KAFKA-12596, we accidentally add a constraint in --create case, that the partitions/replication-factor are now required. This constraint should only apply for zookeeper case.",,dajac,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 05 08:11:00 UTC 2021,,,,,,,,,,"0|z0w3o8:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"01/Nov/21 06:14;showuon;Set this issue as blocker for v3.0.1/v3.1.0 since it's a regression issue.;;;","05/Nov/21 08:11;dajac;Resolving as [~ijuma]'s merged the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic IDs should be removed from PartitionFetchState if they are no longer sent by the controller,KAFKA-13394,13407997,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jolshan,jolshan,jolshan,22/Oct/21 18:49,18/Nov/21 16:33,13/Jul/23 09:17,18/Nov/21 16:33,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"With KAFKA-13102, we added topic IDs to the InitialFetchState and the PartitionFetchState in order to send fetch requests using topic IDs when ibp is 3.1. 

However, there are some cases where we could initially send topic IDs from the controller and then no longer to do so (controller changes to an IBP < 2.8). If we do not remove from the PartitionFetchState and one broker is still IBP 3.1, it will try to send a version 13 fetch request to brokers that no longer have topic IDs in the metadata cache. This could leave the cluster in a state unable to fetch from these partitions.",,dengziming,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-10-22 18:49:32.0,,,,,,,,,,"0|z0w3aw:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure on Windows due to AccessDeniedAcception when attempting to fsync the parent directory,KAFKA-13391,13407725,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,awilkinson,awilkinson,21/Oct/21 09:40,16/Feb/22 08:46,13/Jul/23 09:17,25/Oct/21 17:18,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,clients,,,,,0,,,,,"There appears to be a regression in Kafka 3.0 due to [the changes|https://github.com/apache/kafka/commit/66b0c5c64f2969dc62362b9f169ad1d18f64efe9] made for KAFKA-3968 that causes a failure on Windows. After upgrading to 3.0.0, we're seeing failures in Spring Boot's Windows CI such as the following:

{code}
Caused by: java.nio.file.AccessDeniedException: C:\Windows\TEMP\spring.kafka.915ab8c1-735c-4c88-8507-8d25fd050621920219824697516859	
	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.newFileChannel(WindowsFileSystemProvider.java:115)
	at java.nio.channels.FileChannel.open(FileChannel.java:287)
	at java.nio.channels.FileChannel.open(FileChannel.java:335)
	at org.apache.kafka.common.utils.Utils.flushDir(Utils.java:953)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:941)
	at kafka.server.BrokerMetadataCheckpoint.liftedTree1$1(BrokerMetadataCheckpoint.scala:214)
	at kafka.server.BrokerMetadataCheckpoint.write(BrokerMetadataCheckpoint.scala:204)
	at kafka.server.KafkaServer.$anonfun$checkpointBrokerMetadata$2(KafkaServer.scala:772)
	at kafka.server.KafkaServer.$anonfun$checkpointBrokerMetadata$2$adapted(KafkaServer.scala:770)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:919)
	at scala.collection.IterableOps$WithFilter.foreach(Iterable.scala:889)
	at kafka.server.KafkaServer.checkpointBrokerMetadata(KafkaServer.scala:770)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:322)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:175)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:170)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
{code}

While I'm [aware that Windows isn't officially supported|https://issues.apache.org/jira/browse/KAFKA-12190?focusedCommentId=17264398&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17264398], I think this problem is likely to be a blocker for anyone who uses Windows for Kafka-based development work.

I suspect that the attempt to fsync the directory should just be skipped on Window. Alternatively, the failure could be ignored on Windows. Lucene [added similar functionality in the past|https://issues.apache.org/jira/browse/LUCENE-5588] where it looks like they opted to ignore IOExceptions on Windows rather than skipping the attempt.",,awilkinson,junrao,kpnc,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13390,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 25 17:18:12 UTC 2021,,,,,,,,,,"0|z0w1mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/21 09:56;awilkinson;This may be a duplicate of KAFKA-13390 which was also opened today. The Stack Overflow question to which it links shows the same stack trace at least.;;;","21/Oct/21 11:21;showuon;Also happened in HDFS, and the solution is also to skip fsync when in Windows. https://issues.apache.org/jira/browse/HDFS-13586;;;","22/Oct/21 06:51;showuon;[~awilkinson] , thanks for reporting the issue. PR is ready for review: [https://github.com/apache/kafka/pull/11426]

Welcome to provide comments. Thanks.;;;","22/Oct/21 07:14;awilkinson;It looks good to me. Thanks.;;;","25/Oct/21 17:18;junrao;merged the PR to 3.0 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Producer nodes stuck in CHECKING_API_VERSIONS,KAFKA-13388,13407632,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dajac,dhofftgt,dhofftgt,20/Oct/21 21:27,26/Jan/22 21:58,13/Jul/23 09:17,21/Jan/22 16:56,,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.1,3.2.0,,,,,,core,,,,,0,,,,,"I have been seeing expired batch errors in my app.
{code:java}
org.apache.kafka.common.errors.TimeoutException: Expiring 51 record(s) for xxx-17:120002 ms has passed since batch creation
{code}
 I would have assumed a request timout or connection timeout should have also been logged. I could not find any other associated errors. 

I added some instrumenting to my app and have traced this down to broker connections hanging in CHECKING_API_VERSIONS state. -It appears there is no effective timeout for Kafka Producer broker connections in CHECKING_API_VERSIONS state.-

In the code see the after the NetworkClient connects to a broker node it makes a request to check api versions, when it receives the response it marks the node as ready. -I am seeing that sometimes a reply is not received for the check api versions request the connection just hangs in CHECKING_API_VERSIONS state until it is disposed I assume after the idle connection timeout.-

Update: not actually sure what causes the connection to get stuck in CHECKING_API_VERSIONS.

-I am guessing the connection setup timeout should be still in play for this, but it is not.- 
 -There is a connectingNodes set that is consulted when checking timeouts and the node is removed- 
 -when ClusterConnectionStates.checkingApiVersions(String id) is called to transition the node into CHECKING_API_VERSIONS-",,dajac,david.mao,dengziming,dhofftgt,kirktrue,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/21 15:30;dhofftgt;Screen Shot 2021-10-25 at 10.28.48 AM.png;https://issues.apache.org/jira/secure/attachment/13035312/Screen+Shot+2021-10-25+at+10.28.48+AM.png","21/Oct/21 18:42;dhofftgt;image-2021-10-21-13-42-06-528.png;https://issues.apache.org/jira/secure/attachment/13035207/image-2021-10-21-13-42-06-528.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 16 16:29:18 UTC 2021,,,,,,,,,,"0|z0w11s:",9223372036854775807,,rajinisivaram@gmail.com,,,,,,,,,,,,,,,,,,"21/Oct/21 06:41;dajac;[~dhofftgt] Thanks for filing this issue. Which client version do you use? I just checked the code in trunk and it seems that the API_VERSIONS request should timeouts based on the `request.timeout.ms` (30s by default) like any other requests. To verify, you could turn on debug logs and you should see `Disconnecting from node {} due to request timeout.`.;;;","21/Oct/21 14:09;dhofftgt;[~dajac] Thanks for looking into this and replying. We are using 2.7.1 at the moment. I do see that the api version request should timeout with the request.timeout.ms which is the default for our app. I can't turn on debug logging where I am seeing this issue due to the volume of logs. I have some code periodically inspecting the states of connections that dumps some info. I have seen connections that have those expired batches stuck in CHECKING_API_VERSIONS for longer than the request timeout. I will keep digging to see why. ;;;","21/Oct/21 15:45;dajac;Interesting... It would be great if you could provide more details (logs, dumps, etc.). There might be a bug somewhere. ;;;","21/Oct/21 18:45;dhofftgt;I have a screenshot of some logging I added that shows the connection stuck in CHECKING_API_VERSIONS state. The logging I added grabbed all the nodeIds from ClusterConnectionStates and determined if the Producer would treat them as 'not ready' by checking the state, selector and whether there were in flight requests. If they were 'not ready' it logged them and some other info. It ran on a schedule every 30 seconds.

For this occurrence, I looked up the node id, and it was the leader of the partition that batches expired for. That's all I have right now. It's relatively rare, happens like 1 or 2 times a day for 32 application instances connecting to 64 kafka brokers.

 I am trying to narrow it down more by adding more info and waiting for it to happen again. Like one question I was wondering is if there is an outstanding in flight request when it's in this state or did that somehow get dropped in the shuffle somewhere.
 !image-2021-10-21-13-42-06-528.png|width=664,height=308!;;;","25/Oct/21 15:30;dhofftgt;looks like the connections are getting into CHECKING_API_VERSIONS without any outstanding in flight requests for that node. I think this would indicate a race condition somewhere. If I understand correctly the connection should never be in CHECKING_API_VERSIONS without an in flight request. I am going to trace through the code looking for how this could be possible.  !Screen Shot 2021-10-25 at 10.28.48 AM.png|width=912,height=164!;;;","25/Oct/21 21:31;dhofftgt;I'm going to try enabling debug logging for NetworkClient to see if there is anything indicating what happened to the api versions request.;;;","05/Nov/21 22:20;david.mao;[~dhofftgt]

Why do we expect a connection in CHECKING_API_VERSIONS to have in-flight requests?

I would expect the opposite: 

if a connection is in CHECKING_API_VERSIONS, it will *not* be ready for requests (at this point, the client does not know what API versions the broker supports, so it can't serialize requests to the appropriate version), so it should not have any inflight requests.;;;","08/Nov/21 14:33;dhofftgt;[~david.mao] It's my understanding that there would be an internal request for the api versions check. This is how the call would be tracked for its timeout. ;;;","08/Nov/21 14:42;dhofftgt;I've been trying to get more info. I enabled debug logging for NetworkClient but it hasn't occurred again in the past 2 weeks. Something must have changed in our environment, it doesn't seem to be happening anymore.;;;","10/Dec/21 01:55;david.mao;[~dhofftgt] 

Looking at where the NetworkClient enters the CHECKING_API_VERSIONS state, we see:

 
{code:java}
if (discoverBrokerVersions) {
 this.connectionStates.checkingApiVersions(node); 
 nodesNeedingApiVersionsFetch.put(node, new ApiVersionsRequest.Builder());
 {code}
which is a separate queue for nodes needing to send the api versions request.

Then in 

 
{code:java}
private void handleInitiateApiVersionRequests(long now) {
    Iterator<Map.Entry<String, ApiVersionsRequest.Builder>> iter = nodesNeedingApiVersionsFetch.entrySet().iterator();
    while (iter.hasNext()) {
        Map.Entry<String, ApiVersionsRequest.Builder> entry = iter.next();
        String node = entry.getKey();
        if (selector.isChannelReady(node) && inFlightRequests.canSendMore(node)) {
            log.debug(""Initiating API versions fetch from node {}."", node);
            ApiVersionsRequest.Builder apiVersionRequestBuilder = entry.getValue();
            ClientRequest clientRequest = newClientRequest(node, apiVersionRequestBuilder, now, true);
            doSend(clientRequest, true, now);
            iter.remove();
        }{code}
we only send out the api versions request if the channel is ready (TLS handshake complete, SASL handshake complete).

This is actually a pretty insidious bug because I think we end up in a state where we do not apply any request timeout to the channel if there is some delay in completing any of the handshaking/authentication steps, since the inflight requests are empty.;;;","10/Dec/21 05:11;lucasbradstreet;I'm pretty sure [~david.mao] is correct. If we ever get into CHECKING_API_VERSIONS state we won't apply the connection timeouts or request timeouts. We really need to be applying a timeout to this state. I have a reproducer where we drop API_VERSIONS at random and can see that our normal timeouts don't apply. I think this is why the client gets all the way to expiring the batches without a corresponding disconnect.;;;","10/Dec/21 09:58;dajac;I had a look at the code as well. [~david.mao] is right. My understanding is that we actually transition to the CHECKING_API_VERSIONS state but we only send out the ApiVersionsRequest if the channel is ready. If it never does, we don't have any timeout apply to it because the request timeout only kicks in when the request is sent out.;;;","10/Dec/21 15:37;david.mao;We should probably bump up the priority of this Jira to Major or Critical since this prevents a producer from being able to recover its connection to a node until it restarts, or the connection gets idle-killed. I'm not sure what the impact is on the consumer or admin client.;;;","10/Dec/21 17:03;dajac;I can think of a few ways to fix this:

1) We could decide to include the ApiVersions step as part of the connection time. This means that we would not reset the connection time while transitioning to the CHECKING_API_VERSIONS state but only when transitioning to the READY state.

2) We could reset the connection time in `handleInitiateApiVersionRequests` only when the ApiVersions request is sent out.

3) We might be able to just remove the `selector.isChannelReady(node) && inFlightRequests.canSendMore(node)` condition in `handleInitiateApiVersionRequests`. I don't really understand what would prevent us from sending the request directly. This way the connection would time out based on the request timeout.;;;","16/Dec/21 16:29;lucasbradstreet;[~dajac] I'm not sure about which approach we should use, but I think we should ensure that whatever timeout is used is reasonable for delivery.timeout.ms e.g. with delivery.timeout.ms of 120s and request.timeout.ms of 30s you generally get 3+ shots at attempts before hitting the delivery timeout. At least unless you're stuck in this state.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Docs] - All reads from the leader of the partition even after KIP-392?,KAFKA-13374,13406368,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,showuon,rmoff,rmoff,13/Oct/21 14:50,03/Jan/22 14:42,13/Jul/23 09:17,20/Dec/21 17:34,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,,,,,,0,,,,,"On `https://kafka.apache.org/documentation/#design_replicatedlog` it says

> All reads and writes go to the leader of the partition.



However with KIP-392 I didn't think this was the case any more. If so, the doc should be updated to clarify. ",,githubbot,jlaskowski,rmoff,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 03 14:42:09 UTC 2022,,,,,,,,,,"0|z0vt9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/21 10:34;jlaskowski;Correct. Found this in [What's New in Apache Kafka 2.4|https://blogs.apache.org/kafka/entry/what-s-new-in-apache1]:

bq. KIP-392: Allow consumers to fetch from closest replica
bq. 
bq. Historically, consumers were only allowed to fetch from leaders. In multi-datacenter deployments, this often means that consumers are forced to incur expensive cross-datacenter network costs in order to fetch from the leader. With KIP-392, Kafka now supports reading from follower replicas. This gives the broker the ability to redirect consumers to nearby replicas in order to save costs.
bq. 
bq. See KIP-392 and this blog post for more details.

It'd be nice to have it in the official docs (so I'll remember it when using the docs during Kafka workshops :)).;;;","19/Oct/21 09:58;showuon;Thank you for reporting and confirming, [~rmoff] [~jlaskowski] , I've submitted PR and ready for review: [https://github.com/apache/kafka/pull/11408]

Welcome to provide comments. Thank you.;;;","19/Oct/21 14:52;rmoff;Looks good to me as a layperson - thanks for picking this up [~showuon].;;;","24/Dec/21 07:18;githubbot;showuon opened a new pull request #391:
URL: https://github.com/apache/kafka-site/pull/391


   After KIP-392, we allow consumers to fetch data from leader or followers. Update the doc.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Dec/21 07:18;githubbot;showuon commented on pull request #391:
URL: https://github.com/apache/kafka-site/pull/391#issuecomment-1000694194


   @dajac @mimaison , please take a look. Thanks.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Dec/21 11:31;githubbot;jaceklaskowski commented on pull request #391:
URL: https://github.com/apache/kafka-site/pull/391#issuecomment-1000802718


   LGTM


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Jan/22 14:42;githubbot;mimaison merged pull request #391:
URL: https://github.com/apache/kafka-site/pull/391


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed authentication due to: SSL handshake failed,KAFKA-13372,13406259,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,maisfloro,maisfloro,13/Oct/21 04:23,24/Feb/23 20:01,13/Jul/23 09:17,24/Feb/23 20:01,2.2.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,clients,,,,,0,,,,,"Hi everyone,
 
I have the next issue about authentication SCRAM + SSL. I’m using the CLI and this is the version of my client (./kafka_2.13-2.8.1/bin/kafka-topics.sh). In this example I will talk about list topics, but another operations (consumer, producer) failed too.
 
 
First, let me describe the current scenario:
 
 * I have 5 Kafka servers with 
 * kafka-broker-0.mydomain.com
 * kafka-broker-1.mydomain.com
 * kafka-broker-2.mydomain.com
 * kafka-broker-3.mydomain.com
 * kafka-broker-4.mydomain.com

 
 * I have a DNS principal configured with Round Robin to IPs broker:
 * kafka-broker-princial.mydomain.com (Round Robin)

 
 I have configured for each broker the next listeners (I'm using 3 ports):
{quote}advertised.listeners=SASL_SSL://kafka-broker-0.mydomain.com:9094,SASL_PLAINTEXT://kafka-broker-0.mydomain.com:9093,PLAINTEXT://kafka-broker-0.mydomain.com:9092{quote}
 * 9092 for PLAINTEXT
 * 9093 for SASL_PLAINTEXT
 * 9094 for SASL_SSL

 
My Kafka broker servers have the next config server.properties:
{quote}advertised.listeners=SASL_SSL://kafka-broker-X.mydomain.com:9094,SASL_PLAINTEXT://kafka-broker-X.mydomain.com:9093,PLAINTEXT://kafka-broker-X.mydomain.com:9092
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
auto.create.topics.enable=false
auto.leader.rebalance.enable=true
background.threads=10
broker.id=X
broker.rack=us-east-1c
compression.type=producer
connections.max.idle.ms=2700000
controlled.shutdown.enable=true
delete.topic.enable=true
host.name=localhost
leader.imbalance.check.interval.seconds=300
leader.imbalance.per.broker.percentage=10
listeners=SASL_SSL://0.0.0.0:9094,SASL_PLAINTEXT://0.0.0.0:9093,PLAINTEXT://0.0.0.0:9092
log.cleaner.enable=true
log.dirs=/var/lib/kafka/log/data1,/var/lib/kafka/log/data2,/var/lib/kafka/log/data3
log.retention.check.interval.ms=300000
log.retention.hours=336
log.segment.bytes=1073741824
message.max.bytes=1000012
min.insync.replicas=2
num.io.threads=8
num.network.threads=3
num.partitions=3
num.recovery.threads.per.data.dir=1
num.replica.fetchers=1
offset.metadata.max.bytes=4096
offsets.commit.timeout.ms=5000
offsets.retention.minutes=129600
offsets.topic.num.partitions=50
offsets.topic.replication.factor=3
port=9092
queued.max.requests=500
replica.fetch.min.bytes=1
replica.fetch.wait.max.ms=500
sasl.enabled.mechanisms=SCRAM-SHA-256,GSSAPI
sasl.kerberos.service.name=xxxxx
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256
security.inter.broker.protocol=SASL_SSL
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
socket.send.buffer.bytes=102400
ssl.client.auth=required
{{ssl.endpoint.identification.algorithm=""""}}
ssl.enabled.protocols=TLSv1.2
ssl.key.password=xxxx
ssl.keystore.location=/etc/ssl/default_keystore.jks
ssl.keystore.password=xxxxxxxx
ssl.truststore.location=/usr/lib/jvm/java-11-adoptopenjdk-hotspot/lib/security/cacerts
ssl.truststore.password= xxxxxxxx
ssl.truststore.type=JKS
super.users=User:xxxxx
zookeeper.connect=kafka-zk-X.mydomain.com:2181,kafka-zk-X.mydomain.com:2181,kafka-zk-X.mydomain.com:2181,kafka-zk-X.mydomain.com :2181,kafka-zk-X.mydomain.com:218/my-environment
zookeeper.connection.timeout.ms=6000
zookeeper.sasl.client=false{quote}
 
 
I was trying the next things:
 
 * (/)*PLAINTEXT:* I can consume directly to broker to broker with port *9092* (Using IP or dns broker) 
 * (/)*PLAINTEXT:* I also can consume directly to DNS principal configured with Round Robin  with port *9092* (Using DNS principal)
 * (/)*SASL_SSL:* I can consume directly to broker to broker with port *9094* (Using only dns broker due it needs to validate the certificate)
 * (x)*SASL_SSL:* I cannot consume directly to DNS principal configured with Round Robin with port *9094*

The issue is: * *(x)SASL_SSL(x):* I cannot consume directly to DNS principal configured with Round Robin with port *9094*. Only I have the issue with I try to connect directly to DNS principal. My certificates contains permissions with all my subdomains under the domain. 

 * I have the next _file.config_ when that I use when I try to connect to  DNS principal. (Is the same file that I used for consume directly to broker to broker with port 9094)

{quote}# Required connection configs for Kafka producer, consumer, and admin{quote}
{quote}ssl.keystore.location=/My/Path/default_keystore.jks
ssl.keystore.password=xxxxx
ssl.truststore.location=/My/Path/cacerts
ssl.truststore.password= xxxxx
ssl.truststore.type=JKS
ssl.enabled.protocols=TLSv1.2
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=‘xxxxx' password=‘xxxxxx';
client.dns.lookup=use_all_dns_ips{quote}
 The command that I'm using to try consume directly principal kafka DNS:
{quote}$ ./kafka_2.13-2.8.1/bin/kafka-topics.sh --bootstrap-server kafka-broker-princial.mydomain.com:9094 --command-config java9094.config --list
[2021-10-13 01:04:58,206] ERROR [AdminClient clientId=adminclient-1] Connection to node -1 (kafka-broker-princial.mydomain.com/10.110.209.136:9094) failed authentication due to: SSL handshake failed (org.apache.kafka.clients.NetworkClient)
[2021-10-13 01:04:58,207] WARN [AdminClient clientId=adminclient-1] Metadata update failed due to authentication error (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failed
Caused by: javax.net.ssl.SSLHandshakeException: No subject alternative DNS name matching kafka-broker-princial.mydomain.com  found.
at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:371)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:314)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:309)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369)
at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396)
at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1277)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1264)
at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1209)
at org.apache.kafka.common.network.SslTransportLayer.runDelegatedTasks(SslTransportLayer.java:430)
at org.apache.kafka.common.network.SslTransportLayer.handshakeUnwrap(SslTransportLayer.java:514)
at org.apache.kafka.common.network.SslTransportLayer.doHandshake(SslTransportLayer.java:368)
at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:291)
at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:178)
at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:543)
at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1333)
at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1264)
at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching kafka-broker-princial.mydomain.com found.
at java.base/sun.security.util.HostnameChecker.matchDNS(HostnameChecker.java:212)
at java.base/sun.security.util.HostnameChecker.match(HostnameChecker.java:103)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:452)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:412)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:292)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:632)
... 19 more
Error while executing topic command : SSL handshake failed
[2021-10-13 01:04:58,212] ERROR org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failed
Caused by: javax.net.ssl.SSLHandshakeException: No subject alternative DNS name matching kafka-broker-princial.mydomain.com  found.
at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:371)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:314)
at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:309)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369)
at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396)
at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1277)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1264)
at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1209)
at org.apache.kafka.common.network.SslTransportLayer.runDelegatedTasks(SslTransportLayer.java:430)
at org.apache.kafka.common.network.SslTransportLayer.handshakeUnwrap(SslTransportLayer.java:514)
at org.apache.kafka.common.network.SslTransportLayer.doHandshake(SslTransportLayer.java:368)
at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:291)
at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:178)
at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:543)
at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1333)
at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1264)
at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching kafka-broker-princial.mydomain.com  found.
at java.base/sun.security.util.HostnameChecker.matchDNS(HostnameChecker.java:212)
at java.base/sun.security.util.HostnameChecker.match(HostnameChecker.java:103)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:452)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:412)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:292)
at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144)
at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:632)
... 19 more
 (kafka.admin.TopicCommand$){quote}
Can you help me with this issue? 
 
Thanks for reading me!
 
@maisfloro ",,maisfloro,psmolinski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 14 18:17:02 UTC 2021,,,,,,,,,,"0|z0vsls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/21 08:39;psmolinski;Try checking the TLS server endpoint with OpenSSL.
 
Get the full presented server certificate chain:
openssl s_client -connect [kafka-broker-princial.mydomain.com|http://kafka-broker-princial.mydomain.com/]:9094 -showcerts </dev/null
 
Decode the first certificate:
openssl s_client -connect [kafka-broker-princial.mydomain.com|http://kafka-broker-princial.mydomain.com/]:9094 </dev/null | openssl x509 -text -noout
 
I guess the presented server certificates have something wrong inside. The issue is on the TLS level, not even
touching SASL/SCRAM. I would check the CN and SAN fields of the server certificate, whether they contain
the expected entries.
 
HTH,
Piotr;;;","14/Oct/21 18:17;maisfloro;Hi Piotr!

Thanks a lot. You're right. I was reviewing my broker's certificates and I had to add as alternate name my main DNS on each broker. Now it works.

Have nice day!

@maisfloro ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offset commit failure percentage metric is not computed correctly (regression),KAFKA-13370,13406190,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,20100g,20100g,12/Oct/21 18:14,28/Nov/22 14:18,13/Jul/23 09:17,16/Nov/21 17:02,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.2,3.0.1,3.1.0,,,,,,KafkaConnect,metrics,,,,0,,,,,"There seems to have been a regression in the way the offset-commit-* metrics are calculated for *source* Kafka Connect connectors since version 2.8.0.

Before this version, any timeout or interruption while trying to commit offsets for source connectors (e.g. MM2 MirrorSourceConnector) would get correctly flagged as an offset commit failure (i.e the *offset-commit-failure-percentage* metric ** would be non-zero). Since version 2.8.0, these errors are considered as successes.

After digging through the code, the commit where this bug was introduced appears to be this one : [https://github.com/apache/kafka/commit/047ad654da7903f3903760b0e6a6a58648ca7715]

I believe removing the boolean *success* argument in the *recordCommit* method of the *WorkerTask* class (argument deemed redundant because of the presence of the Throwable *error* argument) and only considering the presence of a non-null error to determine if a commit is a success or failure might be a mistake. This is because in the *commitOffsets* method of the *WorkerSourceTask* class, there are multiple cases where an exception object is either not available or is not passed to the *recordCommitFailure* method, e.g. :
 * *TImeout #1* : [https://github.com/apache/kafka/blob/2.8/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L519] 
 * *Timeout #2* : [https://github.com/apache/kafka/blob/2.8/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L584] 
 * *Interruption* : [https://github.com/apache/kafka/blob/2.8/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L529] 
 * *Unserializable offset* : [https://github.com/apache/kafka/blob/2.8/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L562] 

 ",Confluent Platform Helm Chart (v6.2.0),20100g,rhauch,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 19 06:21:13 UTC 2021,,,,,,,,,,"0|z0vs68:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"19/Oct/21 09:10;showuon;[~20100g], thanks for reporting the issue. PR is ready now: [https://github.com/apache/kafka/pull/11413]

Welcome to provide comments. Thanks.;;;","20/Oct/21 14:07;20100g;Thank you [~showuon] for tackling this issue so quickly !

The changes look good to me. 

P.S. It is also great to see you modified the unit tests so that regression about how these metrics are calculated is less likely to occur in the future.;;;","01/Nov/21 06:13;showuon;Set this issue as blocker for v3.0.1/v3.1.0 since it's a regression issue.;;;","16/Nov/21 15:46;rhauch;Thanks, [~showuon], creating the PR that fixes this issue. But, I think the best course of action here is actually to revert https://github.com/apache/kafka/pull/9642, for a few reasons:

# It was actually useful to have that boolean success parameter within the protected TaskMetricsGroup.recordCommit(...) helper method, since it allowed the WorkerTask.recordCommitSuccess(...) and recordCommitFailure(...) methods to call that one method with desired behavior.
# Rolling back is also more encapsulated and significantly easier to backport all the way back to the 2.8 branch, especially since we've significantly refactored the source connector offset commit logic only in 3.0 and later branches.

So I think I'm going to close your PR and revert #9642.

However, I think your unit test improvement to verify the expected metric values are very useful. Would you mind creating a new PR with those unit test improvements? Since this is a blocker issue for 3.1, it'd be great to do that quickly. If that's not feasible (or I don't hear back in the next few days), then let's create a new issue for the additional unit test metric verification and associate the PR with that new issue.

Thanks!;;;","16/Nov/21 16:20;rhauch;I reverted the change (https://github.com/apache/kafka/pull/9642) that caused this regression in the following branches:
* `2.8` for inclusion in a future 2.8.2 release
* `3.0` for inclusion in a future 3.0.1 release
* `3.1` for inclusion in the upcoming 3.1.0 release
* `trunk` for inclusion in the next major/minor release (e.g., 3.2.0 or 4.0.0)

;;;","19/Nov/21 06:21;showuon;[~rhauch] , PR: [https://github.com/apache/kafka/pull/11509] is created to add unit tests for offset-commit metrics. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanning for Connect plugins can fail with AccessDeniedException,KAFKA-13337,13404329,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,akatona,htamas,htamas,30/Sep/21 15:45,02/Jun/23 07:46,13/Jul/23 09:17,30/May/23 16:00,2.6.2,2.7.1,2.8.1,3.1.0,,,,,,,,,,,,,,,,,,,3.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"During Connect plugin path scan, if an unreadable file/directory is found, Connect will fail with an {{AccessDeniedException}}. As the directories/files can be unreadable, it is best to skip them in this case. See referenced PR.

 
{noformat}
java.nio.file.AccessDeniedException: /tmp/junit8905851398112785578/plugins/.protected
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:90)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:432)
	at java.base/java.nio.file.Files.newDirectoryStream(Files.java:604)
	at org.apache.kafka.connect.runtime.isolation.PluginUtils.pluginUrls(PluginUtils.java:276)
	at org.apache.kafka.connect.runtime.isolation.PluginUtilsTest.testPluginUrlsWithProtectedDirectory(PluginUtilsTest.java:481)
...
{noformat}

Connect server fails with the following exception, (I created an ""aaa"" directory only readable by root
{noformat}
Could not get listing for plugin path: /var/lib/kafka. Ignoring.
java.nio.file.AccessDeniedException: /var/lib/kafka/aaaa
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
	at java.nio.file.Files.newDirectoryStream(Files.java:589)
	at org.apache.kafka.connect.runtime.isolation.PluginUtils.pluginUrls(PluginUtils.java:231)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.registerPlugin(DelegatingClassLoader.java:241)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:222)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:199)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:60)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
{noformat}

Additional note:
Connect server would not stop normally but an extension couldn't be found because of this in my case which killed connect at later point.",,akatona,htamas,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14627,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 07:46:26 UTC 2023,,,,,,,,,,"0|z0vgp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/21 15:49;htamas;https://github.com/apache/kafka/pull/11371;;;","19/May/23 12:53;viktorsomogyi;[~akatona] took this task over in a new PR: https://github.com/apache/kafka/pull/13733;;;","02/Jun/23 07:46;akatona;This MINOR PR is fixing the plugin loading error handling:
https://github.com/apache/kafka/pull/13334;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug where validateOnly flag for createTopics is not honored for KRaft,KAFKA-13324,13403400,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,27/Sep/21 00:38,28/Sep/21 20:16,13/Jul/23 09:17,27/Sep/21 23:01,2.8.0,2.8.1,3.0.0,,,,,,,,,,,,,,,,,,,,3.0.1,,,,,,,,,,,,,0,,,,,Fix a bug where the validateOnly flag for createTopics is not honored when KRaft mode is enabled.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-27 00:38:35.0,,,,,,,,,,"0|z0vayo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log layer exception during shutdown that caused an unclean shutdown,KAFKA-13315,13402576,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ccding,ccding,ccding,21/Sep/21 21:17,23/Sep/21 23:39,13/Jul/23 09:17,23/Sep/21 23:38,,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,,,,,,0,,,,,"We have seen an exception caused by shutting down scheduler before shutting down LogManager.

When LogManager was closing partitons one by one, scheduler called to delete old segments due to retention. However, the old segments could have been closed by the LogManager, which subsequently marked logdir as offline and didn't write the clean shutdown marker. Ultimately the broker would take hours to restart.",,ccding,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13070,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 23 23:38:09 UTC 2021,,,,,,,,,,"0|z0v5w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/21 23:38;junrao;merged the PR to trunk and 3.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'NetworkDegradeTest#test_rate' should wait until iperf server is listening,KAFKA-13312,13402144,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dajac,dajac,dajac,20/Sep/21 09:40,10/Jan/22 11:20,13/Jul/23 09:17,10/Jan/22 11:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"We have seen multiple failures with the following logs:
{noformat}
[DEBUG - 2021-09-17 09:57:58,603 - remoteaccount - _log - lineno:160]: ubuntu@worker26: Running ssh command: iperf -i 1 -t 20 -f k -c worker25 [INFO - 2021-09-17 09:57:58,611 - network_degrade_test - test_rate - lineno:114]: iperf output connect failed: Connection refused
{noformat}
The iperf client can not connect to the iperf server. The test launches the server and then immediately launches the client but there is not guarantee that the server listens when the client is started.

It seems that we should add a condition to wait until the server prints `Server listening` before starting the client.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-20 09:40:06.0,,,,,,,,,,"0|z0v380:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaConsumer cannot jump out of the poll method, and the consumer is blocked in the ConsumerCoordinator method maybeAutoCommitOffsetsSync(Timer timer). Cpu and traffic of  Broker's side increase sharply",KAFKA-13310,13401979,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,RivenSun,RivenSun,RivenSun,18/Sep/21 08:10,20/Jun/22 02:57,13/Jul/23 09:17,09/Feb/22 07:07,2.8.1,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,consumer,,,,,0,new-consumer-threading-should-fix,,,,"h2. Foreword

      Because our consumers' consumption logic is sometimes heavier, we refer to the configuration of Kafka stream [https://kafka.apache.org/documentation/#upgrade_10201_notable]
 Set max.poll.interval.ms to Integer.MAX_VALUE
 Our consumers have adopted method : consumer.subscribe(Pattern.compile("".*riven.*""));

 
h2. Recurrence of the problem scene

operate steps are
 (1) Test environment Kafka cluster: three brokers
 (2) Topics conforming to regular expressions include rivenTest1, rivenTest2, and rivenTest88
 (3) Only one consumer is needed, group.id is ""rivenReassign"", consumer.subscribe(Pattern.compile("".*riven.*""));
 (4) At the beginning, the group status is stable, and everything is normal for consumers, then I delete topic: rivenTest88

 
h2. Phenomenon

      Problem phenomenon
  (1) The consumer is blocked in the poll method, no longer consume any messages, and the consumer log is always printing
 [main] WARN org.apache.kafka.clients.consumer.internals.ConsumerCoordinator-[Consumer clientId=consumer-rivenReassign-1, groupId=rivenReassign] Offset commit failed on partition rivenTest88-1 at offset 0: This server does not host this topic-partition.
 (2) The describe consumerGroup interface of Adminclient  has always timed out, and the group status is no longer stable
 (3) The cpu and traffic of the broker are *significantly increased*

 

 
h2. Problem tracking

   By analyzing the kafkaConsumer code, the version is 2.8.1.
 I found that you introduced the waitForJoinGroup variable in the updateAssignmentMetadataIfNeeded method. For the reason, I attached the comment on the method: ""try to update assignment metadata BUT do not need to block on the timer for join group"". See as below:

 
{code:java}
 if (includeMetadataInTimeout) {
    // try to update assignment metadata BUT do not need to block on the timer for join group
    updateAssignmentMetadataIfNeeded(timer, false);
} else {
    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {
        log.warn(""Still waiting for metadata"");
    }
}{code}
 

 

By tracing the code back layer by layer, it is found that the function of this variable is to construct a time.timer(0L) and pass it back to the method joinGroupIfNeeded (final Timer timer) in AbstractCoordinator. See as below:
{code:java}
// if not wait for join group, we would just use a timer of 0
      if (!ensureActiveGroup(waitForJoinGroup ? timer : time.timer(0L))) {
// since we may use a different timer in the callee, we'd still need 
// to update the original timer's current time after the call 
      timer.update(time.milliseconds()); 
      return false; 
}
{code}
 But you will find that there is a submethod onJoinPrepare in the method stack of joinGroupIfNeeded, and then there is a line of code in the onJoinPrepare method
 maybeAutoCommitOffsetsSync(time.timer(rebalanceConfig.rebalanceTimeoutMs)), the value of rebalanceConfig.rebalanceTimeoutMs is actually max.poll.interval.ms.
 Finally, I tracked down ConsumerCoordinator's method commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer)
 The input parameter offsets is subscriptions.allConsumed(), when I delete the topic: rivenTest88, commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer) method will *fall into an infinite loop! !*
{code:java}
public boolean commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer) {
 invokeCompletedOffsetCommitCallbacks();

 if (offsets.isEmpty())
 return true;

 do {
 if (coordinatorUnknown() && !ensureCoordinatorReady(timer)) {
 return false;
 }

 RequestFuture<Void> future = sendOffsetCommitRequest(offsets);
 client.poll(future, timer);

 // We may have had in-flight offset commits when the synchronous commit began. If so, ensure that
 // the corresponding callbacks are invoked prior to returning in order to preserve the order that
 // the offset commits were applied.
 invokeCompletedOffsetCommitCallbacks();

 if (future.succeeded()) {
 if (interceptors != null)
 interceptors.onCommit(offsets);
 return true;
 }

 if (future.failed() && !future.isRetriable())
 throw future.exception();

 timer.sleep(rebalanceConfig.retryBackoffMs);
 } while (timer.notExpired());

 return false;
}{code}
 

 

*The reason for the endless loop is:*
 (1) The expiration time of the timer is too long, which is max.poll.interval.ms
 (2) The offsets to be submitted contain dirty data and TopicPartition that no longer exists
 (3) The response future of sendOffsetCommitRequest(final Map<TopicPartition, OffsetAndMetadata> offsets) has always failed, and the exception in the future is UnknownTopicOrPartitionException. This exception is allowed to be retried.

Then since the infinite loop interval above is 100ms by default, timer.sleep(rebalanceConfig.retryBackoffMs);
 If a large number of consumers have this problem at the same time, a large number of network requests will be generated to the Kafka broker, *resulting in a sharp increase in the cpu and traffic of the broker machine!*

 

 
h2. Suggest

1.maybeAutoCommitOffsetsSync(time.timer(rebalanceConfig.rebalanceTimeoutMs)), the time of this method is recommended not to use max.poll.interval.ms,
 This parameter is open to users to configure. Through the explanation of this parameter on the official website, I would never think that this parameter will be used in this place. At the same time, it will block KafkaConsumer's poll (final Duration timeout), even if I set consumer.poll (Duration.ofMillis(1000)).
 2. In fact, in the poll (Timer timer, boolean waitForJoinGroup) method of ConsumerCoordinatord, before calling the ensureActiveGroup method, the consumer ensures that the local metadata is up to date, see the code

 
{code:java}
if (rejoinNeededOrPending()) {
    // due to a race condition between the initial metadata fetch and the initial rebalance,
    // we need to ensure that the metadata is fresh before joining initially. This ensures
    // that we have matched the pattern against the cluster's topics at least once before joining.
    if (subscriptions.hasPatternSubscription()) {
        // For consumer group that uses pattern-based subscription, after a topic is created,
        // any consumer that discovers the topic after metadata refresh can trigger rebalance
        // across the entire consumer group. Multiple rebalances can be triggered after one topic
        // creation if consumers refresh metadata at vastly different times. We can significantly
        // reduce the number of rebalances caused by single topic creation by asking consumer to
        // refresh metadata before re-joining the group as long as the refresh backoff time has
        // passed.
        if (this.metadata.timeToAllowUpdate(time.milliseconds()) == 0) {
            this.metadata.requestUpdate();
        }

        if (!client.ensureFreshMetadata(timer)) {
            return false;
        }
    }

    if (!ensureActiveGroup(timer)) {
        return false;
    }
}
{code}
 

That is to say, the consumer knows which topic/topicPartition is legal before onJoinPrepare. In this case, why didn't you find the UnknownTopicOrPartitionException in the commitOffsetsSync method mentioned above,do not put the submitted offsets and the latest local metadata together for analysis, remove the non-existent topicpartitions, and then try to submit the offsets again. I think I can break out of the infinite loop by doing this

3. Why must the offset be submitted synchronously in the onJoinPrepare method? Can't the offset be submitted asynchronously? Or provide a parameter for the user to choose whether to submit synchronously or asynchronously. Or provide a new parameter to control the maximum number of retries for synchronous submission here, instead of using the Timer constructed by max.poll.interval.ms.
 And if you don’t really submit the offset here, it will not have much impact. It may cause repeated consumption of some messages. I still suggest to provide a new parameter to control whether you need to submit the offset.",prod,ableegoldman,dajac,RivenSun,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12256,,,,,,,,,,,,,,,,,,,,"18/Sep/21 13:50;RivenSun;SecondDeleteConsumerLog.png;https://issues.apache.org/jira/secure/attachment/13033798/SecondDeleteConsumerLog.png","18/Sep/21 13:51;RivenSun;SecondDeleteDebugLog.png;https://issues.apache.org/jira/secure/attachment/13033799/SecondDeleteDebugLog.png","19/Sep/21 11:08;RivenSun;ThirdDebugLog1.png;https://issues.apache.org/jira/secure/attachment/13033813/ThirdDebugLog1.png","19/Sep/21 11:03;RivenSun;ThirdDebugLog2.png;https://issues.apache.org/jira/secure/attachment/13033811/ThirdDebugLog2.png","18/Sep/21 08:09;RivenSun;brokerCpu.png;https://issues.apache.org/jira/secure/attachment/13033794/brokerCpu.png","18/Sep/21 08:09;RivenSun;brokerNetBytes.png;https://issues.apache.org/jira/secure/attachment/13033793/brokerNetBytes.png","18/Sep/21 08:08;RivenSun;kafkaConsumerLog.png;https://issues.apache.org/jira/secure/attachment/13033795/kafkaConsumerLog.png",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Mon Sep 20 13:01:58 UTC 2021,,,,,,,,,,"0|z0v27c:",9223372036854775807,,guozhang,,,,,,,,,,,,,,,,,,"18/Sep/21 08:36;RivenSun;[~guozhang] hi Guozhang, can you help deal with this issue? Thanks a lot.;;;","18/Sep/21 12:50;showuon;Nice RCA and good suggestions.

 

My 2 cents:

For suggestion(1), I think, in general, we set ""rebalanceTimeout"" as ""maxPollInterval"" makes sense, because the rebalance period implies the delay between 2 polls (before and after rebalance). But I agree that, during poll(duration), it's not good to have another timer (set as max poll interval) to wait for commit offsets, which will delay the poll process. Maybe we can pass the timer from poll() to the `onJoinPrepare`?

 

For suggestion(2), I think even we use the metadata got before `ensureActiveGroup`, there's still possibility to have race condition after committing offsets. It's pretty difficult to identify the `UnknownTopicOrPartitionException` is topic deleted or not ready yet or other reasons, so I think the point here should be: set a good wait time (so, back to suggestion(1))

 

For suggestion(3), I also think we should set the wait time properly as described in (1).

 

Thank you.

 ;;;","18/Sep/21 13:51;RivenSun;Thank you [~showuon] very much for your reply

I performed the second scene reappearance,  consumerGroup still only has *one consumer*, and the phenomenon remains the same as before.
 For consumer logs, see the attachment: *_SecondDeleteConsumerLog_*.

 

In fact, when kafkaConsumer falls into the above-mentioned infinite loop code, please refer to the attached *_SecondDeleteDebugLog_*.
 In this endless loop code, we can find that the *Set<String> subscription* in the instance variable metadata in ConsumerCoordinator has been refreshed to the latest valid *subscription*.  SourceCode for update  *subscription* can refer as below:

ConsumerCoordinator.java
{code:java}

void maybeUpdateSubscriptionMetadata() {
    int version = metadata.updateVersion();
    if (version > metadataSnapshot.version) {
        Cluster cluster = metadata.fetch();

        if (subscriptions.hasPatternSubscription())
            updatePatternSubscription(cluster);

        // Update the current snapshot, which will be used to check for subscription
        // changes that would require a rebalance (e.g. new partitions).
        metadataSnapshot = new MetadataSnapshot(subscriptions, cluster, version);
    }
}
{code}
SubscriptionState.java
{code:java}
private boolean changeSubscription(Set<String> topicsToSubscribe) {
    if (subscription.equals(topicsToSubscribe))
        return false;

    subscription = topicsToSubscribe;
    return true;
}
{code}
 

So if there is an exception of `UnknownTopicOrPartitionException`, we can use this valid *subscription* to clean up the *dirty topicPartitions* in the offsets we will submit. If you are not sure whether `UnknownTopicOrPartitionException` really means that the topic is deleted, you can further call the listTopics(final ListTopicsOptions options) method in KafkaAdminClient to confirm.

 

Later I will download the sourceCode of Kafka-client 2.8.1 version, try to write the code change scheme I mentioned above, build and generate the test jar package for testing, and hope that the problem will be fixed.

Thanks again.;;;","18/Sep/21 15:50;RivenSun;And I think the most important and critical point is *not to find a good wait time* for the method commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer), but to *immediately clean up/give up* unknownTopicPartitions when submitting offsets in this infinite loop code.

Even if you think that `UnknownTopicOrPartitionException` may not really mean that the topic is really deleted. But when we encounter this exception, shouldn't we temporarily give up submitting offsets for these unknownTopicPartitions?  What is the point of submitting offsets for these unknownTopicPartitions repeatedly?

*The worst effect* is that if unknownTopicPartitions may not be deleted by the broker, we will consume a small portion of the partition messages (if we give up submitting these unknownTopicPartitions)

What do you think? [~showuon];;;","19/Sep/21 00:00;RivenSun;[~showuon]

After thinking about it, here is my latest points.
 Hope you can reply me as soon as possible, thank you.

     1. Because the poll method of KafkaConsumer promises to return within the time specified by the customer, unless the customer sets the ConsumerRebalanceListener to perform a time-consuming operation. See the comment of poll(final Duration timeout) method as below:

 
{code:java}
This method returns immediately if there are records available. Otherwise, it will await the passed timeout. If the timeout expires, an empty record set will be returned. Note that this method may block beyond the timeout in order to execute custom ConsumerRebalanceListener callbacks.{code}
 

In order to keep this promise, I also suggest passing the *Timer set by the customer for poll()* to the maybeAutoCommitOffsetsSync(*Timer timer*) method of ConsumerCoordinator
And the poll()'s Timer should be applied to *onJoinPrepare*.
However, due to other comprehensive considerations, the timers *in other code blocks* in ensureActiveGroup (final Timer timer) *should remain as they are*, using the Timer passed from the upper layer, perhaps time.timer(0L)

 

     2. Modify ConsumerCoordinator's maybeAutoCommitOffsetsSync(Timer timer) method
 Below are my preliminary code changes：

 
{code:java}
   private void maybeAutoCommitOffsetsSync(Timer timer) {
    if (autoCommitEnabled) {
        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();
        
        cleanUpConsumedOffsets(allConsumedOffsets);
        
        try {
            log.debug(""Sending synchronous auto-commit of offsets {}"", allConsumedOffsets);
            if (!commitOffsetsSync(allConsumedOffsets, timer))
                log.debug(""Auto-commit of offsets {} timed out before completion"", allConsumedOffsets);
        } catch (WakeupException | InterruptException e) {
            log.debug(""Auto-commit of offsets {} was interrupted before completion"", allConsumedOffsets);
            // rethrow wakeups since they are triggered by the user
            throw e;
        } catch (Exception e) {
            // consistent with async auto-commit failures, we do not propagate the exception
            log.warn(""Synchronous auto-commit of offsets {} failed: {}"", allConsumedOffsets, e.getMessage());
        }
    }
}{code}
{code:java}
  private void cleanUpConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> willCommitOffsets) {

    if (willCommitOffsets.isEmpty())
        return;

    Set<String> subscription = subscriptions.subscription();
    Set<TopicPartition> toGiveUpTopicPartitions = new HashSet<>();

    Iterator<Map.Entry<TopicPartition, OffsetAndMetadata>> iterator = willCommitOffsets.entrySet().iterator();

    while (iterator.hasNext()) {

        Map.Entry<TopicPartition, OffsetAndMetadata> entry = iterator.next();

        if (!subscription.contains(entry.getKey().topic())) {

            toGiveUpTopicPartitions.add(entry.getKey());
            iterator.remove();
        }

    }

    if (toGiveUpTopicPartitions.size() > 0) {

        //Because toGiveUpTopicPartitions may receive `UnknownTopicOrPartitionException` when submitting their offsets.
        //We are prepared to abandon them. The worst effect is that these partitions may repeatedly consume some messages
        log.warn(""Synchronous auto-commit of offsets {} will be abandoned"", toGiveUpTopicPartitions);

    }
}

{code}
 ;;;","19/Sep/21 02:47;showuon;[~RivenSun], thanks for your response. For your latest comment, I understand what you are trying to achieve, to update the offset metadata (to remove non-existed partitions), before go to the ""possible"" infinite commit offset loop before timeout. But unfortunately, it could still have possibility to cause the issue you reported, because there's still a race condition. Simply put, thread A tried to commit offsets, thread B tried to delete topics, we never know who will reach broker first, and which command will be processed first. So, even you update to the ""latest"" metadata before committing offset in thread A, it's still possible thread B comes and deletes the some topics before thread A committing offsets.

 

So, to achieve what you wanted, we need to `cleanUpConsumedOffsets` inside the while loop in the `commitOffsetsSync`. Each time there's an `UnknownTopicOrPartitionException` returned, we tried to update the metadata. And in the `cleanUpConsumedOffsets`, we need to make sure to get an up-to-date subscription there.

 

This is a way to fix this issue, of course. However, this way, it will make the commitOffsets method more complex, and might have other errors to handle (during request metadata update).

 

For me, I still prefer to control the commitOffset method via the timeout, and if the metadata is already updated by other threads (ex: topics deleted), this commit will fail with timeout. And the caller should do their own metadata update for next retry.

 

Let's see if there are other opinions from other experts. :);;;","19/Sep/21 06:02;RivenSun; 

Thank you [~showuon]  very much for your reply, I very much agree with your points of view above

I also considered what you said about a race condition,
 So I suggest to modify the *two points* together

1. Try `cleanUpConsumedOffsets` before submitting the offsets, maybe 90% of abnormal situations can be avoided.
 I didn't do it (`cleanUpConsumedOffsets`) *in commitOffsetsSync*. As you said, that would make the infinite loop more complicated. We just need to *try to* clean up unKnownTopicPartitions before entering the loop

2. Pass the poll()'s timer set by the customer to commitOffsetsSync(allConsumedOffsets, timer). to avoid *exceeding the timeout expected by the customer*

I am also very happy to invite other experts to analyze this problem, thank you everyone for making Kafka more perfect:D;;;","19/Sep/21 11:02;RivenSun;helo [~showuon] , here is my latest progress

1. I have pulled the latest version of Kafka code, modified the code locally, compiled and tested, the problem can be resolved.
The result of the debugTest can refer to the attachments ThirdDebugLog1 and ThirdDebugLog2

2. I have submitted a PullRequest, please help to review it, or have a better solution, thank you very much.
https://github.com/apache/kafka/pull/11340;;;","20/Sep/21 07:58;showuon;Before dive into the PR, I'd like to hear comments from other experts. [~guozhang] [~dajac] [~ableegoldman] , any comments?;;;","20/Sep/21 11:26;RivenSun;Hi [~showuon]

Thank you for your reply, after my test subscribe(Collection<String> topics) also has the same problem.
 But we can no longer use the *SubscriptionState subscriptions* variable in  'cleanUpConsumedOffsets' method. Because in the AUTO_TOPIC type, the subscriptions variable will not be updated to the latest valid value. SourceCode see as below:

 
{code:java}
   void maybeUpdateSubscriptionMetadata() {
    int version = metadata.updateVersion();
    if (version > metadataSnapshot.version) {
        Cluster cluster = metadata.fetch();

        if (subscriptions.hasPatternSubscription())
            updatePatternSubscription(cluster);

        // Update the current snapshot, which will be used to check for subscription
        // changes that would require a rebalance (e.g. new partitions).
        metadataSnapshot = new MetadataSnapshot(subscriptions, cluster, version);
    }
}{code}
 

So the most elegant way to clean up unKnownTopicPartitions for AUTO_PATTERN and AUTO_TOPICS is to use *metadata.fetch().topics()*，see as below
{code:java}
  private void cleanUpConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> willCommitOffsets) {

    if (willCommitOffsets.isEmpty())
        return;

    Set<String> validTopics = metadata.fetch().topics();
    Set<TopicPartition> toGiveUpTopicPartitions = new HashSet<>();

    Iterator<Map.Entry<TopicPartition, OffsetAndMetadata>> iterator = willCommitOffsets.entrySet().iterator();

    while (iterator.hasNext()) {

        Map.Entry<TopicPartition, OffsetAndMetadata> entry = iterator.next();

        if (!validTopics.contains(entry.getKey().topic())) {

            toGiveUpTopicPartitions.add(entry.getKey());
            iterator.remove();
        }

    }

    if (toGiveUpTopicPartitions.size() > 0) {

        //Because toGiveUpTopicPartitions may receive `UnknownTopicOrPartitionException` when submitting their offsets.
        //We are prepared to abandon them. The worst effect is that these partitions may repeatedly consume some messages
        log.warn(""Synchronous auto-commit of offsets {} will be abandoned"", toGiveUpTopicPartitions);

    }
}{code}
 

 ;;;","20/Sep/21 12:19;dajac;[~RivenSun] Thanks for the bug report. The ticket says that this affects 2.8.1, which was released during the weekend. I suppose that you don't use it already, right? Therefore, I suppose that it affects older versions as well? Do you know which ones, if not all?;;;","20/Sep/21 13:01;RivenSun;Hi [~dajac], 

May all current versions have this issue

The version of Kafka-client used in my production environment is *2.2.2*.  The consumer block in the poll method,cannot consume any messages, and the broker-side cpu and traffic increase sharply.

After the local test using 2.2.2 version, the phenomenon of the production can be reproduced, I want to know whether the latest version of Kafka-client fixes this issue. So I downloaded the *2.8.1* version for kafka-client from the maven repository. After local debugging, I found that the production problem can still be reproduced, even if you introduced the new `*waitForJoinGroup*`  variable in the KafkaConsumer's *updateAssignmentMetadataIfNeeded* method in the hign version SDK.
{code:java}
if (includeMetadataInTimeout) {
    // try to update assignment metadata BUT do not need to block on the timer for join group
    updateAssignmentMetadataIfNeeded(timer, false);
} else {
    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {
        log.warn(""Still waiting for metadata"");
    }
}{code}
{code:java}
  boolean updateAssignmentMetadataIfNeeded(final Timer timer, final boolean waitForJoinGroup) {
    if (coordinator != null && !coordinator.poll(timer, waitForJoinGroup)) {
        return false;
    }

    return updateFetchPositions(timer);
}{code}
 

So I folked a copy of the latest code from GitHub([https://github.com/apache/kafka/]). 
 After local code modification and debug, this Jira issue can be fixed finally :D.

You can see my code changes here
[https://github.com/apache/kafka/pull/11340]

Maybe you can consider some of the suggestions I mentioned above. 
Thanks a lot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemorySessionStore#fetch/backwardFetch doesn't return in correct order,KAFKA-13309,13401860,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,17/Sep/21 12:33,30/Sep/21 00:44,13/Jul/23 09:17,29/Sep/21 00:35,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,streams,,,,,0,,,,,"We supported backward iterator for SessionStore in KAFKA-9929. But we cannot return the correct order when fetch/backwardFetch the key range when there are multiple records in the same session window.

For example:

We have a session window inactivity gap with 10 ms, and the records:

key: ""A"", value: ""AA"", timestamp: 0 --> with SessionWindow(0, 0)

key: ""B"", value: ""BB"", timestamp: 0 --> with SessionWindow(0, 0)

key: ""C"", value: ""CC"", timestamp: 0 --> with SessionWindow(0, 0)

key: ""D"" value: ""DD"", timestamp: 100 --> with SessionWindow(100, 100)

 

So, when fetch(""A"" /\*key from\*/, ""D"" /\*key to\*/), we expected to have [A, B, C, D], but we'll have [C, B A, D ]

 

And the reason is here:

[https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java#L276-L295]

 
{code:java}
public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes keyFrom, final Bytes keyTo) {
                return registerNewIterator(keyFrom, keyTo, Long.MAX_VALUE, endTimeMap.entrySet().iterator(), false); // <-- the final param is ""isFarwarded"", which should be true for ""fetch"" case, and false for ""backwardFetch"" case
            }
{code}
We pass ""false"" in the ""is forward"" parameter for `fetch` method, and ""true"" for ""backwardFetch"" method, which obviously is wrong.

 ",,ableegoldman,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-17 12:33:41.0,,,,,,,,,,"0|z0v1gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Null connector config value passes validation, but fails creation",KAFKA-13306,13401658,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hunyady,hunyady,hunyady,16/Sep/21 12:28,11/Feb/22 15:14,13/Jul/23 09:17,11/Feb/22 15:14,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,KafkaConnect,,,,,0,,,,,"When validating a connector config containing a property with a null value the validation passes, but when creating a connector with the same config the worker fails to start the connector because of an invalid config.

Steps to reproduce:
 # Send PUT request to
 \{\{connectRest\}\}/connector-plugins/FileStreamSource/config/validate
 Request body:
{code}
{
  ""connector.class"": ""FileStreamSource"",
  ""name"": ""file-source"",
  ""topic"": ""target-topic"",
  ""file"":""/source.txt"",
  ""tasks.max"": ""1"",
  ""foo"": null
}
{code}
Response:
{code}
{ ""name"": ""FileStreamSource"", ""error_count"": 0, ... }
{code}

 #  OPTION A:
 Send PUT request to
 \{\{connectRest}}/connectors/file-source/config
 Request body:
{code}
{
  ""connector.class"": ""FileStreamSource"",
  ""name"": ""file-source"",
  ""topic"": ""target-topic"",
  ""file"":""/source.txt"",
  ""tasks.max"": ""1"",
  ""foo"": null
}
{code}
OPTION B:
 Send POST request to
 \{\{connectRest}}/connectors/
 Request Body:
{code}
{
  ""connector.class"": ""FileStreamSource"",
  ""name"": ""file-source"",
  ""topic"": ""target-topic"",
  ""file"":""/source.txt"",
  ""tasks.max"": ""1"",
  ""foo"": null
}
{code}
Result:
Connector is created but connector fails to start, with below exception that indicates an invalid config:
{noformat}
ERROR org.apache.kafka.connect.runtime.WorkerConnector: [file-source|worker] WorkerConnector\{id=file-source} Error initializing connector
java.lang.ClassCastException: Non-string value found in original settings for key foo: null
        at org.apache.kafka.common.config.AbstractConfig.originalsStrings(AbstractConfig.java:234)
        at org.apache.kafka.connect.runtime.WorkerConnector.initialize(WorkerConnector.java:77)
        at org.apache.kafka.connect.runtime.Worker.startConnector(Worker.java:256)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startConnector(DistributedHerder.java:1190)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.processConnectorConfigUpdates(DistributedHerder.java:548)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:395)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:289)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}
 ",,hunyady,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-16 12:28:09.0,,,,,,,,,,"0|z0v080:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NullPointerException in LogCleanerManager ""uncleanable-bytes"" gauge",KAFKA-13305,13401439,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vincent81jiang,vincent81jiang,vincent81jiang,15/Sep/21 17:29,27/Sep/21 16:52,13/Jul/23 09:17,27/Sep/21 16:52,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,log cleaner,,,,,0,,,,,"We've seen following exception in production environment:
{quote} java.lang.NullPointerException: Cannot invoke ""kafka.log.UnifiedLog.logStartOffset()"" because ""log"" is null at

kafka.log.LogCleanerManager$.cleanableOffsets(LogCleanerManager.scala:599)
{quote}
Looks like uncleanablePartitions never has partitions removed from it to reflect partition deletion/reassignment.

 

We should fix the NullPointerException and removed deleted partitions from uncleanablePartitions.

 ",,junrao,vincent81jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 27 16:52:35 UTC 2021,,,,,,,,,,"0|z0uyvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/21 16:52;junrao;merged the PR trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long restoration times for new tasks can lead to transaction timeouts,KAFKA-13295,13400897,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sagarrao,ableegoldman,ableegoldman,14/Sep/21 00:48,13/Jul/23 05:14,13/Jul/23 09:17,12/Jul/23 23:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,eos,new-streams-runtime-should-fix,,,"In some EOS applications with relatively long restoration times we've noticed a series of ProducerFencedExceptions occurring during/immediately after restoration. The broker logs were able to confirm these were due to transactions timing out.

In Streams, it turns out we automatically begin a new txn when calling {{send}} (if there isn’t already one in flight). A {{send}} occurs often outside a commit during active processing (eg writing to the changelog), leaving the txn open until the next commit. And if a StreamThread has been actively processing when a rebalance results in a new stateful task without revoking any existing tasks, the thread won’t actually commit this open txn before it goes back into the restoration phase while it builds up state for the new task. So the in-flight transaction is left open during restoration, during which the StreamThread only consumes from the changelog without committing, leaving it vulnerable to timing out when restoration times exceed the configured transaction.timeout.ms for the producer client.",,ableegoldman,cadonna,dajac,gphilipp,guozhang,MarkC0x,mjsax,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 23:26:18 UTC 2023,,,,,,,,,,"0|z0uvj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/21 00:50;ableegoldman;Note that https://issues.apache.org/jira/browse/KAFKA-13249, while not directly related, makes it more likely for an application to hit this bug since it can result in Streams restoring additional records and thus longer restoration times;;;","14/Sep/21 22:21;guozhang;As we talked about the potential fix in the near term, one possibility is that `onPartitionAssigned` when new tasks are created, we always blindly do a commitOffsetOrTransaction as well so that if there's any ongoing transactions, it would be committed before we potentially go on to restore the newly assigned tasks.;;;","15/Sep/21 12:29;sagarrao;[~guozhang]/ [~ableegoldman] I assigned this to myself.. Would go through the code and see if I can find something :D ;;;","28/Sep/21 23:05;ableegoldman;{quote}we always blindly do a commitOffsetOrTransaction as well so that if there's any ongoing transactions, it would be committed before we potentially go on to restore the newly assigned tasks.
{quote}
I still think we want to keep the scope of committing offsets as narrow as possible, since committing adds a nontrivial overhead to the rebalance. The ideal fix would be to just start checking on the transaction time during restoration, and if we find ourselves getting too close to the timeout then we commit. But that's going to introduce some unnecessary complexity, so a good middle ground might be to just check if `Task#needsCommit` is true for any of the active tasks. And maybe also to check if we even have any new tasks to restore. I think those two checks would be simple enough while still keeping the committing sufficiently narrow in scope.

[~sagarrao] does that make sense?;;;","29/Sep/21 17:39;sagarrao;[~ableegoldman], yes it does. I wanted to ask the same question, do we need to commit offsets for all the tasks or reduce it somehow and you have answered it. I have a few of more questions ->

1) [~guozhang] has mentioned that we can invoke commitOffsetOrTransaction in the onPartitionAssigned method. However, in StreamPartitionRebalanceListener, there's a comment stating that:

 
{code:java}
//all task management is already handled by:
// org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment{code}
So, do we need to add the logic in onAssignment? I haven't quite looked at the flow entirely and hence asking this question

2) How do we handle cases if a TaskCorruptedException or TimeoutException is thrown during commiting. I see these have been handled in handleRevocation whereby these are considered dirtyTasks and then closed. So, how do we handle failure scenarios for offset committing ?

 

3) Do we also need to write checkpoints post committing?;;;","08/Oct/21 18:31;sagarrao;hey [~ableegoldman], whenever you get the chance, plz help me with the couple of questions from the comment above. ;;;","15/Oct/21 00:46;ableegoldman;1) The rebalance callbacks can take a bit to wrap your head around, hence the comment trying to explain more or less the flow of task management...but basically, at the end of each rebalance the following are invoked in this order:
 # StreamsRebalanceListener#onPartitionsRevoked (only if there are revoked partitions)
 # StreamsPartitionAssignor#onAssignment (always)
 # StreamsRebalanceListener#onPartitionsAssigned (always)

Callback #1 invokes TaskManager#handleRevocation, which cleans up and commits any active tasks that were revoked. Then callback #2 invokes TaskManager#handleAssignment, which is what that comment is referring to. This is where you'll want to add the necessary logic

2) It's probably most instructive to look at the #handleRevocation implementation, which also covers the case of committing tasks that are not revoked (because if we have to commit one active task that's revoked, we have to commit all of them) and how to handle exceptions there. In this case, since it's EOS by definition, the only possible exceptions are TaskCorruptedException and a general RuntimeException. For the former case, you can just closeDirtyAndRevive, while for the latter you should keep track of the exception and then rethrow it at the end of #handleAssignment

3) Yes, but only if the commit succeeded (ie didn't throw either of the above);;;","15/Nov/21 10:04;dajac;Moving it to the next release as we are part the code freeze for 3.1.0.;;;","04/Apr/22 16:50;cadonna;Removing from 3.2. release since code freeze passed.;;;","28/Dec/22 23:46;mjsax;[~ableegoldman] – I am wondering if this issue is resolved via https://issues.apache.org/jira/browse/KAFKA-14294 implicitly?;;;","30/Dec/22 05:15;ableegoldman;[~mjsax] unfortunately no, it's a separate issue as we would not even attempt the commit to begin with in this case.

 

I've been chatting with Sagar about it but didn't want to push something into 3.4 at the last minute, however I do think we should try to fix in 3.5;;;","03/Jan/23 23:51;mjsax;Thanks. SG.;;;","09/Mar/23 11:42;sagarrao;Had a discussion with [~guozhang] and the suggestion is to wait for 4.0 release when EOS-v1 would be deprecated. It would simpler to implement the changes in EOS-v2.;;;","21/Mar/23 23:11;guozhang;[~sagarrao] I had a chat with others driving the 4.0 release and it seems like it will still take some time, given that I will resume helping you to finish fixing this bug and not wait for that release.;;;","23/Mar/23 19:02;sagarrao;Thanks [~guozhang] , glad to know that!;;;","12/Jul/23 23:26;mjsax;With the new restore-thread, this issue should be resolved implicilty.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Netty to 4.1.68 for CVE fixes,KAFKA-13294,13400777,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dongjin,51n15t9r,51n15t9r,13/Sep/21 12:26,26/Nov/21 14:12,13/Jul/23 09:17,29/Oct/21 04:43,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,core,,,,,0,,,,,"netty has reported a couple of CVEs regarding the usage of Bzip2Decoder and SnappyFrameDecoder. 

Reference :

[CVE-2021-37136 - https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv|https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv]

[CVE-2021-37137 - https://github.com/netty/netty/security/advisories/GHSA-9vjp-v76f-g363|https://github.com/netty/netty/security/advisories/GHSA-9vjp-v76f-g363]

 

Can we upgrade Netty to version 4.1.68.Final to fix this ? ",,51n15t9r,dominique,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13413,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 26 14:12:03 UTC 2021,,,,,,,,,,"0|z0uusg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/21 12:27;51n15t9r;I can push a PR for updating netty. ;;;","28/Oct/21 13:11;mimaison;I see https://github.com/apache/kafka/pull/11324 has been merged, is there anything else to do or can we resolve this ticket? cc [~dongjin];;;","29/Oct/21 04:43;dongjin;[~mimaison] Here it is.;;;","15/Nov/21 15:01;dominique;Hi, what will be the fix version for this CVE ?;;;","15/Nov/21 15:47;mimaison;Thanks [~dongjin] I've updated the fix version field. The fix is going to be in Kafka 3.1.0;;;","15/Nov/21 22:23;dongjin;[~mimaison] +1.;;;","26/Nov/21 14:12;dominique;Thanks [~mimaison] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id,KAFKA-13292,13400721,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,neeraj.vaidya,neeraj.vaidya,13/Sep/21 06:50,17/Sep/21 04:08,13/Jul/23 09:17,17/Sep/21 04:08,2.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"I have a KafkaStreams application which consumes from a topic which has 12 partitions. The incoming message rate into this topic is very low, perhaps 3-4 per minute. Also, some partitions will not receive messages for more than 7 days.
 
Exactly after 7 days of starting this application, I seem to be getting the following exception and the application shuts down, without processing anymore messages :
 
{code:java}
2021-09-10T12:21:59.636 [kafka-producer-network-thread | mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer] INFO  o.a.k.c.p.i.TransactionManager - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer, transactionalId=mtx-caf-0_2] Transiting to abortable error state due to org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
2021-09-10T12:21:59.642 [kafka-producer-network-thread | mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer] ERROR o.a.k.s.p.i.RecordCollectorImpl - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] task [0_2] Error encountered sending record to topic mtx-caf-DuplicateCheckStore-changelog for task 0_2 due to:
org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
Exception handler choose to FAIL the processing, no more records would be sent.
2021-09-10T12:21:59.740 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] ERROR o.a.k.s.p.internals.StreamThread - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:
org.apache.kafka.streams.errors.StreamsException: Error encountered sending record to topic mtx-caf-DuplicateCheckStore-changelog for task 0_2 due to:
org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
Exception handler choose to FAIL the processing, no more records would be sent.
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError(RecordCollectorImpl.java:214)
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.lambda$send$0(RecordCollectorImpl.java:186)
        at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1363)
        at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:231)
        at org.apache.kafka.clients.producer.internals.ProducerBatch.abort(ProducerBatch.java:159)
        at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortUndrainedBatches(RecordAccumulator.java:781)
        at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:425)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:313)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
2021-09-10T12:21:59.740 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
{code}
 
After this, I can see that all 12 tasks (because there are 12 partitions for all topics) get shutdown and this brings down the whole application.
 
I understand that the transactional.id.expiration.ms = 7 days (default) will likely cause the application thread from getting expired, but why does this specific thread/task not get fenced or respawned.
Why shutdown the entire Streams processing application just because one task has been idle ??
 
Is there a way to keep my application up and running without causing it to shutdown ?",,ableegoldman,mjsax,neeraj.vaidya,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10733,,,,,,,,,,,,,,,KAFKA-13267,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 17 04:08:53 UTC 2021,,,,,,,,,,"0|z0uug0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/21 17:27;mjsax;[https://cwiki.apache.org/confluence/display/KAFKA/KIP-671%3A+Introduce+Kafka+Streams+Specific+Uncaught+Exception+Handler] might help – it's part of AK 2.8 though.;;;","13/Sep/21 17:30;mjsax;Closing this as ""invalid"" as it seems to be a question, not a bug report. Please use the mailing lists to ask questions. Thanks.;;;","14/Sep/21 05:39;neeraj.vaidya;Thanks [~mjsax] for the reference to the KIP. It might not be possible for my application to be upgraded soon.

However, I still feel this is a defect because I believed that stream processing should continue even when transactional.id.expiration.ms has elapsed and should not bring down the entire application.

I noticed from the logs that the application does try to get a new epoch and producer Id, but then still suspends and stops all threads. As you will observe below towards the end of the log messages (at timestamp : 2021-09-10T12:21:59.766) , for the task[ 0_2], the call to ""*Invoking InitProducerId*"" is made, but still the flow ends up in the UnCaughtExceptionHandler.

I was of the understanding that once the producerId epoch is bumped, stream processing should resume.

However, it starts shutting down the entire application by an uncaught exception.

*FYI - I am using exactly_once processing and enable.idempotence=true*

 
{code:java}
2021-09-10T12:21:59.636 [kafka-producer-network-thread | mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer] INFO  o.a.k.c.p.i.TransactionManager - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-Stre
amThread-1-0_2-producer, transactionalId=mtx-caf-0_2] Transiting to abortable error state due to org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its tra
nsactional id.
2021-09-10T12:21:59.642 [kafka-producer-network-thread | mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer] ERROR o.a.k.s.p.i.RecordCollectorImpl - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamT
hread-1] task [0_2] Error encountered sending record to topic mtx-caf-DuplicateCheckStore-changelog for task 0_2 due to:
org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
Exception handler choose to FAIL the processing, no more records would be sent.
2021-09-10T12:21:59.740 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] ERROR o.a.k.s.p.internals.StreamThread - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] Encountered the following exception
 during processing and the thread is going to shut down: 
org.apache.kafka.streams.errors.StreamsException: Error encountered sending record to topic mtx-caf-DuplicateCheckStore-changelog for task 0_2 due to:
org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
Exception handler choose to FAIL the processing, no more records would be sent.
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError(RecordCollectorImpl.java:214)
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.lambda$send$0(RecordCollectorImpl.java:186)
        at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1363)
        at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:231)
        at org.apache.kafka.clients.producer.internals.ProducerBatch.abort(ProducerBatch.java:159)
        at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortUndrainedBatches(RecordAccumulator.java:781)
        at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:425)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:313)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
2021-09-10T12:21:59.740 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] State transition from RUNNING to PE
NDING_SHUTDOWN
2021-09-10T12:21:59.741 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] Shutting down
2021-09-10T12:21:59.743 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  c.m.a.k.t.SessionBasedDataUsageAccumulator - MSG=Shutting Down
2021-09-10T12:21:59.744 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.internals.StreamTask - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] task [0_8] Suspended running
2021-09-10T12:21:59.747 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.clients.consumer.KafkaConsumer - MSG=[Consumer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-restore-consumer, groupId=nu
ll] Subscribed to partition(s): mtx-caf-DuplicateCheckStore-changelog-10, mtx-caf-DuplicateCheckStore-changelog-6, mtx-caf-DuplicateCheckStore-changelog-4, mtx-caf-DuplicateCheckStore-changelog-2, mtx-caf-AggregatedRecordStore-changelog-10, m
tx-caf-DuplicateCheckStore-changelog-0, mtx-caf-AggregatedRecordStore-changelog-4, mtx-caf-AggregatedRecordStore-changelog-6, mtx-caf-AggregatedRecordStore-changelog-0, mtx-caf-AggregatedRecordStore-changelog-2, mtx-caf-DuplicateCheckStore-ch
angelog-11, mtx-caf-DuplicateCheckStore-changelog-9, mtx-caf-DuplicateCheckStore-changelog-7, mtx-caf-AggregatedRecordStore-changelog-11, mtx-caf-DuplicateCheckStore-changelog-5, mtx-caf-DuplicateCheckStore-changelog-3, mtx-caf-AggregatedReco
rdStore-changelog-7, mtx-caf-DuplicateCheckStore-changelog-1, mtx-caf-AggregatedRecordStore-changelog-9, mtx-caf-AggregatedRecordStore-changelog-3, mtx-caf-AggregatedRecordStore-changelog-5, mtx-caf-AggregatedRecordStore-changelog-1
2021-09-10T12:21:59.753 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.i.RecordCollectorImpl - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] task [0_8] Closing record collector 
dirty
...
...
...
...
2021-09-10T12:21:59.763 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.internals.StreamTask - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] task [0_2] Suspended running
2021-09-10T12:21:59.764 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.clients.consumer.KafkaConsumer - MSG=[Consumer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): mtx-caf-DuplicateCheckStore-changelog-11, mtx-caf-DuplicateCheckStore-changelog-10, mtx-caf-DuplicateCheckStore-changelog-9, mtx-caf-DuplicateCheckStore-changelog-7, mtx-caf-DuplicateCheckStore-changelog-6, mtx-caf-AggregatedRecordStore-changelog-11, mtx-caf-DuplicateCheckStore-changelog-4, mtx-caf-AggregatedRecordStore-changelog-7, mtx-caf-AggregatedRecordStore-changelog-10, mtx-caf-DuplicateCheckStore-changelog-0, mtx-caf-AggregatedRecordStore-changelog-9, mtx-caf-AggregatedRecordStore-changelog-4, mtx-caf-AggregatedRecordStore-changelog-6, mtx-caf-AggregatedRecordStore-changelog-0
2021-09-10T12:21:59.765 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.s.p.i.RecordCollectorImpl - MSG=stream-thread [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] task [0_2] Closing record collector dirty
2021-09-10T12:21:59.765 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.clients.producer.KafkaProducer - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer, transactionalId=mtx-caf-0_2] Aborting incomplete transaction
2021-09-10T12:21:59.766 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.c.p.i.TransactionManager - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer, transactionalId=mtx-caf-0_2] Invoking InitProducerId with current producer ID and epoch (producerId=1005, epoch=16) in order to bump the epoch
2021-09-10T12:21:59.769 [kafka-producer-network-thread | mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer] INFO  o.a.k.c.p.i.TransactionManager - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer, transactionalId=mtx-caf-0_2] ProducerId set to 1006 with epoch 0
...
...
...
2021-09-10T12:21:59.798 [mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1] INFO  o.a.k.clients.producer.KafkaProducer - MSG=[Producer clientId=mtx-caf-53dc7e96-90f1-4ae9-8af6-236d22c88e08-StreamThread-1-0_2-producer, transactionalId=mtx-caf-0_2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.

{code}
 ;;;","14/Sep/21 05:50;neeraj.vaidya;As indicated in my previous comments.;;;","14/Sep/21 07:23;mjsax;Fair enough. In the end, {{InvalidPidMappingException}} is not a fatal exception according to [https://cwiki.apache.org/confluence/display/KAFKA/KIP-691%3A+Enhance+Transactional+Producer+Exception+Handling] that proposed to improve error handling. However as it's a public API change it won't be contained in any bug-fix release.

It's also very unlikely that there will be a 2.7.2 bug-fix release, do in the end, upgrading might be your only option after KIP-691 is done.

Thus, I still would propose to close this ticket at ""fixed by"" https://issues.apache.org/jira/browse/KAFKA-10733 ;;;","14/Sep/21 21:29;neeraj.vaidya;Thanks [~mjsax]

So, to summarize : Will KIP-671 and upgrading to 2.8.0 be sufficient to avoid this _InvalidPidMappingException_ ? Or do I need to wait for KIP-691 and 3.x to be available ?

Do I need to just upgrade the kafka-clients, kafka-streams, etc. library dependencies to use the 2.8 version or do I need to update the Brokers as well from 2.7.0 to 2.8.0 ?;;;","14/Sep/21 23:39;mjsax;KIP-671 won't avoid the exception, but it gives you the ability to avoid that the thread dies, by using the newly added exception handler. (Well, technically the thread dies, but you can just restart a new one.)

Also KIP-691 won't really _avoid_ the exception, but we plan to have build-it support to handle the exception within Kafka Streams such that the thread won't die and thus it does not surface to the exception handler. 

If you upgrade client side, you need to bump all versions for producer/consumer/admin/KafkaStreams within the same application, because you cannot mix-and-match those, but they must always be the same. – You don't need to upgrade the brokers though. Clients are (forward and) backward compatible to (newer) older brokers in general (for details, read the docs – there are some limitation what version work together – for your particular case, there should be no limitations IIRC).;;;","15/Sep/21 12:37;neeraj.vaidya;Thanks [~mjsax]

I will look at upgrading the client libraries to 2.8.0.;;;","17/Sep/21 04:08;neeraj.vaidya;Upgraded the client libraries to 2.8.0 while still using 2.7.0 broker.

Using the new StreamsExceptionHandler, added code to return REPLACE_THREAD in case of InvalidPidMappingException.

This causes the application to create a new thread and continue processing Event.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction find-hanging command with --broker-id excludes internal topics,KAFKA-13288,13400338,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,09/Sep/21 23:51,10/Sep/21 22:11,13/Jul/23 09:17,10/Sep/21 22:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"We use the vanilla `Admin.listTopics()` in this command if `--broker-id` is specified. By default, this excludes internal topics.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 10 16:31:06 UTC 2021,,,,,,,,,,"0|z0us2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/21 16:31;hachikuji;Note that it is still possible to use it with the --topic option instead. For example: 
{code}
kafka-transaction.sh --bootstrap-server localhost:9092 find-hanging --topic __consumer_offsets
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization of long tagged string in request/response throws BufferOverflowException,KAFKA-13277,13399796,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rsivaram,rsivaram,rsivaram,07/Sep/21 15:43,08/Sep/21 09:03,13/Jul/23 09:17,08/Sep/21 08:47,2.4.1,2.5.1,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,clients,,,,,0,,,,,Size computation for tagged strings in the message generator is incorrect and hence it works only for small strings (126 bytes or so) where the length happens to be correct.,,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 07 15:52:41 UTC 2021,,,,,,,,,,"0|z0uoqg:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,"07/Sep/21 15:49;ijuma;[~rsivaram] isn't this a blocker for 3.0?;;;","07/Sep/21 15:52;rsivaram;[~ijuma] Yes, it will be good include in 3.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Public DescribeConsumerGroupsResult constructor refers to KafkaFutureImpl,KAFKA-13276,13399613,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,tombentley,tombentley,tombentley,06/Sep/21 16:11,09/Sep/21 08:35,13/Jul/23 09:17,08/Sep/21 19:16,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,The new public DescribeConsumerGroupsResult constructor refers to the non-public API KafkaFutureImpl,,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-06 16:11:55.0,,,,,,,,,,"0|z0unls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kafka may fail to connect to ZooKeeper, retry forever, and never start",KAFKA-13270,13399047,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ijuma,rndgstn,rndgstn,03/Sep/21 01:12,06/Sep/21 16:21,13/Jul/23 09:17,06/Sep/21 16:21,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The implementation of https://issues.apache.org/jira/browse/ZOOKEEPER-3593 in ZooKeeper version 3.6.0 decreased the default value for the ZooKeeper client's `jute.maxbuffer` configuration from 4MB to 1MB.  This can cause a problem if Kafka tries to retrieve a large amount of data across many znodes -- in such a case the ZooKeeper client will repeatedly emit a message of the form ""java.io.IOException: Packet len <####> is out of range"" and the Kafka broker will never connect to ZooKeeper and fail to make progress on the startup sequence.  We can avoid the potential for this issue to occur by explicitly setting the value to 4MB whenever we create a new ZooKeeper client as long as no explicit value has been set via the `jute.maxbuffer` system property.",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-03 01:12:24.0,,,,,,,,,,"0|z0uk48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`InitialFetchState` should be created after partition is removed from the fetchers,KAFKA-13266,13398958,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,02/Sep/21 14:05,08/Sep/21 16:37,13/Jul/23 09:17,08/Sep/21 16:37,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,," `ReplicationTest.test_replication_with_broker_failure` in KRaft mode sometimes fails with the following error in the log:
{noformat}
[2021-08-31 11:31:25,092] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Unexpected error occurred while processing data for partition __consumer_offsets-1 at offset 31727 (kafka.server.ReplicaFetcherThread)java.lang.IllegalStateException: Offset mismatch for partition __consumer_offsets-1: fetched offset = 31727, log end offset = 31728. at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:194) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$8(AbstractFetcherThread.scala:545) at scala.Option.foreach(Option.scala:437) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:533) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7$adapted(AbstractFetcherThread.scala:532) at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62) at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry(JavaCollectionWrappers.scala:359) at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry$(JavaCollectionWrappers.scala:355) at scala.collection.convert.JavaCollectionWrappers$AbstractJMapWrapper.foreachEntry(JavaCollectionWrappers.scala:309) at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:532) at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:216) at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:215) at scala.Option.foreach(Option.scala:437) at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:215) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:197) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:99)[2021-08-31 11:31:25,093] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Partition __consumer_offsets-1 marked as failed (kafka.server.ReplicaFetcherThread)
{noformat}
 The issue is due to a race condition in `ReplicaManager#applyLocalFollowersDelta`. The `InitialFetchState` is created and populated before the partition is removed from the fetcher threads. This means that the fetch offset of the `InitialFetchState` could be outdated when the fetcher threads are re-started because the fetcher threads could have incremented the log end offset in between.

The partitions must be removed from the fetcher threads before the `InitialFetchStates` are created.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-02 14:05:47.0,,,,,,,,,,"0|z0ujkg:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
backwardFetch in InMemoryWindowStore doesn't return in reverse order,KAFKA-13264,13398882,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,02/Sep/21 07:47,13/Sep/21 21:41,13/Jul/23 09:17,13/Sep/21 21:41,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,,,,,0,,,,,"When working on another PR, I found currently, the backwardFetch in InMemoryWindowStore doesn't return in reverse order when there are records in the same window.

ex: window size = 500,

input records:

key: ""a"", value: ""aa"", timestamp: 0 ==> will be in [0, 500] window

key: ""b"", value: ""bb"", timestamp: 10 ==> will be in [0, 500] window

 

So, internally, the ""a"" and ""b"" will be in the same segment.

when fetch in forward order:

""a"" -> ""b"", which is expected

when fetch in backward order:

""a"" -> ""b"", which is NOT expected (because we didn't make the segment iterator as descendingMap)",,ableegoldman,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-02 07:47:33.0,,,,,,,,,,"0|z0uj3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mock Clients Now Have Final close() Methods,KAFKA-13262,13398779,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ijuma,grussell,grussell,01/Sep/21 18:40,07/Sep/21 19:19,13/Jul/23 09:17,07/Sep/21 19:19,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,clients,,,,,0,,,,,"Subclasses can no longer clean up resources when the consumer is closed.

Caused by https://github.com/apache/kafka/commit/2f3600198722dd5a01a210bc78b7d43b33967c7f",,grussell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-09-01 18:40:59.0,,,,,,,,,,"0|z0uigo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTable to KTable foreign key join loose events when using several partitions,KAFKA-13261,13398684,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vcrfxia,xnix,xnix,01/Sep/21 13:42,04/Nov/21 01:52,13/Jul/23 09:17,03/Nov/21 17:56,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,,,,,0,kip,,,,"KIP-775: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-775%3A+Custom+partitioners+in+foreign+key+joins]

 

Two incoming streams A and B. 

Stream A uses a composite key [a, b]

Stream B has key [b]

Stream B has 4 partitions and steams A has 1 partition.

What we try to do is repartition stream A to have 4 partitions too, then put both A and B into KTable and do a foreign key join on from A to B

When doing this, all messages does not end up in the output topic.

Repartitioning both to only use 1 partition each solve the problem so it seem like it has something to do with the foreign key join in combination with several partitions. 

One suspicion would be that it is not possible to define what partitioner to use for the join.

Any insight or help is greatly appreciated.

*Example code of the problem*
{code:java}
static Topology createTopoology(){
    var builder = new StreamsBuilder();

    KTable<String, String> tableB = builder.table(""B"",  stringMaterialized(""table.b""));

    builder
        .stream(""A"", Consumed.with(Serde.of(KeyA.class), Serde.of(EventA.class)))
        .repartition(repartitionTopicA())
        .toTable(Named.as(""table.a""), aMaterialized(""table.a""))
        .join(tableB, EventA::getKeyB, topicAandBeJoiner(), Named.as(""join.ab""), joinMaterialized(""join.ab""))
        .toStream()
        .to(""output"", with(...));

    return builder.build();
}

private static Materialized<KeyA, EventA> aMaterialized(String name) {
  Materialized<KeyA, EventA, KeyValueStore<Bytes, byte[]>> table = Materialized.as(name);
  return table.withKeySerde(Serde.of(KeyA.class)).withValueSerde(Serde.of(EventA.class));
}

private static Repartitioned<DriverPeriod, DriverCosts> repartitionTopicA() {
    Repartitioned<DriverPeriod, DriverCosts> repartitioned = Repartitioned.as(""driverperiod"");
    return repartitioned.withKeySerde(Serde.of(KeyA.class)).withValueSerde(Serde.of(EventA.class))
        .withStreamPartitioner(topicAPartitioner())
        .withNumberOfPartitions(4);
}

private static StreamPartitioner<DriverPeriod, DriverCosts> topicAPartitioner() {
    return (topic, key, value, numPartitions) -> Math.abs(key.getKeyB().hashCode()) % numPartitions;
}

private static Materialized<KeyA, EventA, KeyValueStore<Bytes, byte[]>> joinMaterialized(String name) {
    Materialized<DriverPeriod, DriverCosts, KeyValueStore<Bytes, byte[]>> table = Materialized.as(name);
    return table.withKeySerde(Serde.of(KeyA.class)).withValueSerde(Serde.of(EventA.class));
}

{code}",,abellemare,ableegoldman,guozhang,mjsax,showuon,vcrfxia,vvcephei,xnix,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13268,,,,,,,,,,,,,,KAFKA-13268,,,,,,"02/Sep/21 13:22;xnix;KafkaTest.java;https://issues.apache.org/jira/secure/attachment/13032900/KafkaTest.java",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 12 06:34:13 UTC 2021,,,,,,,,,,"0|z0uhvk:",9223372036854775807,,mjsax,,,,,,,,,,,,,,,,,,"01/Sep/21 17:31;guozhang;Thanks for reporting this [~xnix].

I've re-read the design doc in https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable, although it's illustrated where the foreign key is encoded on the values, not as part of the key, I think that should still work with the round-trip re-partitioning, and hence I'm not sure why foreign-key encoded as part of the key would not work. Ping [~abellemare] [~vvcephei] could you take a look to see if this is a bug?;;;","01/Sep/21 17:55;abellemare;Hi [~xnix]. We'll need some more information:

Have you already verified that the repartitioned Stream A is correctly co-partitioned with Stream B? This would be your first step, as if you can consistently reproduce this, it could very well be an incorrect repartitioning. 

Assuming that the events are correctly repartitioned and keyed, you may be seeing the following:
 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable#KIP213SupportnonkeyjoininginKTable-JoinerPropagationofStaleData]

Due to the distributed and independent-processing nature of Kafka Streams (tasks), it is possible that newer events are joined prior to older events. This is due to context switching between tasks at inopportune times, crashing instances, failures, etc. Stale events, from the perspective of the Left-Hand Side, will not be propagated, because they would otherwise overwrite newer data. This is one of the tradeoffs of this implementation.

For example, your Stream B inputs may be: 
(1, [b], foo )
(2, [b], bar )
(1, [b], bar )

But depending on how Kafka streams loads the events into the KTable (eg: loads them all in in a single batch, writes to the KTable, and writes to the subscription topic), you will either get all 3 joined events out, OR, you will only get:
(2, joinedResults[b], bar )
(1, joinedResults[b], bar ) (<--- this is the ""final"" state given the current inputs).

You _should_ always see the final join result, and it should always be the same value each time you run the test. If this is not occurring, could you please post a simple test demonstrating the inputs? I think you could probably get away [with copying this test|https://github.com/a0x8o/kafka/blob/master/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableForeignKeyJoinScenarioTest.java#L64] and modifying it to showcase your inputs. 

Finally, the reason I suspect its either the partitioner or the discarding of stale events is that with singular partitions, all data is necessarily both co-located and is guaranteed to be processed sequentially. In this case I would expect to always see every join result, in sequential order, without anything missing (as you so observed). ;;;","02/Sep/21 12:38;xnix;Hi [~guozhang] and [~abellemare], thank you for your answers.

Yes, A and B use a partitioner that use the same value from respective topic key.

In the scenario we have the output topic is compacted on the same key as Topic A, so if intermediate events would not come - it would be fine. We want the final result. What we're seeing though is combinations missing completely. 

We've read out all events from the A,B and output topics but also the internal topics created by the join. Expected is that all would have 66 ids. As said, running with same data with one partition create a perfect match where all events has passed through. 
{noformat}
#Using 4 partitions
A                 : ids: 66
B                 : ids: 66
table.b-changelog : ids: 66
table.a-changelog : ids: 66
join.ab-changelog : ids: 20
output            : ids: 20{noformat}
Attached KafkaTest.java, but we've written several junit test cases with the TopologyTestDriver and different amount of test data but are unable to reproduce the problem. (does the test driver consider several partitions?)

However, in a local docker environment we can reproduce the above scenario every time when using 4 partitions and the problem goes away when using 1 partition.

Below is when reading from the different topics directly for a specific id ""ID123"" 

Columns are 'offset, timestamp, partition | [key]value'

Using 4 partitions
{noformat}
table.a-changelog 
5 1612435945196 0 | [ID123, 202101] A01 
15 1614863137136 0 | [ID123, 202102] A02 
25 1617882052260 0 | [ID123, 202103] A03 
35 1620299210336 0 | [ID123, 202104] A04 
45 1622804606823 0 | [ID123, 202105] A05 
table.b-changelog 
6 1622617868856 0 | [ID123] BBB 
join.ab-changelog 
0 1622617868856 0 | [ID123, 202104] A04, BBB 
output 
0 1622617868856 0 | [ID123, 202104] A04, BBB 
{noformat}
 Using 1 partition
{noformat}
table.a-changelog 
28 1612435945196 0 | [ID123, 202101] A01 
88 1614863137136 0 | [ID123, 202102] A02 
149 1617882052260 0 | [ID123, 202103] A03 
210 1620299210336 0 | [ID123, 202104] A04 
269 1622804606823 0 | [ID123, 202105] A05 
table.b-changelog 
7 1622617868856 0 | [ID123] BBB 
join.ab-changelog 
28 1622617868856 0 | [ID123, 202101] A01, BBB 
88 1622617868856 0 | [ID123, 202102] A02, BBB 
149 1622617868856 0 | [ID123, 202103] A03, BBB 
210 1622617868856 0 | [ID123, 202104] A04, BBB 
269 1622804606823 0 | [ID123, 202105] A05, BBB 
output 
5 1622617868856 0 | [ID123, 202101] A01, BBB 
15 1622617868856 0 | [ID123, 202102] A02, BBB 
25 1622617868856 0 | [ID123, 202103] A03, BBB 
35 1622617868856 0 | [ID123, 202104] A04, BBB 
45 1622804606823 0 | [ID123, 202105] A05, BBB{noformat}
 ;;;","02/Sep/21 16:33;guozhang;Hello [~xnix] regarding ""However, in a local docker environment we can reproduce the above scenario every time when using 4 partitions and the problem goes away when using 1 partition."" Is that a test done through the TopologyTestDriver or is it a full-fledged integration test with Kafka clusters and Kafka Streams clients?

;;;","02/Sep/21 16:43;vvcephei;Hi [~xnix] ,

 

Your suspicion is correct. TopologyTestDriver doesn't simulate partitions at all, so you won't be able to use it to test this case.

 

When it comes to a repro, you might be interested in this class, which verifies that foreign-key joins perform correctly when the input topics have different partitions: [https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java]

If we have a bug, my suspicion would be whether we're correctly capturing the partitioner that you're setting via Repartitioned. I'd suggest modifying that test to be closer to your example and seeing whether or not we still get the correct result.

 

On the subject of Repartitioned, I didn't quite follow why you're doing it. To be clear, when you're doing foreign-key joins, you do not need the two tables to have the same number of partitions, nor do you need them to be co-partitioned. This should work just fine:
{code:java}
  KTable<String, String> tableB = builder.table(""B"",  stringMaterialized(""table.b""));

    builder
        .stream(""A"", Consumed.with(Serde.of(KeyA.class), Serde.of(EventA.class)))
        .toTable(Named.as(""table.a""), aMaterialized(""table.a""))
        .join(tableB, EventA::getKeyB, topicAandBeJoiner(), Named.as(""join.ab""), joinMaterialized(""join.ab""))
        .toStream()
        .to(""output"", with(...));
{code}
Unless you have some other requirement for which you need the repartition operation, I'd suggest just completely dropping those repartition steps. At least, I'd suggest trying out removing them from the topology and verifying if you get the correct join results.

 

I hope this helps!;;;","04/Sep/21 06:38;xnix;Hi [~guozhang], the local one is full-fledged, however scaled down, setup running the actual services needed to reproduce the problem. We do have test environment and staging environment with complete setup where we also can reproduce the problem.

Thanks [~vvcephei], i'll look into setting up a test case based on the one you sent me.

About removing the partitioned, the example I've given is the minimal one needed to show the problem. Our topologies are bigger in general and we where hoping to be able to use several partitions, with custom partitioner, instead of just one. 
I will follow your suggestion and try without it.

 

 

 ;;;","05/Sep/21 22:16;xnix;I've added a test that seem to verify a problem with the join. I need to look at it more tomorrow. If anyone's curious it's here [https://github.com/apache/kafka/commit/2e6edf2951f482029aea80acd2f0960085a5f396]

Branch: https://github.com/tomas-forsman/kafka/tree/KAFKA-13261;;;","06/Sep/21 14:45;xnix;I think I need some help to verify if that test does what it should. I also tried to look at the code for the foreign key join, but I couldn't figure out where a partitioner could be passed on to the internal topics for the foreign key join.;;;","06/Sep/21 22:08;abellemare;Hi [~xnix] - Thank you for the test code to reproduce it! It is very helpful. I can confirm that when I run it, it also fails to meet the expected values you supplied. Additionally, if I take out the `repartition` steps, it seems to join properly and work fine. Does this match your expectations?

If so, then I am a bit perplexed. I am not 100% sure how repartitioning KTables upstream can affect the joins downstream. I _think_ that  it could be possible that the FK joiner is using the _original_ repartitioner logic for the `Subscription` and `SubscriptionResponse` topics (and not the logic in the `repartition` command), and thus be routing the subscription and subscription responses to the wrong instances. This would certainly cause loss of events if this is indeed the case. 

I will need to look into the Repartition aspects a bit more because I am not familiar with them. Both the repartition command and the FKJ were added around the same time (developed in parallel), so my inclination is to start inspecting there. [~vvcephei], are you more familiar with the `repartition` chunk of code? What do you think about this theory?;;;","15/Sep/21 06:16;vcrfxia;Hey [~vvcephei] [~abellemare] [~guozhang] I had a look at the code and it seems to support Adam's theory that the custom partitioners from the repartition() step aren't taken into account by the foreign key join. In particular, both the subscription sink topic and the response sink topic are created without partitioners specified in the StreamSinkNode:

[https://github.com/apache/kafka/blob/75795d1ed8402f185e00b5f3aedcc2bcbb914ca9/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L1051] 
[https://github.com/apache/kafka/blob/75795d1ed8402f185e00b5f3aedcc2bcbb914ca9/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L1122]

IIUC, this means the default partitioner is used for both topics despite the custom partitioners on the source tables, which explains the missing join results.

One thing I don't understand: even if we fix this bug by propagating the partitioner information from the repartition() step to the foreign key join, wouldn't we still have an analogous bug if either of the topics for the source tables had custom partitioning logic created from outside Streams (i.e., without a repartition() step in the Streams topology)? In this case, Streams has no way of determining the partitioning of the source tables, which means we need an update to the interface for foreign key joins so that users can specify a partitioner to use in order to ensure copartitioning of the subscription and response topics with the relevant tables. Is this reasoning sound?

If so, does it make sense to add logic into Streams to propagate information about the partitioner from the repartition() step to the foreign key join, or would it be better to require users to use the new interface to pass the same partitioner from the repartition() step(s) to the foreign key join as well? The latter seems more consistent with how copartitioning for joins is typically the user's responsibility, and also avoids the need to update Streams with logic for tracking partitioners throughout the topology.;;;","15/Sep/21 18:55;mjsax;I would prefer to do a KIP and allow user to pass in a custom partitioner.

If we really get follow up requests, we could extend the DSL logic to auto-forward an upstream partitioner later. Overall, it seems not that there is no bug in the strong sense, but a missing feature. We never designed FK-join to support custom partitioning.

Might be worth to update the docs for 3.0 and earlier to point out this limitation.;;;","15/Sep/21 20:41;guozhang;> wouldn't we still have an analogous bug if either of the topics for the source tables had custom partitioning logic created from outside Streams (i.e., without a repartition() step in the Streams topology)? In this case, Streams has no way of determining the partitioning of the source tables, which means we need an update to the interface for foreign key joins so that users can specify a partitioner to use in order to ensure copartitioning of the subscription and response topics with the relevant tables. Is this reasoning sound?

Yeah I think that's faire; KS assumes the source topics are partitioned by key, but does not require it has to be partitioned with default mechanism. However when getting back to the source tables from the subscription table it simply assumes default partitioning is used. For that, I agree allowing users to pass in the partitioner in FK would be good, so that if users know the source tables are not partitioned with the default partitioner, they should be responsible for passing that custom partitioner in FK.;;;","15/Sep/21 22:52;vcrfxia;Thanks for the confirmation, Matthias and Guozhang! I've opened a small KIP with the proposed interface changes to allow users to pass in custom partitioners for FK joins: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-775%3A+Custom+partitioners+in+foreign+key+joins]

I tried sending an email to the [dev@kafka.apache.org|mailto:dev@kafka.apache.org] mailing list to start discussion but either it's taking a while to send or my permissions aren't yet set up correctly. Hopefully it will be ready for discussion soon.

UPDATE: Email for KIP discussion has been sent.;;;","17/Sep/21 15:57;vcrfxia;Hey [~mjsax] [~guozhang], after opening the KIP above I noticed a slight ""problem"" with the interfaces for users to pass custom partitioners. Can you double-check my reasoning below?

The ""problem"" with the interfaces in the KIP is that interfaces currently ask users to provide {{StreamPartitioner<K, V> thisPartitioner }}(representing the partitioning strategy for the right/foreign-key table) – as these are the partitioners that users should know – but the FK join implementation actually needs a partitioner of the form {{StreamPartitioner<KO, SubscriptionWrapper<K>>}} instead of {{otherPartitioner}}, and a partitioner of the form {{StreamPartitioner<K, SubscriptionResponseWrapper<VO>>}} instead of {{thisPartitioner}}, based on the actual messages being passed in the subscription and response topics.

It doesn't make sense to ask users to pass these other partitioners since {{SubscriptionWrapper}} and {{SubscriptionResponseWrapper}} are internal implementation details. What we really want is to require that any custom partitioners select a partition based only on the message key, without taking the message value into consideration. I think this is a reasonable requirement for the right/foreign-key table since the subscription store mechanism (in the FK join implementation) won't work at all unless all messages with the same (foreign) key are always sent to the same partition. We don't technically need to require this for the left table since, if we wanted to, we could send the original value along with the subscription in order to ensure that the response is routed back to the correct partition, but this seems unnecessarily complicated and also bloats the size of the subscription store and topic.

As such, I think we should require that any custom partitioner used for tables must partition messages based only on message key (and not value), in order to be used in FK joins. Do you agree?

If so, we could make this requirement explicit in the FK join interfaces by asking users for {{StreamPartitioner<K, Void> thisPartitioner }}and{{ }}{{StreamPartitioner<KO, Void> otherPartitioner }}instead of the original partitioners. Or we could keep the interfaces as is ({{StreamPartitioner<K, V> thisPartitioner and }}{{StreamPartitioner<KO, VO> otherPartitioner}}), since users probably already have these handy anyway, and leave a note in the javadocs to explain the requirement. I have a preference for the latter but am curious what you think. Thanks!;;;","18/Sep/21 18:06;mjsax;Let keep the discussion on the mailing list. – But I agree to the problem you describe. The FK-join can only work, if both tables are only partitioned using their respective key. I don't think it will become a problem in practice, but it's worth to point out in the JavaDocs.

Furthermore, this problem seems to be very fundamental and I don't think we could solve it. As you pointed out, we don't have access to the value of the foreign key and thus could not pass it into the partitioner. I also agree that we should not send the full left-hand value to the right side just for this purpose. As a matter of fact, in the original FK-KIP, there was a discussion about sending the left value and computing the join result on the right side to avoid the response topics – this idea was rejected for various reasons.

After I read the first paragraphs, my thought was also ""let's make the value-type {{Void}}"", so I am happy that you propose the exact same thing! I am on-board with it! – I'll reply to the mailing list, too.;;;","01/Oct/21 17:49;vcrfxia;https://github.com/apache/kafka/pull/11368;;;","01/Oct/21 17:52;vcrfxia;Hi [~xnix], KIP-775 for adding interfaces to support custom partitioners in foreign key joins has been accepted and I've opened a PR with an implementation. The integration test you shared for demonstrating the feature gap was super convenient to work with, and I've also checked it in as part of the PR. Please let me know if you'd like to be added as a contributor on the PR. Thanks again for raising this gap! :);;;","04/Oct/21 20:22;xnix;[~vcrfxia] thank you so much for solving this issue, it has been very interesting to follow the progress and how the process works. Great job :)

I'm glad the integration test could be of use! If it cause no trouble, then it would be an honor to be added as contributor. ;;;","08/Oct/21 15:50;vcrfxia;Hi [~xnix] , of course! It's no trouble at all. Could you please share an email address to include as a coauthor on the commit when the PR is merged? [https://docs.github.com/en/github/committing-changes-to-your-project/creating-and-editing-commits/creating-a-commit-with-multiple-authors#required-co-author-information] ;;;","12/Oct/21 06:34;xnix;Thank you [~vcrfxia]. Email [tomas-forsman@users.noreply.github.com|mailto:tomas-forsman@users.noreply.github.com] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE in ConfigDef when rendering (enriched) RST or HTML when documentation is not set/NULL,KAFKA-13256,13398459,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rk3rn3r,rk3rn3r,rk3rn3r,31/Aug/21 12:33,10/Sep/21 07:16,13/Jul/23 09:17,09/Sep/21 16:40,2.8.0,3.0.0,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,clients,,,,,0,,,,,"While working on Debezium I discovered the following issue:

When Kafka's ConfigDef renders the HTML or RST documentation representation of the config definition, it requires `ConfigKey.documentation` member variable to be a java.lang.String instance that's set to an actual value different than NULL, else NPE happens:
{code:java}
 b.append(key.documentation.replaceAll(""\n"", ""<br>""));
{code}
{code:java}
 for (String docLine : key.documentation.split(""\n"")) {
{code}
 

When `documentation` is not set/NULL I suggest to either set a valid String like ""No documentation available"" or skip that config key.

 

I could provide a PR to fix this soon.",,mimaison,mjsax,rk3rn3r,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 10 07:16:12 UTC 2021,,,,,,,,,,"0|z0ughk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 12:47;rk3rn3r;PR available: https://github.com/apache/kafka/pull/11287;;;","07/Sep/21 23:23;mjsax;[~rk3rn3r] Thanks for reporting this issue and for the PR. I am not familiar with the details of this tool, so my question is, for what cases can the key actually be {{null}}? ;;;","08/Sep/21 09:14;rk3rn3r;[~mjsax] Currently there's no way to prevent that users of  ConfigDef do NOT provide a documentation.

I don't know who's consuming `org.apache.kafka.common.config.ConfigDef#toRst`, `org.apache.kafka.common.config.ConfigDef#toEnrichedRst` and `org.apache.kafka.common.config.ConfigDef#toHtml` but calling these methods in ConfigDef will fail with an NPE when there are ConfigKeys that don't have `documentation` set.

An alternative would be to add null-checks into all `org.apache.kafka.common.config.ConfigDef#define` methods, but this would be a drastic API breaking change.;;;","10/Sep/21 04:11;mjsax;Thanks. Just wondering if this PR fixes a non-problem or not (maybe just make the rendering more robust, what seems to be good). Congrats to your first contribution! Thanks [~mimaison] for reviewing and merging!;;;","10/Sep/21 07:16;mimaison;While this does not affect Apache Kafka itself, ConfigDef is public API and used by Connect connectors and third party code like Debezium. In a perfect world, you can argue the documentation should always be set but as it was a small fix, it made sense to merge it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirrormaker config property config.properties.exclude is not working as expected ,KAFKA-13255,13398374,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bdesert,anadkarni,anadkarni,31/Aug/21 06:41,29/Apr/22 13:18,13/Jul/23 09:17,15/Nov/21 10:47,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,,,,mirrormaker,,,,,1,,,,,"Objective - Use MM2 (kafka connect in distributed cluster) for data migration between cluster hosted in private data center and aws msk cluster.

Steps performed -
 # Started kafka-connect service.
 # Created 3 MM2 connectors (i.e. source connector, checkpoint connector and heartbeat connector). Curl commands used to create connectors are in the attached file.  To exclude certain config properties while topic replication, we are using the 'config.properties.exclude' property in the MM2 source connector.

Expected -

Source topic 'dev.portlandDc.anamika.helloMsk' should be successfully created in destination cluster.

Actual -

Creation of the source topic 'dev.portlandDc.anamika.helloMsk' in destination cluster fails with an error. Error is
{code:java}
[2021-08-06 06:13:40,944] WARN [mm2-msc|worker] Could not create topic dev.portlandDc.anamika.helloMsk. (org.apache.kafka.connect.mirror.MirrorSourceConnector:371)
org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: confluent.value.schema.validation{code}",,anadkarni,bdesert,ecomar,mimaison,ryannedolan,showuon,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 29 13:18:24 UTC 2022,,,,,,,,,,"0|z0ufyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 06:46;anadkarni;I keep getting HTTP error 403 when trying to attach file with curl commands to this jira. As a work around I am pasting the file contents below.
{code:java}
curl -X POST http://10.xx.xx.xx:8083/connectors -H ""Content-Type: application/json"" -d '{
   ""name"":""mm2-msc"",
   ""config"":{
      ""connector.class"":""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
      ""clusters"":""mysource,mydestination"",
      ""source.cluster.alias"":""mysource"",
      ""target.cluster.alias"":""mydestination"",
      ""replication.policy.class"":""com.amazonaws.kafka.samples.CustomMM2ReplicationPolicy"",
      ""target.cluster.bootstrap.servers"":""b-2.mydestination.amazonaws.com:9096,b-3.mydestination.amazonaws.com:9096,b-1.mydestination.amazonaws.com:9096"",
      ""source.cluster.bootstrap.servers"":""10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093"",
      ""topics"":""dev.portlandDc.anamika.helloMsk"",
      ""config.properties.exclude"":""^confluent\\..*$"",
      ""tasks.max"":""1"",
      ""key.converter"":"" org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""value.converter"":""org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""replication.factor"":""3"",
      ""offset-syncs.topic.replication.factor"":""3"",
      ""sync.topic.acls.interval.seconds"":""20"",
      ""sync.topic.configs.interval.seconds"":""20"",
      ""refresh.topics.interval.seconds"":""20"",
      ""refresh.groups.interval.seconds"":""20"",
      ""consumer.group.id"":""mm2-msc-cons"",
      ""producer.enable.idempotence"":""true"",
      ""source.cluster.security.protocol"":""SASL_SSL"",
      ""source.cluster.sasl.mechanism"":""PLAIN"",
      ""source.cluster.ssl.truststore.location"":""/home/ec2-user/dc/kafka.client.truststore.jks"",
      ""source.cluster.ssl.truststore.password"":""xxxxx"",
      ""source.cluster.ssl.endpoint.identification.algorithm"":"""",
      ""source.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.plain.PlainLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";"",
      ""target.cluster.security.protocol"":""SASL_SSL"",
      ""target.cluster.sasl.mechanism"":""SCRAM-SHA-512"",
      ""target.cluster.ssl.truststore.location"":""/home/ec2-user/kafka_2.13-2.8.0/config/msk/kafka.client.truststore.jks"",
      ""target.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";""
   }
}' | jq .
curl -X POST http://10.xx.xx.xx:8083/connectors -H ""Content-Type: application/json"" -d '{
   ""name"":""mm2-cpc"",
   ""config"":{
      ""connector.class"":""org.apache.kafka.connect.mirror.MirrorCheckpointConnector"",
      ""clusters"":""mysource,mydestination"",
      ""source.cluster.alias"":""mysource"",
      ""target.cluster.alias"":""mydestination"",
      ""config.properties.exclude"":""confluent.value.schema.validation"",
      ""config.properties.blacklist"":""confluent.value.schema.validation"",
      ""target.cluster.bootstrap.servers"":""b-2.mydestination.amazonaws.com:9096,b-3.mydestination.amazonaws.com:9096,b-1.mydestination.amazonaws.com:9096"",
      ""source.cluster.bootstrap.servers"":""10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093"",
      ""tasks.max"":""1"",
      ""key.converter"":"" org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""value.converter"":""org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""replication.policy.class"":""com.amazonaws.kafka.samples.CustomMM2ReplicationPolicy"",
      ""replication.factor"":""3"",
      ""sync.topic.configs.enabled"":""false"",
      ""checkpoints.topic.replication.factor"":""3"",
      ""emit.checkpoints.interval.seconds"":""20"",
      ""source.cluster.security.protocol"":""SASL_SSL"",
      ""source.cluster.sasl.mechanism"":""PLAIN"",
      ""source.cluster.ssl.truststore.location"":""/home/ec2-user/dc/kafka.client.truststore.jks"",
      ""source.cluster.ssl.truststore.password"":""xxxxx"",
      ""source.cluster.ssl.endpoint.identification.algorithm"":"""",
      ""source.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.plain.PlainLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";"",
      ""target.cluster.security.protocol"":""SASL_SSL"",
      ""target.cluster.sasl.mechanism"":""SCRAM-SHA-512"",
      ""target.cluster.ssl.truststore.location"":""/home/ec2-user/kafka_2.13-2.8.0/config/msk/kafka.client.truststore.jks"",
      ""target.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";""
   }
}' | jq .
curl -X POST http://10.xx.xx.xx:8083/connectors -H ""Content-Type: application/json"" -d '{
   ""name"":""mm2-hbc"",
   ""config"":{
      ""connector.class"":""org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"",
      ""clusters"":""mysource,mydestination"",
      ""source.cluster.alias"":""mysource"",
      ""target.cluster.alias"":""mydestination"",
      ""target.cluster.bootstrap.servers"":""b-2.mydestination.amazonaws.com:9096,b-3.mydestination.amazonaws.com:9096,b-1.mydestination.amazonaws.com:9096"",
      ""source.cluster.bootstrap.servers"":""10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093,10.xx.xx.xx:9093"",
      ""tasks.max"":""1"",
      ""key.converter"":"" org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""value.converter"":""org.apache.kafka.connect.converters.ByteArrayConverter"",
      ""replication.factor"":""3"",
      ""heartbeats.topic.replication.factor"":""3"",
      ""emit.heartbeats.interval.seconds"":""20"",
      ""source.cluster.security.protocol"":""SASL_SSL"",
      ""source.cluster.ssl.truststore.location"":""/home/ec2-user/dc/kafka.client.truststore.jks"",
      ""source.cluster.ssl.truststore.password"":""xxxxx"",
      ""source.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.plain.PlainLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";"",
      ""source.cluster.sasl.mechanism"":""PLAIN"",
      ""target.cluster.security.protocol"":""SASL_SSL"",
      ""target.cluster.ssl.truststore.location"":""/home/ec2-user/kafka_2.13-2.8.0/config/msk/kafka.client.truststore.jks"",
      ""target.cluster.sasl.mechanism"":""SCRAM-SHA-512"",
      ""target.cluster.sasl.jaas.config"":""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""xxxxx\"" password=\""xxxxx\"";""
   }
}
' | jq .
{code};;;","31/Aug/21 06:52;anadkarni;To replicate source topic in destination cluster with no prefix (<sourcealias>), I am using custom replication policy while creating source connector. You can generate custom replication policy jar from below link. I am getting 'HTTP - 403 Forbidden' , when I am trying to attach the jar.

https://github.com/aws-samples/mirrormaker2-msk-migration/tree/master/CustomMM2ReplicationPolicy

 ;;;","31/Aug/21 17:09;anadkarni;I have fixed this issue and would like to contribute my changes to apache kafka code. I am trying to attach a file but I am not able to do it.

Please suggest, how to upload a file?;;;","01/Sep/21 01:32;ryannedolan;hey [~anadkarni], you'll need to create a pull request on github (https://github.com/apache/kafka) so that we can review and merge the changes. Let me know if you need help.;;;","16/Sep/21 00:36;anadkarni;[~ryannedolan] Submitted PR. Thank you !!;;;","22/Sep/21 23:18;anadkarni;[~ryannedolan] How much time usually it takes to get PR approved on kafka?

https://github.com/apache/kafka/pull/11328;;;","14/Oct/21 21:59;bdesert;[~ryannedolan], I recommend to fix this bug report:

Subject: Mirrormaker doesn't honor exclude filters for new topics

The bug was indeed introduced in 2.8, when MM started creating topics WITH configs. Prior to this, MM would create new topic on target without any config from source, and then `syncTopicConfigs` would replicate config while honoring the exclude filter defined in `DefaultConfigPropertyFilter` or explicitly specified in MM property `config.properties.exclude`.

After the update in 2.8, MM started creating new topics along with their configs from source cluster, but the configs aren't being filtered against exclude param.

I've submitted PR with a little update to use the same method as used for `syncTopicConfigs` to apply the filter.

 

https://github.com/apache/kafka/pull/11401;;;","15/Dec/21 13:12;showuon;[~mimaison] [~bdesert] , do you know if there's any workaround for this issue? If no, do you think we should get this fix cherry-picked back to v3.1.0, since it blocked users using MM2?

Thank you.;;;","15/Dec/21 17:16;mimaison;It's not a regression from 3.0 so it's probably too late for 3.1.0. David Jacot is the release manager and has the final say. You can ask him if you think this should be included.
I'm not sure if there's a workaround apart from not mirroring topics with custom configurations
;;;","15/Dec/21 18:29;bdesert;the workaround would be to pre-create topics on a target cluster. Also, this bug is critical (show stopper) only if MM2 configured to replicate topics cross-flavors cluster, as reported in the original case above (Confluent to MSK, or to vanilla Apache Kafka). Once topic is created on target, this bug won't affect data mirroring process.;;;","16/Dec/21 01:43;showuon;Thanks for your response. In that case, I agree we don't need to include this fix in v3.1.0. Thank you.;;;","28/Apr/22 17:25;ecomar;Would have been really nice to have this fix in 3.1 ;;;","29/Apr/22 07:35;tombentley;OK [~ecomar] I've cherry-picked it to 3.1;;;","29/Apr/22 13:18;ecomar;Thanks Tom !!! [~tombentley];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock when expanding ISR,KAFKA-13254,13398330,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,30/Aug/21 23:45,20/Sep/21 19:15,13/Jul/23 09:17,20/Sep/21 16:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Found this when debugging downgrade system test failures. The patch for https://issues.apache.org/jira/browse/KAFKA-13091 introduced a deadlock. Here are the jstack details:
{code}
""data-plane-kafka-request-handler-4"":                                                                                                                                                                              
  waiting for ownable synchronizer 0x00000000fcc00020, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),                                                                                                   
  which is held by ""data-plane-kafka-request-handler-5""                                                                                                                                                            

""data-plane-kafka-request-handler-5"":
  waiting for ownable synchronizer 0x00000000c9161b20, (a 
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""data-plane-kafka-request-handler-4""

""data-plane-kafka-request-handler-4"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000fcc00020> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
        at kafka.server.DelayedOperation.safeTryComplete(DelayedOperation.scala:121)
        at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:362)
        at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:264)
        at kafka.cluster.DelayedOperations.checkAndCompleteAll(Partition.scala:59)
        at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:907)
        at kafka.cluster.Partition.handleAlterIsrResponse(Partition.scala:1421)
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1(Partition.scala:1340)
        at kafka.cluster.Partition.$anonfun$sendAlterIsrRequest$1$adapted(Partition.scala:1340)
        at kafka.cluster.Partition$$Lambda$1496/2055478409.apply(Unknown Source)
        at kafka.server.ZkIsrManager.submit(ZkIsrManager.scala:74)
        at kafka.cluster.Partition.sendAlterIsrRequest(Partition.scala:1345)
        at kafka.cluster.Partition.expandIsr(Partition.scala:1312)
        at kafka.cluster.Partition.$anonfun$maybeExpandIsr$2(Partition.scala:755)
        at kafka.cluster.Partition.maybeExpandIsr(Partition.scala:754)
        at kafka.cluster.Partition.updateFollowerFetchState(Partition.scala:672)
        at kafka.server.ReplicaManager.$anonfun$updateFollowerFetchState$1(ReplicaManager.scala:1806)
        at kafka.server.ReplicaManager$$Lambda$1075/1996432270.apply(Unknown Source)
        at scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:99)
        at scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:86)
        at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:42)
        at kafka.server.ReplicaManager.updateFollowerFetchState(ReplicaManager.scala:1790)
        at kafka.server.ReplicaManager.readFromLog$1(ReplicaManager.scala:1025)
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:1029)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:970)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:173)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:75)
        at java.lang.Thread.run(Thread.java:748)

""data-plane-kafka-request-handler-5"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000c9161b20> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)
        at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)
        at kafka.cluster.Partition.fetchOffsetSnapshot(Partition.scala:1183)
        at kafka.server.DelayedFetch.$anonfun$tryComplete$1(DelayedFetch.scala:96)
        at kafka.server.DelayedFetch.$anonfun$tryComplete$1$adapted(DelayedFetch.scala:89)
        at kafka.server.DelayedFetch$$Lambda$1115/1987378797.apply(Unknown Source)
        at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
        at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:919)
        at kafka.server.DelayedFetch.tryComplete(DelayedFetch.scala:89)
        at kafka.server.DelayedOperation.safeTryComplete(DelayedOperation.scala:121)
        at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:362)
        at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:264)
        at kafka.server.ReplicaManager.$anonfun$appendRecords$6(ReplicaManager.scala:622)
        at kafka.server.ReplicaManager$$Lambda$1150/40125541.apply(Unknown Source)
        at scala.collection.mutable.HashMap$Node.foreach(HashMap.scala:627)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:489)
        at kafka.server.ReplicaManager.$anonfun$appendRecords$5(ReplicaManager.scala:611)
        at kafka.server.ReplicaManager$$Lambda$1134/1761219075.apply$mcV$sp(Unknown Source)
        at kafka.server.ActionQueue.tryCompleteActions(ActionQueue.scala:49)
        at kafka.server.ReplicaManager.tryCompleteActions(ReplicaManager.scala:569)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:245)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:75)
        at java.lang.Thread.run(Thread.java:748)
{code}


Basically one thread holds the LeaderAndIsr write lock (from maybeExpandIsr) and is trying to grab the lock for a delayed operation in order to complete it. The other thread has the lock for the delayed operation and is trying to grab the LeaderAndIsr read lock.

Note that this does not affect 3.0 or any other released version.",,dengziming,hachikuji,ivanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-30 23:45:53.0,,,,,,,,,,"0|z0ufow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoints do not contain latest offsets on shutdown when using EOS,KAFKA-13249,13398060,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hutchiko,hutchiko,hutchiko,29/Aug/21 21:48,30/Sep/21 18:04,13/Jul/23 09:17,14/Sep/21 23:12,2.7.0,2.8.0,3.0.0,,,,,,,,,,,,,,,,,,,,2.8.2,3.0.0,,,,,,,streams,,,,,0,,,,,"When using EOS the {{.checkpoint}} file created when a stateful streams app is shutdown does not always contain changelog offsets which represent the latest state of the state store. The offsets can often be behind the end of the changelog - sometimes quite significantly.

This leads to a state restore being required when the streams app restarts after shutting down cleanly as streams thinks (based on the incorrect offsets in the checkpoint) that the state store is not up to date with the changelog. 

This is increasing the time we see it takes to do a clean restart of a single instance streams app from around 10 second to sometime over 2 minutes in our case.

I suspect the bug appears because an assumption about the {{commitNeeded}} field in the following method in {{StreamTask}}:
{code:java}
protected void maybeWriteCheckpoint(final boolean enforceCheckpoint) {
  // commitNeeded indicates we may have processed some records since last commit
  // and hence we need to refresh checkpointable offsets regardless whether we should checkpoint or not
  if (commitNeeded) {
    stateMgr.updateChangelogOffsets(checkpointableOffsets());
  }

  super.maybeWriteCheckpoint(enforceCheckpoint);
}
{code}
In a steady state case for a simple single instance single thread stream app where an app simply starts, runs and then shuts down the {{if (commitNeeded)}} test always fails when running with EOS which results in the latest checkpoint offsets never getting updated into the {{stateMgr}}.

Tracing back to the callers of {{maybeWriteCheckpoint}} it's easy to see this is the case as there's only 1 place in the code which calls {{maybeWriteCheckpoint}} during this steady state. The {{postCommit(final boolean enforceCheckpoint)}} method, specifically the call in the {{RUNNING}} state.
{code:java}
case RUNNING:
  if (enforceCheckpoint || !eosEnabled) {
    maybeWriteCheckpoint(enforceCheckpoint);
  }
  log.debug(""Finalized commit for {} task with eos {} enforce checkpoint {}"", state(), eosEnabled, enforceCheckpoint);
  break;
{code}
We can see from this code that {{maybeWriteCheckpoint}} will only ever to called if {{enforceCheckpoint=true}} because we know {{eosEnabled=true}} as we're running with EOS.

So then where does {{postCommit}} get called with {{enforceCheckpoint=true}}? Again looking only at the steady state case we find that it's only called from {{TaskManager.tryCloseCleanAllActiveTasks}} which is only called from {{TaskManager.shutdown}}.

The thing about the call in {{tryCloseCleanAllActiveTasks}} is that it happens *after* all active tasks have commited. Which means that {{StreamTask.commitNeeded=false}} for all tasks so it follows that the test back in {{maybeWriteCheckpoint}} always fails and we don't end up getting the latest offsets stored into the state manager.

I think the fix is to simply change the test in {{maybeWriteCheckpoint}} to be {{if (commitNeeded || enforceCheckpoint) { ...}} as we know we must always update the changelog offserts before we write the checkpoint.",,ableegoldman,hutchiko,Lerh,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12634,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 30 23:03:37 UTC 2021,,,,,,,,,,"0|z0ue0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/21 22:07;hutchiko;I have created a PR with a test which shows this behaviour https://github.com/apache/kafka/pull/11283 

The test {{EosIntegrationTest.shouldWriteLatestOffsetsToCheckpointOnShutdown}} verifies that the {{.checkpoint}} file and the state store data are up to date with the changelog after shutdown. ;;;","30/Aug/21 23:03;ableegoldman;Thanks for the detailed report! I agree with your analysis, it does appear that we can end up writing stale offsets if the thread is shut down immediately following a ""normal"" commit that occurs during active processing. And given the small commit interval typical of EOS applications, we almost always perform this kind of commit before breaking out of the StreamThread's processing loop, meaning most of the time we will indeed have just committed all tasks when we get to checking the shutdown signal and entering {{TaskManager.shutdown()}}

I'll give your PR a pass -- thanks also for submitting a patch with the bug report :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopologyTestDriver crashes with EOS-beta config,KAFKA-13236,13397673,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,mjsax,mjsax,27/Aug/21 00:55,28/Aug/21 03:15,13/Jul/23 09:17,27/Aug/21 20:23,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,2.6.3,2.7.2,2.8.1,,,,,,streams-test-utils,,,,,0,,,,,"If EOS-beta is configured, we rely on the `-StreamThread` name part to create `StreamsProducer`. (cf [https://github.com/apache/kafka/commit/20e4a74c3579837df7b46ce1a5dcea37b6e1e452)] However, TopologyTestDriver sets the name only as `-Thread` and thus it crashed for EOS-beta is configured.

The issue is already fixed in 3.0 via [https://github.com/apache/kafka/commit/3805f3706f8f3ebba81b80915c9259590525fb05] but we should still fix it for older release, too.",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-27 00:55:10.0,,,,,,,,,,"0|z0ubn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction system tests should check URPs between broker bounces,KAFKA-13234,13397623,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,26/Aug/21 17:52,01/Sep/21 15:40,13/Jul/23 09:17,01/Sep/21 15:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"We've seen a number of failing transaction system tests. After investigating a few of these, I found that this came down to transaction timeouts which were due to partition unavailability. The test rolls the brokers in the cluster, but it does not verify that URPs have cleared after restarting each broker. This means that it is possible for partitions to go under min isr or even offline. In the case of a hard bounce, the time to recover may be quite long because of the default session timeout of 18s. It would be good wait for URPs to clear instead.",,hachikuji,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 26 20:31:17 UTC 2021,,,,,,,,,,"0|z0ubc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 20:31;lucasbradstreet;We do something similar in upgrade_test and downgrade_test. We should move this functionality into kafka.py for use by all tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`TransactionalMessageCopier.start_node` should wait until the process if fully started,KAFKA-13231,13397506,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,26/Aug/21 09:51,27/Aug/21 06:33,13/Jul/23 09:17,27/Aug/21 06:33,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.1,3.1.0,,,,,,,,,,,0,,,,,"There is a subtile race condition in the code which bounces the transaction message copier. As you can see in the log snippet above, it is possible that the copier get bounced before it even starts. Note that the process is stated by a separate thread. In this case, the method which stops the current process miss it because the PID is not there yet. However, the stop hangs because the thread does not stop as expected.

It seems that we should wait until the process is fully started in `restart` or `start_node` to avoid this issue.


{noformat}

[INFO  - 2021-08-24 07:48:25,882 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 5250, remaining 28083
[INFO  - 2021-08-24 07:48:26,121 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 6000, remaining 27333
[INFO  - 2021-08-24 07:48:26,379 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 6750, remaining 26583
[INFO  - 2021-08-24 07:48:26,536 - transactions_test - bounce_copiers - lineno:144]: copier-0 - progress: 20.25020250202502
[DEBUG - 2021-08-24 07:48:26,536 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: jps | grep -i TransactionalMessageCopier | awk '{print $1}'
[DEBUG - 2021-08-24 07:48:26,692 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: kill -9 1567
[DEBUG - 2021-08-24 07:48:26,733 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: jps | grep -i TransactionalMessageCopier | awk '{print $1}'
[INFO  - 2021-08-24 07:48:27,021 - background_thread - start_node - lineno:57]: Running TransactionalMessageCopier-0-139963594423096 node 1 on worker22
[DEBUG - 2021-08-24 07:48:27,021 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: mkdir -p /mnt/transactional_message_copier
[DEBUG - 2021-08-24 07:48:27,068 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: java -version
[INFO  - 2021-08-24 07:48:27,163 - kafka - bootstrap_servers - lineno:2606]: Bootstrap client port is: 9092
[DEBUG - 2021-08-24 07:48:27,163 - transactional_message_copier - _worker - lineno:85]: TransactionalMessageCopier 1 command: export LOG_DIR=/mnt/transactional_message_copier/logs; export KAFKA_OPTS=; export KAFKA_LOG4J_OPTS=""-Dlog4j.configuration=file:/mnt/transactional_message_copier/tools-log4j.properties""; /opt/kafka-dev/bin/kafka-run-class.sh org.apache.kafka.tools.TransactionalMessageCopier --broker-list worker13:9092,worker9:9092,worker20:9092 --transactional-id copier-0 --consumer-group transactions-test-consumer-group --input-topic input-topic --output-topic output-topic --input-partition 0 --transaction-size 750 --transaction-timeout 40000 --enable-random-aborts 2>> /mnt/transactional_message_copier/transactional_message_copier.stderr | tee -a /mnt/transactional_message_copier/transactional_message_copier.stdout &
[DEBUG - 2021-08-24 07:48:27,163 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: export LOG_DIR=/mnt/transactional_message_copier/logs; export KAFKA_OPTS=; export KAFKA_LOG4J_OPTS=""-Dlog4j.configuration=file:/mnt/transactional_message_copier/tools-log4j.properties""; /opt/kafka-dev/bin/kafka-run-class.sh org.apache.kafka.tools.TransactionalMessageCopier --broker-list worker13:9092,worker9:9092,worker20:9092 --transactional-id copier-0 --consumer-group transactions-test-consumer-group --input-topic input-topic --output-topic output-topic --input-partition 0 --transaction-size 750 --transaction-timeout 40000 --enable-random-aborts 2>> /mnt/transactional_message_copier/transactional_message_copier.stderr | tee -a /mnt/transactional_message_copier/transactional_message_copier.stdout &
[INFO  - 2021-08-24 07:49:08,280 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 0, remaining 26583
[INFO  - 2021-08-24 07:49:08,763 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 750, remaining 25833
[INFO  - 2021-08-24 07:49:09,034 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 1500, remaining 25083
[INFO  - 2021-08-24 07:49:09,279 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 1500, remaining 25083
[INFO  - 2021-08-24 07:49:09,553 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 2250, remaining 24333
[INFO  - 2021-08-24 07:49:09,793 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 2250, remaining 24333
[INFO  - 2021-08-24 07:49:10,059 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 3000, remaining 23583
[INFO  - 2021-08-24 07:49:10,312 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 3750, remaining 22833
[INFO  - 2021-08-24 07:49:10,661 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 4500, remaining 22083
[INFO  - 2021-08-24 07:49:11,056 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 5250, remaining 21333
[INFO  - 2021-08-24 07:49:11,402 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 5250, remaining 21333
[INFO  - 2021-08-24 07:49:11,698 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 6000, remaining 20583
[INFO  - 2021-08-24 07:49:11,876 - transactions_test - bounce_copiers - lineno:144]: copier-0 - progress: 22.570815934996048
[DEBUG - 2021-08-24 07:49:11,876 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: jps | grep -i TransactionalMessageCopier | awk '{print $1}'
[INFO  - 2021-08-24 07:49:11,882 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 6000, remaining 20583
[DEBUG - 2021-08-24 07:49:12,089 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: kill -9 2195
[DEBUG - 2021-08-24 07:49:12,129 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: jps | grep -i TransactionalMessageCopier | awk '{print $1}'
[INFO  - 2021-08-24 07:49:12,425 - background_thread - start_node - lineno:57]: Running TransactionalMessageCopier-0-139963594423096 node 1 on worker22
[DEBUG - 2021-08-24 07:49:12,426 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: mkdir -p /mnt/transactional_message_copier
[DEBUG - 2021-08-24 07:49:12,472 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: java -version
[INFO  - 2021-08-24 07:49:12,526 - transactions_test - bounce_copiers - lineno:144]: copier-0 - progress: 22.570815934996048
[DEBUG - 2021-08-24 07:49:12,526 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: jps | grep -i TransactionalMessageCopier | awk '{print $1}'
[INFO  - 2021-08-24 07:49:12,567 - kafka - bootstrap_servers - lineno:2606]: Bootstrap client port is: 9092
[DEBUG - 2021-08-24 07:49:12,567 - transactional_message_copier - _worker - lineno:85]: TransactionalMessageCopier 1 command: export LOG_DIR=/mnt/transactional_message_copier/logs; export KAFKA_OPTS=; export KAFKA_LOG4J_OPTS=""-Dlog4j.configuration=file:/mnt/transactional_message_copier/tools-log4j.properties""; /opt/kafka-dev/bin/kafka-run-class.sh org.apache.kafka.tools.TransactionalMessageCopier --broker-list worker13:9092,worker9:9092,worker20:9092 --transactional-id copier-0 --consumer-group transactions-test-consumer-group --input-topic input-topic --output-topic output-topic --input-partition 0 --transaction-size 750 --transaction-timeout 40000 --enable-random-aborts 2>> /mnt/transactional_message_copier/transactional_message_copier.stderr | tee -a /mnt/transactional_message_copier/transactional_message_copier.stdout &
[DEBUG - 2021-08-24 07:49:12,567 - remoteaccount - _log - lineno:160]: ubuntu@worker22: Running ssh command: export LOG_DIR=/mnt/transactional_message_copier/logs; export KAFKA_OPTS=; export KAFKA_LOG4J_OPTS=""-Dlog4j.configuration=file:/mnt/transactional_message_copier/tools-log4j.properties""; /opt/kafka-dev/bin/kafka-run-class.sh org.apache.kafka.tools.TransactionalMessageCopier --broker-list worker13:9092,worker9:9092,worker20:9092 --transactional-id copier-0 --consumer-group transactions-test-consumer-group --input-topic input-topic --output-topic output-topic --input-partition 0 --transaction-size 750 --transaction-timeout 40000 --enable-random-aborts 2>> /mnt/transactional_message_copier/transactional_message_copier.stderr | tee -a /mnt/transactional_message_copier/transactional_message_copier.stdout &
[INFO  - 2021-08-24 07:49:58,379 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 0, remaining 20583
[INFO  - 2021-08-24 07:49:58,842 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 0, remaining 20583
[INFO  - 2021-08-24 07:49:59,112 - transactional_message_copier - _worker - lineno:95]: copier-0: consumed 0, remaining 20583
{noformat}
",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-26 09:51:14.0,,,,,,,,,,"0|z0uam0:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApiVersionRequest are not correctly handled in kraft mode,KAFKA-13228,13397388,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,dengziming,dengziming,26/Aug/21 01:48,05/Jul/22 22:19,13/Jul/23 09:17,05/Jul/22 22:19,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,"I'am trying to describe quorum in kraft mode but got `org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support DESCRIBE_QUORUM`.

This happens because we only concerns `ApiKeys.zkBrokerApis()` when we call `NodeApiVersions.create()`",,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-26 01:48:21.0,,,,,,,,,,"0|z0u9vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller skips sending UpdateMetadataRequest when shutting down broker doesnt host partitions ,KAFKA-13225,13396831,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,david.mao,david.mao,david.mao,24/Aug/21 06:46,02/Sep/21 20:45,13/Jul/23 09:17,02/Sep/21 20:45,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,controller,,,,,0,,,,,"If a broker not hosting replicas for any partitions is shut down while there are offline partitions, the controller can fail to send out metadata updates to other brokers in the cluster.

 

Since this is a very niche scenario, I will leave the priority as Minor.",,david.mao,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 02 20:45:32 UTC 2021,,,,,,,,,,"0|z0u6g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 20:45;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BrokerState metric not working for KRaft clusters,KAFKA-13219,13396210,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,19/Aug/21 21:53,23/Aug/21 21:22,13/Jul/23 09:17,23/Aug/21 21:17,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,kraft,,,,,0,,,,,"The BrokerState metric always has a value of 0, for NOT_RUNNING, in KRaft clusters",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-19 21:53:26.0,,,,,,,,,,"0|z0u2m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams left/outer joins cause new internal changelog topic to grow unbounded,KAFKA-13216,13395989,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,spena,spena,18/Aug/21 22:25,17/Sep/21 17:38,13/Jul/23 09:17,16/Sep/21 23:45,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,,,,,0,,,,,"This bug is caused by the improvements made in https://issues.apache.org/jira/browse/KAFKA-10847, which fixes an issue with stream-stream left/outer joins. The issue is only caused when a stream-stream left/outer join is used with the new `JoinWindows.ofTimeDifferenceAndGrace()` API that specifies the window time + grace period. This new API was added in AK 3.0. No previous users are affected.

The issue causes that the internal changelog topic used by the new OUTERSHARED window store keeps growing unbounded as new records come. The topic is never cleaned up nor compacted even if tombstones are written to delete the joined and/or expired records from the window store. The problem is caused by a parameter required in the window store to retain duplicates. This config causes that tombstones records have a new sequence ID as part of the key ID in the changelog making those keys unique. Thus causing the cleanup policy not working.

In 3.0, we deprecated {{JoinWindows.of(size)}} in favor of {{JoinWindows.ofTimeDifferenceAndGrace()}} -- the old API uses the old semantics and is thus not affected while the new API enable the new semantics; the problem is that we deprecated the old API and thus tell users that they should switch to the new broken API.

We have two ways forward:
 * Fix the bug (non trivial)
 * Un-deprecate the old {{JoinWindow.of(size)}} API (and tell users not to use the new but broken API)",,ableegoldman,altery,marcelboldt,mjsax,showuon,spena,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13248,,,,,,,,KAFKA-10847,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 20 00:58:56 UTC 2021,,,,,,,,,,"0|z0u194:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 00:58;mjsax;We disabled the feature in 3.0.0, and will fix forward in 3.1.0: [https://github.com/apache/kafka/pull/11233] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectEndOffsetInformation,KAFKA-13215,13395941,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson5,kkarantasis,kkonstantine,18/Aug/21 16:33,24/Aug/21 23:40,13/Jul/23 09:17,24/Aug/21 16:14,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,flaky-test,,,,"Integration test {{test org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectCommittedOffsetInformation()}} sometimes fails with

{code:java}
java.lang.AssertionError: only one task
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.getTaskMetadata(TaskMetadataIntegrationTest.java:163)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectEndOffsetInformation(TaskMetadataIntegrationTest.java:144)
{code}",,kkonstantine,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13010,,,,,,KAFKA-13010,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 24 16:13:05 UTC 2021,,,,,,,,,,"0|z0u0yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 16:13;wcarlson5;[GitHub Pull Request #11083|https://github.com/apache/kafka/pull/11083]

This Pr should take case of this ticket as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer should not reset group state after disconnect,KAFKA-13214,13395937,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,hachikuji,hachikuji,hachikuji,18/Aug/21 16:23,06/Jun/22 21:11,13/Jul/23 09:17,24/Aug/21 19:05,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,,,,,,0,new-consumer-threading-should-fix,,,,"When the consumer disconnects from the coordinator while a rebalance is in progress, we currently reset the memberId and generation. The coordinator then must await the session timeout in order to expire the old memberId. This was apparently a regression from https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832R478. It would be better to keep the memberId/generation.",,ableegoldman,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-18 16:23:33.0,,,,,,,,,,"0|z0u0xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fetch/findSessions queries with open endpoints for SessionStore/WindowStore,KAFKA-13210,13395779,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,18/Aug/21 02:25,12/Oct/21 23:14,13/Jul/23 09:17,12/Oct/21 23:14,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,,,,,0,,,,,"This is the implementation of KIP-766: [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186876596]

 ",,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-18 02:25:31.0,,,,,,,,,,"0|z0tzyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong assignor selected if the assignor name is identical,KAFKA-13204,13395405,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,16/Aug/21 07:01,19/Aug/21 01:50,13/Jul/23 09:17,19/Aug/21 01:50,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,clients,,,,,0,,,,,"We used the partition assignor name to identify which assignor to use in consumer coordinator. But we didn't do any assignor name conflict check, which will cause the wrong assignor got selected when performing assignment.",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-16 07:01:11.0,,,,,,,,,,"0|z0txnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicsDelta doesn't update deleted topic when processing PartitionChangeRecord,KAFKA-13198,13394966,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,12/Aug/21 20:35,18/Aug/21 17:01,13/Jul/23 09:17,17/Aug/21 20:17,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,kraft,replication,,,,0,kip-500,,,,"In KRaft when a replica gets reassigned away from a topic partition we are not notifying the {{ReplicaManager}} to stop the replica.

On solution is to track those topic partition ids when processing {{PartitionChangeRecord}} and to returned them as {{deleted}} when the replica manager calls {{calculateDeltaChanges}}.",,dengziming,jagsancio,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 17 20:17:22 UTC 2021,,,,,,,,,,"0|z0tuxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 20:17;junrao;merged the PR to 3.0 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogCleaner may clean past highwatermark,KAFKA-13194,13394750,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lucasbradstreet,lucasbradstreet,lucasbradstreet,11/Aug/21 19:55,13/Aug/21 22:36,13/Jul/23 09:17,13/Aug/21 22:36,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"Here we have the cleaning point being bounded to the active segment base offset and the first unstable offset. Which makes sense:

 
{code:java}
   // find first segment that cannot be cleaned
    // neither the active segment, nor segments with any messages closer to the head of the log than the minimum compaction lag time
    // may be cleaned
    val firstUncleanableDirtyOffset: Long = Seq(      // we do not clean beyond the first unstable offset
      log.firstUnstableOffset,      // the active segment is always uncleanable
      Option(log.activeSegment.baseOffset),      // the first segment whose largest message timestamp is within a minimum time lag from now
      if (minCompactionLagMs > 0) {
        // dirty log segments
        val dirtyNonActiveSegments = log.localNonActiveLogSegmentsFrom(firstDirtyOffset)
        dirtyNonActiveSegments.find { s =>
          val isUncleanable = s.largestTimestamp > now - minCompactionLagMs
          debug(s""Checking if log segment may be cleaned: log='${log.name}' segment.baseOffset=${s.baseOffset} "" +
            s""segment.largestTimestamp=${s.largestTimestamp}; now - compactionLag=${now - minCompactionLagMs}; "" +
            s""is uncleanable=$isUncleanable"")
          isUncleanable
        }.map(_.baseOffset)
      } else None
    ).flatten.min


{code}
 

But LSO starts out as None.
{code:java}
@volatile private var firstUnstableOffsetMetadata: Option[LogOffsetMetadata] = None

private[log] def firstUnstableOffset: Option[Long] = firstUnstableOffsetMetadata.map(_.messageOffset){code}
For most code depending on the LSO, fetchLastStableOffsetMetadata is used to default it to the hwm if it's not set.

 
{code:java}

  private def fetchLastStableOffsetMetadata: LogOffsetMetadata = {
    checkIfMemoryMappedBufferClosed()    // cache the current high watermark to avoid a concurrent update invalidating the range check
    val highWatermarkMetadata = fetchHighWatermarkMetadata    firstUnstableOffsetMetadata match {
      case Some(offsetMetadata) if offsetMetadata.messageOffset < highWatermarkMetadata.messageOffset =>
        if (offsetMetadata.messageOffsetOnly) {
          lock synchronized {
            val fullOffset = convertToOffsetMetadataOrThrow(offsetMetadata.messageOffset)
            if (firstUnstableOffsetMetadata.contains(offsetMetadata))
              firstUnstableOffsetMetadata = Some(fullOffset)
            fullOffset
          }
        } else {
          offsetMetadata
        }
      case _ => highWatermarkMetadata
    }
  }

{code}
 

 

This means that in the case where the hwm is prior to the active segment base, the log cleaner may clean past the hwm. This is most likely to occur after a broker restart when the log cleaner may start cleaning prior to replication becoming active.",,jolshan,junrao,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 13 22:36:52 UTC 2021,,,,,,,,,,"0|z0ttls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/21 20:00;jolshan;There was code to change this in [https://github.com/apache/kafka/pull/9590,] but if we want to fix faster, we should just make a new PR. 

Replacing
 // we do not clean beyond the first unstable offset
 log.firstUnstableOffset,

with
 // we do not clean beyond the lastStableOffset (and therefore the high watermark)
 Option(log.lastStableOffset),;;;","13/Aug/21 22:36;junrao;Merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica manager doesn't update partition state when transitioning from leader to follower with unknown leader,KAFKA-13193,13394744,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,jagsancio,jagsancio,11/Aug/21 18:37,04/Feb/22 19:13,13/Jul/23 09:17,04/Feb/22 19:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,kraft,replication,,,,0,kip-500,,,,"This issue applies to both the ZK and KRaft implementation of the replica manager. In the rare case when a replica transition from leader to follower with no leader the partition state is not updated.

This is because when handling makeFollowers the ReplicaManager only updates the partition state if the leader is alive. The solution is to always transition to follower but not start the fetcher thread if the leader is unknown or not alive.",,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-11 18:37:11.0,,,,,,,,,,"0|z0ttkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The topic is marked for deletion, create topic with the same name throw exception topic already exists.",KAFKA-13175,13393861,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,yangshengwei,yangshengwei,06/Aug/21 09:45,07/Sep/21 20:50,13/Jul/23 09:17,01/Sep/21 17:16,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"After a topic is deleted, the topic is marked for deletion, create topic with the same name throw exception topic already exists. It should throw exception the topic is marked for deletion. I can choose to wait for the topic to be completely deleted. If the topic is still not deleted for a long time, we need to check the reason why it is not deleted.

 
 ",,yangshengwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/21 08:00;yangshengwei;kafka (2).jpg;https://issues.apache.org/jira/secure/attachment/13032108/kafka+%282%29.jpg","18/Aug/21 08:00;yangshengwei;zookeeper.jpg;https://issues.apache.org/jira/secure/attachment/13032109/zookeeper.jpg",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-06 09:45:09.0,,,,,,,,,,"0|z0to48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft controller does not handle simultaneous broker expirations correctly,KAFKA-13173,13393781,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,Niket Goel,hachikuji,hachikuji,05/Aug/21 20:36,12/Aug/21 18:07,13/Jul/23 09:17,12/Aug/21 18:07,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In `ReplicationControlManager.fenceStaleBrokers`, we find all of the current stale replicas and attempt to remove them from the ISR. However, when multiple expirations occur at once, we do not properly accumulate the ISR changes. For example, I ran a test where the ISR of a partition was initialized to [1, 2, 3]. Then I triggered a timeout of replicas 2 and 3 at the same time. The records that were generated by `fenceStaleBrokers` were the following:

{code}
ApiMessageAndVersion(PartitionChangeRecord(partitionId=0, topicId=_seg8hBuSymBHUQ1sMKr2g, isr=[1, 3], leader=1, replicas=null, removingReplicas=null, addingReplicas=null) at version 0), ApiMessageAndVersion(FenceBrokerRecord(id=2, epoch=102) at version 0), 
ApiMessageAndVersion(PartitionChangeRecord(partitionId=0, topicId=_seg8hBuSymBHUQ1sMKr2g, isr=[1, 2], leader=1, replicas=null, removingReplicas=null, addingReplicas=null) at version 0), 
ApiMessageAndVersion(FenceBrokerRecord(id=3, epoch=103) at version 0)]
{code}

First the ISR is shrunk to [1, 3] as broker 2 is fenced. We also see the record to fence broker 2. Then the ISR is modified to [1, 2] as the fencing of broker 3 is handled. So we did not account for the fact that we had already fenced broker 2 in the request. 

A simple solution for now is to change the logic to handle fencing only one broker at a time. ",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 05 21:56:34 UTC 2021,,,,,,,,,,"0|z0tnmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 21:56;hachikuji;I upgraded this bug to a blocker because I think it can result in data loss. For example, in the example above, the second ISR change would be interpreted as an expansion, but there may have been committed writes to the log between the two ISR changes which were not reflected in the expansion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test InternalTopicManagerTest.shouldRetryDeleteTopicWhenTopicUnknown,KAFKA-13170,13393637,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ableegoldman,ableegoldman,05/Aug/21 07:29,10/Aug/21 21:38,13/Jul/23 09:17,10/Aug/21 21:38,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,unit tests,,,,0,,,,,"[https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11176/2/testReport/org.apache.kafka.streams.processor.internals/InternalTopicManagerTest/Build___JDK_8_and_Scala_2_12___shouldRetryDeleteTopicWhenTopicUnknown_2/]
{code:java}
Stacktracejava.lang.AssertionError: unexpected exception type thrown; expected:<org.apache.kafka.streams.errors.StreamsException> but was:<org.apache.kafka.common.errors.TimeoutException>
  at org.junit.Assert.assertThrows(Assert.java:1020)
  at org.junit.Assert.assertThrows(Assert.java:981)
  at org.apache.kafka.streams.processor.internals.InternalTopicManagerTest.shouldRetryDeleteTopicWhenRetriableException(InternalTopicManagerTest.java:526)
  at org.apache.kafka.streams.processor.internals.InternalTopicManagerTest.shouldRetryDeleteTopicWhenTopicUnknown(InternalTopicManagerTest.java:497)
{code}",,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 05 22:53:09 UTC 2021,,,,,,,,,,"0|z0tmqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 22:53;guozhang;I think I know the reason of this flaky test. I have a PR https://github.com/apache/kafka/pull/11155 for another case within this class but it should be able to fix this one as well. [~ableegoldman] could you help me reviewing that one?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft observers should not have a replica id,KAFKA-13168,13393589,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rdielhenn,jagsancio,jagsancio,04/Aug/21 22:53,01/Jun/22 10:12,13/Jul/23 09:17,05/Aug/21 17:30,,,,,,,,,,,,,,,,,,,,,,,3.0.0,3.1.0,,,,,,,kraft,,,,,0,kip-500,,,,To avoid miss configuration of a broker affecting the quorum of the cluster metadata partition when a Kafka node is configure as broker only the replica id for the KRaft client should be set to {{Optional::empty()}}.,,dengziming,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-04 22:53:19.0,,,,,,,,,,"0|z0tmfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft broker should heartbeat immediately during controlled shutdown,KAFKA-13167,13393558,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,04/Aug/21 19:17,05/Aug/21 16:53,13/Jul/23 09:17,05/Aug/21 16:53,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"Controlled shutdown in KRaft is signaled through a heartbeat request with the `shouldShutDown` flag set to true. When we begin controlled shutdown, we should immediately schedule the next heartbeat instead of waiting for the next periodic heartbeat so that we can shutdown more quickly. Otherwise controlled shutdown can be delayed by several seconds.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-04 19:17:04.0,,,,,,,,,,"0|z0tm8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException when Controller handles unknown API,KAFKA-13166,13393550,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,04/Aug/21 18:06,22/Aug/22 17:42,13/Jul/23 09:17,22/Aug/22 17:37,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,,,,,,0,,,,,"When ControllerApis handles an unsupported RPC, it silently drops the request due to an unhandled exception. 

The following stack trace was manually printed since this exception was suppressed on the controller. 
{code}
java.util.NoSuchElementException: key not found: UpdateFeatures
	at scala.collection.MapOps.default(Map.scala:274)
	at scala.collection.MapOps.default$(Map.scala:273)
	at scala.collection.AbstractMap.default(Map.scala:405)
	at scala.collection.mutable.HashMap.apply(HashMap.scala:425)
	at kafka.network.RequestChannel$Metrics.apply(RequestChannel.scala:74)
	at kafka.network.RequestChannel.$anonfun$updateErrorMetrics$1(RequestChannel.scala:458)
	at kafka.network.RequestChannel.$anonfun$updateErrorMetrics$1$adapted(RequestChannel.scala:457)
	at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62)
	at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry(JavaCollectionWrappers.scala:359)
	at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry$(JavaCollectionWrappers.scala:355)
	at scala.collection.convert.JavaCollectionWrappers$AbstractJMapWrapper.foreachEntry(JavaCollectionWrappers.scala:309)
	at kafka.network.RequestChannel.updateErrorMetrics(RequestChannel.scala:457)
	at kafka.network.RequestChannel.sendResponse(RequestChannel.scala:388)
	at kafka.server.RequestHandlerHelper.sendErrorOrCloseConnection(RequestHandlerHelper.scala:93)
	at kafka.server.RequestHandlerHelper.sendErrorResponseMaybeThrottle(RequestHandlerHelper.scala:121)
	at kafka.server.RequestHandlerHelper.handleError(RequestHandlerHelper.scala:78)
	at kafka.server.ControllerApis.handle(ControllerApis.scala:116)
	at kafka.server.ControllerApis.$anonfun$handleEnvelopeRequest$1(ControllerApis.scala:125)
	at kafka.server.ControllerApis.$anonfun$handleEnvelopeRequest$1$adapted(ControllerApis.scala:125)
	at kafka.server.EnvelopeUtils$.handleEnvelopeRequest(EnvelopeUtils.scala:65)
	at kafka.server.ControllerApis.handleEnvelopeRequest(ControllerApis.scala:125)
	at kafka.server.ControllerApis.handle(ControllerApis.scala:103)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:75)
	at java.lang.Thread.run(Thread.java:748)
{code}

This is due to a bug in the metrics code in RequestChannel.

The result is that the request fails, but no indication is given that it was due to an unsupported API on either the broker, controller, or client.
",,hachikuji,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 17:37:21 UTC 2022,,,,,,,,,,"0|z0tm74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 17:37;hachikuji;I am going to resolve this jira. There were two issues that we have fixed:
 * [https://github.com/apache/kafka/pull/12403]: This patch fixes error handling in `ControllerApis` so that we catch errors which occur during serialization and sending of the response. Prior to this, any error occurred during this phase would be silently ignored and the connection would be left hanging.
 * [https://github.com/apache/kafka/pull/12538:] This patch fixes a bug during kraft shutdown which can cause a response to try to be sent after the controller's socket server has been shutdown. When this occurs, we get an exception trace which matches that in the description above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElectLeader API must be forwarded to Controller,KAFKA-13162,13393357,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,04/Aug/21 00:31,15/Sep/21 15:53,13/Jul/23 09:17,15/Sep/21 15:53,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,We're missing the logic to forward ElectLeaders requests to the controller. This means that `kafka-leader-election.sh` does not work correctly.,,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 12 18:01:30 UTC 2021,,,,,,,,,,"0|z0tl08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/21 18:01;hachikuji;This has turned out to be more work than expected. I am going to change the target version to 3.0.1. cc [~kkonstantine];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Follower leader and ISR state not updated after partition change in KRaft,KAFKA-13161,13393356,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,hachikuji,hachikuji,04/Aug/21 00:23,10/Aug/21 19:33,13/Jul/23 09:17,10/Aug/21 19:33,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In KRaft when we detect a partition change, we first verify whether any leader or follower transitions are needed. Depending on the case, we call either `applyLocalLeadersDelta` or `applyLocalFollowersDelta`. In the latter case, we are missing a call to `Partition.makeFollower` which is responsible for updating LeaderAndIsr state for the partitions. As a result of this, the partition state may be left stale. 

The specific consequences of this bug are 1) follower fetching fails since the epoch is never updated, and 2) a stale leader may continue to accept Produce requests. The latter is the bigger issue since it can lead to log divergence if we are appending from both the client and from the fetcher thread at the same time. I tested this locally and confirmed that it is possible.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-04 00:23:40.0,,,,,,,,,,"0|z0tl00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the code that calls the broker's config handler to pass the expected default resource name when using KRaft.,KAFKA-13160,13393318,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rdielhenn,rdielhenn,rdielhenn,03/Aug/21 18:48,26/Aug/21 04:37,13/Jul/23 09:17,04/Aug/21 23:45,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In a ZK cluster, dynamic default broker configs are stored in the zNode /brokers/<default>. Without this fix, when dynamic configs from snapshots are processed by the KRaft brokers, the BrokerConfigHandler checks if the resource name is ""<default>"" to do a default update and converts the resource name to an integer otherwise to do a per-broker config update.

In KRaft, dynamic default broker configs are serialized in metadata with empty string instead of ""<default>"". This was causing the BrokerConfigHandler to throw a NumberFormatException for dynamic default broker configs since the resource name for them is not ""<default>"" or a single integer. The code that calls the handler method for config changes should be fixed to pass ""<default>"" instead of empty string to the handler method if using KRaft.",,rdielhenn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-03 18:48:17.0,,,,,,,,,,"0|z0tkrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in TransactionalMessageCopier,KAFKA-13155,13393154,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,02/Aug/21 22:47,10/Aug/21 16:44,13/Jul/23 09:17,10/Aug/21 16:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Caught this exception in a system test run:
{code}
[2021-08-02 07:51:54,528] ERROR Uncaught exception in thread 'transactional-message-copier-shutdown-hook': (org.apache.kafka.common.utils.KafkaThread)
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
        at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2447)
        at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2330)
        at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2308)
        at org.apache.kafka.tools.TransactionalMessageCopier.lambda$main$1(TransactionalMessageCopier.java:331)
        at java.lang.Thread.run(Thread.java:748)
{code}
The pattern for closing the consumer is not safe.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-08-02 22:47:02.0,,,,,,,,,,"0|z0tjrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null Pointer Exception for record==null when handling a produce request,KAFKA-13149,13392447,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ccding,ccding,29/Jul/21 03:37,14/Sep/21 16:48,13/Jul/23 09:17,14/Sep/21 16:48,,,,,,,,,,,,,,,,,,,,,,,3.0.1,,,,,,,,log,,,,,0,,,,,"In production, we have seen an exception
{code:java}
java.lang.NullPointerException: Cannot invoke ""org.apache.kafka.common.record.Record.hasMagic(byte)"" because ""record"" is null{code}
which is triggered by
 [https://github.com/apache/kafka/blob/bfc57aa4ddcd719fc4a646c2ac09d4979c076455/core/src/main/scala/kafka/log/LogValidator.scala#L191]
 when handling a produce request.

The reason is that [https://github.com/apache/kafka/blob/bfc57aa4ddcd719fc4a646c2ac09d4979c076455/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L294-L296] returns record==null, which is possibly caused by a bad client. However, we have no idea about the client in our multi-tenant environment.

We should let the broker throw an invalid record exception and notify clients.",,ccding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-29 03:37:36.0,,,,,,,,,,"0|z0tfeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft Controller doesn't handle scheduleAppend returning Long.MAX_VALUE,KAFKA-13148,13392393,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Niket Goel,jagsancio,jagsancio,28/Jul/21 18:54,03/Aug/21 16:46,13/Jul/23 09:17,03/Aug/21 16:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,controller,kraft,,,,0,kip-500,,,,"In some cases the RaftClient will return Long.MAX_VALUE:
{code:java}
      /**
       * Append a list of records to the log. The write will be scheduled for some time
       * in the future. There is no guarantee that appended records will be written to
       * the log and eventually committed. However, it is guaranteed that if any of the
       * records become committed, then all of them will be.
       *
       * If the provided current leader epoch does not match the current epoch, which
       * is possible when the state machine has yet to observe the epoch change, then
       * this method will return {@link Long#MAX_VALUE} to indicate an offset which is
       * not possible to become committed. The state machine is expected to discard all
       * uncommitted entries after observing an epoch change.
       *
       * @param epoch the current leader epoch
       * @param records the list of records to append
       * @return the expected offset of the last record; {@link Long#MAX_VALUE} if the records could
       *         be committed; null if no memory could be allocated for the batch at this time
       * @throws org.apache.kafka.common.errors.RecordBatchTooLargeException if the size of the records is greater than the maximum
       *         batch size; if this exception is throw none of the elements in records were
       *         committed
       */
      Long scheduleAtomicAppend(int epoch, List<T> records);
 {code}
The controller doesn't handle this case:
{code:java}
                  // If the operation returned a batch of records, those records need to be
                  // written before we can return our result to the user.  Here, we hand off
                  // the batch of records to the raft client.  They will be written out
                  // asynchronously.
                  final long offset;
                  if (result.isAtomic()) {
                      offset = raftClient.scheduleAtomicAppend(controllerEpoch, result.records());
                  } else {
                      offset = raftClient.scheduleAppend(controllerEpoch, result.records());
                  }
                  op.processBatchEndOffset(offset);
                  writeOffset = offset;
                  resultAndOffset = ControllerResultAndOffset.of(offset, result);
                  for (ApiMessageAndVersion message : result.records()) {
                      replay(message.message(), Optional.empty(), offset);
                  }
                  snapshotRegistry.getOrCreateSnapshot(offset);
                  log.debug(""Read-write operation {} will be completed when the log "" +
                      ""reaches offset {}."", this, resultAndOffset.offset());
 {code}
 ",,dengziming,hachikuji,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12158,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 03 16:46:18 UTC 2021,,,,,,,,,,"0|z0tf2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/21 19:04;jagsancio;The controller also doesn't handle the raft client returning `null`.;;;","29/Jul/21 06:21;dengziming;KAFKA-12158 will change the return value of `scheduleAppend` and throws an exception if append failed to let the `QuorumController` handle it, so we no longer need to handle Null and Long.MaxValue since `QuorumController` will resign leadership and on any error type.;;;","03/Aug/21 16:46;hachikuji;Closing this since it was fixed by KAFKA-12158.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable Metadata endpoint for KRaft controller,KAFKA-13143,13392202,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,Niket Goel,hachikuji,hachikuji,27/Jul/21 20:37,29/Jul/21 16:24,13/Jul/23 09:17,29/Jul/21 16:24,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The controller currently implements Metadata incompletely. Specifically, it does not return the metadata for any topics in the cluster. This may tend to cause confusion to users. For example, if someone used the controller endpoint by mistake in `kafka-topics.sh --list`, then they would see no topics in the cluster, which would be surprising. It would be better for 3.0 to disable Metadata on the controller since we currently expect clients to connect through brokers anyway.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-27 20:37:58.0,,,,,,,,,,"0|z0tdw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader should not update follower fetch offset if diverging epoch is present,KAFKA-13141,13392195,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rsivaram,hachikuji,hachikuji,27/Jul/21 18:57,28/Jul/21 21:29,13/Jul/23 09:17,28/Jul/21 21:29,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,,,,,,0,,,,,"In 2.7, we began doing fetcher truncation piggybacked on the Fetch protocol instead of using the old OffsetsForLeaderEpoch API. When truncation is detected, we return a `divergingEpoch` field in the Fetch response, but we do not set an error code. The sender is expected to check if the diverging epoch is present and truncate accordingly.

All of this works correctly in the fetcher implementation, but the problem is that the logic to update the follower fetch position on the leader does not take into account the diverging epoch present in the response. This means the fetch offsets can be updated incorrectly, which can lead to either log divergence or the loss of committed data.

For example, we hit the following case with 3 replicas. Leader 1 is elected in epoch 1 with an end offset of 100. The followers are at offset 101

Broker 1: (Leader) Epoch 1 from offset 100
Broker 2: (Follower) Epoch 1 from offset 101
Broker 3: (Follower) Epoch 1 from offset 101

Broker 1 receives fetches from 2 and 3 at offset 101. The leader detects the divergence and returns a diverging epoch in the fetch state. Nevertheless, the fetch positions for both followers are updated to 101 and the high watermark is advanced.

After brokers 2 and 3 had truncated to offset 100, broker 1 experienced a network partition of some kind and was kicked from the ISR. This caused broker 2 to get elected, which resulted in the following state at the start of epoch 2.

Broker 1: (Follower) Epoch 2 from offset 101
Broker 2: (Leader) Epoch 2 from offset 100
Broker 3: (Follower) Epoch 2 from offset 100

Broker 2 was then able to write a new entry at offset 100 and the old record which may have been exposed to consumers was deleted by broker 1.
",,hachikuji,jack_foy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-27 18:57:25.0,,,,,,,,,,"0|z0tdug:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty response after requesting to restart a connector without the tasks results in NPE,KAFKA-13139,13391972,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,kkonstantine,kkonstantine,kkonstantine,26/Jul/21 19:58,27/Jul/21 22:33,13/Jul/23 09:17,27/Jul/21 21:00,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,KafkaConnect,,,,,0,,,,,"After https://issues.apache.org/jira/browse/KAFKA-4793 a response to restart only the connector (without any tasks) returns OK with an empty body. 

As system test runs revealed, this causes an NPE in [https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L135]

We should return 204 (NO_CONTENT) instead. 

This is a regression from previous behavior, therefore the ticket is marked as a blocker candidate for 3.0",,kkonstantine,kpatelatwork,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 27 22:33:09 UTC 2021,,,,,,,,,,"0|z0tcgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/21 14:50;rhauch;Just to clarify what appears to have happened.

As [~kpatelatwork] [mentions in a comment on the PR|https://github.com/apache/kafka/pull/11132], the behavior of the Connect restart API in AK 2.8 and earlier was always to return ""204 NO CONTENT"", not ""200 OK"" as mentioned in [KIP-745|https://cwiki.apache.org/confluence/display/KAFKA/KIP-745%3A+Connect+API+to+restart+connector+and+tasks]. Although the code used `Response.ok().build()`, the `RestClient` always processed the absence of a response body as `204 NO CONTENT`. So, to maintain the actual AK 2.x behavior in this branch of the code, we should instead return `204 NO CONTENT`.

I've corrected the KIP to reflect this older actual behavior of returning ""204 NO CONTENT"". It was a minor but necessary correction.

Note that we have *not* changed the KIP or the behavior of returning ""202 ACCEPTED"" when `includeTasks=true` and/or `failedOnly=true`. These cases correspond to the new behavior added in KIP-745.;;;","27/Jul/21 22:33;kpatelatwork;Thanks a lot [~rhauch] and [~kkonstantine] for taking care of the fix and unblocking the release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controller Metric MBean names are incorrectly quoted,KAFKA-13137,13391936,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,26/Jul/21 17:29,29/Jul/21 20:16,13/Jul/23 09:17,29/Jul/21 20:16,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,,,,,0,,,,,"QuorumControllerMetrics is letting com.yammer.metrics.MetricName create the MBean names for all of the controller metrics, and that adds quotes.  We have typically used KafkaMetricsGroup to explicitly create the MBean name, and we do not add quotes there.  The controller metric names that are in common between the old and new controller must remain the same, but they are not.  For example, this non-KRaft MBean name:

kafka.controller:type=KafkaController,name=OfflinePartitionsCount

has morphed into this when using KRaft:

""kafka.controller"":type=""KafkaController"",name=""OfflinePartitionsCount""

",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-26 17:29:38.0,,,,,,,,,,"0|z0tc8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-connect task.max : active task in consumer group is limited by the bigger topic to consume,KAFKA-13136,13391853,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,raphaelauv,raphaelauv,26/Jul/21 09:33,17/Mar/22 16:27,13/Jul/23 09:17,17/Mar/22 16:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"In kafka-connect 2.7

*The maximum number of active task for a sink connector is equal to the topic with the biggest number of partitions to consume*

An active task is a task with partitions attributed in the consumer-group of the sink connector

example :

With 2 topics where each have 10 partitions ( 20 partitions in total )

The maximum number of active task is 10 ( if I set task.max at 12 ,there is 10 members of the consumer group consuming partitions and  2 members in the consumer-group that do not have partitions to consume).

If I add a third topic with 15 partitions to the connector conf then the 12 members of the consumer group are consuming partitions, and then if I set now task.max at 17 only 15 members are active in the consumer-group.",,ChrisEgerton,raphaelauv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 17 08:00:59 UTC 2022,,,,,,,,,,"0|z0tbqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/22 03:31;ChrisEgerton;This is not a bug in Kafka Connect per se, but rather a side effect of the default consumer partition assignor, which at the moment is the [RangeAssignor|https://github.com/apache/kafka/blob/fbe7fb941173c0907792a8b48e8e9122aabecbd8/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java]. That assignor gives the first partition of every topic to a single consumer, the second partition of every topic to a single consumer, etc. This makes it particularly ill-suited for consumer groups (or multi-task sink connectors) that read from a large number of small topics, or more generally, any situation where the number of consumers (or sink tasks) is greater than the maximum number of partitions in any single topic being consumed, and there are multiple consumers in the group (or sink tasks for the connector).

This should be automatically addressed once [KIP-726|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177048248]/KAFKA-12473 get merged, as that will cause the default partition assignor to be updated to the [CooperativeStickyAssignor|https://github.com/apache/kafka/blob/fbe7fb941173c0907792a8b48e8e9122aabecbd8/clients/src/main/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.java], which has more intelligent assignment logic.

Until then, or if running on older versions of Kafka Connect that in turn use older versions of the Kafka clients library which still use the {{RangeAssignor}} as the default, there are a few options to work around this problem:
 # You can use a new default partition assignor for every connector on the worker by setting the {{consumer.partition.assignment.strategy}} property in your Kafka Connect worker config file.
 # You can configure the partition assignor on a per-connector basis by setting the {{consumer.override.partition.assignment.strategy}} property in your connector config (as long as the worker is configured with a connector client override policy that permits this, which should be possible with the default override policy as of 3.0.0).

As far as which assignor to use goes--if all you need is a guarantee that the spread of partitions across sink tasks is as even as possible, then you can use the [RoundRobinAssignor|https://github.com/apache/kafka/blob/fbe7fb941173c0907792a8b48e8e9122aabecbd8/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java]. Beyond that, it's probably beyond the scope of this ticket to make recommendations, but you can do some research of your own starting with the [docs for the consumer partition.assignment.strategy property|https://kafka.apache.org/31/documentation.html#consumerconfigs_partition.assignment.strategy].

 

Given that there is a known workaround for this issue and an approved, permanent fix in the works for an upcoming release, this may be safe to close. [~raphaelauv] thoughts?;;;","17/Mar/22 08:00;raphaelauv;thank you very much for that explanation , it's very clear :+1

yes we can close that ticket, thank you;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading to topic IDs in LISR requests has gaps introduced in 3.0,KAFKA-13132,13391697,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jolshan,jolshan,jolshan,25/Jul/21 03:59,06/Aug/21 01:14,13/Jul/23 09:17,06/Aug/21 01:14,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"With the change in 3.0 to how topic IDs are assigned to logs, a bug was inadvertently introduced. Now, topic IDs will only be assigned on the load of the log to a partition in LISR requests. This means we will only assign topic IDs for newly created topics/partitions, on broker startup, or potentially when a partition is reassigned.

 

In the case of upgrading from an IBP before 2.8, we may have a scenario where we upgrade the controller to IBP 3.0 (or even 2.8) last. (Ie, the controller is IBP < 2.8 and all other brokers are on the newest IBP) Upon the last broker upgrading, we will elect a new controller but its LISR request will not result in topic IDs being assigned to logs of existing topics. They will only be assigned in the cases mentioned above.

*Keep in mind, in this scenario, topic IDs will be still be assigned in the controller/ZK to all new and pre-existing topics and will show up in metadata.*  This means we are not ensured the same guarantees we had in 2.8. *It is just the LISR/partition.metadata part of the code that is affected.* 

 

The problem is two-fold
 1. We ignore LISR requests when the partition leader epoch has not increased (previously we assigned the ID before this check)
 2. We only assign the topic ID when we are associating the log with the partition in replicamanager for the first time. Though in the scenario described above, we have logs associated with partitions that need to be upgraded.

 

We should check the if the LISR request is resulting in a topic ID addition and add logic to logs already associated to partitions in replica manager.",,jolshan,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 03 21:28:49 UTC 2021,,,,,,,,,,"0|z0tars:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 16:16;kkonstantine;Marked as Blocker for now targeting 3.0, as it seems to be a regression. ;;;","03/Aug/21 21:28;jolshan;Found an issue where we don't sufficiently cover case 2. I have a plan to properly cover.

 cc: [~kkonstantine] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken system tests relate to the ConfigCommand change,KAFKA-13129,13391459,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,showuon,showuon,23/Jul/21 02:10,23/Jul/21 20:50,13/Jul/23 09:17,23/Jul/21 20:50,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,system tests,,,,,0,,,,,"After KAFKA-12598, the system tests failed in {{upgrade_test}}, {{zookeeper_tls_encrypt_only_test.py,}} and {{zookeeper_tls_test.py}}. Fix them.",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-23 02:10:32.0,,,,,,,,,,"0|z0t9aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreQueryIntegrationTest.shouldQueryStoresAfterAddingAndRemovingStreamThread,KAFKA-13128,13391453,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,ableegoldman,ableegoldman,23/Jul/21 01:27,24/Mar/22 14:34,13/Jul/23 09:17,24/Mar/22 14:34,2.8.1,3.0.0,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,streams,,,,,0,flaky-test,,,,"h3. Stacktrace

java.lang.AssertionError: Expected: is not null but: was null 
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) 
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
  at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.lambda$shouldQueryStoresAfterAddingAndRemovingStreamThread$19(StoreQueryIntegrationTest.java:461)
  at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.until(StoreQueryIntegrationTest.java:506)
  at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQueryStoresAfterAddingAndRemovingStreamThread(StoreQueryIntegrationTest.java:455)

 

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11085/5/testReport/org.apache.kafka.streams.integration/StoreQueryIntegrationTest/Build___JDK_16_and_Scala_2_13___shouldQueryStoresAfterAddingAndRemovingStreamThread_2/",,ableegoldman,cadonna,josep.prat-inactive,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 24 12:50:57 UTC 2022,,,,,,,,,,"0|z0t99k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 01:48;ableegoldman;The failure is from the second line in this series of assertions 
{code:java}
assertThat(store1.get(key), is(notNullValue()));
assertThat(store1.get(key2), is(notNullValue()));
assertThat(store1.get(key3), is(notNullValue()));
{code}
which is basically the first time we attempt IQ after starting up. The test setup includes starting Streams and waiting for it to reach RUNNING, then adding a new thread, and finally producing a set of 100 records for each of the three keys. After that it waits for all records to be processed *for _key3_* and then proceeds to the above assertions.

I suspect the problem is that we only wait for all data to be processed for _key3_, but not the other two keys. In theory this should work, since the data for _key3_ is produced last and would have the largest timestamps meaning the keys should be processed more or less in order. However the input topic actually has two partitions, so it could be that _key1_ and _key3_ correspond to task 1 while _key2_ corresponds to task 2. Again, that shouldn't affect the order in which records are processed – as long as the tasks are on the same thread.

But we started up a new thread in between waiting for Streams to reach RUNNING and producing data to the input topics. This new thread has to be assigned one of the tasks, but due to cooperative rebalancing it will take two full (though short) rebalances before the new thread can actually start processing any tasks. Therefore as long as the original thread continues to own the task corresponding to _key3_ after the new thread is added, it can easily get through all records for _key3_. Which would mean the test can proceed to the above assertions while the new thread is still waiting to start processing any data for _key2_ at all.

There are a few ways we can address this given how many things had to happen exactly right in order to see this failure, but the simplest fix is to just wait on all three keys to be fully processed rather than just the one. This seems to align with the original intention of the test best as well;;;","06/Aug/21 06:25;ableegoldman;Failed again for a different reason – just flaky, seems we need to wait for the thread to fully start up

 

{{java.lang.AssertionError: Unexpected exception thrown while getting the value from store.
Expected: is (a string containing ""Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING"" or a string containing ""The state store, source-table, may have migrated to another instance"")
     but: was ""Cannot get state store source-table because the stream thread is STARTING, not RUNNING""}};;;","09/Aug/21 14:02;wcarlson5;I fixed the second failure in this PR a little bit ago https://github.com/apache/kafka/pull/11153;;;","08/Sep/21 12:43;jlprat;Sorry to reopen this issue, it just occurred in this PR [https://github.com/apache/kafka/pull/11302]

It's a different error though:

{{}}
{code:java}
java.lang.AssertionError: Unexpected exception thrown while getting the value from store.
Expected: is (a string containing ""Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING"" or a string containing ""The state store, source-table, may have migrated to another instance"" or a string containing ""Cannot get state store source-table because the stream thread is STARTING, not RUNNING"")
  but: was ""Cannot get state store source-table because the stream thread is PARTITIONS_REVOKED, not RUNNING""{code}
[https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11302/3/testReport/junit/org.apache.kafka.streams.integration/StoreQueryIntegrationTest/Build___JDK_8_and_Scala_2_12___shouldQueryStoresAfterAddingAndRemovingStreamThread/?cloudbees-analytics-link=scm-reporting%2Ftests%2Ffailed]

Let me know if I should have opened a new issue instead of reopening this one.

{{}};;;","10/Sep/21 11:52;cadonna;And also here:
[https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11250/9/testReport/junit/org.apache.kafka.streams.integration/StoreQueryIntegrationTest/Build___JDK_11_and_Scala_2_13___shouldQueryStoresAfterAddingAndRemovingStreamThread/]

{code:java}
java.lang.AssertionError: 
Expected: is not null
 but: was null
 at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
 at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
 at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.lambda$shouldQueryStoresAfterAddingAndRemovingStreamThread$15(StoreQueryIntegrationTest.java:494)
 at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.until(StoreQueryIntegrationTest.java:545)
 at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQueryStoresAfterAddingAndRemovingStreamThread(StoreQueryIntegrationTest.java:488)
{code};;;","16/Nov/21 02:17;ableegoldman;[~wcarlson5]  unassigning you in case someone else wants to pick this up and fix the remaining source(s?) of failure;;;","24/Nov/21 10:21;jlprat;It might be that [https://github.com/apache/kafka/pull/11334] fixed this flaky test;;;","24/Mar/22 12:50;cadonna;[~wcarlson5] Do you think we can close this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix stray partition lookup logic,KAFKA-13127,13391451,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,23/Jul/21 00:37,23/Jul/21 22:05,13/Jul/23 09:17,23/Jul/21 22:05,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,The result of `BrokerMetadataPublisher.findGhostReplicas` is inverted. It returns all of the non-stray replicas. This causes all of these partitions to get deleted on startup by mistake.,,hachikuji,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-23 00:37:12.0,,,,,,,,,,"0|z0t994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overflow in joinGroupTimeoutMs when max.poll.interval.ms is MAX_VALUE leads to missing rebalances,KAFKA-13126,13391442,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,22/Jul/21 22:22,23/Jul/21 23:23,13/Jul/23 09:17,23/Jul/21 23:23,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,consumer,,,,,0,,,,,"In older versions of Kafka Streams, the {{max.poll.interval.ms}} config was overridden by default to {{Integer.MAX_VALUE}}. Even after we removed this override, users of both the plain consumer client and kafka streams still set the poll interval to MAX_VALUE somewhat often. Unfortunately, this causes an overflow when computing the {{joinGroupTimeoutMs}} and results in it being set to the {{request.timeout.ms}} instead, which is much lower.

This can easily make consumers drop out of the group, since they must rejoin now within 30s (by default) but have no obligation to almost ever call poll() given the high {{max.poll.interval.ms}} – basically they will only do so after processing the last record from the previously polled batch. So in heavy processing cases, where each record takes a long time to process, or when using a very large  {{max.poll.records}}, it can be difficult to make any progress at all before dropping out and needing to rejoin. And of course, the rebalance that is kicked off upon this member rejoining can result in many of the other members in the group dropping out as well, leading to an endless cycle of missed rebalances.

We just need to check for overflow and fix it to {{Integer.MAX_VALUE}} when it occurs. The workaround until then is of course to just set the {{max.poll.interval.ms}} to MAX_VALUE - 5000 (5s is the JOIN_GROUP_TIMEOUT_LAPSE)",,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-22 22:22:25.0,,,,,,,,,,"0|z0t974:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
resource leak due to not close KeyValueIterator implemented instances,KAFKA-13122,13391266,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,22/Jul/21 07:07,26/Jul/21 23:27,13/Jul/23 09:17,26/Jul/21 23:27,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,streams,,,,,0,,,,,"Found there are ""many"" KeyValueIterator implemented instances don't explicitly get closed, which will cause resource leak.

From the java doc in KeyValueIterator:

{color:#808080}* Users must call its {{color}{color:#808080}@code {color}{color:#808080}close} method explicitly upon completeness to release resources{color}

 

This issue mostly happen in tests because we usually query state store to get result iterator, and then do verification, but forgot close it. This issue also *appear in the example code in our developer guide docs*.

 

I'll use try-with-resource to fix them. To avoid huge PR created, I split this bug into 3 sub-tasks.",,ableegoldman,guozhang,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 26 23:27:10 UTC 2021,,,,,,,,,,"0|z0t840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 17:29;mjsax;Great catch!;;;","23/Jul/21 23:29;guozhang;Nice findings, thanks [~showuon]!;;;","23/Jul/21 23:55;showuon;Thank you, I hope I've fixed all the resource leaks!;;;","26/Jul/21 23:27;mjsax;All sub-tasks are resolved. Closing this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate the KRaft controllerListener config on startup,KAFKA-13119,13391177,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,Niket Goel,cmccabe,cmccabe,21/Jul/21 17:43,21/Jul/21 17:43,13/Jul/23 09:17,21/Jul/21 17:43,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-21 17:43:02.0,,,,,,,,,,"0|z0t7k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller's committed offset get out of sync with raft client listener context,KAFKA-13112,13391025,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,21/Jul/21 02:06,02/Aug/21 14:12,13/Jul/23 09:17,02/Aug/21 14:12,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,kraft,,,,0,kip-500,,,,"The active controller creates an in-memory snapshot for every offset returned by RaftClient::scheduleAppend and RaftClient::scheduleAtomicAppend. For RaftClient::scheduleAppend, the RaftClient is free to split those records into multiple batches. Because of this when scheduleAppend is use there is no guarantee that the active leader will always have an in-memory snapshot for every ""last committed offset"".

To get around this problem, when the active controller renounces from leader if there is no snapshot at the last committed offset it will instead.
 # Reset the snapshot registry
 # Unregister the listener from the RaftClient
 # Register a new listener with the RaftClient",,dengziming,jagsancio,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 02 14:12:30 UTC 2021,,,,,,,,,,"0|z0t6mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 05:30;kkonstantine;[~jagsancio] the two subtasks of this issue have been resolved. Does this make this blocker issue resolved as well?;;;","02/Aug/21 14:12;jagsancio;Yes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offsets deletion error,KAFKA-13106,13390884,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,robert.janda,robert.janda,20/Jul/21 10:00,20/Jul/21 10:19,13/Jul/23 09:17,20/Jul/21 10:19,2.3.1,2.7.0,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,admin,,,,,0,,,,,"When I use:

kafka-consumer-groups.sh --bootstrap-server broker:9092 --delete-offsets --group myGroup --topic myTopic

 

I have a error:

 

SLF4J: Class path contains multiple SLF4J bindings.

SLF4J: Found binding in [jar:file:/kafka/libs/slf4j-log4j12-1.7.26.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: Found binding in [jar:file:/kafka/libs/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: See [http://www.slf4j.org/codes.html#multiple_bindings] for an explanation.

SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Exception in thread ""main"" joptsimple.UnrecognizedOptionException: delete-offsets is not a recognized option

        at joptsimple.OptionException.unrecognizedOption(OptionException.java:108)

        at joptsimple.OptionParser.handleLongOptionToken(OptionParser.java:510)

        at joptsimple.OptionParserState$2.handleArgument(OptionParserState.java:56)

        at joptsimple.OptionParser.parse(OptionParser.java:396)

        at kafka.admin.ConsumerGroupCommand$ConsumerGroupCommandOptions.<init>(ConsumerGroupCommand.scala:917)

        at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:46)

        at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)

 


 When I use kafka protocol base on:
 [https://kafka.apache.org/protocol#The_Messages_OffsetDelete]
 
 I have kafka log:
 
 [2021-07-20 07:23:37,832] ERROR Closing socket for 10.1.1.20:9092-192.168.65.3:56050-0 because of error (kafka.network.Processor)

org.apache.kafka.common.errors.InvalidRequestException: Unknown API key 47

[2021-07-20 07:23:37,835] ERROR Exception while processing request from 10.1.1.20:9092-192.168.65.3:56050-0 (kafka.network.Processor)

org.apache.kafka.common.errors.InvalidRequestException: Unknown API key 47
 
 
 Base on:
 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-496%3A+Administrative+API+to+delete+consumer+offsets]
 
 Those things should be working.",Linux,robert.janda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 20 10:19:01 UTC 2021,,,,,,,,,,"0|z0t5r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/21 10:19;robert.janda;version 2.3.1 does not suport it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller should notify the RaftClient when it resigns,KAFKA-13104,13390752,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rdielhenn,jagsancio,jagsancio,19/Jul/21 17:21,21/Jul/21 17:07,13/Jul/23 09:17,21/Jul/21 17:07,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,kraft,,,,0,kip-500,,,,"{code:java}
      private Throwable handleEventException(String name,
                                             Optional<Long> startProcessingTimeNs,
                                             Throwable exception) {
          ...
          renounce();
          return new UnknownServerException(exception);
      }
 {code}
When the active controller encounters an event exception it attempts to renounce leadership. Unfortunately, this doesn't tell the {{RaftClient}} that it should attempt to give up leadership. This will result in inconsistent state with the {{RaftClient}} as leader but with the controller as inactive.

We should change this implementation so that the active controller asks the {{RaftClient}} to resign. The active controller waits until {{handleLeaderChange}} before calling {{renounce()}}",,cmccabe,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 21 17:07:34 UTC 2021,,,,,,,,,,"0|z0t4xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/21 17:07;cmccabe;Fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic IDs not propagated to metadata cache quickly enough for Fetch path,KAFKA-13102,13390287,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,jolshan,jolshan,18/Jul/21 03:08,24/Sep/21 08:51,13/Jul/23 09:17,24/Sep/21 08:51,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"Currently, the fetch path for replicas relies on the topic IDs in the metadata cache. However, the propagation of topic ID information is done through the UpdateMetadata request and is too slow. At first the topic will have no ID in the metadata cache and we will send an older request and then we get the ID and have to close the session. This will likely happen on broker startup and with new topics. This has resulted in increased partitions in error, frequent closing of sessions and made tests like ConsumerBounceTest#testCloseDuringRebalance extremely flaky.

A quick test with topic IDs stored in the replica manager during the handling of LISR requests showed that significantly fewer errors and made ConsumerBounceTest#testCloseDuringRebalance much less flaky (passing 50/50 runs vs. 11/50 runs).

The task now is figuring out the best strategy to store topic IDs for the fetch path using the IDs from the LISR request.",,ableegoldman,calohmn,chia7712,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-18 03:08:35.0,,,,,,,,,,"0|z0t22o:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller cannot revert to an in-memory snapshot,KAFKA-13100,13390264,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,17/Jul/21 18:36,20/Jul/21 17:13,13/Jul/23 09:17,20/Jul/21 17:13,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,kraft,,,,0,kip-500,,,,"{code:java}
  [2021-07-16 16:34:55,578] DEBUG [Controller 3002] Executing handleRenounce[3]. (org.apache.kafka.controller.QuorumController)
  [2021-07-16 16:34:55,578] WARN [Controller 3002] Renouncing the leadership at oldEpoch 3 due to a metadata log event. Reverting to last committed offset 214. (org.apache.kafka.controller.QuorumController)
  [2021-07-16 16:34:55,579] WARN [Controller 3002] org.apache.kafka.controller.QuorumController@646b1289: failed with unknown server exception RuntimeException at epoch -1 in 1510 us.  Reverting to last committed offset 214. (org.apache.kafka.controller.  QuorumController)
  java.lang.RuntimeException: No snapshot for epoch 214. Snapshot epochs are: -1, 1, 3, 5, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 94, 96, 97, 107, 108, 112, 125, 126, 128, 135, 171, 208, 213
          at org.apache.kafka.timeline.SnapshotRegistry.getSnapshot(SnapshotRegistry.java:173)
          at org.apache.kafka.timeline.SnapshotRegistry.revertToSnapshot(SnapshotRegistry.java:203)
          at org.apache.kafka.controller.QuorumController.renounce(QuorumController.java:784)
          at org.apache.kafka.controller.QuorumController.access$2500(QuorumController.java:121)
          at org.apache.kafka.controller.QuorumController$QuorumMetaLogListener.lambda$handleLeaderChange$3(QuorumController.java:769)
          at org.apache.kafka.controller.QuorumController$ControlEvent.run(QuorumController.java:311)
          at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:121)
          at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
          at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
          at java.lang.Thread.run(Thread.java:748)
  [2021-07-16 16:34:55,580] ERROR [Controller 3002] Unexpected exception in handleException (org.apache.kafka.queue.KafkaEventQueue)
  java.lang.RuntimeException: No snapshot for epoch 214. Snapshot epochs are: -1, 1, 3, 5, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 94, 96, 97, 107, 108, 112, 125, 126, 128, 135, 171, 208, 213
          at org.apache.kafka.timeline.SnapshotRegistry.getSnapshot(SnapshotRegistry.java:173)
          at org.apache.kafka.timeline.SnapshotRegistry.revertToSnapshot(SnapshotRegistry.java:203)
          at org.apache.kafka.controller.QuorumController.renounce(QuorumController.java:784)
          at org.apache.kafka.controller.QuorumController.handleEventException(QuorumController.java:287)
          at org.apache.kafka.controller.QuorumController.access$500(QuorumController.java:121)
          at org.apache.kafka.controller.QuorumController$ControlEvent.handleException(QuorumController.java:317)                                                                                                                                                       at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:126)
          at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:200)
          at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:173)
          at java.lang.Thread.run(Thread.java:748) {code}",,cmccabe,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 20 17:13:57 UTC 2021,,,,,,,,,,"0|z0t1xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/21 16:53;jagsancio;Here is an example that triggers this error:

1. Controller 3002 starts as inactive and replays a batch with a last offset of 214
{code:java}
[2021-07-16 16:34:46,950] DEBUG [RaftManager nodeId=3002] Follower high watermark updated to 215 (org.apache.kafka.raft.KafkaRaftClient)
[2021-07-16 16:34:46,951] DEBUG [Controller 3002] Executing handleCommits[baseOffset=214]. (org.apache.kafka.controller.QuorumController)
[2021-07-16 16:34:46,951] DEBUG [Controller 3002] Replaying commits from the active node up to offset 214. (org.apache.kafka.controller.QuorumController)
{code}
2. Controller 3002 becomes leader for epoch 3
{code:java}
[2021-07-16 16:34:51,852] INFO [RaftManager nodeId=3002] Completed transition to Leader(localId=3002, epoch=3, epochStartOffset=215, highWatermark=Optional.empty, voterStates={3001=ReplicaState(nodeId=3001, endOffset=Optional.empty, lastFetchTimestamp=  OptionalLong.empty, hasAcknowledgedLeader=false), 3002=ReplicaState(nodeId=3002, endOffset=Optional.empty, lastFetchTimestamp=OptionalLong.empty, hasAcknowledgedLeader=true), 3003=ReplicaState(nodeId=3003, endOffset=Optional.empty, lastFetchTimestamp=O  ptionalLong.empty, hasAcknowledgedLeader=false)}) (org.apache.kafka.raft.QuorumState)
[2021-07-16 16:34:51,852] DEBUG [Controller 3002] Executing handleClaim[3]. (org.apache.kafka.controller.QuorumController)
[2021-07-16 16:34:51,852] DEBUG [LeaderEpochCache @metadata-0] Appended new epoch entry EpochEntry(epoch=3, startOffset=215). Cache now contains 2 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2021-07-16 16:34:51,852] WARN [Controller 3002] Becoming active at controller epoch 3. (org.apache.kafka.controller.QuorumController)
[2021-07-16 16:34:51,855] DEBUG [Controller 3002] Processed handleClaim[3] in 2117 us (org.apache.kafka.controller.QuorumController){code}
3. Controller 3002 lost leadership
{code:java}
[2021-07-16 16:34:55,578] DEBUG [Controller 3002] Executing handleRenounce[3]. (org.apache.kafka.controller.QuorumController)
[2021-07-16 16:34:55,578] WARN [Controller 3002] Renouncing the leadership at oldEpoch 3 due to a metadata log event. Reverting to last committed offset 214. (org.apache.kafka.controller.QuorumController){code}
4. Controller couldn't revert to the committed offset because it didn't generate an in-memory snapshot at the committed offset when it transition to leader.
{code:java}
[2021-07-16 16:34:55,579] WARN [Controller 3002] org.apache.kafka.controller.QuorumController@646b1289: failed with unknown server exception RuntimeException at epoch -1 in 1510 us. Reverting to last committed offset 214. (org.apache.kafka.controller. QuorumController){code}
 

An active controller assumes that there is an in-memory snapshot at the committed offset. The inactive controller only generates an in-memory snapshot when it needs to create an on-disk snapshot.

To fix this the active controller needs to generate an in-memory snapshot at the committed offset when it transition from inactive to active.;;;","20/Jul/21 17:13;cmccabe;Fixed. Thanks, [~jagsancio]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message too large error when expiring transactionalIds,KAFKA-13099,13390138,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,16/Jul/21 18:21,03/Aug/21 17:25,13/Jul/23 09:17,28/Jul/21 18:27,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,2.5.1,2.6.1,2.7.1,2.8.0,,,,,,,,,,,,,,2.5.2,2.6.3,2.7.2,2.8.1,3.0.0,,,,,,,,,0,,,,,We have seen a couple reports of MESSAGE_TOO_LARGE errors when writing tombstones for expired transactionalIds. This is possible because we collect all expired IDs into a single batch. We should ensure that the created batches are smaller than the max message size. Any expired IDs that cannot fit can be expired later.,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-16 18:21:53.0,,,,,,,,,,"0|z0t15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No such file exception when recovering snapshots in metadata log dir,KAFKA-13098,13390137,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,16/Jul/21 18:14,17/Jul/21 18:34,13/Jul/23 09:17,17/Jul/21 18:34,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,kip-500,,,,"{code:java}
RaftClusterTest > testCreateClusterAndCreateListDeleteTopic() FAILED
    java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/kafka-286994548094074875/broker_0_data0/@metadata-0/partition.metadata.tmp
        at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
        at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
        at java.util.Iterator.forEachRemaining(Iterator.java:115)
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)                                       
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)                                                                                                                                                                                   
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
        at kafka.raft.KafkaMetadataLog$.recoverSnapshots(KafkaMetadataLog.scala:616)
        at kafka.raft.KafkaMetadataLog$.apply(KafkaMetadataLog.scala:583)                  
        at kafka.raft.KafkaRaftManager.buildMetadataLog(RaftManager.scala:257)
        at kafka.raft.KafkaRaftManager.<init>(RaftManager.scala:132)                                                                                                                                                                                                             
        at kafka.testkit.KafkaClusterTestKit$Builder.build(KafkaClusterTestKit.java:227)
        at kafka.server.RaftClusterTest.testCreateClusterAndCreateListDeleteTopic(RaftClusterTest.scala:87) {code}",,jagsancio,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-16 18:14:09.0,,,,,,,,,,"0|z0t15c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryableStoreProvider is not updated when threads are added/removed/replaced rendering IQ impossible,KAFKA-13096,13389937,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,ableegoldman,ableegoldman,16/Jul/21 01:25,21/Jul/21 20:37,13/Jul/23 09:17,21/Jul/21 20:37,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,,,,,"The QueryableStoreProviders class is used to route queries to the correct state store on the owning StreamThread, making it a critical piece of IQ. It gets instantiated when you create a new KafkaStreams, and is passed in a list of StreamThreadStateStoreProviders which it then copies and stores. Because it only stores a copy it only ever contains a provider for the StreamThreads that were created during the app's startup, and unfortunately is never updated during an add/remove/replace thread event. 

This means that IQ can’t get a handle on any stores that belong to a thread that wasn’t in the original set. If the app is starting up new threads through the #addStreamThread API or following a REPLACE_THREAD event, none of the data in any of the stores owned by that new thread will be accessible by IQ. If a user is removing threads through #removeStreamThread, or threads die and get replaced, you can fall into an endless loop of {{InvalidStateStoreException}} from doing a lookup into stores that have been closed since the thread was removed/died.

If over time all of the original threads are removed or replaced, then IQ won’t work at all.",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-16 01:25:25.0,,,,,,,,,,"0|z0szww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perf regression in LISR requests,KAFKA-13092,13389734,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jolshan,jolshan,jolshan,15/Jul/21 01:53,31/Aug/21 15:26,13/Jul/23 09:17,15/Jul/21 23:51,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,0,,,,,"With the addition of partition metadata files, we have an extra operation to do when handling LISR requests. This really slows down the processing, so we should flush asynchronously to fix this regression.",,dengziming,jolshan,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 15 23:51:42 UTC 2021,,,,,,,,,,"0|z0syns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/21 23:51;junrao;merged the PR to 3.0 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increment HW after shrinking ISR through AlterIsr,KAFKA-13091,13389726,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,15/Jul/21 00:21,31/Aug/21 00:30,13/Jul/23 09:17,23/Aug/21 18:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"After we have shrunk the ISR, we have an opportunity to advance the high watermark. We do this currently in `maybeShrinkIsr` after the synchronous update through ZK. For the AlterIsr path, however, we cannot rely on this call since the request is sent asynchronously. Instead we should attempt to advance the high watermark in the callback when the AlterIsr response returns successfully.",,dengziming,hachikuji,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-15 00:21:35.0,,,,,,,,,,"0|z0sym0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port sticky assignor fixes (KAFKA-12984) back to 2.8,KAFKA-13081,13389502,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,ableegoldman,ableegoldman,14/Jul/21 01:50,19/Aug/21 00:03,13/Jul/23 09:17,19/Aug/21 00:03,,,,,,,,,,,,,,,,,,,,,,,2.8.1,,,,,,,,consumer,,,,,0,,,,,"We should make sure that fix #1 and #2 of [#10985|https://github.com/apache/kafka/pull/10985] make it back to the 2.8 sticky assignor, since it's pretty much impossible to smoothly cherrypick that commit from 3.0 to 2.8 due to all the recent improvements and refactoring in the AbstractStickyAssignor. Either we can just extract and apply those two fixes to 2.8 directly, or go back and port all the commits that made this cherrypick difficult over to 2.8 as well. If we do so then cherrypicking the original commit should be easy.

See [this comment|https://github.com/apache/kafka/pull/10985#issuecomment-879521196]",,ableegoldman,dajac,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12984,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 16 08:07:32 UTC 2021,,,,,,,,,,"0|z0sx94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/21 01:57;ableegoldman;Personally I would probably advocate for just applying the two fixes to 2.8 rather than porting over the full set of improvements and refactorings. While the improvements are great to have, they're not strictly necessary and relatively complex, so porting them back carries some risk in case of as-yet-undiscovered bugs that were introduced during the large-scale refactoring of this assignor. Given that it's hard to be 100% confident in the correctness of such a complicated algorithm, regardless of how good the test coverage is, I would prefer to just apply the minimal required fixes and leave 2.8 as a ""safe"" branch. That way we can fall back to it in case of unexpected issues in the 3.0 assignor, and recommend users who want to downgrade to the last stable version with a good ""cooperative-sticky""/""sticky"" assignor they can continue to use until the new assignor is stabilized.

Just my 2 cents;;;","14/Jul/21 02:06;showuon;> I would prefer to just apply the minimal required fixes and leave 2.8 as a ""safe"" branch

Agree!;;;","16/Aug/21 07:35;dajac;[~ableegoldman] [~showuon] This is the only remaining blocker for 2.8.1. Would you already have an ETA for it?;;;","16/Aug/21 08:07;showuon;My PR is ready for [~ableegoldman] 's review, I think this PR should be fast since it's just a back port PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch snapshot request are not directed to kraft in controller,KAFKA-13080,13389472,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,13/Jul/21 21:03,14/Jul/21 22:39,13/Jul/23 09:17,14/Jul/21 22:39,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,kraft,,,,0,kip-500,,,,"Kraft followers and observer are seeing the following error
{code:java}
[2021-07-13 18:15:47,289] ERROR [RaftManager nodeId=2] Unexpected error UNKNOWN_SERVER_ERROR in FETCH_SNAPSHOT response: InboundResponse(correlationId=29862, data=FetchSnapshotResponseData(throttleTimeMs=0, errorCode=-1, topics=[]), sourceId=3001) (org.apache.kafka.raft.KafkaRaftClient) {code}
This is because ControllerApis is not directing FetchSnapshost request to the raft manager.",,dengziming,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-13 21:03:59.0,,,,,,,,,,"0|z0sx2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forgotten Topics in Fetch Requests may incorrectly use topic IDs,KAFKA-13079,13389469,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,jolshan,jolshan,13/Jul/21 20:27,27/Aug/21 16:29,13/Jul/23 09:17,27/Aug/21 16:29,3.1.0,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,0,,,,,"In the new code for Fetch, we only check if the topics contained in the session have IDs to decide whether to send a version < 13 (topic names) or version 13+ (topic IDs) request. However, if we have an empty session that previously did not use IDs, we will try to send a request to forget the topics. Since all topics in the session (none) were not missing topic ids, we will send a version 13 request. This request will have the Zero UUID and fail.

The result is that we close the session and mark any partitions in it as errored, but the message is confusing and the request is not correct. We should somehow also track forgotten topics when deciding what version to use.",,dengziming,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-13 20:27:52.0,,,,,,,,,,"0|z0sx1s:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Closing FileRawSnapshotWriter too early,KAFKA-13078,13389437,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jagsancio,jagsancio,13/Jul/21 17:58,14/Jul/21 22:40,13/Jul/23 09:17,14/Jul/21 22:40,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,kraft,,,,,0,kip-500,,,,"We are getting the following error
{code:java}
  [2021-07-13 17:23:42,174] ERROR [kafka-raft-io-thread]: Error due to (kafka.raft.KafkaRaftManager$RaftIoThread)
  java.io.UncheckedIOException: Error calculating snapshot size. temp path = /mnt/kafka/kafka-metadata-logs/@metadata-0/00000000000000000062-0000000002-3249768281228588378.checkpoint.part, snapshotId = OffsetAndEpoch(offset=62, epoch=2).
      at org.apache.kafka.snapshot.FileRawSnapshotWriter.sizeInBytes(FileRawSnapshotWriter.java:63)
      at org.apache.kafka.raft.KafkaRaftClient.maybeSendFetchOrFetchSnapshot(KafkaRaftClient.java:2044)
      at org.apache.kafka.raft.KafkaRaftClient.pollFollowerAsObserver(KafkaRaftClient.java:2032)
      at org.apache.kafka.raft.KafkaRaftClient.pollFollower(KafkaRaftClient.java:1995)
      at org.apache.kafka.raft.KafkaRaftClient.pollCurrentState(KafkaRaftClient.java:2104)
      at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:2217)
      at kafka.raft.KafkaRaftManager$RaftIoThread.doWork(RaftManager.scala:52)
      at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
  Caused by: java.nio.channels.ClosedChannelException
      at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110)
      at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:300)
      at org.apache.kafka.snapshot.FileRawSnapshotWriter.sizeInBytes(FileRawSnapshotWriter.java:60)
      ... 7 more
 {code}
This is because the {{FollowerState}} is closing the snapshot write passed through the argument instead of the one being replaced.",,dengziming,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-13 17:58:10.0,,,,,,,,,,"0|z0swuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simulation test fails due to inconsistency in MockLog's implementation,KAFKA-13073,13389295,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,jagsancio,jagsancio,13/Jul/21 03:30,14/Jul/21 21:55,13/Jul/23 09:17,14/Jul/21 21:55,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,replication,,,,0,kip-500,,,,"We are getting the following error on trunk
{code:java}
RaftEventSimulationTest > canRecoverAfterAllNodesKilled STANDARD_OUT
    timestamp = 2021-07-12T16:26:55.663, RaftEventSimulationTest:canRecoverAfterAllNodesKilled =
      java.lang.RuntimeException:
        Uncaught exception during poll of node 1                                  |-------------------jqwik-------------------
    tries = 25                    | # of calls to property
    checks = 25                   | # of not rejected calls
    generation = RANDOMIZED       | parameters are randomly generated
    after-failure = PREVIOUS_SEED | use the previous seed
    when-fixed-seed = ALLOW       | fixing the random seed is allowed
    edge-cases#mode = MIXIN       | edge cases are mixed in
    edge-cases#total = 108        | # of all combined edge cases
    edge-cases#tried = 4          | # of edge cases tried in current run
    seed = 8079861963960994566    | random seed to reproduce generated values    Sample
    ------
      arg0: 4002
      arg1: 2
      arg2: 4{code}
I think there are a couple of issues here:
 # The {{ListenerContext}} for {{KafkaRaftClient}} uses the value returned by {{ReplicatedLog::startOffset()}} to determined the log start and when to load a snapshot while the {{MockLog}} implementation uses {{logStartOffset}} which could be a different value.
 # {{MockLog}} doesn't implement {{ReplicatedLog::maybeClean}} so the log start offset is always 0.
 # The snapshot id validation for {{MockLog}} and {{KafkaMetadataLog}}'s {{createNewSnapshot}} throws an exception when the snapshot id is less than the log start offset.

Solutions:

Fix the error quoted above we only need to fix bullet point 3. but I think we should fix all of the issues enumerated in this Jira.

For 1. we should change the {{MockLog}} implementation so that it uses {{startOffset}} both externally and internally.

For 2. I will file another issue to track this implementation.

For 3. I think this validation is too strict. I think it is safe to simply ignore any attempt by the state machine to create an snapshot with an id less that the log start offset. We should return a {{Optional.empty()}}when the snapshot id is less than the log start offset. This tells the user that it doesn't need to generate a snapshot for that offset. ",,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13074,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-13 03:30:22.0,,,,,,,,,,"0|z0svz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`AlterConsumerGroupOffsetsHandler` does not handle partition errors correctly.,KAFKA-13058,13388868,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,11/Jul/21 09:39,20/Jul/21 12:00,13/Jul/23 09:17,19/Jul/21 14:49,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"While reviewing https://github.com/apache/kafka/pull/10973, we have noticed that `AlterConsumerGroupOffsetsHandler` does not handle the partition errors correctly. The issue is that any partition error fails the entire future returned in the results instead of failing only the future of the corresponding partition. This is a regression that was introduced by KIP-699.",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-11 09:39:19.0,,,,,,,,,,"0|z0stc8:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many broker RPCs are not enable in KRaft mode,KAFKA-13057,13388734,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,mumrah,mumrah,09/Jul/21 19:30,10/Jul/21 17:45,13/Jul/23 09:17,10/Jul/21 17:45,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,kraft,,,,0,,,,,Many RPCs have not yet been enabled in KRaft mode. We should enable the ones that are intended to be supported for the 3.0 release.,,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-09 19:30:32.0,,,,,,,,,,"0|z0ssig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker should not generate snapshots when controller is co-resident,KAFKA-13056,13388733,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,09/Jul/21 19:30,10/Jul/21 17:48,13/Jul/23 09:17,10/Jul/21 17:48,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,We added broker snapshot support in https://github.com/apache/kafka/pull/10990. We should have logic to disable the broker snapshot when the controller is running on the same host since there is no need to have them both doing it.,,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-09 19:30:26.0,,,,,,,,,,"0|z0ssi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump frame version for KRaft records,KAFKA-13053,13388481,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,08/Jul/21 20:43,09/Jul/21 19:45,13/Jul/23 09:17,09/Jul/21 19:45,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"We have at least one compatibility break to the record format of KRaft records (we added a version field to the LeaderChange schema). Since there was never an expectation of compatibility between 2.8 and 3.0, we want to make this explicit by bumping the frame version. At the same time, we will reset the record versions back to 0.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-08 20:43:03.0,,,,,,,,,,"0|z0sqyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log recovery threads use default thread pool naming,KAFKA-13049,13388372,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,08/Jul/21 11:28,14/Jul/21 09:23,13/Jul/23 09:17,14/Jul/21 09:23,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,core,,,,,0,,,,,The threads used for log recovery use a pool {{Executors.newFixedThreadPool(int)}} and hence pick up the naming scheme from {{Executors.defaultThreadFactory()}}. It's not so clear in a thread dump taken during log recovery what those threads are. They should have clearer names. ,,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-08 11:28:22.0,,,,,,,,,,"0|z0sqa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test KafkaMetadataLogTest.testDeleteSnapshots(),KAFKA-13042,13388099,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dengziming,dengziming,07/Jul/21 07:13,11/Jul/21 04:36,13/Jul/23 09:17,11/Jul/21 01:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,This test fails many times but succeeds locally.,,dengziming,meher_crok,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 11 04:36:43 UTC 2021,,,,,,,,,,"0|z0solk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/21 22:33;meher_crok;[~dengziming] so this the error you get ?

Error while writing to partition metadata file \AppData\Local\Temp\kafka-6754745891137104415@metadata-0\partition.metadataError while writing to partition metadata file \AppData\Local\Temp\kafka-6754745891137104415@metadata-0\partition.metadataorg.apache.kafka.common.errors.KafkaStorageException: Error while writing to partition metadata file \AppData\Local\Temp\kafka-6754745891137104415@metadata-0\partition.metadata Suppressed: java.nio.file.FileSystemException: \AppData\Local\Temp\kafka-6754745891137104415@metadata-0\00000000000000000000.index: Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus.;;;","11/Jul/21 04:36;dengziming;[~meher_crok] This is a bug of Scala 2.12 and fixed here: https://github.com/apache/kafka/pull/10997;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Thread state is already PENDING_SHUTDOWN"" log spam",KAFKA-13037,13387946,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gray.john,gray.john,gray.john,06/Jul/21 13:52,14/Jul/21 12:12,13/Jul/23 09:17,14/Jul/21 04:09,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,,,,,"KAFKA-12462 introduced a [change|https://github.com/apache/kafka/commit/4fe4cdc4a61cbac8e070a8b5514403235194015b#diff-76f629d0df8bd30b2593cbcf4a2dc80de3167ebf55ef8b5558e6e6285a057496R722] that increased this ""Thread state is already {}"" logger to info instead of debug. We are running into a problem with our streams apps when they hit an unrecoverable exception that shuts down the streams thread, we get this log printed about 50,000 times per second per thread. I am guessing it is once per record we have queued up when the exception happens? We have temporarily raised the StreamThread logger to WARN instead of INFO to avoid the spam, but we do miss the other good logs we get on INFO in that class. Could this log be reverted back to debug? Thank you! ",,ableegoldman,gray.john,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 14 12:12:45 UTC 2021,,,,,,,,,,"0|z0sno0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 22:05;ableegoldman;Hey [~gray.john], would you be interested in submitting a PR for this? I completely agree that logging this at INFO on every iteration is wildly inappropriate, I just didn't push it at the time since I figured someone would file a ticket if it was really bothering people. And here we are :) ;;;","14/Jul/21 04:10;ableegoldman;Thanks for the fix – btw, I added you as a contributor so you should be able to self-assign tickets from now on. ;;;","14/Jul/21 12:12;gray.john;Awesome! Thank you, Sophie.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
coordinator not available error should cause add into unmap list to do a new lookup,KAFKA-13033,13387539,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,showuon,showuon,04/Jul/21 10:14,22/Jul/21 10:59,13/Jul/23 09:17,16/Jul/21 08:01,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In KIP-699, we add some handler to handle different types of operation. In the `handleError`, we didn't make the `COORDINATOR_NOT_AVAILABLE` as unmapped, to do a re-lookup. In [DescribeTransactionsHandler|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandler.java#L172-L186], there's already explained by [~hachikuji] why `COORDINATOR_NOT_AVAILABLE` and `NOT_COORDINATOR` should be listed in unmapped, and `COORDINATOR_LOAD_IN_PROGRESS` should not.

 
{code:java}
case COORDINATOR_LOAD_IN_PROGRESS:
    // If the coordinator is in the middle of loading, then we just need to retry
    log.debug(""DescribeTransactions request for transactionalId `{}` failed because the "" +
            ""coordinator is still in the process of loading state. Will retry"",
        transactionalIdKey.idValue);
    break;

case NOT_COORDINATOR:
case COORDINATOR_NOT_AVAILABLE:
    // If the coordinator is unavailable or there was a coordinator change, then we unmap
    // the key so that we retry the `FindCoordinator` request
    unmapped.add(transactionalIdKey);
    log.debug(""DescribeTransactions request for transactionalId `{}` returned error {}. Will attempt "" +
            ""to find the coordinator again and retry"", transactionalIdKey.idValue, error);
    break;
{code}
We should be consistent with it. Fix it, add logs and comments, and also update the tests.

 

 

 ",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-07-04 10:14:03.0,,,,,,,,,,"0|z0sl5k:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindCoordinators batching can break consumers during rolling upgrade,KAFKA-13029,13387386,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rsivaram,rsivaram,rsivaram,02/Jul/21 18:30,07/Jul/21 08:35,13/Jul/23 09:17,07/Jul/21 08:35,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,consumer,,,,,0,,,,,"The changes made under KIP-699 assume that it is always safe to use unbatched mode since we move from batched to unbatched and cache the value forever in clients if a broker doesn't support batching. During rolling upgrade, if a request is sent to an older broker, we move from batched to unbatched mode. The consumer (admin client as well I think) disables batching and future requests to upgraded brokers will fail because we attempt to use unbatched requests with a newer version of the request.",,dajac,mimaison,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 03 10:44:40 UTC 2021,,,,,,,,,,"0|z0sk80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/21 18:58;mimaison;Right, I missed that!

For the Consumer/Producer, I guess we could reset the batch flag in AbstractCoordinator each time the coordinator changes. 

For AdminClient, we create a new CoordinatorStrategy for each call but I think this could still happen if the coordinator changes during a call while we upgrade brokers. It's a bit more tricky because the strategy doesn't know explicitly if the coordinator just changed.;;;","03/Jul/21 05:41;dajac;For the consumer, I think a simpler version would be to have all the logic in the builder. As the request is always for one group, we could simply build the correct request data based on the version passed to ‘Builder#build’. ;;;","03/Jul/21 10:44;rsivaram;[~mimaison] [~dajac]  Consumer and transaction coordinator always have single group, so the batch flag in those with failover are unnecessary. As David mentioned, we just need to set the appropriate data based on versions. The plan is to do the same for batching OffsetFetchRequest as well. For Admin Client, the PR currently switches to unbatched mode if there is an error. But the unbatched mode now works with any version (by setting appropriate data). So while performance could be improved in case an admin client gets into this state using the version of individual brokers, it didn't seem worthwhile making that change now for admin clients.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_multi_version failures,KAFKA-13018,13386858,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,jolshan,jolshan,30/Jun/21 16:24,30/Jun/21 21:19,13/Jul/23 09:17,30/Jun/21 21:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"This system test fails with the error


{{Exception in thread ""main"" joptsimple.UnrecognizedOptionException: zookeeper is not a recognized option}}

{{joptsimple.OptionException.unrecognizedOption(OptionException.java:108)}}

{{joptsimple.OptionParser.handleLongOptionToken(OptionParser.java:510)}}

{{joptsimple.OptionParserState$2.handleArgument(OptionParserState.java:56)}}

{{ joptsimple.OptionParser.parse(OptionParser.java:396)}}

{{kafka.admin.TopicCommand$TopicCommandOptions.<init>(TopicCommand.scala:516)}}

{{kafka.admin.TopicCommand$.main(TopicCommand.scala:47)}}

{{ kafka.admin.TopicCommand.main(TopicCommand.scala)}}

 

Seems like when we try to run with both 0.8.2.X version and dev version, we have this issue, since {{test_0_8_2 }}which uses only 0.8.2.X passes. Likely this has to do with {{create_topic}} in kafka.py and choose whether or not to use ZK. Maybe it will work if we specify node 0 instead of 1 as the older version node.

{{}}",,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 21:19:48 UTC 2021,,,,,,,,,,"0|z0sgzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/21 21:19;jolshan;Fixed by [#10918|https://github.com/apache/kafka/pull/10918];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive logging on sink task deserialization errors,KAFKA-13017,13386831,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,30/Jun/21 15:20,16/Jul/21 14:57,13/Jul/23 09:17,16/Jul/21 14:40,2.7.0,2.8.0,3.0.0,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,,,,KafkaConnect,,,,,0,,,,,"Even with {{errors.log.enable}} set to {{false}}, deserialization failures are still logged at {{ERROR}} level by the {{org.apache.kafka.connect.runtime.WorkerSinkTask}} namespace. This becomes problematic in pipelines with {{errors.tolerance}} set to {{all}}, and can generate excessive logging of stack traces when deserialization errors are encountered for most if not all of the records being consumed by a sink task.

The logging added to the {{WorkerSinkTask}} class in KAFKA-9018 should be removed and, if necessary, any valuable information from it not already present in the log messages generated by Connect with {{errors.log.enable}} and {{errors.log.include.messages}} set to {{true}} should be added in that place instead.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9018,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-30 15:20:24.0,,,,,,,,,,"0|z0sgtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectCommittedOffsetInformation(),KAFKA-13010,13386493,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson5,cadonna,cadonna,29/Jun/21 10:20,24/Aug/21 23:40,13/Jul/23 09:17,20/Jul/21 21:56,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,flaky-test,,,,"Integration test {{test org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectCommittedOffsetInformation()}} sometimes fails with

{code:java}
java.lang.AssertionError: only one task
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.getTaskMetadata(TaskMetadataIntegrationTest.java:162)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectCommittedOffsetInformation(TaskMetadataIntegrationTest.java:117)
{code}",,ableegoldman,cadonna,josep.prat-inactive,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13215,,,,,,,,,,,,,KAFKA-13215,,,,,,"15/Jul/21 01:54;ableegoldman;TaskMetadataIntegrationTest#shouldReportCorrectEndOffsetInformation.rtf;https://issues.apache.org/jira/secure/attachment/13030645/TaskMetadataIntegrationTest%23shouldReportCorrectEndOffsetInformation.rtf",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 19 23:01:00 UTC 2021,,,,,,,,,,"0|z0seqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 18:00;cadonna;Some logs that might be interesting:

{code}
[2021-06-29 12:19:40,200] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer] Finished unstable assignment of tasks, a followup rebalance will be scheduled. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:818)
[2021-06-29 12:19:40,200] WARN [Consumer clientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer, groupId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation] The following subscribed topics are not assigned to any members: [inputTaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation]  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:611)
[2021-06-29 12:19:40,200] INFO [GroupCoordinator 0]: Assignment received from leader TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer-0a548162-9e3f-4003-98c5-54ece6f5e1b8 for group TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation for generation 2. The group has 2 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2021-06-29 12:19:40,201] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1-consumer] Requested to schedule immediate rebalance for new tasks to be safely revoked from current owner. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1300)
[2021-06-29 12:19:40,201] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] State transition from RUNNING to PARTITIONS_REVOKED (org.apache.kafka.streams.processor.internals.StreamThread:229)
[2021-06-29 12:19:40,201] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] Handle new assignment with:
	New active tasks: []
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:263)
[2021-06-29 12:19:40,201] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:229)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] task [0_0] Suspended RUNNING (org.apache.kafka.streams.processor.internals.StreamTask:1187)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] task [0_0] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask:300)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] partition revocation took 1 ms. (org.apache.kafka.streams.processor.internals.StreamThread:97)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor:1306)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] Handle new assignment with:
	New active tasks: []
	New standby tasks: []
	Existing active tasks: [0_0]
	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager:263)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:268)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask:524)
[2021-06-29 12:19:40,202] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] State transition from PARTITIONS_REVOKED to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread:229)
[2021-06-29 12:19:40,203] INFO [GroupCoordinator 0]: Preparing to rebalance group TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer-0a548162-9e3f-4003-98c5-54ece6f5e1b8 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2021-06-29 12:19:40,274] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] Restoration took 73 ms for all tasks [] (org.apache.kafka.streams.processor.internals.StreamThread:851)
[2021-06-29 12:19:40,274] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:229)
[2021-06-29 12:19:40,274] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] Restoration took 72 ms for all tasks [] (org.apache.kafka.streams.processor.internals.StreamThread:851)
[2021-06-29 12:19:40,274] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread:229)
[2021-06-29 12:19:40,274] INFO stream-client [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams:315)
[2021-06-29 12:19:40,274] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] Triggering the followup rebalance scheduled for 0 ms. (org.apache.kafka.streams.processor.internals.StreamThread:585)
[2021-06-29 12:19:40,274] INFO stream-client [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:315)
[2021-06-29 12:19:40,275] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1063)
[2021-06-29 12:19:40,275] INFO stream-thread [TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorr
{code}

Especially this log message seems suspicious:

{code}
[2021-06-29 12:19:40,200] WARN [Consumer clientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-6889e6c9-c4fa-427c-83bf-469b33a34bb5-StreamThread-2-consumer, groupId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation] The following subscribed topics are not assigned to any members: [inputTaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation]  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:611)
{code};;;","13/Jul/21 22:34;ableegoldman;Failed again [https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-10985/6/testReport/junit/org.apache.kafka.streams.integration/TaskMetadataIntegrationTest/Build___JDK_11_and_Scala_2_13___shouldReportCorrectCommittedOffsetInformation/]

FWIW I didn't see the above log message about that subscribed topic not being assigned to any members. The logs were truncated so it's possible that it actually was there, but I don't think that's the case since AFAICT the truncated logs are mostly from kafka/zookeeper. The relevant logs from the rebalance seem to be present;;;","14/Jul/21 08:43;cadonna;For the future assignee of this ticket: I was able to reproduce this failure multiple times by running the test in IntelliJ in the until failure mode. It failed quite quickly after approx. 25 runs.;;;","14/Jul/21 19:58;ableegoldman;[~wcarlson@confluent.io] I'm guessing you wrote this test so you have the most context, can you reproduce this locally and take a minute or two to look through the logs and see if anything jumps out at you? ;;;","15/Jul/21 01:55;ableegoldman;Not the exact same test, but I did manage to reproduce this same ""only one task"" failure in TaskMetadataIntegrationTest.shouldReportCorrectEndOffsetInformation:
{code:java}
java.lang.AssertionError: only one task
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.getTaskMetadata(TaskMetadataIntegrationTest.java:163)
	at org.apache.kafka.streams.integration.TaskMetadataIntegrationTest.shouldReportCorrectEndOffsetInformation(TaskMetadataIntegrationTest.java:144)
{code}
I saved the logs since they should actually be the full, un-truncated logs – hope this helps: [^TaskMetadataIntegrationTest#shouldReportCorrectEndOffsetInformation.rtf];;;","19/Jul/21 16:12;wcarlson5;This failure should will effect both tests. When it is getting the task list it asserts that there is only one task in the very simple topology. I don't know if this is really a problem with the test or even the feature that the test is targeted at. It looks like the tasks created are not consistent. The task behavior might be intentional but I don't think so. I will see if I can reproduce and look at the logs.;;;","19/Jul/21 16:43;ableegoldman;It's worth noting that this test seemed to be pretty stable for roughly a full release cycle, then started failing pretty frequently right after [KIP-744|https://cwiki.apache.org/confluence/display/KAFKA/KIP-744%3A+Migrate+TaskMetadata+and+ThreadMetadata+to+an+interface+with+internal+implementation] / KAFKA-12849 was merged (June 25th). [~wcarlson5] I wonder if the changes to the metadata hierarchy could have introduced a potentially serious bug?;;;","19/Jul/21 17:20;wcarlson5;It is possible [~ableegoldman] . I ran till failure (75 runs). And it seems that the metadata was reporting no tasks, so I would agree.
{code:java}
[ThreadMetadata{threadName=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-2, threadState=RUNNING, activeTasks=[], standbyTasks=[], consumerClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-2-consumer, restoreConsumerClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-2-restore-consumer, producerClientIds=[TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-2-producer], adminClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-admin}, ThreadMetadata{threadName=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-1, threadState=RUNNING, activeTasks=[], standbyTasks=[], consumerClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-1-consumer, restoreConsumerClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-1-restore-consumer, producerClientIds=[TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-StreamThread-1-producer], adminClientId=TaskMetadataTest_TaskMetadataIntegrationTestshouldReportCorrectCommittedOffsetInformation-c1a902ee-54b1-4b8e-a195-98d4a6143c1e-admin}]
{code};;;","19/Jul/21 17:55;jlprat;If it happened after KIP-744 probably I introduced the regression .

I will also try to look if I can see where is the mistake. If I remember correctly, most of the code I wrote was just copied over from one place to another.;;;","19/Jul/21 18:37;ableegoldman;Yeah it's honestly pretty hard to imagine how that could have introduced a bug like this, but the timing definitely is suspicious. Seeing as Walker was able to reproduce it after a ""reasonable"" number of retries, it should be easy to confirm the suspicion by just running the test again with sufficient repeats on the commit just before KIP-744 was merged.;;;","19/Jul/21 18:54;jlprat;To add more to the mystery, I'm running the test until failure and I'm already over 250 repetitions and can't reproduce the bug (on a i7 CPU 4 cores, 8 threads).

Maybe what I'm about to say it's pretty obvious, but according to the logs you provide, it seems that when the test fails, TaskManager prints:
{code:java}
Handle new assignment with:
	New active tasks: []
	New standby tasks: []
	Existing active tasks: [0_0]
	Existing standby tasks: [] 
{code}
instead of:
{code:java}
Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: [] 
{code}
(or at least it's what's been printed on my machine when test run successfully). Notice the ""0_0"" task being in the ""existing active tasks"" when failing instead of being in ""new active tasks"".;;;","19/Jul/21 18:58;wcarlson5;That does seem to be the case. I think somehow those ""existing active tasks"" are getting excluded from the `activeTasks()` list in the `ThreadMetadata`;;;","19/Jul/21 19:26;jlprat;One thing that was modified under that PR that might cause some race conditions is the fact that StreamsMetadataImpl now saves all collections as immutable ones during creation instead of doing it inside the getters. Similar thing ThreadMetadataImpl that now saves producerClientIds as an immutable collection within the constructor.

However, TaskMetadataImpl behaves the same in regards of immutable collections. I doubt it is related, but it's worth a shot.;;;","19/Jul/21 19:30;wcarlson5;That sounds likely as the ThreadMetadata is retrieved using `metadataForLocalThreads()`;;;","19/Jul/21 19:37;jlprat;If this is the reason why we are seeing this bug, by replacing line 60 in ThreadMetadataImpl the following might cause the test to not fail:
{code:java}
this.producerClientIds = producerClientIds;
{code}
As I can't reproduce the test locally would you be able to try this [~wcarlson5] ?;;;","19/Jul/21 20:18;wcarlson5;it took about 126 runs but it still failed;;;","19/Jul/21 20:25;jlprat;The only other difference left now is in StreamsMetadataImpl where the immutable collections were created within the getters instead of the constructor. A diff between StreamsMetadataImpl and the Deprecated StreamsMetadata class will show the places where it changed

I guess it's another long shot;;;","19/Jul/21 20:28;wcarlson5;This doesn't use the StreamsMetadata so I don't think that would be related ;;;","19/Jul/21 20:39;jlprat;I only mentioned because it seems to be the only other change in that PR that is not just moving implementations around.

If you run the test before the KIP-744 changes were introduced, does it also fail after, let's say 200 iterations?;;;","19/Jul/21 21:54;wcarlson5;I ran it for the commit before KIP-744 and it did fail. I am going to do a binary search from when the test was introduced to see where it started failing.

 

EDIT: It looks like it always would fail eventually.

 

I am seeing if this is an error that will persist beyond one try;;;","19/Jul/21 23:01;wcarlson5;It looks like the issue only shows up between cooperative rebalances and just retrying will fix it. I will make a PR with a fix shortly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream will stop processing data for a long time while waiting for the partition lag,KAFKA-13008,13386435,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,showuon,showuon,29/Jun/21 06:44,23/Jul/21 23:47,13/Jul/23 09:17,23/Jul/21 23:47,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,"In KIP-695, we improved the task idling mechanism by checking partition lag. It's a good improvement for timestamp sync. But I found it will cause the stream stop processing the data for a long time while waiting for the partition metadata.

 

I've been investigating this case for a while, and figuring out the issue will happen in below situation (or similar situation):
 # start 2 streams (each with 1 thread) to consume from a topicA (with 3 partitions: A-0, A-1, A-2)
 # After 2 streams started, the partitions assignment are: (I skipped some other processing related partitions for simplicity)
 stream1-thread1: A-0, A-1 
 stream2-thread1: A-2
 # start processing some data, assume now, the position and high watermark is:
 A-0: offset: 2, highWM: 2
 A-1: offset: 2, highWM: 2
 A-2: offset: 2, highWM: 2
 # Now, stream3 joined, so trigger rebalance with this assignment:
 stream1-thread1: A-0 
 stream2-thread1: A-2
 stream3-thread1: A-1
 # Suddenly, stream3 left, so now, rebalance again, with the step 2 assignment:
 stream1-thread1: A-0, *A-1* 
 stream2-thread1: A-2
 (note: after initialization, the  position of A-1 will be: position: null, highWM: null)
 # Now, note that, the partition A-1 used to get assigned to stream1-thread1, and now, it's back. And also, assume the partition A-1 has slow input (ex: 1 record per 30 mins), and partition A-0 has fast input (ex: 10K records / sec). So, now, the stream1-thread1 won't process any data until we got input from partition A-1 (even if partition A-0 is buffered a lot, and we have `{{max.task.idle.ms}}` set to 0).

 

The reason why the stream1-thread1 won't process any data is because we can't get the lag of partition A-1. And why we can't get the lag? It's because
 # In KIP-695, we use consumer's cache to get the partition lag, to avoid remote call
 # The lag for a partition will be cleared if the assignment in this round doesn't have this partition. check [here|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java#L272]. So, in the above example, the metadata cache for partition A-1 will be cleared in step 4, and re-initialized (to null) in step 5
 # In KIP-227, we introduced a fetch session to have incremental fetch request/response. That is, if the session existed, the client(consumer) will get the update only when the fetched partition have update (ex: new data). So, in the above case, the partition A-1 has slow input (ex: 1 record per 30 mins), it won't have update until next 30 mins, or wait for the fetch session become inactive for (default) 2 mins to be evicted. Either case, the metadata won't be updated for a while.

 

In KIP-695, if we don't get the partition lag, we can't determine the partition data status to do timestamp sync, so we'll keep waiting and not processing any data. That's why this issue will happen.

 

*Proposed solution:*
 # If we don't get the current lag for a partition, or the current lag > 0, we start to wait for max.task.idle.ms, and reset the deadline when we get the partition lag, like what we did in previous KIP-353
 # Introduce a waiting time config when no partition lag, or partition lag keeps > 0 (need KIP)

[~vvcephei] [~guozhang] , any suggestions?

 

cc [~ableegoldman]  [~mjsax] , this is the root cause that in [https://github.com/apache/kafka/pull/10736,] we discussed and thought there's a data lose situation. FYI.",,ableegoldman,cmccabe,guozhang,mjsax,showuon,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9295,,,,,,"07/Jul/21 03:19;showuon;image-2021-07-07-11-19-55-630.png;https://issues.apache.org/jira/secure/attachment/13030208/image-2021-07-07-11-19-55-630.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 13 01:56:35 UTC 2021,,,,,,,,,,"0|z0sedk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/21 09:49;showuon;[~vvcephei] [~guozhang] , any suggestions for this issue?

IMO, this is not the expected behavior because what we expected to wait is the partition info not retrieved, yet. But in this case, the partition info is indeed retrieved (in fetch session's perspective), but the partition cache for that partition is cleared.

Thank you.;;;","06/Jul/21 20:51;ableegoldman;Nice find! I agree, this does not seem like the expected behavior, and given that it's been causing a test to fail regularly I think we can assert that this should not happen. 

One thing I don't understand, and maybe this is because I don't have much context on the incremental fetch internals, is: why would we not get the metadata again after the partition is re-assigned in step 5? Surely if a partition was removed from the assignment and then added back, this should constitute a new 'session', and thus it should get the metadata again on assignment? If not that sounds like a bug in the incremental fetch to me, but again, I'm not too familiar with it so there could be a valid reason it works this way. 

If so, then maybe we should consider allowing metadata to remain around after a partition is unassigned, in case it gets this same partition back within the session? Could there be other consequences of this lack of metadata, outside of Streams?;;;","07/Jul/21 03:28;showuon;Actually, I'm not very familiar with detailed incremental session, too. But from the , [KIP-227|https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability]: we can see the old session will be evict only when matching 1 of the following 3 condition:

!image-2021-07-07-11-19-55-630.png|width=762,height=161!

 

And because in the step 5, all above 3 conditions won't match, the new session won't evict the old session. Also, during that time, the old session already contain the ""up-to-date"" partition info of the partition A-1 (because partition A-1 was assigned to this session), so no partition A-1 update will be received.

 

This is my understanding. Please correct me if I'm wrong. Thank you.;;;","08/Jul/21 23:41;cmccabe;Thanks, [~showuon]... this is a good find.

[~ableegoldman] wrote:
bq. Surely if a partition was removed from the assignment and then added back, this should constitute a new 'session', and thus it should get the metadata again on assignment

Sessions may be changed without creating a new session. They wouldn't be much use otherwise, since many consumers often change their subscriptions or mute partitions, etc.

bq. If so, then maybe we should consider allowing metadata to remain around after a partition is unassigned, in case it gets this same partition back within the session? Could there be other consequences of this lack of metadata, outside of Streams?

The issue is that the metadata would be incorrect. If the fetch session doesn't contain a partition, we really can't say anything about what its lag is, other than potentially caching a stale value. But we don't know how long the partition could be muted (it could be hours, for example).

Ultimately there is a tradeoff here between having the most up-to-date lag information for each partition, and efficiency. I'm not totally sure what the best way to resolve this is (we'd have to look at the Streams use-case more carefully).;;;","09/Jul/21 22:42;vvcephei;Woah, this is an excellent find, [~showuon] !

You and Sophie are absolutely right. We chose to use the cached lag so that we wouldn't have to wait for a full round-trip every time we want to know whether we're approximately caught up.

Colin is right, too; we don't want to use a metadata cache that can be arbitrarily old. There are plenty of applications that go days without a rebalance, which would certainly violate the semantics we're going for here.

Re-reading KIP-227, it seems like there should be a way for the client to add re-acquired partitions like this to the incremental fetch request so that it can reinitialize its metadata cache. In other words, it seems like getting a partition assigned that you haven't owned for a while is effectively the same case as getting a partition that you've never owned, and there does seem to be a mechanism for the latter.

Does that sound right to you, [~cmccabe] ?;;;","12/Jul/21 23:22;ableegoldman;{quote}Re-reading KIP-227, it seems like there should be a way for the client to add re-acquired partitions like this to the incremental fetch request so that it can reinitialize its metadata cache. In other words, it seems like getting a partition assigned that you haven't owned for a while is effectively the same case as getting a partition that you've never owned, and there does seem to be a mechanism for the latter.
{quote}
Thanks John, that is exactly what I was trying to suggest above, but I may have mungled it with my lack of understanding of the incremental fetch design. Given how long this bug went unnoticed and the in-depth investigation it took to uncover the bug (again, nicely done [~showuon]), it seems like any user of the plain consumer client in addition to Streams could be easily tripped up by this. And just personally, I had to read the analysis twice to really understand what was going on, since the behavior was/is so unintuitive to me.;;;","13/Jul/21 00:34;guozhang;Thanks for the great find [~showuon]!

I took a look at the server-side code, and I think we can consider two things:

1) slightly augment the session handling logic so that within a session, if a partition was newly requested (here, we would not try to distinguish whether it was requested for the very first time, or it was re-added after a while), even if the requested position has reached the log end i.e. there's no data to return, we still return in the response to encode the log end information. WDYT [~cmccabe]?

2) since 1) would be a broker change and even if we do that, it may not help all cases for streams, we would still need some remedies. One (somewhat hacky..) idea is to actually pay the round-trip in such cases (only when the config is set to >= 0), but that since fetch request would not do for old versioned brokers, we would use the offset request (either consumer or admin has the API to do that) to get the log end offset. In fact, in Streams when we get the assigned partitions we always need to get the log end offset for changes at first to check if any restoration is needed, we can, just add source topic partitions as well in that phase and expose as the initial values for the main consumer as well. Note this is only done once after every rebalance, and no more.;;;","13/Jul/21 01:56;ableegoldman;Thanks Guozhang, +1 on that approach (though I'll let Colin confirm whether #1 does make sense or not). We'll definitely need a Streams/client side fix if the 'real' fix is going to be on the broker side. My only question is whether this is something that might trip up other plain consumer client users in addition to Streams, and if so, whether there's anything we could do in the consumer client itself. But AFAIK it's only Streams that really relies on this metadata in this critical way, so I'm happy with the Streams-side fix as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaAdminClient getListOffsetsCalls builds cluster snapshot for every topic partition,KAFKA-13007,13386429,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,29/Jun/21 06:17,01/Jul/21 21:17,13/Jul/23 09:17,01/Jul/21 21:17,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,clients,,,,,0,,,,,"From [KafkaAdminClient#getListOffsetsCalls|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L4215]

```

for (Map.Entry<TopicPartition, OffsetSpec> entry: topicPartitionOffsets.entrySet()) {

...

Node node = mr.cluster().leaderFor(tp);

```

here we build the cluster snapshot for each topic partition. instead, we should reuse a snapshot. this will reduce the time complexity from O( n^2 ) to O( n ).

for manual testing (used AK 2.8), i've passed in a map of 6K topic partitions to listOffsets

without snapshot reuse:
 duration of building futures from metadata response: *15582* milliseconds
 total duration of listOffsets: *15743* milliseconds

with reuse:
 duration of building futures from metadata response: *24* milliseconds
 total duration of listOffsets: *235* milliseconds

Affects all versions since Admin & KafkaAdminClient introduced listOffsets (original PR: [https://github.com/apache/kafka/pull/7296])",,jeffkbkim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-29 06:17:03.0,,,,,,,,,,"0|z0sec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaBroker advertises socket port instead of the configured advertised port,KAFKA-13003,13386281,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ueisele,ueisele,ueisele,28/Jun/21 14:28,12/Jul/21 20:48,13/Jul/23 09:17,12/Jul/21 20:48,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,kip-500,,,,"In Kraft mode Apache Kafka 2.8.0 does advertise the socket port instead of the configured advertised port.

A broker given with the following configuration
{code:java}
listeners=PUBLIC://0.0.0.0:19092,REPLICATION://0.0.0.0:9091
advertised.listeners=PUBLIC://envoy-kafka-broker:9091,REPLICATION://kafka-broker1:9091
{code}
advertises on the _PUBLIC_ listener _envoy-kafka-broker:19092_, however I would expect that _envoy-kafka-broker:9091_ is advertised. In ZooKeeper mode it works as expected.

In a deployment with a L4 proxy in front of the Kafka cluster, it is important, that the advertised port can be different from the actual socket port.

I tested it with a Docker-Compose setup which runs 3 Kafka Broker in Kraft mode and an Envoy proxy in front of them. With Apache Kafka 2.8.0 it does not work, because Kafka does not advertise the configured advertised port. For more details see: https://github.com/ueisele/kafka/tree/fix/kraft-advertisedlisteners-build/proxy-examples/proxyl4-kafkakraft-bug-2.8

_Client -- 909[1-3] --> Envoy Proxy -- 19092 --> Kafka Broker [1-3]_

|| Envoy Host || Envoy Port || Kafka Broker || Kafka Port || Advertised Listener ||
| envoy-kafka-broker | 9091 | kafka-broker1 | 19092 | envoy-kafka-broker:9091 |
| envoy-kafka-broker | 9092 | kafka-broker2 | 19092 | envoy-kafka-broker:9092 |
| envoy-kafka-broker | 9093 | kafka-broker3 | 19092 | envoy-kafka-broker:9093 |

{code:bash}
> docker-compose exec kafkacat kafkacat -b envoy-kafka-broker:9091 -L
Metadata for all topics (from broker -1: envoy-kafka-broker:9091/bootstrap):
 3 brokers:
  broker 101 at envoy-kafka-broker:19092
  broker 102 at envoy-kafka-broker:19092 (controller)
  broker 103 at envoy-kafka-broker:19092
 0 topics:
{code}",,ueisele,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 12 20:48:09 UTC 2021,,,,,,,,,,"0|z0sdfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/21 14:31;ueisele;I create a pull request: https://github.com/apache/kafka/pull/10935

The Docker-Compose setup with the fix (proposed in the pull request) works and advertises the configured advertised port. For more details see: https://github.com/ueisele/kafka/tree/fix/kraft-advertisedlisteners-build/proxy-examples/proxyl4-kafkakraft-fix-2.8;;;","12/Jul/21 20:48;ueisele;Pull Request #10935 has been merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
listOffsets must downgrade immediately for non MAX_TIMESTAMP specs,KAFKA-13002,13386279,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,tom@confluent.io,vvcephei,vvcephei,28/Jun/21 14:20,01/Jul/21 12:42,13/Jul/23 09:17,01/Jul/21 06:36,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,"Note: this is not a report against a released version of AK. It seems to be a problem on the trunk development branch only.

After deploying our soak test against `trunk/HEAD` on Friday, I noticed that Streams is no longer processing:

!soaks.png!

I found this stacktrace in the logs during startup:
{code:java}
5075 [2021-06-25T16:50:44-05:00] (streams-soak-trunk-ccloud-alos_soak_i-0691913411e8c77c3_streamslog) [2021-06-25 21:50:44,499] WARN [i-0691913411e8c77c3-StreamThread-1] The listOffsets request failed. (org.apache.kafka.streams.processor.internals.ClientUtils)
 5076 [2021-06-25T16:50:44-05:00] (streams-soak-trunk-ccloud-alos_soak_i-0691913411e8c77c3_streamslog) java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_OFFSETS with version in range [7,7].       The supported range is [0,6].
 5077         at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
 5078         at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
 5079         at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
 5080         at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
 5081         at org.apache.kafka.streams.processor.internals.ClientUtils.getEndOffsets(ClientUtils.java:147)
 5082         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.populateClientStatesMap(StreamsPartitionAssignor.java:643)
 5083         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assignTasksToClients(StreamsPartitionAssignor.java:579)
 5084         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:387)
 5085         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:589)
 5086         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:689)
 5087         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1000(AbstractCoordinator.java:111)
 5088         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:593)
 5089         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:556)
 5090         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1178)
 5091         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1153)
 5092         at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
 5093         at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
 5094         at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
 5095         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)
 5096         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)
 5097         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)
 5098         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
 5099         at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1297)
 5100         at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1238)
 5101         at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
 5102         at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:932)
 5103         at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:885)
 5104         at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:720)
 5105         at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:583)
 5106         at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:555) {code}
Just eyeballing the recent commits, I'm guessing it was due to [https://github.com/apache/kafka/commit/bd72ef1bf1e40feb3bc17349a385b479fa5fa530] . It looks like that code sets the initial ""minimum version"" to 7, but then should back off into compatibility mode. Therefore, maybe that stacktrace is expected (though it's not great UX regardless).

However, it does not seem like Streams is actually able to back off. The next thing I see is:
{code:java}
[2021-06-25T16:50:44-05:00] (streams-soak-trunk-ccloud-alos_soak_i-0691913411e8c77c3_streamslog) [2021-06-25 21:50:44,507] WARN [i-0691913411e8c77c3-StreamThread-1] Task 3_2 had endOffsetSum=-3 smaller than offsetSum=0 on member 24e46b47-0a01-4b57-9d15-771482869097. This probably means the task is corrupted, which in turn indicates that it will need to restore from scratch if it gets assigned. The assignor will de-prioritize returning this task to this member in the hopes that some other member may be able to re-use its state. (org.apache.kafka.streams.processor.internals.assignment.ClientState) {code}
Which is itself a problem. It looks like there's a sentinel ""-3"" value returned as the end offset, but since that value is lower than any real endOffset Streams will have book-kept, Streams will assume that all its local state is corrupt. The result is that Streams will delete all its local state and rebuild from the changelog. This isn't an ideal behavior on restart.

Finally, I never actually see Streams able to proceed with processing. The only thing it logs after this point (as far as I can tell) is:
{code:java}
[2021-06-25T16:50:54-05:00] (streams-soak-trunk-ccloud-alos_soak_i-0691913411e8c77c3_streamslog) [2021-06-25 21:50:52,463] INFO [i-0691913411e8c77c3-StreamThread-1] stream-thread [i-0691913411e8c77c3-StreamThread-1] End offset for changelog stream-soak-test-trunk-ccloud-alos--KSTREAM-AGGREGATE-STATE-STORE-0000000013-changelog-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader) {code}
So, it seems the version backoff simply isn't working.

Obviously, we'll need to fix these problems before we can release 3.0",,ableegoldman,cadonna,dajac,ijuma,jolshan,mjsax,tom@confluent.io,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12541,,,,,,"28/Jun/21 14:12;vvcephei;soaks.png;https://issues.apache.org/jira/secure/attachment/13027362/soaks.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 01 12:42:36 UTC 2021,,,,,,,,,,"0|z0sdew:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"28/Jun/21 14:26;ijuma;[~vvcephei] If you are using the admin client, it seems that this is an admin client regression. I mentioned this ticket in the original PR.;;;","28/Jun/21 15:10;tom@confluent.io;I think I've determined the problem, I'll get a PR together ASAP;;;","01/Jul/21 06:37;dajac;[~vvcephei] We have fixed the regression. Could you retry in your environment? Sorry for the regression.;;;","01/Jul/21 10:25;cadonna;[~dajac] I deployed the soak cluster that showed the issue from trunk. Until now it looks good. We also run into the same issue with our benchmarks. I also started them to see if everything is OK. We also saw the issue in the Streams system tests {{kafkatest.tests.streams.streams_broker_compatibility_test}}.  ;;;","01/Jul/21 10:42;dajac;[~cadonna] Sounds good, thanks.;;;","01/Jul/21 11:03;cadonna;The soak cluster still looks good and the benchmarks completed successfully their first run. I guess the fix is fine.;;;","01/Jul/21 12:42;dajac;Awesome, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetOutOfRange not handled correctly for diverging epochs when fetch offset less than leader start offset,KAFKA-12996,13385912,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,25/Jun/21 17:48,29/Jun/21 19:45,13/Jul/23 09:17,29/Jun/21 19:45,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,core,,,,,0,,,,,"{color:#24292e}If fetchOffset < startOffset, we currently throw OffsetOutOfRangeException when attempting to read from the log in the regular case. But for diverging epochs, we return Errors.NONE with the new leader start offset, hwm etc.. ReplicaFetcherThread throws OffsetOutOfRangeException when processing responses with Errors.NONE if the leader's offsets in the response are out of range and this moves the partition to failed state. We should add a check for this case when processing fetch requests and ensure OffsetOutOfRangeException is thrown regardless of epochs.{color}",,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-25 17:48:59.0,,,,,,,,,,"0|z0sb5c:",9223372036854775807,,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Formatting of Streams 'Memory Management' docs is messed up ,KAFKA-12993,13385734,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,ableegoldman,ableegoldman,24/Jun/21 23:43,13/Jul/21 01:03,13/Jul/23 09:17,13/Jul/21 01:03,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,docs,streams,,,,0,,,,,"The formatting of this page is all messed up, starting in the RocksDB section. It looks like there's a missing closing tag after the example BoundedMemoryRocksDBConfig class",,ableegoldman,cadonna,githubbot,josep.prat-inactive,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 13 01:02:51 UTC 2021,,,,,,,,,,"0|z0sa1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 02:49;showuon;on it~;;;","25/Jun/21 07:24;githubbot;showuon opened a new pull request #361:
URL: https://github.com/apache/kafka-site/pull/361


   1. add missing closing tag
   2. remove a redundant `span` tag


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Jun/21 07:25;githubbot;showuon commented on a change in pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#discussion_r658532684



##########
File path: 27/streams/developer-guide/memory-mgmt.html
##########
@@ -179,7 +179,7 @@ <h2><a class=""toc-backref"" href=""#id3"">RocksDB</a><a class=""headerlink"" href=""#r
        <span class=""nd"">@Override</span>
        <span class=""kd"">public</span> <span class=""kt"">void</span> <span class=""nf"">setConfig</span><span class=""o"">(</span><span class=""kd"">final</span> <span class=""n"">String</span> <span class=""n"">storeName</span><span class=""o"">,</span> <span class=""kd"">final</span> <span class=""n"">Options</span> <span class=""n"">options</span><span class=""o"">,</span> <span class=""kd"">final</span> <span class=""n"">Map</span><span class=""o"">&lt;</span><span class=""n"">String</span><span class=""o"">,</span> <span class=""n"">Object</span><span class=""o"">&gt;</span> <span class=""n"">configs</span><span class=""o"">)</span> <span class=""o"">{</span>
 
-         <span class=""n"">BlockBasedTableConfig</span> <span class=""n"">tableConfig</span> <span class=""o"">=</span> <span class=""k"">(BlockBasedTableConfig)</span> <span class=""n"">options</span><span><span class=""o"">.</span><span class=""na"">tableFormatConfig</span><span class=""o"">();</span>
+         <span class=""n"">BlockBasedTableConfig</span> <span class=""n"">tableConfig</span> <span class=""o"">=</span> <span class=""k"">(BlockBasedTableConfig)</span> <span class=""n"">options</span><span class=""o"">.</span><span class=""na"">tableFormatConfig</span><span class=""o"">();</span>

Review comment:
       fix 2: remove redundant `span` tag

##########
File path: 27/streams/developer-guide/memory-mgmt.html
##########
@@ -201,6 +201,7 @@ <h2><a class=""toc-backref"" href=""#id3"">RocksDB</a><a class=""headerlink"" href=""#r
          <span class=""c1"">// Cache and WriteBufferManager should not be closed here, as the same objects are shared by every store instance.</span>
        <span class=""o"">}</span>
     <span class=""o"">}</span>
+        </pre>

Review comment:
       fix 1: add missing `pre` tag

##########
File path: 27/streams/developer-guide/memory-mgmt.html
##########
@@ -217,6 +218,7 @@ <h2><a class=""toc-backref"" href=""#id3"">RocksDB</a><a class=""headerlink"" href=""#r
             In addition to the recommended configs above, you may want to consider using partitioned index filters as described by the <a class=""reference external"" href=""https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters"">RocksDB docs</a>.
 
           </dl>
+        </sup>

Review comment:
       add missing `sup` tag




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Jun/21 07:27;githubbot;showuon commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868286033


   @ableegoldman , please take a look. Thanks.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Jun/21 12:59;githubbot;cadonna commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868481708


   @showuon I think this has been already done in PR #10651.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Jun/21 13:00;cadonna;I think this has already been done by https://github.com/apache/kafka/pull/10651. With the next release it should be also merged to kafka-site. 
I am not sure about the process here. Should every PR against docs in kafka also be made against kafka-site? \cc [~ableegoldman] [~mjsax] [~vvcephei];;;","25/Jun/21 13:12;showuon;[~cadonna], yes, I know this is been done by [https://github.com/apache/kafka/pull/10651] as mentioned in my PR. But that needs to wait for next release to fix it. We can make the same change in PR:10651 into Kafka-site repo, or we can just pick my quick fix to add the closing `pre` tag. Both are good to me.

 

I don't think we should make every change to docs into Kafka and Kafka-site repo, especially, there are some new docs for new released feature. But in this case, it's like a bug to the doc in current release, it should be fixed in both repo.

 

Thank you.;;;","25/Jun/21 23:23;ableegoldman;Ah, I didn't realize this was already addressed. Thanks for the heads up Bruno. I agree with Luke's take on this: not every docs change needs to go into kafka-site right away, but for something like this which is a pretty egregious formatting issue that makes it very difficult to read, we should fix it on the live site ASAP.

[~showuon] do you want to port this PR over to kafka-site?;;;","25/Jun/21 23:30;githubbot;ableegoldman commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868882876


   @cadonna this PR is just porting  #10651 over to kafka-site, right? I agree that the basic formatting fix for the memory management docs at least should be ported to the live site ASAP.
   
   @showuon is there also a missing `</code>` tag? See [line 165](https://github.com/apache/kafka/pull/10651/files#diff-90f13939de7269a2ab9e8d0e8596f29afd1f4293710319fdcde35d84bca5c195R165)


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Jun/21 23:31;ableegoldman;Oh, you already have a PR for kafka-site. I should have known :P;;;","26/Jun/21 02:00;githubbot;showuon commented on a change in pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#discussion_r659103580



##########
File path: 27/streams/developer-guide/memory-mgmt.html
##########
@@ -168,7 +168,7 @@
       <h2><a class=""toc-backref"" href=""#id3"">RocksDB</a><a class=""headerlink"" href=""#rocksdb"" title=""Permalink to this headline""></a></h2>
       <p> Each instance of RocksDB allocates off-heap memory for a block cache, index and filter blocks, and memtable (write buffer). Critical configs (for RocksDB version 4.1.0) include
         <code class=""docutils literal""><span class=""pre"">block_cache_size</span></code>, <code class=""docutils literal""><span class=""pre"">write_buffer_size</span></code> and <code class=""docutils literal""><span class=""pre"">max_write_buffer_number</span></code>.  These can be specified through the
-        <code class=""docutils literal""><span class=""pre"">rocksdb.config.setter</span></code> configuration.</li>
+        <code class=""docutils literal""><span class=""pre"">rocksdb.config.setter</span></code> configuration.</li></p>

Review comment:
       add missing `</p>` tag




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Jun/21 02:00;githubbot;showuon commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868926930


   @ableegoldman , I've checked, the line 165 for `</code>` change is to fix the new added doc in this PR https://github.com/apache/kafka/pull/10046, not existed in V2.8. I also added a missing `</p>` tag. Thank you.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Jun/21 03:42;githubbot;ableegoldman commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868940746


   Gotcha, thanks for checking. Can you try setting up a local Apache server to test these changes (see instructions [here](https://cwiki.apache.org/confluence/display/KAFKA/Setup+Kafka+Website+on+Local+Apache+Server))?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Jun/21 08:28;githubbot;showuon commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868968278


   Will do and let you know. Thanks.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Jun/21 09:50;jlprat;Not about the same page, but I have a couple of PRs still open fixing code listings on different pages, in case you find them relevant:
 * [https://github.com/apache/kafka/pull/10769]
 * [https://github.com/apache/kafka/pull/10768]
 * [https://github.com/apache/kafka/pull/10767]

 ;;;","26/Jun/21 10:17;githubbot;showuon commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-868980516


   @ableegoldman , I've updated and confirmed the change can fix the formatting issue.
   ![image](https://user-images.githubusercontent.com/43372967/123509883-b38bcc80-d6aa-11eb-8f98-a6e5b2d98e45.png)
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Jun/21 16:34;githubbot;izzyacademy commented on pull request #361:
URL: https://github.com/apache/kafka-site/pull/361#issuecomment-869027105


   > Gotcha, thanks for checking. Can you try setting up a local Apache server to test these changes (see instructions [here](https://cwiki.apache.org/confluence/display/KAFKA/Setup+Kafka+Website+on+Local+Apache+Server))?
   
   @ableegoldman this is great to know. I was not aware of this before now. @showuon I will review it on Monday and share my feedback as well. I am looking to contribute documentation changes myself as well


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Jul/21 01:02;githubbot;ableegoldman merged pull request #361:
URL: https://github.com/apache/kafka-site/pull/361


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: dev-unsubscribe@kafka.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unsafe access to `AbstractCoordinator.state`,KAFKA-12991,13385669,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,24/Jun/21 15:38,24/Jun/21 18:53,13/Jul/23 09:17,24/Jun/21 15:50,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,0,,,,,,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-24 15:38:12.0,,,,,,,,,,"0|z0s9nc:",9223372036854775807,,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cooperative sticky assignor can get stuck with invalid SubscriptionState input metadata,KAFKA-12984,13385288,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,23/Jun/21 01:57,05/Feb/22 00:44,13/Jul/23 09:17,14/Jul/21 01:40,,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.3,2.7.2,2.8.1,3.0.0,,,,consumer,,,,,1,,,,,"Some users have reported seeing their consumer group become stuck in the CompletingRebalance phase when using the cooperative-sticky assignor. Based on the request metadata we were able to deduce that multiple consumers were reporting the same partition(s) in their ""ownedPartitions"" field of the consumer protocol. Since this is an invalid state, the input causes the cooperative-sticky assignor to detect that something is wrong and throw an IllegalStateException. If the consumer application is set up to simply retry, this will cause the group to appear to hang in the rebalance state.

The ""ownedPartitions"" field is encoded based on the ConsumerCoordinator's SubscriptionState, which was assumed to always be up to date. However there may be cases where the consumer has dropped out of the group but fails to clear the SubscriptionState, allowing it to report some partitions as owned that have since been reassigned to another member.

We should (a) fix the sticky assignment algorithm to resolve cases of improper input conditions by invalidating the ""ownedPartitions"" in cases of double ownership, and (b) shore up the ConsumerCoordinator logic to better handle rejoining the group and keeping its internal state consistent. See KAFKA-12983 for more details on (b)",,ableegoldman,Andy_Dufresne,calohmn,mikebin,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12896,,,,,,,,KAFKA-12983,,KAFKA-12896,,,KAFKA-13081,KAFKA-13406,,KAFKA-13420,,,,,,"25/Oct/21 08:53;Andy_Dufresne;image-2021-10-25-11-53-40-221.png;https://issues.apache.org/jira/secure/attachment/13035302/image-2021-10-25-11-53-40-221.png","26/Oct/21 11:13;Andy_Dufresne;log-events-viewer-result-kafka.numbers;https://issues.apache.org/jira/secure/attachment/13035361/log-events-viewer-result-kafka.numbers","26/Oct/21 11:14;Andy_Dufresne;logs-insights-results-kafka.csv;https://issues.apache.org/jira/secure/attachment/13035362/logs-insights-results-kafka.csv","26/Oct/21 11:10;Andy_Dufresne;logs-insights-results-kafka.numbers;https://issues.apache.org/jira/secure/attachment/13035360/logs-insights-results-kafka.numbers",,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 27 09:26:39 UTC 2021,,,,,,,,,,"0|z0s7aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 08:42;showuon;Good root cause analysis! And I agree the solution (a) that the sticky assignment algorithm should resolve cases of improper input conditions by invalidating the ""ownedPartitions"" in cases of double ownership.;;;","23/Jun/21 17:18;mjsax;[~ableegoldman] – is KS affected by this issue? Even if we use our own assignor, it seems that the issue with regard to ""subscription state"" could also affect KS?;;;","24/Jun/21 01:08;ableegoldman;[~mjsax] Technically yes, the issue with the SubscriptionState potentially providing invalid ""ownedPartitions"" input can affect Kafka Streams as well. However the impact for Streams is be considerably less severe, as the assignment algorithm it doesn't make any assumptions about the previous assignment being valid. The worst that should happen to a Streams application is that the assignment could be slightly sub-optimal, with a partition/active task being assigned to a member that had dropped out of the group since being assigned that partition, instead of its true current owner. ;;;","22/Oct/21 14:56;Andy_Dufresne;Hi, we faced the same issue described above with  app running Kafka client 2.8.1. and broker 2.7.0.

I manually pulled CooperativeStickyAssignor and AbstractStickyAssignor from v 3.0 and placed it in the consumer app as custom 

partition.assignment.strategy and the issue has gone.

Is it possible that the fix wasn't completely backported from 3.0 to 2.8.1 kafka client?

 ;;;","22/Oct/21 23:31;ableegoldman;[~Andy_Dufresne] Hm, well there's a separate commit for this due to merge conflicts, but it does appear to be in the 2.8.1 tag: see commit 3d764ed38709aa0516e226afa8f4dac682bffb59

I suppose it's possible we missed something in the cherrypick. Can you be more specific about what exactly you hit here? An exception message, or full logs would be great. Also can you confirm whether the entire group was on 2.8.1 at the time? For example did you do a rolling bounce up to 2.8.1 and encounter the problem during that? If there was even one member on an older version without this fix, then it would be possible to hit this bug. ;;;","25/Oct/21 09:28;Andy_Dufresne;The entire group used 2.8.1 Kafka-client  and 'CooperativeStickyAssignor'.


Here are broker logs: we see that broker skipped assignment for generations 10-12 since ConsumerCoordinator was stucked on it's side

 

!image-2021-10-25-11-53-40-221.png!

and here are logs from consumers for the same timeframe:


{code:java}
2021-10-20 10:14:27.878 ERROR {spanId=, traceId=} [org.apa.kaf.cli.con.int.ConsumerCoordinator] (smallrye-kafka-consumer-thread-0) [Consumer clientId=qa-qa-cf-executor-transform, groupId=qa-qa-cf-executor-transform] With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions [qa-qa-cf-events-32, qa-qa-cf-events-13, qa-qa-cf-events-30, qa-qa-cf-events-38, qa-qa-cf-events-11] which are still owned by some members
2021-10-20 10:14:30.566 ERROR {spanId=, traceId=} [org.apa.kaf.cli.con.int.ConsumerCoordinator] (smallrye-kafka-consumer-thread-0) [Consumer clientId=qa-qa-cf-executor-transform, groupId=qa-qa-cf-executor-transform] With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions [qa-qa-cf-events-32, qa-qa-cf-events-13, qa-qa-cf-events-30, qa-qa-cf-events-38, qa-qa-cf-events-11] which are still owned by some members
2021-10-20 10:14:34.913 ERROR {spanId=, traceId=} [org.apa.kaf.cli.con.int.ConsumerCoordinator] (smallrye-kafka-consumer-thread-0) [Consumer clientId=qa-qa-cf-executor-transform, groupId=qa-qa-cf-executor-transform] With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions [qa-qa-cf-events-32, qa-qa-cf-events-13, qa-qa-cf-events-30, qa-qa-cf-events-38, qa-qa-cf-events-11] which are still owned by some members
2021-10-20 10:14:34.920 ERROR {spanId=, traceId=} [io.sma.rea.mes.kafka] (smallrye-kafka-consumer-thread-0) SRMSG18217: Unable to read a record from Kafka topics '[qa-qa-cf-events]': java.lang.IllegalStateException: Retries exhausted: 3/3
2021-10-20T13:14:34.928+03:00	Caused by: java.lang.IllegalStateException: Assignor supporting the COOPERATIVE protocol violates its requirements
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.validateCooperativeAssignment(ConsumerCoordinator.java:668)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:592)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:693)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1000(AbstractCoordinator.java:111)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:599)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:562)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1182)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1157)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:247)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:426)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:365)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:508)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1261)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1230)
2021-10-20T13:14:34.928+03:00	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210)
2021-10-20T13:14:34.928+03:00	at io.smallrye.reactive.messaging.kafka.impl.ReactiveKafkaConsumer.lambda$poll$4(ReactiveKafkaConsumer.java:131)
2021-10-20T13:14:34.928+03:00	at io.smallrye.reactive.messaging.kafka.impl.ReactiveKafkaConsumer.lambda$runOnPollingThread$0(ReactiveKafkaConsumer.java:101)
2021-10-20T13:14:34.928+03:00	at io.smallrye.context.impl.wrappers.SlowContextualSupplier.get(SlowContextualSupplier.java:21)
2021-10-20T13:14:34.928+03:00	at io.smallrye.mutiny.operators.uni.builders.UniCreateFromItemSupplier.subscribe(UniCreateFromItemSupplier.java:28)
2021-10-20 10:14:34.921 WARN  {spanId=, traceId=} [io.sma.rea.mes.kafka] (smallrye-kafka-consumer-thread-0) SRMSG18228: A failure has been reported for Kafka topics '[qa-qa-cf-events]': java.lang.IllegalStateException: Retries exhausted: 3/3
{code}

UPD:
We run another tests and despite manually pulling and using 'CooperativeStickyAssignor' from Kafka 3.0 client in our application, now we see the same errors as well, so  it seems the issue still exists (maybe something in ConsumerCoordinator class);;;","25/Oct/21 12:59;showuon;[~Andy_Dufresne], thanks for your response. It looks like there's something wrong in the cooperative sticky assignor. If possible, could you help turn on ""DEBUG"" log for consumer and reproduce the issue and collect the log again?

That'd be helpful if we can see the logs starts with:
{code:java}
Performing constrained assign with partitionsPerTopic: ...
After reassigning previously owned partitions, unfilled members: ...
Final assignment of partitions to consumers: ...
{code}
or
{code:java}
performing general assign. partitionsPerTopic: ...
unassigned Partitions: ...
Final assignment of partitions to consumers: ..{code}
 Please run it in v3.0.0 since in not quite sure if there are such logs in 2.8.1. 

Thank you very much.

 ;;;","26/Oct/21 11:14;Andy_Dufresne;[~showuon] Here are debug logs.
The freeze happened at 15:48:23
we used CooperativeStickyAssignor from v 3.0.0 here

 [^logs-insights-results-kafka.csv] ;;;","26/Oct/21 14:10;showuon;[~Andy_Dufresne], thanks for the log. I have some clue from there now.

 

[~ableegoldman], here's my observation from the log.

At first, the rebalance (generation 24) completed successfully. Here's the final assignment:
{code:java}
- qa-qa-cf-executor-transform-e96cc4a8-e2a4-447a-b033-eb5b244a329a=[qa-qa-cf-events-8, qa-qa-cf-events-9, qa-qa-cf-events-24, qa-qa-cf-events-35], 
- qa-qa-cf-executor-transform-6f9acbca-885e-435c-97f5-6c86f476bd8e=[qa-qa-cf-events-1, qa-qa-cf-events-3, qa-qa-cf-events-5, qa-qa-cf-events-7], 
- qa-qa-cf-executor-transform-84d0f50e-0c97-4bf7-b4cc-311c6d3fbdf9=[qa-qa-cf-events-10, qa-qa-cf-events-11, qa-qa-cf-events-25], 
- qa-qa-cf-executor-transform-199c5bfb-85c8-4ecc-9604-30eb7988b9fe=[qa-qa-cf-events-6, qa-qa-cf-events-30, qa-qa-cf-events-32], 
- qa-qa-cf-executor-transform-c7decca7-e9d1-4d0b-b78e-3dfa5d71ea29=[qa-qa-cf-events-26, qa-qa-cf-events-28, qa-qa-cf-events-29], 
- qa-qa-cf-executor-transform-ac907fc0-33c9-4dfc-8f4d-f0e10b39e8ab=[qa-qa-cf-events-31, qa-qa-cf-events-36, qa-qa-cf-events-37], 
- qa-qa-cf-executor-transform-4c3d7d78-c1de-4d2d-bd18-580a442bc719=[qa-qa-cf-events-34, qa-qa-cf-events-38, qa-qa-cf-events-39], 
- qa-qa-cf-executor-transform-0e14afde-0788-44ef-8b5e-af7182f6a762=[qa-qa-cf-events-14, qa-qa-cf-events-15, qa-qa-cf-events-33], 
- qa-qa-cf-executor-transform-41f4f5c4-da68-40f1-911d-d006ede5c464=[qa-qa-cf-events-16, qa-qa-cf-events-17, qa-qa-cf-events-18, qa-qa-cf-events-19], qa-- - qa-cf-executor-transform-b1777322-80ee-4f28-a224-bf97ee2b2a50=[qa-qa-cf-events-20, qa-qa-cf-events-21, qa-qa-cf-events-22, qa-qa-cf-events-23], qa-- - qa-cf-executor-transform-5a95d1d8-0a47-4d32-bb6b-03531bb92765=[qa-qa-cf-events-0, qa-qa-cf-events-2, qa-qa-cf-events-4], 
- qa-qa-cf-executor-transform-2b6f692a-b557-4cc3-b461-f55fa3e25000=[qa-qa-cf-events-12, qa-qa-cf-events-13, qa-qa-cf-events-27]{code}
 

Here, what I want to highlight is the the assignment for consumer: qa-cf-executor-transform-b1777322-80ee-4f28-a224-bf97ee2b2a50=[**qa-qa-cf-events-20, qa-qa-cf-events-21, qa-qa-cf-events-22, qa-qa-cf-events-23**]. This one causes the issue.

Also, the other consumer: qa-qa-cf-executor-transform-199c5bfb-85c8-4ecc-9604-30eb7988b9fe=[**qa-qa-cf-events-6, qa-qa-cf-events-30, qa-qa-cf-events-32**], it seems not get the final assignment in generation 24. So in the next round of rebalance, ownedPartition is empty (my guess)

And, what will happen next (my guess), is that, in subscription's ""ownedPartition"" variable, we remember these 4 ownedPartition in subscription for the consumer ending with ""bf97ee2b2a50"" above. But somehow, we didn't find them in subscription data field after deserialization (maybe the generation is not the highest). So, in the following assignment, we'll just assign the 4 partitions to 2 consumers (1 is the consumer itself). And the error message in validateCooperativeAssignment proves my suspicion.

 

Next, generation 25 rebalance starts, with 3 retries. The all 3 final assignments are also logged, and here, I highlighted the difference:
 * 
 -- 1st try*, with error:
 _With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions [*qa-qa-cf-events-23, qa-qa-cf-events-21*] which are still owned by some members_

...

qa-qa-cf-executor-transform-199c5bfb-85c8-4ecc-9604-30eb7988b9fe=[qa-qa-cf-events-6, *qa-qa-cf-events-21*, *qa-qa-cf-events-23*]

...

qa-qa-cf-executor-transform-b1777322-80ee-4f28-a224-bf97ee2b2a50=[*qa-qa-cf-events-20, qa-qa-cf-events-22*, qa-qa-cf-events-30]

 
 * 
 -- 2nd try*, with the same error as 1st try, because the assignment is the same.

...

qa-qa-cf-executor-transform-199c5bfb-85c8-4ecc-9604-30eb7988b9fe=[qa-qa-cf-events-6, *qa-qa-cf-events-21, qa-qa-cf-events-23*]

...

qa-qa-cf-executor-transform-b1777322-80ee-4f28-a224-bf97ee2b2a50=[*qa-qa-cf-events-20, qa-qa-cf-events-22*, qa-qa-cf-events-30]

 
 * 
 -- 3rd try*, with error:
 _With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions [*qa-qa-cf-events-20, qa-qa-cf-events-23, qa-qa-cf-events-22*] which are still owned by some members_

...

qa-qa-cf-executor-transform-199c5bfb-85c8-4ecc-9604-30eb7988b9fe=[qa-qa-cf-events-6, *qa-qa-cf-events-20, qa-qa-cf-events-23*]

...

qa-qa-cf-executor-transform-d914fca8-dfe7-420c-98f7-ce7c44727fcd=[qa-qa-cf-events-19, *qa-qa-cf-events-22*, qa-qa-cf-events-32]  <-- new consumer

...

qa-qa-cf-executor-transform-b1777322-80ee-4f28-a224-bf97ee2b2a50=[qa-qa-cf-events-7, *qa-qa-cf-events-21*, qa-qa-cf-events-30]

 

 

====

So, it looks like the out-of-date ""ownedPartition"" in subscription not only might cause the double assignment issue, but also failed the cooperation assignment validation. 

My suggestion:
 # in validateCooperationAssignment, we should deserialize the subscription userData, instead of using the ownedPartition directly.
 # If the assignor is our built-in assignor (i.e. CooperativeStickyAssignor), we ignore the validation.

What do you think?;;;","26/Oct/21 14:51;Andy_Dufresne;[~showuon] Thank you for the fast and complete analysis.
It'd be great if I could cherrypick potential fix later and try the tests once more to prove the issue has finally gone.  :D;;;","26/Oct/21 21:18;ableegoldman;Mm, yeah, I should've remembered that the `ownedPartitions` field was used for this validation as well. I mean we did fix the [onJoinPrepare issue|https://issues.apache.org/jira/browse/KAFKA-12983] which was the only specific known edge case at the time which could lead to the `ownedPartitions` set being incorrect/out of date like this. Not surprised there might be others out there though, in fact I originally wanted to expand the fix in [#10986|https://github.com/apache/kafka/pull/10986] to reset the generation/ownedPartitions in some other cases to be safe but was talked out of it (not trying to shift the blame here, I still should've caught this!)

My point is, even if we do either 1/2, users won't be able to write a custom cooperative assignor without running into this. It's a huge bummer if we can't trust the `ownedPartitions` to be accurate – the whole point of adding it to the ConsumerProtocol was to use it for this verification and also to provide this info to users who may want to write a custom cooperative assignor :/ 

I suppose for the time being we could/should consider putting in a hotfix for the CooperativeStickyAssignor while we work out a better long-term solution – for example a public interface/abstract class like CooperativeConsumerPartitionAssignor that users can implement/extend to for a custom cooperative assignor, which at the very least stores the generation like we do in the CooperativeStickyAssignor and makes it available for the user as well as to the ConsumerCoordinator for it to know when to invalidate a member's ownedPartitions during this validation step – does that make sense? WDYT?

cc also [~guozhang] [~hachikuji]

FWIW I'd prefer either option (2), or possibly a modified version of (1) that only uses the generation rather than trying to get the partitions from the assignor's userdata. But again, all of these will only work for the CooperativeStickyAssignor, so we may need to start thinking about a KIP if we don't feel like we can trust the ownedPartitions to be accurate;;;","27/Oct/21 05:20;ableegoldman;[~showuon] want to prepare a quick fix for the time being? Either via option 2, or option 1/modified option 1 as described so that [~Andy_Dufresne] can test it out?;;;","27/Oct/21 05:40;showuon;Yes, I'm going to create another Jira issue for it. Will work on it soon!

 ;;;","27/Oct/21 07:25;showuon;> a public interface/abstract class like CooperativeConsumerPartitionAssignor that users can implement/extend to for a custom cooperative assignor, which at the very least stores the generation like we do in the CooperativeStickyAssignor and makes it available for the user as well as to the ConsumerCoordinator for it to know when to invalidate a member's ownedPartitions during this validation step – does that make sense? WDYT?

 --> Sounds good to me. Or maybe we can put onto the ConsumerProtocolSubscription protocol to add a ""generation"" field.;;;","27/Oct/21 09:26;showuon;KAFKA-13406 is created for this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
onJoinPrepare is not always invoked before joining the group,KAFKA-12983,13385287,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,23/Jun/21 01:57,06/Jun/22 21:10,13/Jul/23 09:17,13/Jul/21 19:29,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,consumer,,,,,0,new-consumer-threading-should-fix,,,,"As the title suggests, the #onJoinPrepare callback is not always invoked before a member (re)joins the group, but only once when it first enters the rebalance. This means that any updates or events that occur during the join phase can be lost in the internal state: for example, clearing the SubscriptionState (and thus the ""ownedPartitions"" that are used for cooperative rebalancing) after losing its memberId during a rebalance.

We should reset the `needsJoinPrepare` flag inside the resetStateAndRejoin() method",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12896,,,,,,,,,,KAFKA-12984,,,,,,,,,KAFKA-12920,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 28 22:48:32 UTC 2021,,,,,,,,,,"0|z0s7ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/21 19:23;mjsax;[~ableegoldman] Do we actually know if 2.6 is also affected?;;;","28/Sep/21 22:48;ableegoldman;I'm pretty sure if affects all versions. I guess it wasn't that bad of a bug prior to cooperative rebalancing though, so maybe the affects version should be 2.4?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupt segment recovery can delete new producer state snapshots,KAFKA-12964,13384360,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gardnervickers,gardnervickers,gardnervickers,17/Jun/21 13:09,01/Jul/21 21:25,13/Jul/23 09:17,01/Jul/21 21:25,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,,,,,"During log recovery, we may schedule asynchronous deletion in deleteSegmentFiles.

[https://github.com/apache/kafka/blob/fc5245d8c37a6c9d585c5792940a8f9501bedbe1/core/src/main/scala/kafka/log/Log.scala#L2382]

If we're truncating the log, this may result in deletions for segments with matching base offsets to segments which will be written in the future. To avoid asynchronously deleting future segments, we rename the segment and index files, but we do not do this for producer state snapshot files. 

This leaves us vulnerable to a race condition where we could end up deleting snapshot files for segments written after log recovery when async deletion runs.

 

To fix this, we should first remove the `SnapshotFile` from the `ProducerStateManager` and rename the file to have a `Log.DeletedFileSuffix`. Then we can asynchronously delete the snapshot file later.",,gardnervickers,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 01 21:25:45 UTC 2021,,,,,,,,,,"0|z0s1kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 21:25;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite loop while restoring a GlobalKTable,KAFKA-12951,13383956,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,Dabz,Dabz,15/Jun/21 15:05,29/Jun/21 13:30,13/Jul/23 09:17,28/Jun/21 22:31,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,streams,,,,,1,,,,,"We encountered an issue a few time in some of our Kafka Streams application.
 After an unexpected restart of our applications, some instances have not been able to resume operating.

They got stuck while trying to restore the state store of a GlobalKTable. The only way to resume operating was to manually delete their `state.dir`.

We observed the following timeline:
 * After the restart of the Kafka Streams application, it tries to restore its GlobalKTable
 * It seeks to the last checkpoint available on the {{{{state.dir}}}}: 382 ([https://github.com/apache/kafka/blob/2.7.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L259])
 * The watermark ({{endOffset}} results) returned the offset 383 
{code:java}
handling ListOffsetResponse response for XX. Fetched offset 383, timestamp -1{code}

 * We enter the loop: [https://github.com/apache/kafka/blob/2.7.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L279]
 * Then we invoked the {{poll()}}, but the poll returns nothing, so we enter: [https://github.com/apache/kafka/blob/2.7.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java#L306] and we crash (x)
{code:java}
Global task did not make progress to restore state within 300000 ms.{code}

 * The POD restart, and we encounter the same issue until we manually delete the {{state.dir}}

 

Regarding the topic, by leveraging the {{DumpLogSegment}} tool, I can see:
 * {{Offset 381}} - Last business message received
 * {{Offset 382}} - Txn COMMIT (last message)

I think the real culprit is that the checkpoint is {{383}} instead of being {{382}}. For information, the global topic is a *transactional topic*.

While experimenting with the API, it seems that the {{consumer.position()}} call is a bit tricky, after a {{seek()}} and a {{poll()}}, it seems that the {{position()}} is actually returning the seek position. After the {{poll()}} call, even if no data is returned, the {{position()}} is returning the LSO. I did an example on [https://gist.github.com/Dabz/9aa0b4d1804397af6e7b6ad8cba82dcb] .",,ableegoldman,Dabz,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9274,,,,,KAFKA-6607,KAFKA-12980,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-15 15:05:22.0,,,,,,,,,,"0|z0rz3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestRaftServer's scala.MatchError: null on test-kraft-server-start.sh,KAFKA-12949,13383826,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,IgnacioAcunaFri,IgnacioAcunaFri,IgnacioAcunaFri,15/Jun/21 00:14,22/Jun/21 19:15,13/Jul/23 09:17,22/Jun/21 19:15,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,kraft,,,,,0,,,,,"Encounter the following exception when trying to run the TestRaftServer:
{code:java}
bin/test-kraft-server-start.sh --config config/kraft.properties{code}
{code:java}
[2021-06-14 17:15:43,232] ERROR [raft-workload-generator]: Error due to (kafka.tools.TestRaftServer$RaftWorkloadGenerator)
 scala.MatchError: null
 at kafka.tools.TestRaftServer$RaftWorkloadGenerator.doWork(TestRaftServer.scala:220)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
 [2021-06-14 17:15:43,253] INFO [raft-workload-generator]: Stopped (kafka.tools.TestRaftServer$RaftWorkloadGenerator){code}
That happens on the followin match:
{code:java}
eventQueue.poll(eventTimeoutMs, TimeUnit.MILLISECONDS) match {
  case HandleClaim(epoch) =>
      claimedEpoch = Some(epoch)
      throttler.reset()
      pendingAppends.clear()
      recordCount.set(0)    
  case HandleResign =>
      claimedEpoch = None
      pendingAppends.clear()    case HandleCommit(reader) =>
      try {
        while (reader.hasNext) {
          val batch = reader.next()
          claimedEpoch.foreach { leaderEpoch =>
            handleLeaderCommit(leaderEpoch, batch)
          }
        }
      } finally {
        reader.close()
      }    
  case HandleSnapshot(reader) =>
      // Ignore snapshots; only interested in records appended by this leader
      reader.close()    
  case Shutdown => // Ignore shutdown command
}
{code}
Full log attached. When the eventQueue.poll returns null (if deque is empty), there isn't a case to match so the thread gets stuck and stops processing events (raft-workload-generator).

Proposal:
 Add a case null to the match so the raft-workload-generator can continue.",,IgnacioAcunaFri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/21 00:19;IgnacioAcunaFri;TestRaftServer.log;https://issues.apache.org/jira/secure/attachment/13026827/TestRaftServer.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-15 00:14:35.0,,,,,,,,,,"0|z0ryag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NetworkClient.close(node) with node in connecting state makes NetworkClient unusable,KAFKA-12948,13383804,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rsivaram,rsivaram,rsivaram,14/Jun/21 20:39,15/Jun/21 08:57,13/Jul/23 09:17,15/Jun/21 08:57,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,network,,,,,0,,,,,"`NetworkClient.close(node)` closes the node and removes it from `ClusterConnectionStates.nodeState`, but not from `ClusterConnectionStates.connectingNodes`. Subsequent `NetworkClient.poll()` invocations throw IllegalStateException and this leaves the NetworkClient in an unusable state until the node is removed from connectionNodes or added to nodeState. We don't use `NetworkClient.close(node)` in clients, but we use it in clients started by brokers for replica fetcher and controller. Since brokers use NetworkClientUtils.isReady() before establishing connections and this invokes poll(), the NetworkClient never recovers.

Exception stack trace:
{code:java}
    java.lang.IllegalStateException: No entry found for connection 0
        at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:409)
        at org.apache.kafka.clients.ClusterConnectionStates.isConnectionSetupTimeout(ClusterConnectionStates.java:446)
        at org.apache.kafka.clients.ClusterConnectionStates.lambda$nodesWithConnectionSetupTimeout$0(ClusterConnectionStates.java:458)
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
        at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1553)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
        at org.apache.kafka.clients.ClusterConnectionStates.nodesWithConnectionSetupTimeout(ClusterConnectionStates.java:459)
        at org.apache.kafka.clients.NetworkClient.handleTimedOutConnections(NetworkClient.java:807)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:564)
        at org.apache.kafka.clients.NetworkClientUtils.isReady(NetworkClientUtils.java:42)
{code}",,dengziming,fvaleri,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 08:57:35 UTC 2021,,,,,,,,,,"0|z0ry5k:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"14/Jun/21 21:45;ijuma;Good catch. So, this regressed in 2.7.0?;;;","15/Jun/21 08:57;rsivaram;Yes, it was due to the changes from [https://cwiki.apache.org/confluence/display/KAFKA/KIP-601%3A+Configurable+socket+connection+timeout+in+NetworkClient] , which went into 2.7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove port, host.name and related configs in 3.0",KAFKA-12945,13383590,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,ijuma,ijuma,12/Jun/21 17:46,08/Jul/21 22:29,13/Jul/23 09:17,08/Jul/21 22:29,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,They have been deprecated since 0.10.0.,,dengziming,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 13 16:08:20 UTC 2021,,,,,,,,,,"0|z0rwu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/21 16:08;ijuma;https://github.com/apache/kafka/pull/10872;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Kafka Streams Documentation (Aggregating),KAFKA-12943,13383528,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,marcolotz,rajeshksv37,rajeshksv37,12/Jun/21 02:21,07/Jul/22 21:00,13/Jul/23 09:17,07/Jul/22 21:00,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,documentation,streams,,,,0,,,,,"In the doc, for aggregating function, the example is incorrect

[https://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#aggregating]

It says
{code:java}
KTable<byte[], Long> aggregatedStream = groupedStream.aggregate(
    () -> 0L, /* initializer */
    (aggKey, newValue, aggValue) -> aggValue + newValue.length(), /* adder */
    Materialized.as(""aggregated-stream-store"") /* state store name */
        .withValueSerde(Serdes.Long()); /* serde for aggregate value */{code}
Generic types are missing. Instead, it should be 
{code:java}
KTable<byte[], Long> aggregatedStream = groupedStream.aggregate(
    () -> 0L, /* initializer */
    (aggKey, newValue, aggValue) -> aggValue + newValue.length(), /* adder */
    Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as(""aggregated-stream-store"") /* state store name */
        .withValueSerde(Serdes.Long()); /* serde for aggregate value */ {code}
Otherwise, code won't work. I myself verified it. 

Reference

[https://stackoverflow.com/questions/51040555/the-method-withvalueserde-in-the-type-materialized-is-not-applicable/51049472]",,josep.prat-inactive,marcolotz,mjsax,rajeshksv37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 20:05:37 UTC 2022,,,,,,,,,,"0|z0rwg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/21 07:51;jlprat;Submitted a patch a while ago: https://github.com/apache/kafka/pull/10766;;;","15/Jun/21 07:57;jlprat;Sorry my bad. This wasn't covered in that patch.;;;","07/Jul/22 20:05;marcolotz;The PR was raised in April and approved twice. Is there anything else missing to merge it to trunk?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerGroupCommand's java.lang.NullPointerException at negative offsets while running kafka-consumer-groups.sh,KAFKA-12926,13383062,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,IgnacioAcunaFri,IgnacioAcunaFri,IgnacioAcunaFri,10/Jun/21 00:15,29/Jun/21 14:07,13/Jul/23 09:17,29/Jun/21 07:01,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,admin,clients,,,,0,,,,,"Hi everybody, hope everyone is doing great.

*i) Introduction:*
I noticed the following exception (on a cluster with brokers running 2.3.1) when trying to describe a consumer group (using the Kafka 2.7.1):

 
{code:java}
./kafka-consumer-groups.sh --describe --group order-validations{code}
{code:java}
Error: Executing consumer group command failed due to null
java.lang.NullPointerException
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$6(ConsumerGroupCommand.scala:579)
 at scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:99)
 at scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:86)
 at scala.collection.convert.JavaCollectionWrappers$JSetWrapper.map(JavaCollectionWrappers.scala:180)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$5(ConsumerGroupCommand.scala:578)
 at scala.collection.immutable.List.flatMap(List.scala:293)
 at scala.collection.immutable.List.flatMap(List.scala:79)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$2(ConsumerGroupCommand.scala:574)
 at scala.collection.Iterator$$anon$9.next(Iterator.scala:575)
 at scala.collection.mutable.Growable.addAll(Growable.scala:62)
 at scala.collection.mutable.Growable.addAll$(Growable.scala:59)
 at scala.collection.mutable.HashMap.addAll(HashMap.scala:117)
 at scala.collection.mutable.HashMap$.from(HashMap.scala:570)
 at scala.collection.mutable.HashMap$.from(HashMap.scala:563)
 at scala.collection.MapOps$WithFilter.map(Map.scala:358)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.collectGroupsOffsets(ConsumerGroupCommand.scala:569)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeGroups(ConsumerGroupCommand.scala:369)
 at kafka.admin.ConsumerGroupCommand$.run(ConsumerGroupCommand.scala:76)
 at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:63)
 at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala){code}
 

When trying on and older version of AdminClient (2.3.1):
{code:java}
Error: Executing consumer group command failed due to java.lang.IllegalArgumentException: Invalid negative offset
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Invalid negative offset
 at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
 at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
 at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
 at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.getCommittedOffsets(ConsumerGroupCommand.scala:595)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$2(ConsumerGroupCommand.scala:421)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$$Lambda$131/000000004CB1EFD0.apply(Unknown Source)
 at scala.collection.TraversableLike$WithFilter.$anonfun$map$2(TraversableLike.scala:827)
 at scala.collection.TraversableLike$WithFilter$$Lambda$132/000000004CD49E20.apply(Unknown Source)
 at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
 at scala.collection.mutable.HashMap$$Lambda$133/000000004CD4A4F0.apply(Unknown Source)
 at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
 at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
 at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
 at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
 at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:826)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.collectGroupsOffsets(ConsumerGroupCommand.scala:419)
 at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeGroups(ConsumerGroupCommand.scala:312)
 at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:63)
 at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
 at org.apache.kafka.clients.consumer.OffsetAndMetadata.<init>(OffsetAndMetadata.java:50)
 at org.apache.kafka.clients.admin.KafkaAdminClient$24$1.handleResponse(KafkaAdminClient.java:2832)
 at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.handleResponses(KafkaAdminClient.java:1032)
 at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1160)
 at java.lang.Thread.run(Thread.java:820){code}
 

The main difference between those outputs is what had been done in KAFKA-9507.

*ii) Problem:* 
commitedOffsets for some partitions are arriving as null to _ConsumerGroupCommand_. Then (for the assigned consumers to those such null OffsetAndMetadata's partitions) getting the offset's value throws an java.lang.NullPointerException, because the ConsumerGroupCommand tries to map over a null value.

*iii) Example*:
a) +GroupID information (from describeConsumerGroups() method):+
(groupId=order-validations, isSimpleConsumerGroup=false, members=(memberId=order-validations-d5fbca62-ab2b-48d7-96ba-0ae72dff72a6, groupInstanceId=null, clientId=order-validations, host=/127.0.0.1, assignment=(topicPartitions=rtl_orderReceive-0,rtl_orderReceive-1,rtl_orderReceive-2,rtl_orderReceive-3,rtl_orderReceive-4,rtl_orderReceive-5,rtl_orderReceive-6,rtl_orderReceive-7,rtl_orderReceive-8,rtl_orderReceive-9)), partitionAssignor=RoundRobinAssigner, state=Stable, coordinator=f0527.cluster.cl:31047 (id: 1 rack: null), authorizedOperations=[])

b) +Commited Offsets information (from getCommittedOffsets() method):+
Map(rtl_orderReceive-0 -> null, rtl_orderReceive-1 -> OffsetAndMetadata\{offset=39, leaderEpoch=null, metadata=''}, rtl_orderReceive-2 -> null, rtl_orderReceive-3 -> OffsetAndMetadata\{offset=33, leaderEpoch=null, metadata=''}, rtl_orderReceive-4 -> null, rtl_orderReceive-5 -> null, rtl_orderReceive-7 -> null, rtl_orderReceive-8 -> null)

As seen, member order-validations-d5fbca62-ab2b-48d7-96ba-0ae72dff72a6 is assigned to all partitions, but the commited offsets reported for the the partition 0,2,4,5,7,8 are null.
Then getting commited offsets for rtl_orderReceive-0 throws an error at .map(_.offset), because it translates to null.map(_.offset). This is happening because the offset of that partions is -1 and that gets map to null (as defined on KAFKA-9507).

*iv) Proposals:*

a) +Fix locally on the ConsumerGroupCommand:+
Add a filter to the Commited Offsets arriving from upstreams to catch border cases. In that way, even if upstreams cames with null values instead of a OffsetAndMetadata, the describeGroups would work and get the consumer group description.

From
{code:java}
val committedOffsets = getCommittedOffsets(groupId){code}
To:
{code:java}
val committedOffsets = getCommittedOffsets(groupId).filter(_._2.isInstanceOf[OffsetAndMetadata]){code}
 


b) +Fix upstreams on KafkaAdmin's listConsumerGroupOffsets method:+
Related to KAFKA-9507. In that issue, the solution to handle negative offsets was to explicitly set null to the topicPartition:
{code:java}
if (offset < 0) {
 groupOffsetsListing.put(topicPartition, null);
 } else {
 groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));
 }{code}
 

That approach solves org.apache.kafka.clients.consumer.OffsetAndMetadata for throwing an _'Invalid negative offset'_ error, but affects downstreams methods that use KafkaAdminClient's listConsumerGroupOffsets method (as the one at kafka-consumer-groups.sh).

The proposal is to skip returning offset for topic partitions where offsets are negative:
{code:java}
if (offset >= 0) {
 groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));
 }{code}
 

This would remove the negative offsets from the the listConsumerGroupOffsets and guarantee that the results are valids OffsetAndMetadata (not only handling negative offsets as KAFKA-9507, but not impacting other downstreams methods which expects an OffsetAndMetadata instead of a null value).

I think the second approach is cleaner because let the downstreams methods without having to handle the null's border case, which may lead to expecions (as seen).

I had been working on the both approaches, and I ready to prepare a PR. What do you think?",,IgnacioAcunaFri,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-10 00:15:09.0,,,,,,,,,,"0|z0rtko:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
prefixScan missing from intermediate interfaces,KAFKA-12925,13383060,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sagarrao,mviamari,mviamari,09/Jun/21 23:27,15/Jul/21 02:17,13/Jul/23 09:17,15/Jul/21 02:17,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,,,,,"[KIP-614|https://cwiki.apache.org/confluence/display/KAFKA/KIP-614%3A+Add+Prefix+Scan+support+for+State+Stores] and [KAFKA-10648|https://issues.apache.org/jira/browse/KAFKA-10648] introduced support for {{prefixScan}} to StateStores.

It seems that many of the intermediate {{StateStore}} interfaces are missing a definition for {{prefixScan}}, and as such is not accessible in all cases.

For example, when accessing the state stores through a the processor context, the {{KeyValueStoreReadWriteDecorator}} and associated interfaces do not define {{prefixScan}} and it falls back to the default implementation in {{KeyValueStore}}, which throws {{UnsupportedOperationException}}.",,ableegoldman,mjsax,mviamari,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 13 18:48:36 UTC 2021,,,,,,,,,,"0|z0rtk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/21 23:44;mjsax;[~sagarrao] [~cadonna] [~guozhang] – I had a quick look into this, and it seems `prefixScan` is fundamentally ""broken"" and unusable?

We have a lot of internal ""wrapper"" classes that implement the base interface and none of them implements the new method to forward the corresponding calls... Seems having a default implementation hid this issue on the original PR...;;;","10/Jun/21 00:50;ableegoldman;[~sagarrao] do you think you'll be able to get in a fix for this in the next few weeks? It would be good to have this working in full capacity by 3.0

(I wouldn't consider it a blocker for 3.0, but we have enough time before code freeze that this should not be an issue, provided someone can pick it up);;;","10/Jun/21 02:20;sagarrao;Sure [~ableegoldman], [~mjsax] I have assigned it to myself. Will take it up.

TBH I did struggle to figure out where all should this method be implemented and having a default implementation masked the misses. What I will do is, find all places where a method like range() has been implemented and try to add it there if I can. Would that work?;;;","10/Jun/21 04:45;mjsax;Thanks [~sagarrao]!

Maybe the easiest way is to remove the default implementation and see where it fails to compile. After all issues are fixed and it compiles again you can add back the default implementation. Me might also want to add more tests?

[~mviamari] Could you share the code that triggered the issue as a starting point for writing tests?;;;","10/Jun/21 16:17;mviamari;I can flesh out a larger example if needed, but the basic usage for me was getting a reference to the state store using {{context.getStateStore()}} inside {{Transformer#init}}, and then when attempting to use {{TimestampedKeyValueStore#prefixScan}}, the exception was thrown.
{code:java}
public class TransformerPrefixScan<K, V, KS, VS> implements Transformer<K, V, KeyValue<K, V>> {

        private ProcessorContext context;
        private TimestampedKeyValueStore<KS, VS> lookupStore;

        public TransformerPrefixScan() {}

        @Override
        @SuppressWarnings(""unchecked"")
        public void init(ProcessorContext context) {
            this.context = context;
            lookupStore = context.getStateStore(lookupStoreName);
        }

        @Override
        public KeyValue<K, V> transform(K key, V value) {

            String keyPrefix = extractPrefix(key);
            try (KeyValueIterator<KS, ValueAndTimestamp<VS>> lookupIterator = lookupStore.prefixScan(keyPrefix, Serdes.String())) {
                //....handle results....
            }

            return null;
        }

        @Override
        public void close() {

        }
}
{code};;;","13/Jun/21 18:48;sagarrao;Thanks [~mviamari] .

[~mjsax]/[~ableegoldman]: I have created a PR with the implementations for the remaining classes. Now, only 2 test classes are missing namely GenericInMemoryKeyValueStore and GenericInMemoryTimestampedKeyValueStore. For them, it will be difficult to add the implementation. Plz review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamSourceNode.toString() throws with StreamsBuilder.stream(Pattern) ctor,KAFKA-12914,13382745,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,will118,will118,08/Jun/21 15:44,15/Jun/21 08:54,13/Jul/23 09:17,15/Jun/21 08:00,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,,,,,"Hi, 

I came across what looks like a bug.
h2. Repro
{code:java}
import org.apache.kafka.common.serialization.Serdes
import org.apache.kafka.streams.KafkaStreams
import org.apache.kafka.streams.StreamsBuilder
import org.apache.kafka.streams.kstream.Consumed
import java.util.*
import java.util.regex.Pattern

fun main() {
    val builder = StreamsBuilder()
    builder.stream(Pattern.compile(""foo""), Consumed.with(Serdes.Long(), Serdes.Long()))
    val streams = KafkaStreams(builder.build(), Properties())
    streams.start()
}
{code}
{code:bash}
SLF4J: Failed toString() invocation on an object of type [java.util.LinkedHashSet]
Reported exception:
java.lang.NullPointerException
	at java.base/java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1030)
	at java.base/java.util.Collections$UnmodifiableSet.<init>(Collections.java:1132)
	at java.base/java.util.Collections.unmodifiableSet(Collections.java:1122)
	at org.apache.kafka.streams.kstream.internals.graph.SourceGraphNode.topicNames(SourceGraphNode.java:55)
	at org.apache.kafka.streams.kstream.internals.graph.StreamSourceNode.toString(StreamSourceNode.java:65)
	at java.base/java.lang.String.valueOf(String.java:3352)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:166)
	at java.base/java.util.AbstractCollection.toString(AbstractCollection.java:457)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:277)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:249)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:211)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:161)
	at ch.qos.logback.classic.spi.LoggingEvent.getFormattedMessage(LoggingEvent.java:293)
	at ch.qos.logback.classic.spi.LoggingEvent.prepareForDeferredProcessing(LoggingEvent.java:206)
	at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:223)
	at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:102)
	at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:84)
	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51)
	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270)
	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257)
	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421)
	at ch.qos.logback.classic.Logger.filterAndLog_2(Logger.java:414)
	at ch.qos.logback.classic.Logger.debug(Logger.java:490)
	at org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder.buildAndOptimizeTopology(InternalStreamsBuilder.java:305)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:624)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:613)
	at ApplicationKt.main(Application.kt:11)
	at ApplicationKt.main(Application.kt)
SLF4J: Failed toString() invocation on an object of type [org.apache.kafka.streams.kstream.internals.graph.StreamSourceNode]
Reported exception:
java.lang.NullPointerException
	at java.base/java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1030)
	at java.base/java.util.Collections$UnmodifiableSet.<init>(Collections.java:1132)
	at java.base/java.util.Collections.unmodifiableSet(Collections.java:1122)
	at org.apache.kafka.streams.kstream.internals.graph.SourceGraphNode.topicNames(SourceGraphNode.java:55)
	at org.apache.kafka.streams.kstream.internals.graph.StreamSourceNode.toString(StreamSourceNode.java:65)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:277)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:249)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:211)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:161)
	at ch.qos.logback.classic.spi.LoggingEvent.getFormattedMessage(LoggingEvent.java:293)
	at ch.qos.logback.classic.spi.LoggingEvent.prepareForDeferredProcessing(LoggingEvent.java:206)
	at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:223)
	at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:102)
	at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:84)
	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51)
	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270)
	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257)
	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421)
	at ch.qos.logback.classic.Logger.filterAndLog_2(Logger.java:414)
	at ch.qos.logback.classic.Logger.debug(Logger.java:490)
	at org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder.buildAndOptimizeTopology(InternalStreamsBuilder.java:305)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:624)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:613)
	at ApplicationKt.main(Application.kt:11)
	at ApplicationKt.main(Application.kt)
{code}
Kotlin but you get the idea.
h2. Brief debug

StreamSourceNode.toString() calls topicNames():

[https://github.com/apache/kafka/blob/1dadb6db0c6848a8a1d2eee1497f9b79b6e04e0e/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java#L64-L70]

Which will be null if constructed with a Pattern rather than a String:

[https://github.com/apache/kafka/blob/1dadb6db0c6848a8a1d2eee1497f9b79b6e04e0e/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/SourceGraphNode.java#L49]

Which causes UnmodifiableCollection to throw.

 ",,ableegoldman,mjsax,will118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 08:54:43 UTC 2021,,,,,,,,,,"0|z0rrm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/21 08:54;will118;Thanks for the quick turnaround!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect's validate REST endpoint uses incorrect timeout,KAFKA-12904,13382500,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,rhauch,rhauch,07/Jun/21 14:51,22/Jun/21 14:17,13/Jul/23 09:17,22/Jun/21 14:17,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.3,2.7.2,2.8.1,3.0.0,,,,,KafkaConnect,,,,,0,,,,,"The fix for KAFKA-9374 changed how the `ConnectorPluginsResource` and its method to validate connector configurations used the `ConnectorsResource.REQUEST_TIMEOUT_MS` constant (90 seconds). However, in doing so it introduced a bug where the timeout was actually 1000x longer than desired/specified.

In particular, the following line is currently:

{code:java}
            return validationCallback.get(ConnectorsResource.REQUEST_TIMEOUT_MS, TimeUnit.SECONDS);
{code}

but should be:
{code:java}
            return validationCallback.get(ConnectorsResource.REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);
{code}

Users may run into this whenever validating a connector configuration where the connector implementation takes more than the 90 seconds to actually validate the configuration. 
* Without this fix, the `PUT /connector-plugins/(string:name)/config/validate` REST requests might **_not_** return `500 Internal Server Error` and may block (the request thread) for a long period of time. 
* With this fix, the `PUT /connector-plugins/(string:name)/config/validate` REST requests might **_not_** return `500 Internal Server Error` if the connector does not complete the validation of a connector configuration within 90 seconds.

The user will not see a difference between the behavior before or after this fix if/when the connectors complete validation of connector configurations before 90 seconds, since the method will return those results to the client.
",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9374,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 14:17:36 UTC 2021,,,,,,,,,,"0|z0rq48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 14:17;rhauch;Merged to `trunk` and backported.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Owned partitions in the subscription must be sorted,KAFKA-12898,13382177,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,04/Jun/21 15:37,19/Aug/22 15:54,13/Jul/23 09:17,24/Jun/21 08:10,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,1,,,,,"While investigating https://issues.apache.org/jira/browse/KAFKA-12896, I have noticed that the leader was always sending the same subscribed partitions but not always in the same order. The group coordinator compares the provided subscription with the store subscription based on their bytes representation. So if the subscribed partitions are not in the same order, the group coordinator would consider that they are different and rebalance the group.",,ableegoldman,amuraru,dajac,ijuma,wenbing.shen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-04 15:37:22.0,,,,,,,,,,"0|z0ro4g:",9223372036854775807,,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft Controller cannot create topic with multiple partitions on a single broker cluster,KAFKA-12897,13382168,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rndgstn,rndgstn,rndgstn,04/Jun/21 15:06,07/Jun/21 21:14,13/Jul/23 09:17,07/Jun/21 21:14,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,controller,,,,,0,,,,,https://github.com/apache/kafka/pull/10494 introduced a bug in the KRaft controller where the controller will loop forever in `StripedReplicaPlacer` trying to identify the racks on which to place partition replicas if there is a single unfenced broker in the cluster and the number of requested partitions in a CREATE_TOPICS request is greater than 1.,,junrao,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 07 21:14:17 UTC 2021,,,,,,,,,,"0|z0ro2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 21:14;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group rebalance loop caused by repeated group leader JoinGroups,KAFKA-12896,13382160,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,lucasbradstreet,lucasbradstreet,04/Jun/21 14:14,15/Jul/21 07:33,13/Jul/23 09:17,15/Jul/21 07:33,2.6.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,clients,,,,,2,,,,,"We encountered a strange case of a rebalance loop with the ""cooperative-sticky"" assignor. The logs show the following for several hours:

 

{{Apr 7, 2021 @ 03:58:36.040	[GroupCoordinator 7]: Stabilized group mygroup generation 19830137 (__consumer_offsets-7)}}

{{Apr 7, 2021 @ 03:58:35.992	[GroupCoordinator 7]: Preparing to rebalance group mygroup in state PreparingRebalance with old generation 19830136 (__consumer_offsets-7) (reason: Updating metadata for member mygroup-1-7ad27e07-3784-4588-97e1-d796a74d4ecc during CompletingRebalance)}}

{{Apr 7, 2021 @ 03:58:35.988	[GroupCoordinator 7]: Stabilized group mygroup generation 19830136 (__consumer_offsets-7)}}

{{Apr 7, 2021 @ 03:58:35.972	[GroupCoordinator 7]: Preparing to rebalance group mygroup in state PreparingRebalance with old generation 19830135 (__consumer_offsets-7) (reason: Updating metadata for member mygroup during CompletingRebalance)}}

{{Apr 7, 2021 @ 03:58:35.965	[GroupCoordinator 7]: Stabilized group mygroup generation 19830135 (__consumer_offsets-7)}}

{{Apr 7, 2021 @ 03:58:35.953	[GroupCoordinator 7]: Preparing to rebalance group mygroup in state PreparingRebalance with old generation 19830134 (__consumer_offsets-7) (reason: Updating metadata for member mygroup-7ad27e07-3784-4588-97e1-d796a74d4ecc during CompletingRebalance)}}

{{Apr 7, 2021 @ 03:58:35.941	[GroupCoordinator 7]: Stabilized group mygroup generation 19830134 (__consumer_offsets-7)}}

{{Apr 7, 2021 @ 03:58:35.926	[GroupCoordinator 7]: Preparing to rebalance group mygroup in state PreparingRebalance with old generation 19830133 (__consumer_offsets-7) (reason: Updating metadata for member mygroup during CompletingRebalance)}}

Every single time, it was the same member that triggered the JoinGroup and it was always the leader of the group.{{}}

The leader has the privilege of being able to trigger a rebalance by sending `JoinGroup` even if its subscription metadata has not changed. But why would it do so?

It is possible that this is due to the same issue or a similar bug to https://issues.apache.org/jira/browse/KAFKA-12890.",,ableegoldman,amuraru,dajac,ijuma,lucasbradstreet,xenogears,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12984,KAFKA-12983,,,,,,,,,KAFKA-12984,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 15 07:33:55 UTC 2021,,,,,,,,,,"0|z0ro0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/21 15:38;dajac;I have found a bug while investigating this one: https://issues.apache.org/jira/browse/KAFKA-12898. It does not explain why the leader tries to rejoin the group but it does explain why the group coordinator rebalances the group.;;;","23/Jun/21 02:00;ableegoldman;I believe this is caused by https://issues.apache.org/jira/browse/KAFKA-12984, which in turn is caused by https://issues.apache.org/jira/browse/KAFKA-12983. I'll prepare a fix for both of these issues;;;","15/Jul/21 06:34;dajac;[~ableegoldman] With you two fixes, I think that we can resolve this one, isn't it?;;;","15/Jul/21 07:33;ableegoldman;Yep, that should do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer group stuck in `CompletingRebalance`,KAFKA-12890,13382081,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,04/Jun/21 08:12,10/Aug/21 17:51,13/Jul/23 09:17,17/Jun/21 12:05,2.6.1,2.6.2,2.7.0,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,1,,,,,"We have seen recently multiple consumer groups stuck in `CompletingRebalance`. It appears that those group never receives the assignment from the leader of the group and remains stuck in this state forever.

When a group transitions to the `CompletingRebalance` state, the group coordinator sets up `DelayedHeartbeat` for each member of the group. It does so to ensure that the member sends a sync request within the session timeout. If it does not, the group coordinator rebalances the group. Note that here, `DelayedHeartbeat` is used here for this purpose. `DelayedHeartbeat` are also completed when member heartbeats.

The issue is that https://github.com/apache/kafka/pull/8834 has changed the heartbeat logic to allow members to heartbeat while the group is in the `CompletingRebalance` state. This was not allowed before. Now, if a member starts to heartbeat while the group is in the `CompletingRebalance`, the heartbeat request will basically complete the pending `DelayedHeartbeat` that was setup previously for catching not receiving the sync request. Therefore, if the sync request never comes, the group coordinator does not notice anymore.

We need to bring that behavior back somehow.

",,ableegoldman,dajac,ijuma,xenogears,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 17 12:05:56 UTC 2021,,,,,,,,,,"0|z0rnj4:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"17/Jun/21 12:05;dajac;I will backport the patch to 2.8 branch as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log clean group consider empty log segment to avoid empty log left,KAFKA-12889,13382073,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,iamgd67,iamgd67,04/Jun/21 07:04,22/Jun/21 02:12,13/Jul/23 09:17,19/Jun/21 22:34,0.10.1.1,2.8.0,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,log cleaner,,,,,0,,,,,"to avoid log index 4 byte relative offset overflow, log cleaner group check log segments offset to make sure group offset range not exceed Int.MaxValue.

this offset check currentlly not cosider next is next log segment is empty, so there will left empty log files every about 2^31 messages.

the left empty logs will be reprocessed every clean cycle, which will rewrite it with same empty content, witch cause little no need io.

for __consumer_offsets topic, normally we can set cleanup.policy to compact,delete to get rid of this.

my cluster is 0.10.1.1, but after aylize trunk code, it should has same problem too.

 

some of my left empty logs,(run ls -l)

-rw-r----- 1 u g 0 Dec 16 2017 00000000000000000000.index
-rw-r----- 1 u g 0 Dec 16 2017 00000000000000000000.log
-rw-r----- 1 u g 0 Dec 16 2017 00000000000000000000.timeindex
-rw-r----- 1 u g 0 Jan  15 2018 00000000002148249632.index
-rw-r----- 1 u g 0 Jan  15 2018 00000000002148249632.log
-rw-r----- 1 u g 0 Jan  15 2018 00000000002148249632.timeindex
-rw-r----- 1 u g 0 Jan  27 2018 00000000004295766494.index
-rw-r----- 1 u g 0 Jan  27 2018 00000000004295766494.log
-rw-r----- 1 u g 0 Jan  27 2018 00000000004295766494.timeindex

 ",,guozhang,iamgd67,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 02:12:43 UTC 2021,,,,,,,,,,"0|z0rnhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/21 07:12;iamgd67;create a pull request on github;;;","19/Jun/21 22:34;guozhang;Thanks [~iamgd67] for reporting the issue and for the fix too!;;;","22/Jun/21 02:12;showuon;Nice find! Thanks for the fix! [~iamgd67];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated Count and SampledTotal in 3.0,KAFKA-12880,13381703,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,ijuma,ijuma,02/Jun/21 13:26,08/Jul/21 22:30,13/Jul/23 09:17,08/Jul/21 22:30,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,,,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-06-02 13:26:45.0,,,,,,,,,,"0|z0rl74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compatibility break in Admin.listOffsets(),KAFKA-12879,13381643,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,tombentley,tombentley,02/Jun/21 08:59,24/Mar/22 21:45,13/Jul/23 09:17,10/Mar/22 21:21,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.4,2.7.3,2.8.2,3.0.2,3.1.1,3.2.0,,admin,,,,,0,,,,,"KAFKA-12339 incompatibly changed the semantics of Admin.listOffsets(). Previously it would fail with {{UnknownTopicOrPartitionException}} when a topic didn't exist. Now it will (eventually) fail with {{TimeoutException}}. It seems this was more or less intentional, even though it would break code which was expecting and handling the {{UnknownTopicOrPartitionException}}. A workaround is to use {{retries=1}} and inspect the cause of the {{TimeoutException}}, but this isn't really suitable for cases where the same Admin client instance is being used for other calls where retries is desirable.

Furthermore as well as the intended effect on {{listOffsets()}} it seems that the change could actually affect other methods of Admin.

More generally, the Admin client API is vague about which exceptions can propagate from which methods. This means that it's not possible to say, in cases like this, whether the calling code _should_ have been relying on the {{UnknownTopicOrPartitionException}} or not.",,chia7712,ChrisEgerton,cmccabe,ijuma,kirktrue,rhauch,scholzj,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12339,,KAFKA-13770,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 10 21:14:57 UTC 2022,,,,,,,,,,"0|z0rku0:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"06/Jul/21 00:12;kirktrue;I'm trying to understand the reason to _*not*_ throw the {{UnknownTopicOrPartitionException}} if it determines that a topic doesn't exist 🤔

Are meta data operations in Kafka (e.g. topic creation) atomic across the cluster? Or is the admin client code retrying optimistically, hoping that the topic will soon be available across the cluster?;;;","06/Jul/21 00:17;kirktrue;Also, there's code elsewhere in Kafka that will perform retries but note any {{UnknownTopicOrPartitionException}} to throw if all of the retries are exhausted:

[https://github.com/apache/kafka/blob/trunk/trogdor/src/main/java/org/apache/kafka/trogdor/common/WorkerUtils.java#L273]

 ;;;","07/Jul/21 00:23;kirktrue;Almost all of the {{Admin}} client's public API methods are meant to be consumed asynchronously. Most of the exceptions that can occur are thrown when the {{Future#get}} method is invoked, not when the {{Admin}} API call itself is invoked.

However, there are some exceptions (pun intended):
 # Creation of the client propagates errors if any occur during configuration
 # {{close}} throws an error if the given {{timeout}} is negative
 # {{deleteTopics}} throws an {{IllegalArgumentException}} if the {{TopicCollection}} parameter is not of an expected sub-class
 # {{updateFeatures}} throws an {{IllegalArgumentException}} if the the {{featureUpdates}} map is empty or contains a blank feature name

The above are just those that are thrown directly in the {{KafkaAdminClient}} itself.

Of the above list, item four seems like it stands out as being extra ""picky"" compared to, say, {{describeTransactions}}, which doesn't check the collection its given for emptiness.;;;","19/Jan/22 19:06;kirktrue;[~tombentley] - is there a specific way you're invoking {{{}listOffsets{}}}? Are you doing it via one of the scripts or a custom program? Just looking for the best way to reproduce. Thanks!;;;","21/Jan/22 01:27;cmccabe;cc [~rhauch], [~chia7712], [~kkonstantine];;;","25/Jan/22 17:51;rhauch;The original intent of [KAFKA-12339|https://issues.apache.org/jira/browse/KAFKA-12339]'s changes were to retry the `listOffsets(...)` if a retriable exception were thrown, as other methods within the AdminClient automatically handle retries. In hindsight, I should have sought clarification on that change, since [KIP-396|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97551484] that added `listOffsets(...)` was ambiguous about retries while [KIP-117|https://cwiki.apache.org/confluence/display/KAFKA/KIP-117%3A+Add+a+public+AdminClient+API+for+Kafka+admin+operations] that added `AdminClient` included automatic retry support.

Having said that, we need to decide whether to:
1. Revert the changes from [KAFKA-12339|https://issues.apache.org/jira/browse/KAFKA-12339] so that `listOffsets(...)` does not retry. IMO this would leave the `AdminClient` in a strange state where some methods retry and others don't, with no documentation about which methods do and do not retry. We would also have to change the Connect code that uses this to perform the retries, though that's doable.
2. Keep the changes from [KAFKA-12339|https://issues.apache.org/jira/browse/KAFKA-12339] so that `listOffset(...)` that does retry on retriable exceptions, but throws `UnknownTopicOrPartitionException` when the topic does not exist (after successive retries) rather than the timeout exception.
3. Keep as-is and simply better document the behavior, perhaps by making an addendum to KIP-396.

WDYT, [~mimaison], [~cmccabe], and others?;;;","01/Feb/22 21:23;ijuma;My suggestion is to do 3 and to make sure the cause of `TimeoutException` is `UnknownTopicOrPartitionException`. Since 2.5.2 and newer have the updated behavior, it's now more disruptive to change it back.;;;","01/Feb/22 21:28;cmccabe;Let me give a little context here on the behavior.

The Producer and Consumer typically retry most operations if the partition in question doesn't exist. The thinking there is that if the user specified they want to consume from topic foo-0, they knew what they were doing, and we should just wait for foo-0 to appear. This is particularly useful because Kafka has eventually consistent metadata -- even after creating a topic, it may take a few seconds for every broker to become aware of the new topic.

For the AdminClient, we usually don't retry if a topic doesn't exist. For example, if you try to delete a topic, we don't loop forever if the topic doesn't exist -- we just return UNKNOWN_TOPIC_OR_PARTITION immediately. You could view this as inconsistent, but being consistent with Producer / Consumer here would result in a somewhat useless API. People do not want their topic deletes to take a long time and then fail with TimeoutException if the topic doesn't exist.

I would argue that listOffsets is more similar to the second case here. It's very rare that you would be invoking listOffsets on a partition that had just been created. Looping forever if the partition doesn't exist isn't really a useful behavior in most scenarios. It seems like Connect has a use case for this -- since Connect knows for sure that the topic exists (or will exist), it should do the retries itself, rather than pushing this into AdminClient.

So I would argue we should just revert the change.

Also, as to the ""without documentation"" part -- we do make an effort to document the exceptions admin methods can throw. We're missing a lot of them (PRs would be very welcome here!) For example, listPartitionReassignments documents that it can return UnknownTopicOrPartitionException, ClusterAuthorizationException, TimeoutException, etc. If we revert the change, we should also add this kind of documentation to the listOffsets function.;;;","01/Feb/22 21:33;cmccabe;bq. My suggestion is to do 3 and to make sure the cause of `TimeoutException` is `UnknownTopicOrPartitionException`. Since 2.5.2 and newer have the updated behavior, it's now more disruptive to change it back.

I think this is still quite harmful for people who want to build tools to poll partition offsets. It's really a very disruptive behavior to time out, which could take several minutes and many retries.

I almost would suggest adding an option for this... but I just think the retry behavior is wrong and not consistent with how most admin operations work.;;;","01/Feb/22 22:31;ijuma;[~cmccabe] It's straightforward to do `describe topics` before `list offsets` if you want to check if a topic exists or not, right? It is not clear to me that calling `listOffsets` on a topic that was recently created is that rare. Do we have evidence for that?;;;","09/Mar/22 21:48;ChrisEgerton;Are there any practical use cases for listing the offsets of a just-created topic? Are any of these use cases more likely than ones that would involve describing a just-created topic?

It seems a little heavy-handed to suggest to users that they invoke {{Admin::describeTopics}} before {{Admin::listOffsets}} in order to handle non-existing topics, at least if this pattern hasn't already been documented as a best practice for people using the Java admin client.

Preserving existing behavior (which IMO is valid for the reasons Colin has laid out) seems like the correct move here.;;;","09/Mar/22 22:37;rhauch;The approach we decided to take was to revert the previous admin client changes from KAFKA-12339 to bring the admin client behavior back to previous expectations, and to implement retries within the KafkaBasedLog to handle cases like those identified in that issue.

For example, a likely root cause of KAFKA-12339 was a Connect worker instantiates its KafkaConfigBackingStore (and other internal topic stores), which creates a KafkaBasedLog that as part of start() creates the topic if it doesn't exist and then immediately tries to read the offsets. That reading of offsets can fail if the metadata for the newly created topic hasn't been propagated to all of the brokers. We can solve this particular root cause easily by retrying the reading of offsets within the KafkaBasedLog's start() method, and since topic metadata should be propagated relatively quickly, we don't need to retry for that long – and most of the time we'd probably successfully retry within a few retries.

I've just merged to trunk a PR that does this. When trying to backport this, some of the newer tests were flaky, so [~pnee] created another PR (plus another) to hopefully eliminate that flakiness, and it seemed to work. 

I'm in the process of backporting this all the way back to 2.6 -2.5- branch, since that's how far back the regression from KAFKA-12339 was backported.;;;","10/Mar/22 21:14;rhauch;Update: I'm having trouble with backporting the Connect changes to the 2.5 branch. Because this branch is so old, I'm going to just revert the AdminClient behavior to the 2.5 branch, and _NOT_ backport the Connect retry changes.

Note: the break due to KAFKA-12339 was never released in the 2.5 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecordAccumulator stuck in a flushing state,KAFKA-12870,13381410,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,niclaslockner,niclaslockner,01/Jun/21 09:17,18/Jun/21 23:03,13/Jul/23 09:17,18/Jun/21 22:56,2.5.1,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,producer ,streams,,,,0,,,,,"After a Kafka Stream with exactly once enabled has performed its first commit, the RecordAccumulator within the stream's internal producer gets stuck in a state where all subsequent ProducerBatches that get allocated are immediately flushed instead of being held in memory until they expire, regardless of the stream's linger or batch size config.

This is reproduced in the example code found at [https://github.com/niclaslockner/kafka-12870] which can be run with ./gradlew run --args=<bootstrap servers>

The example has a producer that sends 1 record/sec to one topic, and a Kafka stream with EOS enabled that forwards the records from that topic to another topic with the configuration linger = 5 sec, commit interval = 10 sec.

 

The expected behavior when running the example is that the stream's ProducerBatches will expire (or get flushed because of the commit) every 5th second, and that the stream's producer will send a ProduceRequest every 5th second with an expired ProducerBatch that contains 5 records.

The actual behavior is that the ProducerBatch is made immediately available for the Sender, and the Sender sends one ProduceRequest for each record.

 

The example code contains a copy of the RecordAccumulator class (copied from kafka-clients 2.8.0) with some additional logging added to
 * RecordAccumulator#ready(Cluster, long)
 * RecordAccumulator#beginFlush()
 * RecordAccumulator#awaitFlushCompletion()

These log entries show (see the attached RecordsAccumulator.log)
 * that the batches are considered sendable because a flush is in progress
 * that Sender.maybeSendAndPollTransactionalRequest() calls RecordAccumulator's beginFlush() without also calling awaitFlushCompletion(), and that this makes RecordAccumulator's flushesInProgress jump between 1-2 instead of the expected 0-1.

 

This issue is not reproducible in version 2.3.1 or 2.4.1.

 

 ",,ableegoldman,dengziming,guozhang,hachikuji,ijuma,mjsax,niclaslockner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/21 09:30;niclaslockner;RecordAccumulator.log;https://issues.apache.org/jira/secure/attachment/13026233/RecordAccumulator.log","01/Jun/21 09:30;niclaslockner;full.log;https://issues.apache.org/jira/secure/attachment/13026232/full.log",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 23:03:42 UTC 2021,,,,,,,,,,"0|z0rjeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/21 16:26;mjsax;Could this be related to https://issues.apache.org/jira/browse/KAFKA-10888 ?;;;","10/Jun/21 14:10;ijuma;I think the claim is that there's a bug in the `Sender` when exactly-once is used.;;;","18/Jun/21 18:36;guozhang;This is a great find. Though it might not related to KAFKA-10888, I think it could help resolving the perf degradation with EOS (since the bug effectively disables any batching).;;;","18/Jun/21 23:03;hachikuji;I agree this was a great find. Thanks also for the detailed investigation [~niclaslockner]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trogdor ConsumeBenchWorker quits prematurely with maxMessages config,KAFKA-12867,13381230,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kprakasam,kprakasam,kprakasam,31/May/21 07:53,02/Jun/21 18:33,13/Jul/23 09:17,02/Jun/21 18:33,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The trogdor [ConsumeBenchWorker|https://github.com/apache/kafka/commits/fc405d792de12a50956195827eaf57bbf64444c9/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java] has a bug. If one of the consumption tasks completes executing successfully due to [maxMessages being consumed|https://github.com/apache/kafka/blob/fc405d792de12a50956195827eaf57bbf64444c9/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java#L245], then, the consumption task [notifies the doneFuture|https://github.com/apache/kafka/blob/fc405d792de12a50956195827eaf57bbf64444c9/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java#L285] causing the ConsumeBenchWorker to halt. This becomes a problem when more than 1 consumption task is running in parallel, because the successful completion of 1 of the tasks shuts down the entire worker while the other tasks are still running. When the worker is shut down, it [kills|https://github.com/apache/kafka/blob/fc405d792de12a50956195827eaf57bbf64444c9/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java#L482] all the active consumption tasks, which is not the desired behavior.

The fix is to not notify the doneFuture when 1 of the consumption tasks complete without error. Instead, we should defer the notification to the [CloseStatusUpdater|https://github.com/apache/kafka/blob/fc405d792de12a50956195827eaf57bbf64444c9/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java#L299] thread.",,kprakasam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-31 07:53:53.0,,,,,,,,,,"0|z0ribs:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka requires ZK root access even when using a chroot,KAFKA-12866,13381158,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,soarez,soarez,soarez,30/May/21 14:34,01/Jun/21 13:41,13/Jul/23 09:17,01/Jun/21 13:41,2.6.1,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,zkclient,,,,0,,,,,"When a Zookeeper chroot is configured, users do not expect Kafka to need Zookeeper access outside of that chroot.
h1. Why is this important?

A zookeeper cluster may be shared with other Kafka clusters or even other applications. It is an expected security practice to restrict each cluster/application's access to it's own Zookeeper chroot.
h1. Steps to reproduce
h2. Zookeeper setup

Using the zkCli, create a chroot for Kafka, make it available to Kafka but lock the root znode.

 
{code:java}
[zk: localhost:2181(CONNECTED) 1] create /somechroot
Created /some
[zk: localhost:2181(CONNECTED) 2] setAcl /somechroot world:anyone:cdrwa
[zk: localhost:2181(CONNECTED) 3] addauth digest test:12345
[zk: localhost:2181(CONNECTED) 4] setAcl / digest:test:Mx1uO9GLtm1qaVAQ20Vh9ODgACg=:cdrwa{code}
 
h2. Kafka setup

Configure the chroot in broker.properties:

 
{code:java}
zookeeper.connect=localhost:2181/somechroot{code}
 

 
h2. Expected behavior

The expected behavior here is that Kafka will use the chroot without issues.
h2. Actual result

Kafka fails to start with a fatal exception:
{code:java}
    org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /chroot
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:120)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
        at kafka.zookeeper.AsyncResponse.maybeThrow(ZooKeeperClient.scala:583)
        at kafka.zk.KafkaZkClient.createRecursive(KafkaZkClient.scala:1729)
        at kafka.zk.KafkaZkClient.makeSurePersistentPathExists(KafkaZkClient.scala:1627)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1957)
        at kafka.zk.ZkClientAclTest.testChrootExistsAndRootIsLocked(ZkClientAclTest.scala:60)
{code}
 

 ",,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-30 14:34:50.0,,,,,,,,,,"0|z0rhw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation error for Admin Client API in describe ACLs,KAFKA-12865,13381084,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sachan.rohit@gmail.com,sachan.rohit@gmail.com,sachan.rohit@gmail.com,29/May/21 07:19,29/May/21 11:34,13/Jul/23 09:17,29/May/21 11:32,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,clients,,,,,0,,,,,"There is a documentation bug in *Admin.java's* `describeAcls` and its overloaded variation, function's return type shows `*DeleteAclsResult*` instead of `*DescribeAclResult*`. ",,sachan.rohit@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-29 07:19:58.0,,,,,,,,,,"0|z0rhfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Jackson to 2.12.3,KAFKA-12856,13380790,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,ijuma,ijuma,27/May/21 14:19,27/May/21 17:08,13/Jul/23 09:17,27/May/21 17:08,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"2.10.x is no longer supported, so we should move to 2.12 for the 3.0 release.",,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-27 14:19:28.0,,,,,,,,,,"0|z0rfm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test RaftEventSimulationTest.canMakeProgressIfMajorityIsReachable,KAFKA-12851,13380597,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,ableegoldman,ableegoldman,26/May/21 17:33,28/Jul/21 16:29,13/Jul/23 09:17,28/Jul/21 16:29,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,kraft,,,,0,kip-500,,,,"Failed twice on a [PR build|https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-10755/6/testReport/]
h3. Stacktrace

org.opentest4j.AssertionFailedError: expected: <true> but was: <false> at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40) at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:35) at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:162) at org.apache.kafka.raft.RaftEventSimulationTest.canMakeProgressIfMajorityIsReachable(RaftEventSimulationTest.java:263)",,ableegoldman,jagsancio,loboxu,meher_crok,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/21 13:31;meher_crok;Capture.PNG;https://issues.apache.org/jira/secure/attachment/13030427/Capture.PNG",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 26 15:44:18 UTC 2021,,,,,,,,,,"0|z0refk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/21 00:50;mjsax;Failed again.;;;","11/Jul/21 13:31;meher_crok;it s working fine  !Capture.PNG!;;;","24/Jul/21 23:35;jagsancio;Thanks for the report and screenshot with the seed. Next time please include at least the seed in the report. The seed can be used to deterministically reproduce the failure.

The seed is {{-2333626563705276482}}.;;;","24/Jul/21 23:44;jagsancio;Test passes with that seed against trunk. Testing before the commit that I think fixed this.
{code:java}
Gradle Test Executor 3 STANDARD_ERROR
    Jul 24, 2021 4:40:13 PM org.junit.platform.launcher.core.EngineDiscoveryOrchestrator lambda$logTestDescriptorExclusionReasons$7
    INFO: 0 containers and 6 tests were Method or class mismatch

RaftEventSimulationTest > canMakeProgressIfMajorityIsReachable STANDARD_OUT
    timestamp = 2021-07-24T16:40:24.928, RaftEventSimulationTest:canMakeProgressIfMajorityIsReachable =
                                  |-------------------jqwik-------------------
    tries = 100                   | # of calls to property
    checks = 100                  | # of not rejected calls
    generation = RANDOMIZED       | parameters are randomly generated
    after-failure = SAMPLE_ONLY   | only try the previously failed sample
    when-fixed-seed = ALLOW       | fixing the random seed is allowed
    edge-cases#mode = MIXIN       | edge cases are mixed in
    edge-cases#total = 36         | # of all combined edge cases
    edge-cases#tried = 15         | # of edge cases tried in current run
    seed = -2333626563705276482   | random seed to reproduce generated values

Gradle Test Executor 3 finished executing tests.
 {code};;;","24/Jul/21 23:59;jagsancio; The seed in the PR build
{code:java}
timestamp = 2021-05-26T04:37:01.424183, RaftEventSimulationTest:canMakeProgressIfMajorityIsReachable = 
  org.opentest4j.AssertionFailedError:
    expected: <true> but was: <false>

                              |-------------------jqwik-------------------
tries = 3                     | # of calls to property
checks = 3                    | # of not rejected calls
generation = RANDOMIZED       | parameters are randomly generated
after-failure = PREVIOUS_SEED | use the previous seed
when-fixed-seed = ALLOW       | fixing the random seed is allowed
edge-cases#mode = MIXIN       | edge cases are mixed in
edge-cases#total = 36         | # of all combined edge cases
edge-cases#tried = 0          | # of edge cases tried in current run
seed = 137014923570865933     | random seed to reproduce generated values

Sample
------
  arg0: 807
  arg1: 2 {code};;;","25/Jul/21 00:03;jagsancio;It still fails against trunk
{code:java}
RaftEventSimulationTest > canMakeProgressIfMajorityIsReachable STANDARD_OUT
    timestamp = 2021-07-24T17:01:57.485, RaftEventSimulationTest:canMakeProgressIfMajorityIsReachable =
      org.opentest4j.AssertionFailedError:
        expected: <true> but was: <false>

                                  |-------------------jqwik-------------------
    tries = 1                     | # of calls to property
    checks = 1                    | # of not rejected calls
    generation = RANDOMIZED       | parameters are randomly generated
    after-failure = SAMPLE_ONLY   | only try the previously failed sample
    when-fixed-seed = ALLOW       | fixing the random seed is allowed
    edge-cases#mode = MIXIN       | edge cases are mixed in
    edge-cases#total = 0          | # of all combined edge cases
    edge-cases#tried = 0          | # of edge cases tried in current run
    seed = 137014923570865933     | random seed to reproduce generated values

    Sample
    ------
      arg0: 807
      arg1: 2

org.apache.kafka.raft.RaftEventSimulationTest.canMakeProgressIfMajorityIsReachable failed, log available in /home/jsancio/work/kafka/raft/build/reports/testOutput/org.apache.kafka.raft.RaftEventSimulationTest.canMakeProgressIfMajorityIsReachable.test.stdout

RaftEventSimulationTest > canMakeProgressIfMajorityIsReachable FAILED
    org.opentest4j.AssertionFailedError: expected: <true> but was: <false>
        at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
        at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40)
        at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:35)
        at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:162)
        at org.apache.kafka.raft.RaftEventSimulationTest.canMakeProgressIfMajorityIsReachable(RaftEventSimulationTest.java:264)1 test completed, 1 failed
 {code};;;","26/Jul/21 15:44;jagsancio;The issue is with the test and not with the Raft implementation. At a high level the test runs with 5 voters until it reaches at least a high watermark of 10. At this point it partition the network so that 3 nodes cannot send request to 2 nodes and vice versa. With the seed {{137014923570865933}} right before the partition the nodes have the following state:
{code:java}
Node(id=0, hw=14, logEndOffset=25)
Node(id=1, hw=10, logEndOffset=14)
Node(id=2, hw=10, logEndOffset=14)
Node(id=3, hw=10, logEndOffset=22)
Node(id=4, hw=10, logEndOffset=14)
Node(id=5, hw=10, logEndOffset=14)
Node(id=6, hw=6, logEndOffset=18){code}
Nodes 5 and 6 are observers and do not participate in quorum or hw values. Notices that two nodes have a log end offset greater than 20 (0 and 3).

The tests now partitions the network so that nodes 0, 1 can send request to each other and nodes 2, 3, 4 can send request to each other. That means that only node 1 needs to reach offset 20 before the election timeout so that the leader 0 can advance the high watermark.
{code:java}
Node(id=0, hw=22, logEndOffset=34)
Node(id=1, hw=18, logEndOffset=22)
Node(id=2, hw=14, logEndOffset=18)
Node(id=3, hw=10, logEndOffset=25)
Node(id=4, hw=14, logEndOffset=18)
Node(id=5, hw=18, logEndOffset=25)
Node(id=6, hw=18, logEndOffset=29){code}
 

I think that the best way to fix the test, after the partition, is to wait for the high-watermark to reach a value must larger than the LEO before the partition. In the trace above it would be a value much greater than 25.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from the provided metadata in client callback in case of ApiException,KAFKA-12841,13380072,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,ayouk,ayouk,24/May/21 11:08,27/Apr/22 08:19,13/Jul/23 09:17,26/Apr/22 18:07,2.6.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,clients,,,,,0,,,,,"1.
org.apache.kafka.clients.producer.Callback interface has method onCompletion(...)
Which says as part of the documentation :

*The metadata for the record that was sent (i.e. the partition and offset). *An empty metadata with -1 value for all fields* except for topicPartition will be returned if an error occurred.


We got an NPE from doSend(...) method in org.apache.kafka.clients.producer.KafkaProducer 
Which can occur in case ApiException was thrown ...
In case of ApiException it uses the regular callback instead of InterceptorCallback which also may cover the NPE.

2. More over RecordMetadata has method partition() which return int but can also throw NPE because TopicPartition might be null.

Stack trace attached.

 ",Prod,ayouk,junrao,kirktrue,pnee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13448,,,,,,,,,,,,,,,,,,,"24/May/21 11:07;ayouk;NPE.production;https://issues.apache.org/jira/secure/attachment/13025849/NPE.production",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Tue Apr 26 18:07:07 UTC 2022,,,,,,,,,,"0|z0rb74:",9223372036854775807,,showuon,,,,,,,,,,,,,,,,,,"30/Jun/21 22:08;kirktrue;I'd like to take a stab at this bug. Can someone kindly assign it to me? TIA.;;;","01/Jul/21 18:20;kirktrue;There are some conditions inside the {{KafkaProducer}}'s {{doSend}} method that will result in an {{ApiException}} being thrown before the {{TopicPartition}} ({{tp}}) is created. As a result, {{tp}} is null and case #2 listed in the initial bug report occurs.

Would it be OK to create a new, ""dummy"" {{TopicPartition}} in this case? We could use the topic from the {{ProducerRecord}}. But perhaps the partition from the {{ProducerRecord}}, if non-null, can be used? Or should we just set the partition to `-1`?;;;","01/Jul/21 18:31;ayouk;[~kirktrue] This is what i did in this [[PR|https://github.com/apache/kafka/pull/10728]].
 Might be the right approach is to create the tp with default values;;;","01/Jul/21 19:01;kirktrue;I have submitted a [pull request|https://github.com/apache/kafka/pull/10951] for review.;;;","26/Apr/22 18:07;junrao;merged [https://github.com/apache/kafka/pull/11689] and a followup fix [https://github.com/apache/kafka/pull/12064] to trunk and 3.2 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic IDs can mismatch on brokers (after interbroker protocol version update),KAFKA-12835,13379791,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,ivanyu,ivanyu,21/May/21 14:36,30/Aug/22 18:25,13/Jul/23 09:17,18/Jun/21 16:08,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,core,,,,,0,,,,,"We had a Kafka cluster running 2.8 version with interbroker protocol set to 2.7. It had a number of topics and everything was fine.
Then we decided to update the interbroker protocol to 2.8 by the following procedure:
1. Run new brokers with the interbroker protocol set to 2.8.
2. Move the data from the old brokers to the new ones (normal partition reassignment API).
3. Decommission the old brokers.

At the stage 2 we had the problem: old brokers started failing on {{LeaderAndIsrRequest}} handling with
{code:java}
ERROR [Broker id=<...>] Topic Id in memory: <...> does not match the topic Id for partition <...> provided in the request: <...>. (state.change.logger)
{code}
for multiple topics. Topics were not recreated.

We checked {{partition.metadata}} files and IDs there were indeed different from the values in ZooKeeper. It was fixed by deleting the metadata files (and letting them be recreated).

 


The logs, unfortunately, didn't show anything that might point to the cause of the issue (or it happened longer ago than we store the logs).

We tried to reproduce this also, but no success.

If the community can point out what to check or beware of in future, it will be great. We'll be happy to provide additional information if needed. Thank you! 

Sorry for the ticket that might be not very actionable. We hope to at least rise awareness of this issue.

 ",,ableegoldman,ivanyu,jolshan,mrMigles,polaris.alioth,suriyav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 18:25:12 UTC 2022,,,,,,,,,,"0|z0r9gw:",9223372036854775807,,dajac,,,,,,,,,,,,,,,,,,"21/May/21 15:56;jolshan;Hi [~ivanyu]. Thanks for pointing this out.
I was curious about your upgrade process. Is there a reason you moved from old brokers to new brokers rather than doing a rolling restart of the same brokers? (Described in the documentation here: https://kafka.apache.org/documentation/#upgrade_2_7_0);;;","21/May/21 21:00;jolshan;I believe I found the cause to this issue and will be working on a fix.;;;","24/May/21 09:57;ivanyu;bq. I was curious about your upgrade process. Is there a reason you moved from old brokers to new brokers rather than doing a rolling restart of the same brokers?

We try to keep machines immutable, nothing specific in Kafka itself.

bq. I believe I found the cause to this issue and will be working on a fix.

Great to hear this! Could you please the idea at the higher level?;;;","24/May/21 23:14;jolshan;Hi [~ivanyu]. Previously we could lose topic IDs in the Znode when reassigning partitions. This could occur if we switched between controllers with IBP 2.8 back to 2.7 (where we reassign partitions) and back to 2.8. If the partition.metadata file still existed, we would have the old ID in that, but the new controller would see a topic ID missing in the ZNode and assign a new one. 

I've opened a PR to prevent this loss of topic ID regardless of the IBP of the controller.;;;","18/Jun/22 07:29;polaris.alioth;[~jolshan] Hello, I find an error similar to this issue mentioned below when I update my kafka from 2.7.0 to 2.8.1.

[2022-06-17 09:34:56,599] ERROR [Broker id=0] Topic Id in memory: PV_7diK7RJaS45KsEH4sng does not match the topic Id for partition __consumer_offsets-14 provided in the request: X_hrEUetShKyQ_RR_pEo9A. (state.change.logger)

I found that the topic id of __consumer_offsets in partition.metadata is PV_7diK7RJaS45KsEH4sng. And in zookeeper the topic id is X_hrEUetShKyQ_RR_pEo9A.

Then I delete all partition.metadata of topic __consumer_offsets, restart kafka, and fix this probelm.

I try to recurrence this probelm the second time, but failed.

I want to ask in which situation will make the topic id of __consumer_offsets diffirent between partition.metadata and zookeeper. Because when topic __consumer_offsets have this problem, the consumer will not work completely. I had to make attention on it.

By the way, I found another situation recently, the leader of all partitions of __consumer_offsets always on the same broker.;;;","30/Aug/22 18:25;mrMigles;Hi All,

It seems this ticket already solved, but I'd like to add my case too.


We use Kafka 2.8.1, last upgrade from 2.7.1 to 2.8.1 was performed a few weeks ago. 
And yesterday we found that Kafka consumes all provided CPU, and a lot of Kafka clients don't work and failed by timeout. Default logs didn't show any useful information, but in state-change logs we found a lot of rows with:


{code:java}
[2022-08-30 17:33:19,804] ERROR [Broker id=3] Topic Id in memory: n1ld3W-pSMqWMS5ceX_v-Q does not match the topic Id for partition __consumer_offsets-14 provided in the request: LzjEgQA5Sw29x-cayfXYEw. (state.change.logger){code}

for different topics, but in the most part for __consumer_offsets. We checked and yes, topic_id was different for ""patition.metadata"" on broker side and zookeeper. 

Also we found, that a lof of topics were in under ISR status, for example:
{code:java}
Topic: __consumer_offsets       TopicId: LzjEgQA5Sw29x-cayfXYEw PartitionCount: 50      ReplicationFactor: 3    Configs: compression.type=producer,min.insync.replicas=2,cleanup.policy=compact,segment.bytes=104857600,max.message.bytes=10000000,retention.bytes=1610612735
        Topic: __consumer_offsets       Partition: 0    Leader: 1       Replicas: 1,3,2 Isr: 1
        Topic: __consumer_offsets       Partition: 1    Leader: 3       Replicas: 2,1,3 Isr: 3
        Topic: __consumer_offsets       Partition: 2    Leader: 1       Replicas: 3,2,1 Isr: 1
        Topic: __consumer_offsets       Partition: 3    Leader: 1       Replicas: 1,2,3 Isr: 1
        Topic: __consumer_offsets       Partition: 4    Leader: 3       Replicas: 2,3,1 Isr: 3
        Topic: __consumer_offsets       Partition: 5    Leader: 1       Replicas: 3,1,2 Isr: 1
        Topic: __consumer_offsets       Partition: 6    Leader: 2       Replicas: 1,3,2 Isr: 2
....
        Topic: __consumer_offsets       Partition: 48   Leader: 2       Replicas: 1,3,2 Isr: 2
        Topic: __consumer_offsets       Partition: 49   Leader: 3       Replicas: 2,1,3 Isr: 3{code}
and this didn't change for a lot of time. 


While Kafka was loaded in max CPU it couldn't answer clients correctly, especially consumers, because offsets topic was under ISR we think. So, all related to Kafka services are stuck.

Thanks to this ticket, we solved a part of issues with removing ""partition.metadata"" and restart cluster. But Kafka still feels bad and some of partitions still have only one ISR with issue ""Non-monotonic update of high watermark"". It looks like another issue KAFKA-13077

Is it expected, that we got this issue in 2.8.1 Kafka server?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTable.transformValue might have incorrect record metadata,KAFKA-12815,13379376,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,mjsax,mjsax,19/May/21 21:48,16/Jun/21 16:57,13/Jul/23 09:17,19/May/21 22:11,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,"In the DSL, Kafka Streams applies an optimization for non-materialized tables: when these are queried an upstream state store is accessed. To ensure that the correct value is returned from the lookup, all intermediate processors after the materialized store, and before the processor that triggers the lookup are re-applied (cf `KTableValueGetter`).

For re-applying DSL operators like filter/mapValues that works fine. However, for transformValue(), the method is executed with the incorrect `RecordContext` (note that DSL operators like filter don't have access to the `RecordContext` and thus, are not subject to this bug). Instead of using the record context from the value that was received from the upstream state store (and that is re-processed), the transformer would see the context from the record that triggered the lookup.

Thus, the information about timestamp, offset, partition, topic name, and headers is incorrect.",,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8377,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 17:23:11 UTC 2021,,,,,,,,,,"0|z0r6wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/21 22:41;ableegoldman;[~mjsax] should we fix back to 2.8 at least? If it's a critical bug for FKJ or potentially any custom transformValues on a KTable, we should consider cherrypicking it back at least one version.;;;","20/May/21 00:45;guozhang;For the longer term, I feel that we either need to 1) persist the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to opt-in / opt-out if they know the risks of calling context.topic() / offset(). Personally, I'd lean towards 2). [~mjsax] maybe we should create a new ticket open for this longer term solution?;;;","20/May/21 04:36;mjsax;[~ableegoldman] This bug does not affect FK-joins. I am actually also not sure, how many users would actually user `KTable.transformValues` _and_ use the record context metadata (I would assume that if people to this, we would have gotten a bug report from them). Don't have a strong opinion on back-porting to `2.8`, but I don't think that it's a critical bug, so maybe we can just leave it as-is?

For the long term, I am not sure either – I think it's ok to stay passive and only improve if users really start to raise it as an issue. Note, that the `RecordContext` JavaDocs already state that record information might not be available. So it's not a regression we are introducing with this fix, but it's a real fix, because we omit missing information instead of passing in incorrect information.;;;","20/May/21 05:49;guozhang;Yup, we are definitely fixing forward rather than regressing here as previously people would just get the incorrect result silently. As for the longer term, I kinda also agree that we can wait and see how large of an impact it would be to not have topic/offsets be available; but what I'm pointing out is that this is not an impossible task to do, we just need to be careful about our trade-offs here. For some advanced users that may be using `transformValues` heavily but do not leverage on topic/offsets, at least we should allow them to choose based on their own knowledge.;;;","20/May/21 05:55;mjsax;Well, user can still force to materialize the result of `transformValues()` to avoid this issue. Or they can add topic/offset into the value to preserve the information.;;;","20/May/21 17:23;ableegoldman;Ack, sorry I made a bad assumption about how FKJ are implemented internally. I wasn't the one to review them

Given the limited scope of this bug I would say it feels appropriate to just fix in trunk for now, and file a followup ticket pointing out this bug/as an improvement to tighten up the processor context contract w.r.t topic() and offset(). If there are users currently out there for whom this is an essential feature, it will be pretty obvious once their apps start failing with an NPE, and they will probably let us know. We can re-evaluate whether to do a ""full fix"" if/when user reports come in;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix metrics bug and introduce TimelineInteger,KAFKA-12792,13378632,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,16/May/21 06:07,09/Jul/21 03:09,13/Jul/23 09:17,09/Jul/21 03:09,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,Introduce a TimelineInteger class which represents a single integer value which can be changed while maintaining snapshot consistency.  Fix a case where a metric value would be corrupted after a snapshot restore.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-16 06:07:11.0,,,,,,,,,,"0|z0r2bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in KafkaProducer constructor,KAFKA-12791,13378608,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,lucasbradstreet,lucasbradstreet,15/May/21 14:47,29/Nov/21 05:05,13/Jul/23 09:17,26/Nov/21 19:44,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"Recently we have noticed multiple instances where KafkaProducers have failed to constructe due to the following exception:
{noformat}
org.apache.kafka.common.KafkaException: Failed to construct kafka producer at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440) at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291) at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318) java.base/java.lang.Thread.run(Thread.java:832) Caused by: java.util.ConcurrentModificationException at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1584) at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1607) at java.base/java.util.AbstractSet.removeAll(AbstractSet.java:171) at org.apache.kafka.common.config.AbstractConfig.unused(AbstractConfig.java:221) at org.apache.kafka.common.config.AbstractConfig.logUnused(AbstractConfig.java:379) at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:433) ... 9 more exception.class:org.apache.kafka.common.KafkaException exception.message:Failed to construct kafka producer
{noformat}

It appears that this is due to the fact that `used` below is a synchronized set:



 
{code:java}
public Set<String> unused() {
 Set<String> keys = new HashSet<>(originals.keySet());
 keys.removeAll(used);
 return keys;
}{code}
It appears that `used` is being modified while removeAll is being called. This may be due to the way that keys are added to it when used:


{code:java}
protected Object get(String key) {
 if (!values.containsKey(key))
 throw new ConfigException(String.format(""Unknown configuration '%s'"", key));
 used.add(key);
 return values.get(key);
}{code}
 ",,lucasbradstreet,tuk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 29 05:05:52 UTC 2021,,,,,,,,,,"0|z0r260:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/21 11:03;tuk;The fix is released in Kafka 3.0. Can this be resolved now?

https://github.com/apache/kafka/blob/3.0.0/clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java;;;","29/Nov/21 05:05;tuk;Can this bug be added in Kafka 3.0 release notes also? The bug is missing in release notes as of now. 

https://downloads.apache.org/kafka/3.0.0/RELEASE_NOTES.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector hangs in distributed mode without logs,KAFKA-12785,13378471,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dishka_krauch,dishka_krauch,14/May/21 11:34,29/May/21 11:25,13/Jul/23 09:17,29/May/21 11:25,2.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,consumer,KafkaConnect,producer ,,,0,newbie,,,,"I've faced strage kafka connector attitude using distributed mode trying to transfer data from Teradata using JDBC to partquet using HDFS. It's done very well with standalone mode but not with distributed.

I have really small cluster with 3 nodes (virtual machines).

Here are settings:

*Server 1 server.properties:*

 
{code:java}
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.
broker.id=0############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for ""listeners"" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
advertised.listeners=PLAINTEXT://10.8.151.70:9092
# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600
############################# Log Basics ############################## A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics ""__consumer_offsets"" and ""__transaction_state""
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. ""127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
#zookeeper.connect=localhost:2181
zookeeper.connect=10.8.151.70:2181,10.8.151.71:2181,10.8.151.72:2181# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000
############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0

{code}
 

*Server 2 server.properties:*
{code:java}
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.
broker.id=1############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for ""listeners"" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092
advertised.listeners=PLAINTEXT://10.8.151.71:9092# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600
############################# Log Basics ############################## A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics ""__consumer_offsets"" and ""__transaction_state""
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. ""127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=localhost:2181
zookeeper.connect=hadoop-dn-02.corp.tander.ru:2181# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000
############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0

{code}
*Server 3 server.properties:*
{code:java}
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.
broker.id=2############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for ""listeners"" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092
advertised.listeners=PLAINTEXT://10.8.151.72:9092
# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600
############################# Log Basics ############################## A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics ""__consumer_offsets"" and ""__transaction_state""
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. ""127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
#zookeeper.connect=localhost:2181
#zookeeper.connect=hadoop-master-1.corp.tander.ru:2181
zookeeper.connect=hadoop-dn-03.corp.tander.ru:2181
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000
############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0

{code}
 

The jdbc drivers for connector and all .properties files are placed at each node, here are settings:

*connect-distributed.properties:*
{code:java}
##
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
### This file contains some of the configurations for the Kafka Connect distributed worker. This file is intended
# to be used with the examples, and some settings may differ from those used in a production system, especially
# the `bootstrap.servers` and those specifying replication factors.# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
bootstrap.servers=localhost:9092# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
group.id=connect-cluster# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
# need to configure these based on the format they want their data in when loaded from or stored into Kafka
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply
# it to
key.converter.schemas.enable=true
value.converter.schemas.enable=true# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
# the topic before starting Kafka Connect if a specific topic configuration is needed.
# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
offset.storage.topic=connect-offsets
offset.storage.replication.factor=1
#offset.storage.partitions=25# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
# and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
# the topic before starting Kafka Connect if a specific topic configuration is needed.
# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
config.storage.topic=connect-configs
config.storage.replication.factor=1# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
# the topic before starting Kafka Connect if a specific topic configuration is needed.
# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
status.storage.topic=connect-status
status.storage.replication.factor=1
#status.storage.partitions=5# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000# These are provided to inform the user about the presence of the REST host and port configs 
# Hostname & Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.
#rest.host.name=
#rest.port=8083# The Hostname & Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.
#rest.advertised.host.name=
#rest.advertised.port=# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
# (connectors, converters, transformations). The list should consist of top level directories that include 
# any combination of: 
# a) directories immediately containing jars with plugins and their dependencies
# b) uber-jars with plugins and their dependencies
# c) directories immediately containing the package directory structure of classes of plugins and their dependencies
# Examples: 
# plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,
plugin.path=/opt/kafka-connect/plugins
{code}
 

*connect-teradata-source-distributed.properties:*
{code:java}
name=teradata-source-test-distributed
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=100
connection.url=jdbc:teradata://server/DATABASE=DB,USER=USER,PASSWORD=PASSWORD,CHARSET=UTF16,TMODE=ANSI,LOGMECH=TD2
query=SELECT * FROM ANP.T_KAFKA_TEST
mode=timestamp
timestamp.column.name=SRC_CHANGE_DTM
batch.max.rows=10000
topic.prefix=topic-test-distributed
poll.interval.ms=1000
{code}
*connect-hdfs-sink-distributed.properties:*
{code:java}
name=hdfs-sink-test-distributed
connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
tasks.max=100
topics=topic-test-distributed
hdfs.url=hdfs://myserver.com:8020/stg/kafka
flush.size=1
rotate.interval.ms=30000
format.class=io.confluent.connect.hdfs.parquet.ParquetFormat
partitioner.class=io.confluent.connect.storage.partitioner.FieldPartitioner
parquet.codec=snappy
partition.field.name=ID
{code}
I've create topic like this:

*bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic topic-test-distributed*

 

Here is topic description:

!topic desc.png!

 

Then I see logs like this after starting connectors at each node:

 

*Node 1:*
{code:java}
[root@hadoop-dn-01 kafka]# bin/connect-distributed.sh config/connect-distributed.properties config/connect-teradata-source-distributed.properties config/connect-txt-sink-distributed.properties
[2021-05-14 12:49:39,655] INFO WorkerInfo values:
        jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/opt/kafka/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
        jvm.spec = Red Hat, Inc., OpenJDK 64-Bit Server VM, 1.8.0_282, 25.282-b08
        jvm.classpath = /opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.6.1.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-cli-1.4.jar:/opt/kafka/bin/../libs/commons-lang3-3.8.1.jar:/opt/kafka/bin/../libs/connect-api-2.8.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.8.0.jar:/opt/kafka/bin/../libs/connect-file-2.8.0.jar:/opt/kafka/bin/../libs/connect-json-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-client-2.8.0.jar:/opt/kafka/bin/../libs/connect-runtime-2.8.0.jar:/opt/kafka/bin/../libs/connect-transforms-2.8.0.jar:/opt/kafka/bin/../libs/hk2-api-2.6.1.jar:/opt/kafka/bin/../libs/hk2-locator-2.6.1.jar:/opt/kafka/bin/../libs/hk2-utils-2.6.1.jar:/opt/kafka/bin/../libs/jackson-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-core-2.10.5.jar:/opt/kafka/bin/../libs/jackson-databind-2.10.5.1.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-paranamer-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.12-2.10.5.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.1.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.5.jar:/opt/kafka/bin/../libs/jakarta.inject-2.6.1.jar:/opt/kafka/bin/../libs/jakarta.validation-api-2.0.2.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/opt/kafka/bin/../libs/javassist-3.27.0-GA.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.31.jar:/opt/kafka/bin/../libs/jersey-common-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.31.jar:/opt/kafka/bin/../libs/jersey-hk2-2.31.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.31.jar:/opt/kafka/bin/../libs/jersey-server-2.31.jar:/opt/kafka/bin/../libs/jetty-client-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-http-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-io-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-security-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-server-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-ajax-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jline-3.12.1.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0-sources.jar:/opt/kafka/bin/../libs/kafka-clients-2.8.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.8.0.jar:/opt/kafka/bin/../libs/kafka-metadata-2.8.0.jar:/opt/kafka/bin/../libs/kafka-raft-2.8.0.jar:/opt/kafka/bin/../libs/kafka-shell-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.8.0.jar:/opt/kafka/bin/../libs/kafka-tools-2.8.0.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.7.1.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.3.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-codec-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-handler-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.3.jar:/opt/kafka/bin/../libs/paranamer-2.8.jar:/opt/kafka/bin/../libs/plexus-utils-3.2.1.jar:/opt/kafka/bin/../libs/reflections-0.9.12.jar:/opt/kafka/bin/../libs/rocksdbjni-5.18.4.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.12-2.3.0.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.12-0.9.1.jar:/opt/kafka/bin/../libs/scala-library-2.12.13.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.2.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.13.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.30.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.30.jar:/opt/kafka/bin/../libs/zookeeper-3.5.9.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.5.9.jar:/opt/kafka/bin/../libs/zstd-jni-1.4.9-1.jar
        os.spec = Linux, amd64, 3.10.0-1160.24.1.el7.x86_64
        os.vcpus = 16
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-05-14 12:49:39,658] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:92)
[2021-05-14 12:49:39,673] INFO Loading plugin from: /opt/kafka-connect/plugins/kafka-connect-jdbc (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-05-14 12:49:40,155] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka-connect/plugins/kafka-connect-jdbc/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:49:40,155] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:40,155] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:40,156] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:40,156] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:40,156] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:40,208] INFO Loading plugin from: /opt/kafka-connect/plugins/kafka-connect-hdfs (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-05-14 12:49:43,071] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka-connect/plugins/kafka-connect-hdfs/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:49:43,071] INFO Added plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,071] INFO Added plugin 'io.confluent.connect.storage.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,071] INFO Added plugin 'io.confluent.connect.hdfs.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,071] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,805] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:49:43,805] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,806] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,807] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,808] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:49:43,809] INFO Added aliases 'HdfsSinkConnector' and 'HdfsSink' to plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,809] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,809] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,809] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,809] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,809] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,810] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,811] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,812] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,812] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,812] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,812] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,812] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,812] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,812] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,812] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,812] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,813] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,813] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,813] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,813] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:49:43,813] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,813] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,813] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:49:43,858] INFO DistributedConfig values:
        access.control.allow.methods =
        access.control.allow.origin =
        admin.listeners = null
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        config.providers = []
        config.storage.replication.factor = 1
        config.storage.topic = connect-configs
        connect.protocol = sessioned
        connections.max.idle.ms = 540000
        connector.client.config.override.policy = None
        group.id = connect-cluster
        header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
        heartbeat.interval.ms = 3000
        inter.worker.key.generation.algorithm = HmacSHA256
        inter.worker.key.size = null
        inter.worker.key.ttl.ms = 3600000
        inter.worker.signature.algorithm = HmacSHA256
        inter.worker.verification.algorithms = [HmacSHA256]
        internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
        internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
        key.converter = class org.apache.kafka.connect.json.JsonConverter
        listeners = null
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        offset.flush.interval.ms = 10000
        offset.flush.timeout.ms = 5000
        offset.storage.partitions = 25
        offset.storage.replication.factor = 1
        offset.storage.topic = connect-offsets
        plugin.path = [/opt/kafka-connect/plugins]
        rebalance.timeout.ms = 60000
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 40000
        response.http.headers.config =
        rest.advertised.host.name = null
        rest.advertised.listener = null
        rest.advertised.port = null
        rest.extension.classes = []
        rest.host.name = 10.8.151.70
        rest.port = 8083
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        scheduled.rebalance.max.delay.ms = 300000
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.client.auth = none
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        status.storage.partitions = 5
        status.storage.replication.factor = 1
        status.storage.topic = connect-status
        task.shutdown.graceful.timeout.ms = 5000
        topic.creation.enable = true
        topic.tracking.allow.reset = true
        topic.tracking.enable = true
        value.converter = class org.apache.kafka.connect.json.JsonConverter
        worker.sync.timeout.ms = 3000
        worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:372)
[2021-05-14 12:49:43,860] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:43,863] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:43,928] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,928] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,929] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,929] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,929] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:43,929] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:43,929] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:43,929] INFO Kafka startTimeMs: 1620985783929 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,228] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,230] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,238] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,239] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,239] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,250] INFO Logging initialized @4927ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-05-14 12:49:44,296] INFO Added connector for http://10.8.151.70:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-05-14 12:49:44,296] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-05-14 12:49:44,303] INFO jetty-9.4.39.v20210325; built: 2021-03-25T14:42:11.471Z; git: 9fc7ca5a922f2a37b84ec9dbc26a5168cee7e667; jvm 1.8.0_282-b08 (org.eclipse.jetty.server.Server:375)
[2021-05-14 12:49:44,330] INFO Started http_10.8.151.708083@60ed0b9d{HTTP/1.1, (http/1.1)}{10.8.151.70:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-05-14 12:49:44,330] INFO Started @5007ms (org.eclipse.jetty.server.Server:415)
[2021-05-14 12:49:44,348] INFO Advertised URI: http://10.8.151.70:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:49:44,348] INFO REST server listening at http://10.8.151.70:8083/, advertising URL http://10.8.151.70:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-05-14 12:49:44,348] INFO Advertised URI: http://10.8.151.70:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:49:44,348] INFO REST admin endpoints at http://10.8.151.70:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-05-14 12:49:44,349] INFO Advertised URI: http://10.8.151.70:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:49:44,351] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,352] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,355] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,355] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,356] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,356] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,356] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,356] INFO Kafka startTimeMs: 1620985784356 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,373] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,374] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,378] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,378] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,378] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,384] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-05-14 12:49:44,392] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,393] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,395] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,395] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,396] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,396] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,396] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,396] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,396] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,396] INFO Kafka startTimeMs: 1620985784396 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,404] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,405] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,407] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,407] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,407] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,411] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,411] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,411] INFO Kafka startTimeMs: 1620985784411 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,515] INFO JsonConverterConfig values:
        converter.type = key
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:49:44,516] INFO JsonConverterConfig values:
        converter.type = value
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:49:44,517] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,517] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,522] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,522] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,523] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,523] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,523] INFO Kafka startTimeMs: 1620985784523 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,531] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,531] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,533] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,533] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,533] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,541] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,542] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,543] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,544] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,544] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,544] INFO Kafka startTimeMs: 1620985784544 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,552] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,552] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,555] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,555] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,555] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,558] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,559] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,560] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,560] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,560] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,560] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,561] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,561] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,561] INFO Kafka startTimeMs: 1620985784561 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,571] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,572] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,574] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,575] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,575] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,588] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:49:44,589] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,590] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,590] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,590] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,591] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,591] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,591] INFO Kafka startTimeMs: 1620985784591 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,600] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:49:44,600] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:49:44,601] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:49:44,602] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:49:44,602] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:49:44,618] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,619] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,619] INFO Kafka startTimeMs: 1620985784618 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,621] INFO Kafka Connect distributed worker initialization took 4963ms (org.apache.kafka.connect.cli.ConnectDistributed:138)
[2021-05-14 12:49:44,621] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-05-14 12:49:44,622] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-05-14 12:49:44,622] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:308)
[2021-05-14 12:49:44,622] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-05-14 12:49:44,622] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:144)
[2021-05-14 12:49:44,622] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:49:44,624] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:49:44,627] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,627] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,627] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,628] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:49:44,629] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,629] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,629] INFO Kafka startTimeMs: 1620985784629 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,662] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-05-14 12:49:44,664] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-1
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:49:44,680] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,681] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,682] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,682] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,682] INFO Kafka startTimeMs: 1620985784681 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,689] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-1
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:49:44,689] INFO [Producer clientId=producer-1] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,713] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,714] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,715] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,715] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,715] INFO Kafka startTimeMs: 1620985784715 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,721] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,727] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:49:44,730] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,730] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,730] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,730] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,731] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,732] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,732] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,732] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,732] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,732] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,746] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-05-14 12:49:44,746] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-05-14 12:49:44,748] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2021-05-14 12:49:44,775] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,775] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,777] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,778] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,779] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,779] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,779] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,779] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,780] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:49:44,780] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:49:44,780] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:146)
[2021-05-14 12:49:44,782] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-05-14 12:49:44,782] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:49:44,790] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-2
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 120000
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 0
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:49:44,794] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,794] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,795] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,795] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,795] INFO Kafka startTimeMs: 1620985784795 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,796] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-2
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:49:44,799] INFO [Producer clientId=producer-2] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,801] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,801] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,802] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,803] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,803] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,803] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,803] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,803] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,804] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,804] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,804] INFO Kafka startTimeMs: 1620985784803 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,807] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,808] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,817] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,817] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,817] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,818] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,818] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,818] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:49:44,818] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:49:44,820] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:275)
[2021-05-14 12:49:44,821] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:49:44,836] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-3
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:49:44,840] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,840] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,840] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,840] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,841] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:49:44,842] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,842] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,842] INFO Kafka startTimeMs: 1620985784842 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,843] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-3
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:49:44,844] INFO [Producer clientId=producer-3] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,846] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,847] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,848] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,848] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,848] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,848] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:49:44,848] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:49:44,848] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:49:44,848] INFO Kafka startTimeMs: 1620985784848 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:49:44,851] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,851] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:49:44,851] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:49:44,859] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:49:44,897] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:49:44,897] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:49:44,897] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:290)
[2021-05-14 12:49:44,898] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:312)
[2021-05-14 12:49:44,904] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:49:44,904] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator 10.8.151.72:9092 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:848)
[2021-05-14 12:49:44,905] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:49:44,905] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:49:44,914] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
May 14, 2021 12:49:45 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored.
May 14, 2021 12:49:45 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored.
May 14, 2021 12:49:45 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored.
May 14, 2021 12:49:45 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored.
May 14, 2021 12:49:45 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.[2021-05-14 12:49:45,234] INFO Started o.e.j.s.ServletContextHandler@13d984ee{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-05-14 12:49:45,234] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-05-14 12:49:45,234] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-05-14 12:49:47,494] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=11, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:49:47,499] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=11, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:49:47,501] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-5bfb91d7-c76b-4725-8bf8-971631d72083', leaderUrl='http://10.8.151.72:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:49:47,502] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1119)
[2021-05-14 12:49:47,502] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 12, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1183)
[2021-05-14 12:49:47,506] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1190)
[2021-05-14 12:49:47,507] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:49:47,507] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
[2021-05-14 12:53:50,533] INFO [Worker clientId=connect-1, groupId=connect-cluster] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1123)
[2021-05-14 12:53:50,534] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:53:50,534] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:53:51,527] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=12, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:53:51,531] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=12, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:53:51,532] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-5bfb91d7-c76b-4725-8bf8-971631d72083', leaderUrl='http://10.8.151.72:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:53:51,532] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:53:51,532] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
[2021-05-14 12:54:03,529] INFO [Worker clientId=connect-1, groupId=connect-cluster] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1123)
[2021-05-14 12:54:03,530] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:54:03,530] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:54:03,534] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=13, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:54:03,549] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=13, memberId='connect-1-a4f80aec-e5fb-4009-a035-830091939b08', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:54:03,549] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', leaderUrl='http://10.8.151.71:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:54:03,550] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:54:03,550] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
{code}
 

*Node 2:*
{code:java}
[root@hadoop-dn-02 kafka]# bin/connect-distributed.sh config/connect-distributed.properties config/connect-teradata-source-distributed.properties config/connect-txt-sink-distributed.properties
[2021-05-14 12:54:05,271] INFO WorkerInfo values:
        jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/opt/kafka/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
        jvm.spec = Red Hat, Inc., OpenJDK 64-Bit Server VM, 1.8.0_282, 25.282-b08
        jvm.classpath = /opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.6.1.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-cli-1.4.jar:/opt/kafka/bin/../libs/commons-lang3-3.8.1.jar:/opt/kafka/bin/../libs/connect-api-2.8.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.8.0.jar:/opt/kafka/bin/../libs/connect-file-2.8.0.jar:/opt/kafka/bin/../libs/connect-json-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-client-2.8.0.jar:/opt/kafka/bin/../libs/connect-runtime-2.8.0.jar:/opt/kafka/bin/../libs/connect-transforms-2.8.0.jar:/opt/kafka/bin/../libs/hk2-api-2.6.1.jar:/opt/kafka/bin/../libs/hk2-locator-2.6.1.jar:/opt/kafka/bin/../libs/hk2-utils-2.6.1.jar:/opt/kafka/bin/../libs/jackson-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-core-2.10.5.jar:/opt/kafka/bin/../libs/jackson-databind-2.10.5.1.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-paranamer-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.12-2.10.5.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.1.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.5.jar:/opt/kafka/bin/../libs/jakarta.inject-2.6.1.jar:/opt/kafka/bin/../libs/jakarta.validation-api-2.0.2.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/opt/kafka/bin/../libs/javassist-3.27.0-GA.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.31.jar:/opt/kafka/bin/../libs/jersey-common-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.31.jar:/opt/kafka/bin/../libs/jersey-hk2-2.31.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.31.jar:/opt/kafka/bin/../libs/jersey-server-2.31.jar:/opt/kafka/bin/../libs/jetty-client-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-http-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-io-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-security-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-server-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-ajax-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jline-3.12.1.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0-sources.jar:/opt/kafka/bin/../libs/kafka-clients-2.8.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.8.0.jar:/opt/kafka/bin/../libs/kafka-metadata-2.8.0.jar:/opt/kafka/bin/../libs/kafka-raft-2.8.0.jar:/opt/kafka/bin/../libs/kafka-shell-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.8.0.jar:/opt/kafka/bin/../libs/kafka-tools-2.8.0.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.7.1.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.3.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-codec-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-handler-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.3.jar:/opt/kafka/bin/../libs/paranamer-2.8.jar:/opt/kafka/bin/../libs/plexus-utils-3.2.1.jar:/opt/kafka/bin/../libs/reflections-0.9.12.jar:/opt/kafka/bin/../libs/rocksdbjni-5.18.4.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.12-2.3.0.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.12-0.9.1.jar:/opt/kafka/bin/../libs/scala-library-2.12.13.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.2.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.13.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.30.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.30.jar:/opt/kafka/bin/../libs/snappy-java-1.1.8.1.jar:/opt/kafka/bin/../libs/zookeeper-3.5.9.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.5.9.jar:/opt/kafka/bin/../libs/zstd-jni-1.4.9-1.jar
        os.spec = Linux, amd64, 3.10.0-1160.24.1.el7.x86_64
        os.vcpus = 16
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-05-14 12:54:05,274] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:92)
[2021-05-14 12:54:06,481] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:54:06,481] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,481] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,482] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,483] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,483] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,483] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,484] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,485] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,486] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,487] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:54:06,488] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,488] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,488] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,488] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,488] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,488] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,489] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,490] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,491] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,491] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,491] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,491] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,491] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,491] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,492] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,492] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,492] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,493] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,493] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,493] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,493] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:54:06,493] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,493] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,493] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:54:06,534] INFO DistributedConfig values:
        access.control.allow.methods =
        access.control.allow.origin =
        admin.listeners = null
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        config.providers = []
        config.storage.replication.factor = 1
        config.storage.topic = connect-configs
        connect.protocol = sessioned
        connections.max.idle.ms = 540000
        connector.client.config.override.policy = None
        group.id = connect-cluster
        header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
        heartbeat.interval.ms = 3000
        inter.worker.key.generation.algorithm = HmacSHA256
        inter.worker.key.size = null
        inter.worker.key.ttl.ms = 3600000
        inter.worker.signature.algorithm = HmacSHA256
        inter.worker.verification.algorithms = [HmacSHA256]
        internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
        internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
        key.converter = class org.apache.kafka.connect.json.JsonConverter
        listeners = null
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        offset.flush.interval.ms = 10000
        offset.flush.timeout.ms = 5000
        offset.storage.partitions = 25
        offset.storage.replication.factor = 1
        offset.storage.topic = connect-offsets
        plugin.path = [/opt/kafka-connect/plugins]
        rebalance.timeout.ms = 60000
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 40000
        response.http.headers.config =
        rest.advertised.host.name = null
        rest.advertised.listener = null
        rest.advertised.port = null
        rest.extension.classes = []
        rest.host.name = 10.8.151.71
        rest.port = 8083
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        scheduled.rebalance.max.delay.ms = 300000
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.client.auth = none
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        status.storage.partitions = 5
        status.storage.replication.factor = 1
        status.storage.topic = connect-status
        task.shutdown.graceful.timeout.ms = 5000
        topic.creation.enable = true
        topic.tracking.allow.reset = true
        topic.tracking.enable = true
        value.converter = class org.apache.kafka.connect.json.JsonConverter
        worker.sync.timeout.ms = 3000
        worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:372)
[2021-05-14 12:54:06,536] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:06,538] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:06,597] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,597] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,597] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,597] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,597] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,598] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:06,599] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:06,599] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:06,599] INFO Kafka startTimeMs: 1620986046598 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:06,908] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:06,909] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:06,917] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:06,917] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:06,917] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:06,929] INFO Logging initialized @1994ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-05-14 12:54:06,974] INFO Added connector for http://10.8.151.71:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-05-14 12:54:06,974] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-05-14 12:54:06,982] INFO jetty-9.4.39.v20210325; built: 2021-03-25T14:42:11.471Z; git: 9fc7ca5a922f2a37b84ec9dbc26a5168cee7e667; jvm 1.8.0_282-b08 (org.eclipse.jetty.server.Server:375)
[2021-05-14 12:54:07,006] INFO Started http_10.8.151.718083@f73dcd6{HTTP/1.1, (http/1.1)}{10.8.151.71:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-05-14 12:54:07,006] INFO Started @2071ms (org.eclipse.jetty.server.Server:415)
[2021-05-14 12:54:07,022] INFO Advertised URI: http://10.8.151.71:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:54:07,022] INFO REST server listening at http://10.8.151.71:8083/, advertising URL http://10.8.151.71:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-05-14 12:54:07,022] INFO Advertised URI: http://10.8.151.71:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:54:07,022] INFO REST admin endpoints at http://10.8.151.71:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-05-14 12:54:07,025] INFO Advertised URI: http://10.8.151.71:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:54:07,028] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,028] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,033] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,033] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,034] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,034] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,034] INFO Kafka startTimeMs: 1620986047034 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,047] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,047] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,051] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,051] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,051] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,057] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-05-14 12:54:07,064] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,064] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,068] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,068] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,069] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,069] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,069] INFO Kafka startTimeMs: 1620986047069 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,081] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,081] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,082] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,082] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,082] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,086] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,086] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,087] INFO Kafka startTimeMs: 1620986047086 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,198] INFO JsonConverterConfig values:
        converter.type = key
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:54:07,199] INFO JsonConverterConfig values:
        converter.type = value
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:54:07,199] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,199] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,201] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,202] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,203] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,203] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,203] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,203] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,203] INFO Kafka startTimeMs: 1620986047203 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,213] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,213] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,215] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,215] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,215] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,221] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,222] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,225] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,225] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,226] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,226] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,226] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,226] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,226] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,226] INFO Kafka startTimeMs: 1620986047226 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,232] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,233] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,235] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,235] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,235] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,238] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,239] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,241] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,241] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,242] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,242] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,242] INFO Kafka startTimeMs: 1620986047242 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,249] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,249] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,250] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,250] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,250] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,262] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:54:07,263] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,265] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,265] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,266] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,266] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,266] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,266] INFO Kafka startTimeMs: 1620986047266 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,272] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:54:07,273] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:54:07,274] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:54:07,274] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:54:07,274] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:54:07,293] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,294] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,294] INFO Kafka startTimeMs: 1620986047293 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,297] INFO Kafka Connect distributed worker initialization took 2024ms (org.apache.kafka.connect.cli.ConnectDistributed:138)
[2021-05-14 12:54:07,297] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-05-14 12:54:07,298] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-05-14 12:54:07,298] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:308)
[2021-05-14 12:54:07,299] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-05-14 12:54:07,299] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:144)
[2021-05-14 12:54:07,299] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:54:07,300] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:54:07,301] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,301] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,301] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,301] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,302] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:54:07,303] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,303] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,304] INFO Kafka startTimeMs: 1620986047303 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,333] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-1
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:54:07,334] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-05-14 12:54:07,349] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,349] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,350] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,350] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,350] INFO Kafka startTimeMs: 1620986047349 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,355] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-1
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:54:07,355] INFO [Producer clientId=producer-1] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,380] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,380] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,381] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,381] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,381] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,381] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,381] INFO Kafka startTimeMs: 1620986047381 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,386] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,391] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:54:07,395] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,395] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,396] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,398] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-05-14 12:54:07,398] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-05-14 12:54:07,399] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-05-14 12:54:07,447] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,447] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,460] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,460] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,461] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,462] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,463] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,464] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,464] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:54:07,465] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:54:07,465] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:146)
[2021-05-14 12:54:07,466] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-05-14 12:54:07,467] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:54:07,472] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-2
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 120000
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 0
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:54:07,475] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,475] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,476] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,476] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,476] INFO Kafka startTimeMs: 1620986047476 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,476] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-2
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:54:07,477] INFO [Producer clientId=producer-2] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,479] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,479] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,480] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,480] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,480] INFO Kafka startTimeMs: 1620986047480 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,484] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,485] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,496] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,496] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,497] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,497] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,497] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,497] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:54:07,497] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:54:07,498] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:275)
[2021-05-14 12:54:07,498] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:54:07,506] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-3
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:54:07,509] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,509] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,510] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,510] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:54:07,510] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,510] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,510] INFO Kafka startTimeMs: 1620986047510 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,510] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-3
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:54:07,513] INFO [Producer clientId=producer-3] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,514] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'rest.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,514] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,515] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,515] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:54:07,515] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:54:07,515] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:54:07,515] INFO Kafka startTimeMs: 1620986047515 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:54:07,518] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,519] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:54:07,519] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:54:07,528] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:54:07,568] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:54:07,568] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:54:07,568] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:290)
[2021-05-14 12:54:07,568] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:312)
[2021-05-14 12:54:07,572] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:54:07,573] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator 10.8.151.72:9092 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:848)
[2021-05-14 12:54:07,574] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:54:07,574] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:54:07,581] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
May 14, 2021 12:54:07 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored.
May 14, 2021 12:54:07 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored.
May 14, 2021 12:54:07 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored.
May 14, 2021 12:54:07 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored.
May 14, 2021 12:54:07 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.[2021-05-14 12:54:07,834] INFO Started o.e.j.s.ServletContextHandler@53cdecf6{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-05-14 12:54:07,835] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-05-14 12:54:07,835] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-05-14 12:54:09,979] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=12, memberId='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:54:09,983] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=12, memberId='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:54:09,984] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-5bfb91d7-c76b-4725-8bf8-971631d72083', leaderUrl='http://10.8.151.72:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:54:09,985] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1119)
[2021-05-14 12:54:09,985] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 12, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1183)
[2021-05-14 12:54:09,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1190)
[2021-05-14 12:54:09,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:54:09,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
[2021-05-14 12:54:21,983] INFO [Worker clientId=connect-1, groupId=connect-cluster] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1123)
[2021-05-14 12:54:21,983] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:54:21,983] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:54:21,987] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=13, memberId='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:54:22,002] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=13, memberId='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:54:22,002] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', leaderUrl='http://10.8.151.71:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:54:22,002] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:54:22,002] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
{code}
 

 

*Node 3:*
{code:java}
[root@hadoop-dn-03 kafka]# bin/connect-distributed.sh config/connect-distributed.properties config/connect-teradata-source-distributed.properties config/connect-txt-sink-distributed.properties
[2021-05-14 12:53:34,566] INFO WorkerInfo values:
        jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/opt/kafka/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
        jvm.spec = Red Hat, Inc., OpenJDK 64-Bit Server VM, 1.8.0_282, 25.282-b08
        jvm.classpath = /opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.6.1.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-cli-1.4.jar:/opt/kafka/bin/../libs/commons-lang3-3.8.1.jar:/opt/kafka/bin/../libs/connect-api-2.8.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.8.0.jar:/opt/kafka/bin/../libs/connect-file-2.8.0.jar:/opt/kafka/bin/../libs/connect-json-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-2.8.0.jar:/opt/kafka/bin/../libs/connect-mirror-client-2.8.0.jar:/opt/kafka/bin/../libs/connect-runtime-2.8.0.jar:/opt/kafka/bin/../libs/connect-transforms-2.8.0.jar:/opt/kafka/bin/../libs/hk2-api-2.6.1.jar:/opt/kafka/bin/../libs/hk2-locator-2.6.1.jar:/opt/kafka/bin/../libs/hk2-utils-2.6.1.jar:/opt/kafka/bin/../libs/jackson-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-core-2.10.5.jar:/opt/kafka/bin/../libs/jackson-databind-2.10.5.1.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.10.5.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.10.5.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-paranamer-2.10.5.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.12-2.10.5.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.1.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.5.jar:/opt/kafka/bin/../libs/jakarta.inject-2.6.1.jar:/opt/kafka/bin/../libs/jakarta.validation-api-2.0.2.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/opt/kafka/bin/../libs/javassist-3.27.0-GA.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.31.jar:/opt/kafka/bin/../libs/jersey-common-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.31.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.31.jar:/opt/kafka/bin/../libs/jersey-hk2-2.31.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.31.jar:/opt/kafka/bin/../libs/jersey-server-2.31.jar:/opt/kafka/bin/../libs/jetty-client-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-http-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-io-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-security-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-server-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jetty-util-ajax-9.4.39.v20210325.jar:/opt/kafka/bin/../libs/jline-3.12.1.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka_2.12-2.8.0-sources.jar:/opt/kafka/bin/../libs/kafka-clients-2.8.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.8.0.jar:/opt/kafka/bin/../libs/kafka-metadata-2.8.0.jar:/opt/kafka/bin/../libs/kafka-raft-2.8.0.jar:/opt/kafka/bin/../libs/kafka-shell-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.8.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.8.0.jar:/opt/kafka/bin/../libs/kafka-tools-2.8.0.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.7.1.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.3.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-codec-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-handler-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.62.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.62.Final.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.3.jar:/opt/kafka/bin/../libs/paranamer-2.8.jar:/opt/kafka/bin/../libs/plexus-utils-3.2.1.jar:/opt/kafka/bin/../libs/reflections-0.9.12.jar:/opt/kafka/bin/../libs/rocksdbjni-5.18.4.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.12-2.3.0.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.12-0.9.1.jar:/opt/kafka/bin/../libs/scala-library-2.12.13.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.2.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.13.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.30.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.30.jar:/opt/kafka/bin/../libs/snappy-java-1.1.8.1.jar:/opt/kafka/bin/../libs/zookeeper-3.5.9.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.5.9.jar:/opt/kafka/bin/../libs/zstd-jni-1.4.9-1.jar
        os.spec = Linux, amd64, 3.10.0-1160.24.1.el7.x86_64
        os.vcpus = 16
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-05-14 12:53:34,569] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:92)
[2021-05-14 12:53:34,584] INFO Loading plugin from: /opt/kafka-connect/plugins/kafka-connect-jdbc (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-05-14 12:53:35,052] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka-connect/plugins/kafka-connect-jdbc/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:53:35,052] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:35,053] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:35,053] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:35,053] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:35,053] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:35,105] INFO Loading plugin from: /opt/kafka-connect/plugins/kafka-connect-hdfs (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2021-05-14 12:53:37,783] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka-connect/plugins/kafka-connect-hdfs/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:53:37,783] INFO Added plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:37,783] INFO Added plugin 'io.confluent.connect.storage.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:37,783] INFO Added plugin 'io.confluent.connect.hdfs.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:37,783] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,413] INFO Registered loader: sun.misc.Launcher$AppClassLoader@764c12b6 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,414] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,415] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,416] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,417] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,418] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-05-14 12:53:38,420] INFO Added aliases 'HdfsSinkConnector' and 'HdfsSink' to plugin 'io.confluent.connect.hdfs.HdfsSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,421] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,421] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,421] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,421] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,421] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,422] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,423] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,424] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,424] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,425] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,425] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,425] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,425] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,425] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,426] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,426] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,426] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,426] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-05-14 12:53:38,426] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,426] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,426] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-05-14 12:53:38,472] INFO DistributedConfig values:
        access.control.allow.methods =
        access.control.allow.origin =
        admin.listeners = null
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        config.providers = []
        config.storage.replication.factor = 1
        config.storage.topic = connect-configs
        connect.protocol = sessioned
        connections.max.idle.ms = 540000
        connector.client.config.override.policy = None
        group.id = connect-cluster
        header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
        heartbeat.interval.ms = 3000
        inter.worker.key.generation.algorithm = HmacSHA256
        inter.worker.key.size = null
        inter.worker.key.ttl.ms = 3600000
        inter.worker.signature.algorithm = HmacSHA256
        inter.worker.verification.algorithms = [HmacSHA256]
        internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
        internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
        key.converter = class org.apache.kafka.connect.json.JsonConverter
        listeners = null
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        offset.flush.interval.ms = 10000
        offset.flush.timeout.ms = 5000
        offset.storage.partitions = 25
        offset.storage.replication.factor = 1
        offset.storage.topic = connect-offsets
        plugin.path = [/opt/kafka-connect/plugins]
        rebalance.timeout.ms = 60000
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 40000
        response.http.headers.config =
        rest.advertised.host.name = 10.8.151.72
        rest.advertised.listener = null
        rest.advertised.port = 8083
        rest.extension.classes = []
        rest.host.name = null
        rest.port = 8083
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        scheduled.rebalance.max.delay.ms = 300000
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.client.auth = none
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        status.storage.partitions = 5
        status.storage.replication.factor = 1
        status.storage.topic = connect-status
        task.shutdown.graceful.timeout.ms = 5000
        topic.creation.enable = true
        topic.tracking.allow.reset = true
        topic.tracking.enable = true
        value.converter = class org.apache.kafka.connect.json.JsonConverter
        worker.sync.timeout.ms = 3000
        worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:372)
[2021-05-14 12:53:38,474] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:38,476] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:38,553] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,553] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,553] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,553] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,554] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,555] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:38,555] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:38,555] INFO Kafka startTimeMs: 1620986018554 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:38,867] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:38,869] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:38,878] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:38,879] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:38,879] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:38,895] INFO Logging initialized @4656ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2021-05-14 12:53:38,939] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-05-14 12:53:38,939] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-05-14 12:53:38,945] INFO jetty-9.4.39.v20210325; built: 2021-03-25T14:42:11.471Z; git: 9fc7ca5a922f2a37b84ec9dbc26a5168cee7e667; jvm 1.8.0_282-b08 (org.eclipse.jetty.server.Server:375)
[2021-05-14 12:53:38,970] INFO Started http_8083@131ba005{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-05-14 12:53:38,971] INFO Started @4731ms (org.eclipse.jetty.server.Server:415)
[2021-05-14 12:53:38,988] INFO Advertised URI: http://10.8.151.72:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:53:38,989] INFO REST server listening at http://10.8.151.72:8083/, advertising URL http://10.8.151.72:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-05-14 12:53:38,989] INFO Advertised URI: http://10.8.151.72:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:53:38,989] INFO REST admin endpoints at http://10.8.151.72:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-05-14 12:53:38,989] INFO Advertised URI: http://10.8.151.72:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-05-14 12:53:38,993] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:38,993] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:38,997] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:38,998] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:38,998] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:38,998] INFO Kafka startTimeMs: 1620986018998 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,014] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,014] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,017] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,017] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,018] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,022] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-05-14 12:53:39,028] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:39,029] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,031] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,031] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,032] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,032] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,032] INFO Kafka startTimeMs: 1620986019032 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,044] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,044] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,046] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,046] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,046] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,049] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,049] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,049] INFO Kafka startTimeMs: 1620986019049 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,148] INFO JsonConverterConfig values:
        converter.type = key
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:53:39,149] INFO JsonConverterConfig values:
        converter.type = value
        decimal.format = BASE64
        schemas.cache.size = 1000
        schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-05-14 12:53:39,150] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:39,150] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,152] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,152] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,153] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,153] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,153] INFO Kafka startTimeMs: 1620986019152 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,164] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,165] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,167] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,168] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,168] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,176] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:39,176] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,178] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,178] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,179] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,179] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,179] INFO Kafka startTimeMs: 1620986019179 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,188] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,188] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,190] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,190] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,190] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,194] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:39,194] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,196] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,196] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,196] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,196] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,197] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,197] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,197] INFO Kafka startTimeMs: 1620986019197 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,211] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,211] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,213] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,213] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,213] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,229] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-05-14 12:53:39,229] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,231] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,231] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,232] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,232] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,232] INFO Kafka startTimeMs: 1620986019232 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,240] INFO Kafka cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-05-14 12:53:39,241] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-05-14 12:53:39,242] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-05-14 12:53:39,242] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-05-14 12:53:39,242] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-05-14 12:53:39,272] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,272] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,272] INFO Kafka startTimeMs: 1620986019272 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,274] INFO Kafka Connect distributed worker initialization took 4705ms (org.apache.kafka.connect.cli.ConnectDistributed:138)
[2021-05-14 12:53:39,275] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-05-14 12:53:39,275] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-05-14 12:53:39,276] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:308)
[2021-05-14 12:53:39,276] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2021-05-14 12:53:39,276] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:144)
[2021-05-14 12:53:39,276] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:53:39,278] INFO AdminClientConfig values:
        bootstrap.servers = [localhost:9092]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-05-14 12:53:39,282] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,282] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,283] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,283] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,283] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,283] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,284] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,284] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,284] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,284] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,284] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-05-14 12:53:39,285] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,285] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,285] INFO Kafka startTimeMs: 1620986019285 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,313] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-05-14 12:53:39,320] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-1
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:53:39,351] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,351] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,352] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,352] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,352] INFO Kafka startTimeMs: 1620986019352 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,359] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-1
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:53:39,366] INFO [Producer clientId=producer-1] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,375] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-05-14 12:53:39,376] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-05-14 12:53:39,377] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2021-05-14 12:53:39,381] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,382] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,382] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,382] INFO Kafka startTimeMs: 1620986019382 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,391] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,397] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,400] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,401] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,401] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,401] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,401] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,401] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,446] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,447] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,447] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,448] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,449] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,450] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,450] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,450] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,450] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,452] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:53:39,452] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:53:39,452] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:146)
[2021-05-14 12:53:39,455] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2021-05-14 12:53:39,456] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:53:39,464] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-2
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 120000
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 0
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:53:39,467] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,467] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,468] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,469] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,469] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,469] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,469] INFO Kafka startTimeMs: 1620986019469 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,470] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-2
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:53:39,473] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,473] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,473] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,474] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,474] INFO [Producer clientId=producer-2] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,475] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,476] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,476] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,476] INFO Kafka startTimeMs: 1620986019476 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,478] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,479] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,495] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,495] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,495] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,496] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,496] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,496] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:53:39,496] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:53:39,501] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:275)
[2021-05-14 12:53:39,502] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:162)
[2021-05-14 12:53:39,516] INFO ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-3
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 2147483647
        enable.idempotence = false
        interceptor.classes = []
        internal.auto.downgrade.txn.commit = false
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 1
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-05-14 12:53:39,519] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,519] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,519] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,520] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-05-14 12:53:39,521] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,521] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,521] INFO Kafka startTimeMs: 1620986019521 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,522] INFO ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [localhost:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-connect-cluster-3
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = connect-cluster
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-05-14 12:53:39,525] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'rest.advertised.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,525] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,526] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,526] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,526] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-05-14 12:53:39,526] INFO Kafka version: 2.8.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-05-14 12:53:39,526] INFO Kafka commitId: ebb1d6e21cc92130 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-05-14 12:53:39,526] INFO Kafka startTimeMs: 1620986019526 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-05-14 12:53:39,527] INFO [Producer clientId=producer-3] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,528] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,529] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2021-05-14 12:53:39,529] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:619)
[2021-05-14 12:53:39,541] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.8.151.70:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-05-14 12:53:39,577] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:202)
[2021-05-14 12:53:39,577] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:204)
[2021-05-14 12:53:39,577] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:290)
[2021-05-14 12:53:39,577] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:312)
[2021-05-14 12:53:39,602] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: V4WtcZcaRAuScx4Yi7ObpQ (org.apache.kafka.clients.Metadata:279)
[2021-05-14 12:53:39,603] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator 10.8.151.72:9092 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:848)
[2021-05-14 12:53:39,606] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:221)
[2021-05-14 12:53:39,606] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
[2021-05-14 12:53:39,614] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:538)
May 14, 2021 12:53:39 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored.
May 14, 2021 12:53:39 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored.
May 14, 2021 12:53:39 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored.
May 14, 2021 12:53:39 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored.
May 14, 2021 12:53:39 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.[2021-05-14 12:53:39,825] INFO Started o.e.j.s.ServletContextHandler@4fdac2a7{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2021-05-14 12:53:39,825] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-05-14 12:53:39,825] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-05-14 12:53:40,386] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=13, memberId='connect-1-05ede048-b620-410e-b442-f73c32e55736', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:594)
[2021-05-14 12:53:40,400] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=13, memberId='connect-1-05ede048-b620-410e-b442-f73c32e55736', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:758)
[2021-05-14 12:53:40,402] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-ce2b6330-7dbb-4f0e-a1d7-a2e37e10fee8', leaderUrl='http://10.8.151.71:8083/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1693)
[2021-05-14 12:53:40,403] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1119)
[2021-05-14 12:53:40,403] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 12, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1183)
[2021-05-14 12:53:40,410] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1190)
[2021-05-14 12:53:40,410] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2021-05-14 12:53:40,410] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1272)
{code}
 

So as you can see Node 1 can see Node 2 and Node 3 as part of cluster, all of them have the same group.ip but connector is frozen after output about starting tasks no mater do I have new data at JDBC side or not.

Can anybody help me? Did I miss something?","- kafka 2.8.0
- zookeeper 3.4.14",dishka_krauch,iakunin,kkonstantine,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/21 11:25;dishka_krauch;topic desc.png;https://issues.apache.org/jira/secure/attachment/13025463/topic+desc.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 29 07:47:57 UTC 2021,,,,,,,,,,"0|z0r1bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/21 15:51;kkonstantine;Hi [~dishka_krauch] 

How do you submit your connectors' configurations? In distributed mode they are not loaded from the command line. You'll have to call the Connect REST API. 
See for example: [https://kafka.apache.org/documentation/#connect_rest] 

As a side note, when you are debugging you might choose to increase your log level to DEBUG or TRACE. Default is INFO. 

Given that I don't see any connectors starting (the message is a generic message that will be displayed even if there are no connectors configured in your cluster), I believe you just need to submit the connector configurations through the Connect REST API. 

For example: 
{{curl -X POST -H ""Content-Type: application/json"" --data @config.json http://localhost:8083/connectors}};;;","17/May/21 08:07;dishka_krauch;[~kkonstantine] thanks for you reply.

I've read docs and tried REST API approach but always get error like ""no route to the host"".;;;","17/May/21 09:13;dishka_krauch;[~kkonstantine] sorry, I forgot to start connect-distributed scripts before API using.;;;","17/May/21 09:32;dishka_krauch;[~kkonstantine] everyting is worked fine, thank you for your advice - you saved my day.;;;","27/May/21 11:30;iakunin;[~dishka_krauch], hi! 

As far as I can see, this issue is resolved and therefore can be closed, right?;;;","29/May/21 07:47;dishka_krauch;Hey, you're absolutely right, thx.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadocs search sends you to a non-existent URL,KAFKA-12782,13378400,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jlprat,ableegoldman,ableegoldman,14/May/21 02:19,05/Jul/23 14:46,13/Jul/23 09:17,05/Sep/21 18:34,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,docs,,,,,0,,,,,"I was looking up a class using the javadocs search functionality, and clicked on the link when TaskId came up, but it sent me to which [https://kafka.apache.org/28/javadoc/undefined/org/apache/kafka/streams/processor/TaskId.html] does not exist.

I noticed the URL had an odd ""undefined"" term inserted before the package name, so I took that out and was able to find the [correct javadocs|https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/processor/TaskId.html]. So the search seems to be broken due to this ""undefined"" term that's being injected somewhere, for some reason.",,ableegoldman,josep.prat-inactive,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 25 12:47:16 UTC 2021,,,,,,,,,,"0|z0r0vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/21 11:35;jlprat;I can confirm this is bug with Java11 for API docs that do not use the modules system. It has been solved on JDK 12 and higher but not yet backported to JDK 11. See [https://bugs.openjdk.java.net/browse/JDK-8215291] for more details. Long story short, the search.js file appends the module name in the URL.

I ran the docs script manually specifying JDK 16 and the links on the search are generated correctly pointing to the right URL.

I will try to find a workaround that would make it work in the meantime. It is possible to disable the module system option for the API docs, however all links pointing to JDK APIs (i.e. String) won't work as they would need the submodule prefix.;;;","25/May/21 12:47;jlprat;PR submitted;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalStateException: Tried to lookup lag for unknown task 2_0,KAFKA-12780,13378369,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,iirekm2,iirekm2,13/May/21 20:45,14/May/21 05:56,13/Jul/23 09:17,14/May/21 05:56,2.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"Whenever I try doing a stream.join(table, joiner), I get the following error ever second or third time I rerun the application. 

Kafka Streams version: 2.8.0

The default stores config is used (simply writing to /tmp directory). Changing state.dir to other location doesn't help.

 
{code:java}

22:36:35.205 [xxxxxx-94ada1ab-8573-4245-9742-3322e412598b-StreamThread-1] WARN  o.a.k.s.p.i.assignment.ClientState - Task 1_22 had endOffsetSum=3 smaller than offsetSum=13 on member 94ada1ab-8573-4245-9742-3322e412598b. This probably means the task is corrupted, which in turn indicates that it will need to restore from scratch if it gets assigned. The assignor will de-prioritize returning this task to this member in the hopes that some other member may be able to re-use its state.22:36:35.205 [xxxxxx-94ada1ab-8573-4245-9742-3322e412598b-StreamThread-1] WARN  o.a.k.s.p.i.assignment.ClientState - Task 1_22 had endOffsetSum=3 smaller than offsetSum=13 on member 94ada1ab-8573-4245-9742-3322e412598b. This probably means the task is corrupted, which in turn indicates that it will need to restore from scratch if it gets assigned. The assignor will de-prioritize returning this task to this member in the hopes that some other member may be able to re-use its state.22:36:35.224 [xxxxxx-94ada1ab-8573-4245-9742-3322e412598b-StreamThread-1] ERROR o.apache.kafka.streams.KafkaStreams - stream-client [xxxxxx-94ada1ab-8573-4245-9742-3322e412598b] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now. java.lang.IllegalStateException: Tried to lookup lag for unknown task 2_0 at org.apache.kafka.streams.processor.internals.assignment.ClientState.lagFor(ClientState.java:318) at java.base/java.util.Comparator.lambda$comparingLong$6043328a$1(Comparator.java:511) at java.base/java.util.Comparator.lambda$thenComparing$36697e65$1(Comparator.java:216) at java.base/java.util.TreeMap.put(TreeMap.java:550) at java.base/java.util.TreeSet.add(TreeSet.java:255) at java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352) at java.base/java.util.TreeSet.addAll(TreeSet.java:312) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.getPreviousTasksByLag(StreamsPartitionAssignor.java:1205) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assignTasksToThreads(StreamsPartitionAssignor.java:1119) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.computeNewAssignment(StreamsPartitionAssignor.java:845) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:405) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:589) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:691) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1000(AbstractCoordinator.java:111) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:597) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:560) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1177) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1152) at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206) at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169) at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1296) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1237) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210) at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:925) at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:885) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:720) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:583) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:556){code}",,ableegoldman,iirekm2,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 14 05:56:02 UTC 2021,,,,,,,,,,"0|z0r0ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/21 21:03;ableegoldman;Hey [~iirekm2]

This exception means there's a discrepancy between the tasks that a client is reporting based on its current assignment + local state on disk, and the tasks that the assignor has parsed from the current topology. It means something has gone wrong with your app, but there can be a few different causes. Given your description of hitting this ""every second or third time [you] rerun the application"", my best guess is that you may have an application that is generating at least some subtopologies in a random order. This has been reported before by [someone who was using Sprint Boot|https://issues.apache.org/jira/browse/KAFKA-5882?focusedCommentId=16554237&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16554237] – is it possible your app is experiencing the same issue? You can print out the topology on startup and compare it across reruns to check.;;;","13/May/21 21:11;iirekm2;I'll check the topologies tomorrow.
For now, I downgraded everything from org.apache.kafka groupId to 2.7.1 and it seems to solve the issue. ;;;","14/May/21 05:56;iirekm2;In my case it is Micronaut issue:
- every KStream in Micronaut has to be registered as bean
- Micronaut sometimes reorders beans, hence the error

Tricks like replacing constructor injection with setter injection helped in my case.

Thanks for the tip.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoTopicCreationManager does not handle response errors,KAFKA-12777,13378328,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,mumrah,mumrah,mumrah,13/May/21 15:29,16/Jul/21 21:25,13/Jul/23 09:17,16/Jul/21 21:24,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The request completion callback in AutoTopicCreationManager assumes the response is present. 

{code:scala}
override def onComplete(response: ClientResponse): Unit = {
        debug(s""Auto topic creation completed for ${creatableTopics.keys} with response ${response.responseBody.toString}."")
        clearInflightRequests(creatableTopics)
      }
{code}

We should at least check to see if the response exists before logging it and clearing the in-flight flag. 

This was found while debugging a separate issue. Here is a log snippet:

{code}
[2021-05-13 11:21:03,890] DEBUG [BrokerToControllerChannelManager broker=1 name=forwarding] Version mismatch when attempting to send EnvelopeRequestData(requestData=java.nio.HeapByteBuffer[pos=0 lim=43 cap=43], requestPrincipal=[0, 0, 5, 85, 115, 101, 114, 10, 65, 78, 79, 78, 89, 77, 79, 85, 83, 0, 0], clientHostAddress=[127, 0, 0, 1]) with correlation id 2 to 0 (org.apache.kafka.clients.NetworkClient:495)
org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support ENVELOPE
[2021-05-13 11:21:03,893] ERROR [BrokerToControllerChannelManager broker=1 name=forwarding]: Request EnvelopeRequestData(requestData=java.nio.HeapByteBuffer[pos=0 lim=43 cap=43], requestPrincipal=[0, 0, 5, 85, 115, 101, 114, 10, 65, 78, 79, 78, 89, 77, 79, 85, 83, 0, 0], clientHostAddress=[127, 0, 0, 1]) failed due to unsupported version error (kafka.server.BrokerToControllerRequestThread:76)
org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support ENVELOPE
[2021-05-13 11:21:03,894] ERROR [BrokerToControllerChannelManager broker=1 name=forwarding] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient:576)
java.lang.NullPointerException
	at kafka.server.DefaultAutoTopicCreationManager$$anon$1.$anonfun$onComplete$1(AutoTopicCreationManager.scala:179)
	at kafka.utils.Logging.debug(Logging.scala:62)
	at kafka.utils.Logging.debug$(Logging.scala:62)
	at kafka.server.DefaultAutoTopicCreationManager.debug(AutoTopicCreationManager.scala:67)
	at kafka.server.DefaultAutoTopicCreationManager$$anon$1.onComplete(AutoTopicCreationManager.scala:179)
	at kafka.server.BrokerToControllerRequestThread.handleResponse(BrokerToControllerChannelManager.scala:355)
	at kafka.server.BrokerToControllerRequestThread.$anonfun$generateRequests$1(BrokerToControllerChannelManager.scala:339)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:574)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545)
	at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:74)
	at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManager.scala:374)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{code}

I suspect the NPE is due to {{response.responseBody}} being null.",,cmccabe,dengziming,ijuma,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 16 21:24:50 UTC 2021,,,,,,,,,,"0|z0r0fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/21 15:34;ijuma;Is this really ""minor""?;;;","13/May/21 15:53;mumrah;I think this is mostly cosmetic since the only time this bug would be seen is if something was misconfigured. E.g., authentication errors and apiVersion errors (like I'm seeing). That said, NPEs are never good to see in production code. I'll change this to major;;;","16/Jul/21 21:24;cmccabe;Committed #11069, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use connection timeout when polling the network for new connections,KAFKA-12762,13377261,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ecomar,mimaison,mimaison,07/May/21 15:28,04/Oct/21 16:46,13/Jul/23 09:17,17/Sep/21 13:36,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,,,,,,0,,,,,"In some cases, when connecting to brokers, we end up calling selector.select() with the wrong timeout. Since 2.7, we should use a timeout computed from socket.connection.setup.timeout.ms to ensure we detect bad hosts quickly.

This is especially relevant now that client.dns.lookup defaults to use_all_dns_ips. In case, one of the IPs returned in currently unavailable, we want the client to quickly discover it and avoid timing out user calls.
h4.  ",,ecomar,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-05-07 15:28:20.0,,,,,,,,,,"0|z0qtv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMetadata endOffsets does not update when the offsets are read,KAFKA-12754,13376827,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson5,wcarlson5,wcarlson5,05/May/21 16:10,14/May/21 20:24,13/Jul/23 09:17,14/May/21 19:18,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,The high water mark in StreamTask is not updated optimally. Also it would be good to have the metadata offsets have a initial value of -1 instead of an empty map that way the set of TopicPartitions won't change.,,ableegoldman,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 14 20:23:54 UTC 2021,,,,,,,,,,"0|z0qr74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/21 19:18;ableegoldman;[~wcarlson5] was this feature released in 2.8 and needs this fix to be cherrypicked back, or is it just in 3.0?;;;","14/May/21 20:07;wcarlson5;[~ableegoldman] I think picking back to 2.8 would be good;;;","14/May/21 20:21;ableegoldman;[~wcarlson5] are you sure? I tried to cherrypick this just now and pretty much everything had merge conflicts. It doesn't look like the feature is in 2.8 after all, for example TaskMetadata doesn't have any of the new APIs like committedOffsets(), endOffsets(), or timeCurrentIdlingStarted();;;","14/May/21 20:23;wcarlson5;Huh, I guess I was wrong. Sorry!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2021-28168 upgrade jersey to 2.34 or 3.02,KAFKA-12752,13376819,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,jrstacy,jrstacy,05/May/21 15:35,07/May/21 12:43,13/Jul/23 09:17,06/May/21 14:54,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.0,,,,,,,,,,,0,CVE,security,,,"[https://nvd.nist.gov/vuln/detail/CVE-2021-28168]

CVE-2021-28168 affects jersey versions <=2.33, <=3.0.1. Upgrading to 2.34 or 3.02 should resolve the issue.",,dongjin,jrstacy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 14:06:14 UTC 2021,,,,,,,,,,"0|z0qr5c:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"06/May/21 14:06;dongjin;https://github.com/apache/kafka/pull/10641;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISRs remain in in-flight state if proposed state is same as actual state,KAFKA-12751,13376812,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rsivaram,rsivaram,rsivaram,05/May/21 15:10,13/Sep/21 13:55,13/Jul/23 09:17,18/May/21 08:42,2.7.0,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,,,,,,,core,,,,,0,,,,,"If proposed ISR state in an AlterIsr request is the same as the actual state, Controller returns a successful response without performing any updates. But the broker code that processes the response leaves the ISR state in in-flight state without committing. This prevents further ISR updates until the next leader election.",,jack_foy,priyavj,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 13 13:55:33 UTC 2021,,,,,,,,,,"0|z0qr3s:",9223372036854775807,,mumrah,,,,,,,,,,,,,,,,,,"13/Sep/21 12:44;priyavj;[~rsivaram] We are currently using kafka 2.8  in production. Can you please let me know what are the impacts of this bug?

also, when will 2.8.1 be released?

appreciate your guidance on this

thanks

Priya Vijay;;;","13/Sep/21 13:55;rsivaram;The bug only happens in an unusual case where the proposed ISR is the same as the expected ISR and the chances of hitting that are very low. And it is an issue only if `inter.broker.protocol.version >= 2.7`. If it does occur, further ISR updates don't occur, so the broker will need to be restarted. Release plan for 2.8.1 is here: [https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+2.8.1,] work is under way and the release is expected in within the next few weeks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog topic config on suppressed KTable lost,KAFKA-12749,13376645,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,vishranganathan,philbour,philbour,04/May/21 22:05,04/Jun/21 08:06,13/Jul/23 09:17,03/Jun/21 18:00,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,newbie,newbie++,,,"When trying to set the changelog configuration on a suppressed KTable, the config is lost if either {{emitEarlyWhenFull}} or {{shutDownWhenFull}} is set after the logging config.

This works - 
{code:java}
.suppress(Suppressed.untilTimeLimit(Duration.ofMillis(maxIdleIntervalMs), BufferConfig.maxRecords(
 maxBufferRecords).emitEarlyWhenFull().withLoggingEnabled(changelogConfig)){code}
but not if you set {{emitEarlyWhenFull}} last.

See comments in https://issues.apache.org/jira/browse/KAFKA-8147

 ",,ableegoldman,mjsax,philbour,vishranganathan,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8147,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 20:09:33 UTC 2021,,,,,,,,,,"0|z0qq2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/21 23:49;ableegoldman;Thanks for filing a ticket – we should try to get this fixed up in 3.0 since it's kind of awkward to just silently drop the user's specified configurations, we're lucky that you happened to notice this at all. Just copying over our observation of the underlying problem:
{quote}Seems like BufferConfigInternal#emitEarlyWhenFull creates a new EagerBufferConfigImpl and passes the two original configs (maxRecords and maxBytes) in to the constructor, but loses the logging configs at that point. Same thing for BufferConfigInternal#shutDownWhenFull 
{quote}
I think we can do a trivial, one-line fix to just pass the logging configs as a parameter to the constructor for an immediate patch in 3.0 (or two lines, one for the EagerBufferConfigImpl and one for the StrictBufferConfigImpl constructed in BufferConfigInternal). And maybe also remove the constructor that doesn't accept a logConfig parameter so you're forced to specify it explicitly, whether it's empty/unspecified or not.

Eventually we probably want to refactor things a bit for a more natural fluent API, and try to future-proof things a bit so we don't accidentally introduce bugs like this again. But that's not as urgent;;;","09/May/21 04:41;vishranganathan;Hi [~ableegoldman]. 

I am interested in picking up this issue and get my feet wet in contributing to the Kafka community. I am currently unable to assign the ticket to myself. Would appreciate it if you can assign this ticket to me. 

Thank you;;;","10/May/21 20:09;ableegoldman;Thanks [~vishranganathan], assigned the ticket to you. I also added you as a contributor so you can self-assign from now on;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test RocksDBStoreTest.shouldReturnUUIDsWithStringPrefix,KAFKA-12747,13376445,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ableegoldman,ableegoldman,03/May/21 20:44,10/May/21 20:09,13/Jul/23 09:17,10/May/21 19:33,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,flaky-test,newbie,newbie++,unit-test,"Stacktrace
java.lang.AssertionError: 
Expected: is <1>
     but: was <2>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.state.internals.RocksDBStoreTest.shouldReturnUUIDsWithStringPrefix(RocksDBStoreTest.java:463)

https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-10597/10/tests/",,ableegoldman,josep.prat-inactive,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 12:19:31 UTC 2021,,,,,,,,,,"0|z0qou8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/21 21:13;ableegoldman;Looks like this test is flaky because it assumes no two UUIDs have the same prefix, where the prefix is only the first four digits of the UUID (which of course greatly cuts down on the UUID's probability of uniqueness). We can fix this up by just testing both UUIDs to see if the prefix should apply to just one of the entries in the RocksDBStore, or both.;;;","10/May/21 11:15;jlprat;I'd like to try to solve this one.;;;","10/May/21 11:59;jlprat;Created PR https://github.com/apache/kafka/pull/10662;;;","10/May/21 12:19;jlprat;There was already another PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A single Kerberos login failure fails all future connections from Java 9 onwards,KAFKA-12730,13375774,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,29/Apr/21 08:22,05/May/21 15:39,13/Jul/23 09:17,29/Apr/21 14:45,,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.3,2.7.2,2.8.1,3.0.0,,,,security,,,,,0,,,,,"The refresh thread for Kerberos performs re-login by logging out and then logging in again. If login fails, we retry after a backoff. Every iteration of the loop performs loginContext.logout() and loginContext.login(). If login fails, we end up with two consecutive logouts. This used to work, but from Java 9 onwards, this results in a NullPointerException due to https://bugs.openjdk.java.net/browse/JDK-8173069. We should check if logout is required before attempting logout.",,dengziming,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-29 08:22:25.0,,,,,,,,,,"0|z0qkps:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
misbehaving Task.stop() can prevent other Tasks from stopping,KAFKA-12726,13375637,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ryannedolan,ryannedolan,ryannedolan,28/Apr/21 16:09,03/May/21 19:13,13/Jul/23 09:17,30/Apr/21 20:10,2.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,0,,,,,"We've observed a misbehaving Task fail to stop in a timely manner (e.g. stuck in a retry loop). Despite Connect supporting a property task.shutdown.graceful.timeout.ms, this is currently not enforced – tasks can take as long as they want to stop, and the only consequence is an error message.

We've seen a Worker's ""task-count"" metric double following a rebalance, which we think is due to Tasks not getting cleaned up when Task.stop() is stuck.

While the Connector implementation is ultimately to blame here – a Task probably shouldn't loop forever in stop() – we believe the Connect runtime should handle this situation more gracefully.",,ChrisEgerton,ryannedolan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10792,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 03 19:13:49 UTC 2021,,,,,,,,,,"0|z0qjvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/21 11:40;ChrisEgerton;[~ryannedolan] can you confirm that this has been observed with 2.8.0? This sounds similar to https://issues.apache.org/jira/browse/KAFKA-10792, which should be fixed in 2.8.0.

I'm also not sure about the statement that ""Workers stop Tasks sequentially""–my understanding is that workers _trigger_ task stops sequentially (see [Worker::stopTasks|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L849-L855], which invokes [Worker::stopTask|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L827-L847] consecutively for each to-be-stopped task), but we can see under the hood in [WorkerTask::stop and WorkerTask::triggerStop|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java#L105-L120] that this should complete almost immediately, and neither [WorkerSinkTask|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L157-L162] nor [WorkerSourceTask|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L215-L219] override that method with anything that should block, either. The graceful shutdown period is then enforced en masse in [Worker::awaitStopTasks|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L880-L887].

Given this, I'd be very curious about a reproduction case in the event that my misunderstanding is complete or there is an edge case that is not accounted for currently. Perhaps we could leverage the existing [BlockingConnectorTest|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java] integration test to try to demonstrate where things break down?;;;","29/Apr/21 15:43;ryannedolan;[~ChrisEgerton] ah, indeed I have conflated WorkerTask.stop() and Task.stop(). If I'm (re-)reading this correctly, Task.stop() is called from WorkerTask.close() [here|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L169] (not WorkTask.stop()...) which is called at the end of the WorkerTask's main loop [here|https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java#L197].

wrt KAFKA-10792, we've observed the problem while only running SinkConnectors, so I don't think that particular fix can be related.

So it appears the problem still exists -- a stuck Task.stop() will prevent the WorkerTask from closing, as we've observed in production -- but it's clear I'm mistaken about 1) my remark that Task.stop()s are sequential (they are not) and 2) where this fix needs to go :)

Lemme relocate this logic to WorkerTask.doClose(). However, this begs the question: should we be doing this for every Task method? Seems any stuck method would yield the same behavior.

wrt BlockingConnectorTest, that is indeed where I started my investigation, but the existing tests don't seem to capture this issue. I'll see if I can add a test to repro and show it failing. My understanding is that BlockingConnectorTest is only testing whether subsequent Tasks can created and run, but doesn't test that stopped Tasks are ever actually stopped: https://github.com/apache/kafka/blob/f9de25f046452b2a6d916e6bca41e31d49bbdecf/connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java#L219

In that test, I believe the blocked Task will be leaked, which is what we're observing in production.;;;","29/Apr/21 16:15;ChrisEgerton;Ahhh, that makes sense. We wrestled with this a bit in KAFKA-9374 and the unfortunate conclusion was that you basically can't forcibly kill a thread in Java without doing [this|https://stackoverflow.com/questions/5241822/is-there-a-good-way-to-forcefully-stop-a-java-thread/32909191#32909191]. The current approach is to allow for resource leakage if a task is irretrievably blocked and allow a new task to be brought up in its place, after the graceful shutdown period has elapsed.

It sounds like you're proposing that, if a task has exhausted its graceful shutdown period, we invoke {{Task::stop}} from a separate thread, even if the task is blocked in the middle of a call to something else like {{preCommit}}, {{commitRecord}}, {{put}}, {{poll}}, etc. Is that correct?

I was thinking that the {{BlockingConnectorTest}} can be a starting point if you'd like to reproduce and/or test for this problem, as opposed to providing a test case for this scenario as-is. Just a thought; it may not be the right tool for this job so no worries if something else comes in handy instead.

 ;;;","29/Apr/21 23:28;ryannedolan;[~ChrisEgerton] Yeah, the problem is that we were seeing, say, 100 tasks on each Worker, then a rebalance, then 200 tasks per Worker (as reported by the tasks-count metric) with nothing to do but restart each Worker -- which, ofc, would cause further rebalances!

I'm not proposing we interrupt any threads here. I agree with you that it's reasonable to just leak a thread if a Task impl is stuck indefinitely. But we can leak a stuck thread while cleaning up everything around it. I'm proposing we continue with the WorkerTask shutdown after the grace period, which includes removing the WorkerTask from the list of current tasks (and thus the tasks-count metric).;;;","30/Apr/21 02:26;ChrisEgerton;Okay, that makes the problem clearer (y). Maybe we can update the ticket title/description to match that since I believe we've ruled out the original issue of hung tasks interfering with the lifecycle of their successors?

As far as resource cleanup goes:
 * I agree that pulling the task out of the {{task-count}} metric is warranted; the current behavior sounds buggy and hopefully that change shouldn't be too controversial. And, if it is and people want more insight into zombies that may be running on their workers, [shameless plug|https://cwiki.apache.org/confluence/display/KAFKA/KIP-611%3A+Improved+Handling+of+Abandoned+Connectors+and+Tasks].
 * If we want to go further, there's already precedent for proactively cleaning up other resources after tasks exceed their graceful shutdown timeout; [a (partial) fix|https://github.com/apache/kafka/pull/10016] for KAFKA-10340 involved closing the producers for source tasks on a separate thread as soon as the worker chose to abandon them. Since the issue you describe involved sink tasks specifically, we could also apply similar logic for them and their consumers.
 * Invoking {{Task::stop}} before a returns control to the worker might also be a viable option as long as we also take care to not commit offsets after that point. If the connector uses the framework-provided offset mechanisms (i.e. internal offsets topic for source connectors and consumer offsets for sink connectors), then we can prevent data loss, and any other unexpected failures that happen as a result of the task not being capable of handling a call to {{stop}} while it's in the middle of something else like a {{put}} or a {{poll}} are unlikely to have much impact since the task will already have been abandoned by the worker and a new one may already be running in its place (in fact, it may be better to cause the hung task to fail than to allow it to try to complete gracefully and potentially interfere with a successor).
 * If we go this far, the trickiest part might be deciding in which order we try to perform these cleanup actions. If the task is already hung and we call {{stop}} on it from another thread, it's entirely possible that that new thread will also become blocked up indefinitely. So it may be necessary to try to clean up resources in descending order of ""likelihood to misbehave""; e.g., Kafka clients first (which, unless they're configured with a troublesome interceptor of some sort, are pretty likely to return control in a timely fashion), then the {{OffsetStorageReader}} (for source tasks only), then the {{RetryWithToleranceOperator}}, then the {{transformationChain}} (contains user-written code; now we're getting into the parts that are most likely to hang), then finally the task itself (via {{Task::stop}}). We could also invoke each of these methods on a separate thread so that one doesn't block up the other, but at that point I think we might be beating a dead horse and wasting more resources than we save.

Up to you how far you want to take this; just some thoughts on how resource cleanup in general might be improved for blocked tasks.;;;","30/Apr/21 20:07;ryannedolan;Thanks for the detailed analysis [~ChrisEgerton]. I added some tests to BlockingConnectorTests that try to recreate the issue, but the tests all pass against trunk. Checking with me team, turns out we were running an experimental SourceConnector at the time. So I think it's very likely that this was fixed in KAFKA-10792 after all.

I'll close this ticket and the PR to avoid confusion, and will open a new PR for the additional tests in a bit.;;;","30/Apr/21 20:10;ryannedolan;Closing as duplicate.;;;","03/May/21 19:13;ryannedolan;Relevant tests here: https://github.com/apache/kafka/pull/10629;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionWindows are closed too early,KAFKA-12718,13375163,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,gonzur,mjsax,mjsax,26/Apr/21 21:08,26/Jul/21 23:38,13/Jul/23 09:17,28/Jun/21 22:40,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,beginner,easy-fix,newbie,,"SessionWindows are defined based on a {{gap}} parameter, and also support an additional {{grace-period}} configuration to handle out-of-order data.

To incorporate the session-gap a session window should only be closed at {{window-end + gap}} and to incorporate grace-period, the close time should be pushed out further to {{window-end + gap + grace}}.

However, atm we compute the window close time as {{window-end + grace}} omitting the {{gap}} parameter.

Because default grace-period is 24h most users might not notice this issues. Even if they set a grace period explicitly (eg, when using suppress()), they would most likely set a grace-period larger than gap-time not hitting the issue (or maybe only realize it when inspecting the behavior closely).

However, if a user wants to disable the grace-period and sets it to zero (on any other value smaller than gap-time), sessions might be close too early and user might notice.",,ableegoldman,gonzur,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 19 05:01:00 UTC 2021,,,,,,,,,,"0|z0qgy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/21 19:42;mjsax;I was just looking into `suppress()` implementation, that obviously does not know anything about semantics of upstream window definitions. It computes the ""expiry-time"" based on window-end time. Thus, I believe that the right fix for session windows would be to enforce that gracePeriod > gap. \cc [~ableegoldman] as you proposed to address this inside the session window processor (I think this approach won't work).;;;","30/Apr/21 00:20;ableegoldman;{quote}I was just looking into `suppress()` implementation, that obviously does not know anything about semantics of upstream window definitions{quote}
[~mjsax] can you elaborate? Suppression does in fact search the processor graph to find the grace period of the windowed operator which is upstream of the suppression. It's called GraphGraceSearchUtil or something;;;","30/Apr/21 02:08;mjsax;Well, `gracePeriod` is a generic concept that applies to all windows. However, windows may have quite different semantics, and thus I don't think we should leak actual semantics into `suppress()`? It does not seem desirable to let `suppress()` inspect the actual window type, and do different things based on it? Especially, because it is possible that users implement custom windows.

Thus, my take it that we should stick with `closeTime = windowEnd + grace`. If we would accept to ""leak"" the semantics into `suppress()` we would need to compute `closeTime = windowEnd + gap + grace` (or add `gap` to the user specified `grace` to avoid branching base on window type in suppress() – or maybe set `gap=0` for non-session windows). I guess I could be convinced _if_ there is a good argument that we don't open Pandoras box this way?

If we stick with `closeTime = windowEnd + grace` and keep window semantics out of suppress and session windows semantics stay encapsulated with `SessionWindows` – and `SessionWindows` understand that it should enforce `grace > gap` to work properly.

Thoughts?

[~byusti] Thanks for your interest. Happy to guide you. I guess, we will first need to agree how we want to address this issue though :)

Couple of pointer so you can follow the discussion:
 * suppress() implementation: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorSupplier.java] 
 * test that ""reveals"" the issue: [https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregateProcessorTest.java#L446-L455] 
 * session-windows: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindows.java] ;;;","30/Apr/21 03:26;vvcephei;Thanks for reporting this, [~mjsax] !

What [~ableegoldman] said is correct. The grace period is defined at the windowing operation and inherited downstream by the suppression operator. Which means that when you add a suppression to the topology it has to search up the graph to learn the grace period it should use. This logic is defined in GraphGraceSearchUtil.

In addition to traversing the graph, this logic actually does inspect the type of window definition. IIRC, this was required at the time because of the awkward definition of Windows, but that has just recently been cleaned up, so maybe this logic can now be cleaned up as well.

The relevant snippet is:
{code:java}
private static Long extractGracePeriod(final GraphNode node) {
    if (node instanceof StatefulProcessorNode) {
        final ProcessorSupplier processorSupplier = ((StatefulProcessorNode) node).processorParameters().oldProcessorSupplier();
        if (processorSupplier instanceof KStreamWindowAggregate) {
            final KStreamWindowAggregate kStreamWindowAggregate = (KStreamWindowAggregate) processorSupplier;
            final Windows windows = kStreamWindowAggregate.windows();
            return windows.gracePeriodMs();
        } else if (processorSupplier instanceof KStreamSessionWindowAggregate) {
            final KStreamSessionWindowAggregate kStreamSessionWindowAggregate = (KStreamSessionWindowAggregate) processorSupplier;
            final SessionWindows windows = kStreamSessionWindowAggregate.windows();
            return windows.gracePeriodMs() + windows.inactivityGap();
        } else if (processorSupplier instanceof KStreamSlidingWindowAggregate) {
            final KStreamSlidingWindowAggregate kStreamSlidingWindowAggregate = (KStreamSlidingWindowAggregate) processorSupplier;
            final SlidingWindows windows = kStreamSlidingWindowAggregate.windows();
            return windows.gracePeriodMs();
        } else {
            return null;
        }
    } else {
        return null;
    }
} {code}
Actually, it seems like your concern is actually resolved by this code block, since the effective grace period for sliding windows is actually ""gap + declared grace"".

However, I'm not happy with how difficult it was to track this down. It should have been easy for any reader to verify the correctness of the implementation. It might be good to consider ways to generalize the grace period logic so that it's defined explicitly in the window definition.;;;","30/Apr/21 20:45;mjsax;I just realized that the issue is actually not within `suppress()` but within `KStreamSessionWindowAggregateProcessor` – note that the test is question test the processor, not suppress().

It seem we would prefer to not to enforce `grace > gap` and thus, we should fix how we compute `closeTime` inside the processor and add the currently missing `gap` parameter.

[~byusti] feel free to start working on a PR. Let us know if you have any further questions. I added you to the list of contributors and assigned the ticket. You can know also self-assign tickets.;;;","08/May/21 20:52;gonzur;Hello Mr. Sax, I see someone else already attempted this issue but left prior to finishing it. Would it be alright if I tried looking over your comments and attempted a fix myself?;;;","11/May/21 04:49;mjsax;[~gonzur] it seems [~byusti] lost interest to work on this ticket. Feel free to pick it up.;;;","12/May/21 04:25;gonzur;Thank you for assigning me. I will begin work on it tomorrow morning. :D;;;","28/May/21 02:06;gonzur;Hello [~mjsax]. Sorry for this late follow up but I've mostly fixed the issue. I've corrected the two test cases that were giving issues due to being tuned to accept erroneous results wherein the gap time was previously absent. Them being located in org.apache.kafka.streams.kstream.internals.KStreamSessionWindowAggregateProcessorTest. But now I have an issue with shouldSupportFinalResultsForSessionWindows in org.apache.kafka.streams.kstream.internals.SuppressScenarioTest that I am unsure of how to fix. I do not understand why the test case has been modeled that way. The error result given is as follows:


{code:java}
java.lang.AssertionError: [
 TestRecord[key=[k1@0/0], value=1, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00Z]
 TestRecord[key=[k1@0/0], value=null, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00Z]
 TestRecord[key=[k1@0/5], value=2, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.005Z]
 TestRecord[key=[k1@0/5], value=null, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.005Z]
 TestRecord[key=[k1@0/5], value=3, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.005Z]
 TestRecord[key=[k2@6/6], value=1, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.006Z]
 TestRecord[key=[k1@0/5], value=null, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.005Z]
 TestRecord[key=[k1@0/5], value=4, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.005Z]
 TestRecord[key=[k1@30/30], value=1, headers=RecordHeaders(headers = [], isReadOnly = false), recordTime=1970-01-01T00:00:00.030Z]
 ] != [KeyValueTimestamp{key=[k1@0/0], value=1, timestamp=0}, KeyValueTimestamp{key=[k1@0/0], value=null, timestamp=0}, KeyValueTimestamp{key=[k1@0/5], value=2, timestamp=5}, KeyValueTimestamp{key=[k1@0/5], value=null, timestamp=5}, KeyValueTimestamp{key=[k1@0/5], value=3, timestamp=5}, KeyValueTimestamp{key=[k2@6/6], value=1, timestamp=6}, KeyValueTimestamp{key=[k1@30/30], value=1, timestamp=30}]
{code}
I can't seem to make sense of what k1@0/5 means or where value is derived from as the original only shows ""v1"" in its value argument to pipeInput or why the original has 6 values but output-raw has 7. If you could offer some assistance it would be greatly appreciated. I will be back tomorrow sometime in the afternoon to check back on this again. In the meantime, if you need me to upload my work so far or require anything else to make it easier on you to help simply suggest it.;;;","02/Jun/21 07:06;mjsax;`key=[k1@0/5]` is the key of a session, with data key `k1` and session start time of 0 and session end time of 5. The format is `[dataKey@windowStart/windowEnd]`.

Given the input data we observe and expected the following: The first record creates a new session `k1@0/0` – the second record extend the existing session (gap is set to 5) – for this case, we get a tombstone for the existing sessions and a second record for the new sessions. Thus after processing the first two input records, we have 3 output records.

Seems the first 6 output records are actually the same as in the expected result, but output records 7 and 8 are not expected in the result. Given that grace-period is zero, the fourth input record `k2` with ts=6 actually closes the session `k1@0/5` (before your fix) and thus the 5th input record was not expected to produce any output – however, with the fix, `k2` does not close the window any longer, and thus we get more result records.

I guess the goal of the test was to verify that the first session gets closed, so I think the right fix is to change the input data, ie, the timestamp of input record key=k2 should be changes from 6 to 11 to bump the time beyond session-end plus gap?;;;","02/Jun/21 22:26;guozhang;Hi folks, I'm bumping up the priority of this ticket for 3.0 for now (note that we still have plenty of time towards the code freeze: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177046466 but still would be good to get this easy fix asap).;;;","02/Jun/21 22:53;mjsax;Thanks [~guozhang] – given the last message from [~gonzur], I expect that we get a PR for it soon.

Btw: I also think that getting https://issues.apache.org/jira/browse/KAFKA-12317 into 3.0 might be worth it? Thoughts [~guozhang]? [~gonzur] would be be interested to pick this ticket up, too?;;;","02/Jun/21 23:30;gonzur;I will try to get this PR tonight or tomorrow night depending on circumstances. This test case is the last I need to update in order to agree with the gap changes within the streams test. As to the new ticket itself I would love to pick it up but I've not investigated it with enough depth to be certain whether or not I could beat the 14th deadline for it. I would be happy to try, though :) [~mjsax].

 

Edit: I thought it was June 14th but it's really July. Yes I'll add myself rn;;;","05/Jun/21 02:12;gonzur;Pull request: https://github.com/apache/kafka/pull/10824;;;","17/Jun/21 00:43;gonzur;Attempting to fix the conflict at the moment. I find it odd that some of the compile step fails here as my local branch is having no issues whatsoever. The test cases for streams all succeeded. Since I don't know how to access the profiling report from the crashes in Jenkins I'm not sure what to do. Any advice? [~mjsax];;;","18/Jun/21 05:32;mjsax;Not sure what you mean. Just looked at your PR (and left a few nit comments), and it seems there are no conflict and that Jenkins passed? (One build failed with a know flaky test, so we can ignore it) – We are only interested in the ""test"" builds, not the ""stags"" builds.;;;","19/Jun/21 02:12;gonzur;I see. But yes that was what I meant. Thanks for looking at the issue and explaining what you're concerned about as it relates to builds. I'd heard about flake tests from the contribution instructions but didn't know which were flakes and which weren't. Does the project have a compiled list of them or is the stages and test build that division itself? 

I'll fix the nits rn;;;","19/Jun/21 05:01;mjsax;For flaky tests that fail more recently, we create Jira tickets. But in general, you don't need to worry about it and the reviewer will guide you. – As long as it's not a compile or checkstyle error, or a test that fails reproducible in your local env, it's most likely a flaky test. In the end, the committer who merges the PR is responsible to ensure we don't merge bad PRs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRaft: Missing controller.quorom.voters config not properly handled,KAFKA-12712,13374701,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,edenhill,edenhill,23/Apr/21 16:46,03/Dec/21 01:30,13/Jul/23 09:17,03/Dec/21 01:30,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,kip-500,,,,"When trying out KRaft in 2.8 I mispelled controller.quorum.voters as controller.quorum.voters, but the broker did not fail to start, nor did it print any warning.

 

Instead it raised this error:

 
{code:java}
[2021-04-23 18:25:13,484] INFO Starting controller (kafka.server.ControllerServer)[2021-04-23 18:25:13,484] INFO Starting controller (kafka.server.ControllerServer)[2021-04-23 18:25:13,485] ERROR [kafka-raft-io-thread]: Error due to (kafka.raft.KafkaRaftManager$RaftIoThread)java.lang.IllegalArgumentException: bound must be positive at java.util.Random.nextInt(Random.java:388) at org.apache.kafka.raft.RequestManager.findReadyVoter(RequestManager.java:57) at org.apache.kafka.raft.KafkaRaftClient.maybeSendAnyVoterFetch(KafkaRaftClient.java:1778) at org.apache.kafka.raft.KafkaRaftClient.pollUnattachedAsObserver(KafkaRaftClient.java:2080) at org.apache.kafka.raft.KafkaRaftClient.pollUnattached(KafkaRaftClient.java:2061) at org.apache.kafka.raft.KafkaRaftClient.pollCurrentState(KafkaRaftClient.java:2096) at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:2181) at kafka.raft.KafkaRaftManager$RaftIoThread.doWork(RaftManager.scala:53) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{code}
which I guess eventually (1 minute later) lead to this error which terminated the broker:
{code:java}
[2021-04-23 18:26:14,435] ERROR [BrokerLifecycleManager id=2] Shutting down because we were unable to register with the controller quorum. (kafka.server.BrokerLifecycleManager)[2021-04-23 18:26:14,435] ERROR [BrokerLifecycleManager id=2] Shutting down because we were unable to register with the controller quorum. (kafka.server.BrokerLifecycleManager)[2021-04-23 18:26:14,436] INFO [BrokerLifecycleManager id=2] registrationTimeout: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)[2021-04-23 18:26:14,437] INFO [BrokerLifecycleManager id=2] Transitioning from STARTING to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)[2021-04-23 18:26:14,437] INFO [broker-2-to-controller-send-thread]: Shutting down (kafka.server.BrokerToControllerRequestThread)[2021-04-23 18:26:14,438] INFO [broker-2-to-controller-send-thread]: Stopped (kafka.server.BrokerToControllerRequestThread)[2021-04-23 18:26:14,438] INFO [broker-2-to-controller-send-thread]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)[2021-04-23 18:26:14,441] ERROR [BrokerServer id=2] Fatal error during broker startup. Prepare to shutdown (kafka.server.BrokerServer)java.util.concurrent.CancellationException at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276) at kafka.server.BrokerLifecycleManager$ShutdownEvent.run(BrokerLifecycleManager.scala:474) at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:174) at java.lang.Thread.run(Thread.java:748)
{code}
But since the client listeners were made available prior to shutting down, the broker was deemed up and operational by the (naiive) monitoring tool.

So..:

 - Broker should fail on startup on invalid/unknown config properties. I understand this is tehcnically tricky, so at least a warning log should be printed.

 - Perhaps not create client listeners before control plane is somewhat happy.

 ",,cmccabe,dajac,edenhill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 03 01:30:26 UTC 2021,,,,,,,,,,"0|z0qe3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/21 21:17;kkonstantine;Postponing to the subsequent release given that this issue is not a blocker and did not make it on time for 3.0 code freeze. ;;;","15/Nov/21 10:10;dajac;Moving to the next release as we are past the 3.1 release code freeze.;;;","03/Dec/21 01:30;cmccabe;This now fails with:

org.apache.kafka.common.config.ConfigException: If using process.roles, controller.quorum.voters must contain a parseable set of voters.

I'll close this JIRA. Thanks, all.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow unencrypted private keys when using PEM files,KAFKA-12703,13374141,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,trobador,trobador,21/Apr/21 13:17,16/May/22 07:26,13/Jul/23 09:17,16/May/22 07:26,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,clients,,,,,0,,,,,"Unencrypted PEM files seem to be internally [supported in the codebase|https://github.com/apache/kafka/blob/a46beb9d29781e0709baf596601122f770a5fa31/clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java#L509] but setting an ssl.key.password is currently enforced by createKeystore (on DefaultSslEngineFactory). I was unable to find a reason for this, so I wonder if this limitation could simply be removed:
 [https://github.com/pera/kafka/commit/8df2feab5fc6955cf8c89a7d132f05d8f562e16b]

 

Thanks",,dongjin,trobador,villevaltonen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 30 13:46:18 UTC 2021,,,,,,,,,,"0|z0qanc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/21 12:04;dongjin;[~trobador] It seems like you are right. According to [KIP-651|https://cwiki.apache.org/confluence/display/KAFKA/KIP-651+-+Support+PEM+format+for+SSL+certificates+and+private+key] which introduced 'ssl.key.password', it states ""If the key is encrypted, key password must be specified using 'ssl.key.password'."" In other words, it allows key password may not be specified.

 [~rsivaram] [~omkreddy] Could you have a look? I thought you must be the best reviewer since you wrote or reviewed the KIP.;;;","30/Nov/21 13:46;villevaltonen;What's the status on this issue? Any active development going on?

I can confirm, that it's possible to configure a client with unencrypted key through the newly added configuration options 'ssl.keystore.key', 'ssl.keystore.certificate.chain' and 'ssl.truststore.certificates"" in [KIP-651|https://cwiki.apache.org/confluence/display/KAFKA/KIP-651+-+Support+PEM+format+for+SSL+certificates+and+private+key], because 'ssl.key.password' is not enforced by validation then. But if you add the same key and certificates through 'ssl.keystore.location' and 'ssl.truststore.location' while the types are set to 'PEM', the configuration validation enforces to add 'ssl.key.password'. By adding a value, e.g. even an empty string, for 'ssl.key.password', the default SSL engine proceeds to try to decrypt the key and eventually throwing an exception. By encrypting the same key with PKCS8 and providing a password, everything works.

Either the requirement for 'ssl.key.password' while using PEM-files should be removed, add a mechanism to recognize whether the key is encrypted or not or consolidate the behavior between the mechanisms of adding the key, i.e requiring an encrypted key always. In the current form the feature and its documentation is hard to interpret.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unhandled exception caught in InterBrokerSendThread,KAFKA-12702,13374054,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,wenbing.shen,wenbing.shen,wenbing.shen,21/Apr/21 08:05,26/Apr/21 06:59,13/Jul/23 09:17,25/Apr/21 13:29,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,0,,,,,"In kraft mode, if listeners and advertised.listeners are not configured with host addresses, the host parameter value of Listener in BrokerRegistrationRequestData will be null. When the broker is started, a null pointer exception will be thrown, causing startup failure.

A feasible solution is to replace the empty host of endPoint in advertisedListeners with InetAddress.getLocalHost.getCanonicalHostName in Broker Server when building networkListeners.

The following is the debug log:

before fixing:

[2021-04-21 14:15:20,032] DEBUG (broker-2-to-controller-send-thread org.apache.kafka.clients.NetworkClient 522) [broker-2-to-controller] Sending BROKER_REGISTRATION request with header RequestHeader(apiKey=BROKER_REGIS
TRATION, apiVersion=0, clientId=2, correlationId=6) and timeout 30000 to node 2: BrokerRegistrationRequestData(brokerId=2, clusterId='nCqve6D1TEef3NpQniA0Mg', incarnationId=X8w4_1DFT2yUjOm6asPjIQ, listeners=[Listener(n
ame='PLAINTEXT', {color:#FF0000}host=null,{color} port=9092, securityProtocol=0)], features=[], rack=null)
[2021-04-21 14:15:20,033] ERROR (broker-2-to-controller-send-thread kafka.server.BrokerToControllerRequestThread 76) [broker-2-to-controller-send-thread]: unhandled exception caught in InterBrokerSendThread
java.lang.NullPointerException
 at org.apache.kafka.common.message.BrokerRegistrationRequestData$Listener.addSize(BrokerRegistrationRequestData.java:515)
 at org.apache.kafka.common.message.BrokerRegistrationRequestData.addSize(BrokerRegistrationRequestData.java:216)
 at org.apache.kafka.common.protocol.SendBuilder.buildSend(SendBuilder.java:218)
 at org.apache.kafka.common.protocol.SendBuilder.buildRequestSend(SendBuilder.java:187)
 at org.apache.kafka.common.requests.AbstractRequest.toSend(AbstractRequest.java:101)
 at org.apache.kafka.clients.NetworkClient.doSend(NetworkClient.java:525)
 at org.apache.kafka.clients.NetworkClient.doSend(NetworkClient.java:501)
 at org.apache.kafka.clients.NetworkClient.send(NetworkClient.java:461)
 at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1(InterBrokerSendThread.scala:104)
 at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1$adapted(InterBrokerSendThread.scala:99)
 at kafka.common.InterBrokerSendThread$$Lambda$259/910445654.apply(Unknown Source)
 at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
 at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:919)
 at kafka.common.InterBrokerSendThread.sendRequests(InterBrokerSendThread.scala:99)
 at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:73)
 at kafka.server.BrokerToControllerRequestThread.doWork(BrokerToControllerChannelManager.scala:368)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2021-04-21 14:15:20,034] INFO (broker-2-to-controller-send-thread kafka.server.BrokerToControllerRequestThread 66) [broker-2-to-controller-send-thread]: Stopped



after fixing:

[2021-04-21 15:05:01,095] DEBUG (BrokerToControllerChannelManager broker=2 name=heartbeat org.apache.kafka.clients.NetworkClient 512) [BrokerToControllerChannelManager broker=2 name=heartbeat] Sending BROKER_REGISTRATI
ON request with header RequestHeader(apiKey=BROKER_REGISTRATION, apiVersion=0, clientId=2, correlationId=0) and timeout 30000 to node 2: BrokerRegistrationRequestData(brokerId=2, clusterId='nCqve6D1TEef3NpQniA0Mg', inc
arnationId=xF29h_IRR1KzrERWwssQ2w, listeners=[Listener(name='PLAINTEXT', host='hdpxxx.cn', port=9092, securityProtocol=0)], features=[], rack=null)

 

 ",,wenbing.shen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 08:03;wenbing.shen;afterFixing.png;https://issues.apache.org/jira/secure/attachment/13024366/afterFixing.png","21/Apr/21 08:03;wenbing.shen;beforeFixing.png;https://issues.apache.org/jira/secure/attachment/13024367/beforeFixing.png","21/Apr/21 09:12;wenbing.shen;image-2021-04-21-17-12-28-471.png;https://issues.apache.org/jira/secure/attachment/13024370/image-2021-04-21-17-12-28-471.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 09:12:34 UTC 2021,,,,,,,,,,"0|z0qa48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 09:12;wenbing.shen;We need to fix this problem, because according to the comments in the configuration file (config/kraft/server.properties), if listeners and advertised.listeners are not configured with an address, the program will automatically obtain java.net.InetAddress.getCanonicalHostName(), but this will actually cause the service to fail to start. !image-2021-04-21-17-12-28-471.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in MetadataRequest when using topic IDs,KAFKA-12701,13374025,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,twmb,twmb,21/Apr/21 06:21,22/Aug/21 03:47,13/Jul/23 09:17,22/Aug/21 03:47,2.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Authorized result checking relies on topic name to not be null, which, when using topic IDs, it is.

Unlike the logic in handleDeleteTopicsRequest, handleMetadataRequest does not check zk for the names corresponding to topic IDs if topic IDs are present.

{noformat}
[2021-04-21 05:53:01,463] ERROR [KafkaApi-1] Error when handling request: clientId=kgo, correlationId=1, api=METADATA, version=11, body=MetadataRequestData(topics=[MetadataRequestTopic(topicId=LmqOoFOASnqQp_4-oJgeKA, name=null)], allowAutoTopicCreation=false, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) (kafka.server.RequestHandlerHelper)
java.lang.NullPointerException: name
	at java.base/java.util.Objects.requireNonNull(Unknown Source)
	at org.apache.kafka.common.resource.ResourcePattern.<init>(ResourcePattern.java:50)
	at kafka.server.AuthHelper.$anonfun$filterByAuthorized$3(AuthHelper.scala:121)
	at scala.collection.Iterator$$anon$9.next(Iterator.scala:575)
	at scala.collection.mutable.Growable.addAll(Growable.scala:62)
	at scala.collection.mutable.Growable.addAll$(Growable.scala:57)
	at scala.collection.mutable.ArrayBuffer.addAll(ArrayBuffer.scala:142)
	at scala.collection.mutable.ArrayBuffer.addAll(ArrayBuffer.scala:42)
	at scala.collection.mutable.ArrayBuffer$.from(ArrayBuffer.scala:258)
	at scala.collection.mutable.ArrayBuffer$.from(ArrayBuffer.scala:247)
	at scala.collection.SeqFactory$Delegate.from(Factory.scala:306)
	at scala.collection.IterableOnceOps.toBuffer(IterableOnce.scala:1270)
	at scala.collection.IterableOnceOps.toBuffer$(IterableOnce.scala:1270)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1288)
	at kafka.server.AuthHelper.filterByAuthorized(AuthHelper.scala:120)
	at kafka.server.KafkaApis.handleTopicMetadataRequest(KafkaApis.scala:1146)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:170)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:74)
	at java.base/java.lang.Thread.run(Unknown Source)
[2021-04-21 05:53:01,464] ERROR [Kafka Request Handler 1 on Broker 1], Exception when handling request (kafka.server.KafkaRequestHandler)
java.lang.NullPointerException
	at org.apache.kafka.common.message.MetadataResponseData$MetadataResponseTopic.addSize(MetadataResponseData.java:1247)
	at org.apache.kafka.common.message.MetadataResponseData.addSize(MetadataResponseData.java:417)
	at org.apache.kafka.common.protocol.SendBuilder.buildSend(SendBuilder.java:218)
	at org.apache.kafka.common.protocol.SendBuilder.buildResponseSend(SendBuilder.java:200)
	at org.apache.kafka.common.requests.AbstractResponse.toSend(AbstractResponse.java:43)
	at org.apache.kafka.common.requests.RequestContext.buildResponseSend(RequestContext.java:111)
	at kafka.network.RequestChannel$Request.buildResponseSend(RequestChannel.scala:132)
	at kafka.server.RequestHandlerHelper.sendResponse(RequestHandlerHelper.scala:185)
	at kafka.server.RequestHandlerHelper.sendErrorOrCloseConnection(RequestHandlerHelper.scala:155)
	at kafka.server.RequestHandlerHelper.sendErrorResponseMaybeThrottle(RequestHandlerHelper.scala:109)
	at kafka.server.RequestHandlerHelper.handleError(RequestHandlerHelper.scala:79)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:229)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:74)
	at java.base/java.lang.Thread.run(Unknown Source)
{noformat}",,dengziming,ijuma,jolshan,showuon,twmb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 20:50:49 UTC 2021,,,,,,,,,,"0|z0q9xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 06:52;dengziming;Describe a topic using topicId is not supported in 2.8.0, can you provide your request or command?;;;","21/Apr/21 07:23;twmb;Metadata v11 is supported in 2.8.0. I've added support for specifying topic IDs in my {{kcl}} command line client [here|https://github.com/twmb/kcl/commit/a09f6f8cca4f87b878d943a2bed4ef8ed2e10a7e] (you'd need to build off of master if you want to test this yourself).

{noformat}
[01:20:51]
twmb@h4x3r:~
$ kcl admin topic create foo
NAME  ID                                MESSAGE
foo   6a62c01129a341c8a0231f7b3c21bc9b  OK

[01:20:57]
twmb@h4x3r:~
$ kcl metadata -t --ids 6a62c01129a341c8a0231f7b3c21bc9b
^C

[01:21:11]
twmb@h4x3r:~
$ kcl metadata -t foo
NAME  ID                                PARTITIONS  REPLICAS
foo   6a62c01129a341c8a0231f7b3c21bc9b  20          1

[01:21:16]
twmb@h4x3r:~
$ kcl admin topic delete --ids 6a62c01129a341c8a0231f7b3c21bc9b
foo   OK
{noformat}

What this is doing is issuing a metadata request that has a single topic where the topic name is null and the topic ID is the hex-decoded ID that is passed in.;;;","21/Apr/21 09:18;dengziming;In fact, we first planned to support topicId in MetadataRequest and MetadataResponse, and we added 2 tickets KAFKA-10547 and  KAFKA-10774.

though KAFKA-10547 add topicId in MetadataResponse and MetadataRequest, describe topic using topicId is supported by KAFKA-10774.

sadly KAFKA-10547 PR was merged but KAFKA-10774 PR was held off, for more details: [https://github.com/apache/kafka/pull/9769#issuecomment-772830472]

I think this is a bug so we should fix this for 2.8.0, I will submit a PR target at 2.8.0 to remove TopicID from MetadataRequest since it isn't supported.;;;","21/Apr/21 15:24;jolshan;Thanks [~twmb] and [~dengziming] for taking a look at this. I am not sure this is something for 2.8.0 since that release has already passed the vote. I'm thinking that this is ok. We should still fix for 3.0 and possibly 2.8.x though. Does that make sense?;;;","21/Apr/21 15:29;ijuma;I suggest fixing it in 3.0.0 and 2.8.1.;;;","21/Apr/21 15:41;jolshan;[~dengziming] Just to clarify, we did not yet implement functionality to handle using topic IDs in MetadataRequest. So even if we get past the NPE, we wouldn't handle the topic ID yet. That will be added in https://github.com/apache/kafka/pull/9769, correct? Maybe for 2.8.1 we can return an error that the operation is not yet supported for v11 requests. We'll have to add this to trunk too. Then in https://github.com/apache/kafka/pull/9769, we bump the protocol to v12 so that the clients know that topic IDs are supported.;;;","21/Apr/21 20:50;jolshan;[~dengziming] I can work on the part where we return an error for earlier versions if you haven't started on that already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The admin.listeners config has wonky valid values in the docs,KAFKA-12700,13373977,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,ableegoldman,ableegoldman,21/Apr/21 01:35,22/Apr/21 03:41,13/Jul/23 09:17,22/Apr/21 03:41,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,docs,KafkaConnect,,,,0,,,,,"Noticed this while updating the docs for the 2.6.2 release, the docs for these configs are generated from the config definition, including info such as default, type, valid values, etc. 

When defining WorkerConfig.ADMIN_LISTENERS_CONFIG we seem to pass an actual `new AdminListenersValidator()` object in for the ""valid values"" parameter, causing this field to display some wonky useless object reference in the docs. See https://kafka.apache.org/documentation/#connectconfigs_admin.listeners:

Valid Values:	org.apache.kafka.connect.runtime.WorkerConfig$AdminListenersValidator@383534aa",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 07:38:34 UTC 2021,,,,,,,,,,"0|z0q9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 07:38;showuon;Nice catch!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams no longer overrides the java default uncaught exception handler  ,KAFKA-12699,13373976,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,wcarlson5,wcarlson5,wcarlson5,21/Apr/21 01:31,15/Aug/22 21:08,13/Jul/23 09:17,19/Jul/22 20:38,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.4.0,,,,,,,,streams,,,,,0,,,,,"If a user used `Thread.setUncaughtExceptionHanlder()` to set the handler for all threads in the runtime streams would override that with its own handler. However since streams does not use the `Thread` handler anymore it will no longer do so. This can cause problems if the user does something like `System.exit(1)` in the handler. 

 

If using the old handler in streams it will still work as it used to",,ableegoldman,cadonna,dajac,marcolotz,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 05 15:50:07 UTC 2022,,,,,,,,,,"0|z0q9mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/21 02:57;ableegoldman;To be fair, I think claiming that this is a bug that Streams ""no longer overrides the java default handler"" is a bit harsh since (a) as you point out, this isn't a regression but just a different behavior in a new feature, and (b) it was never specified or implied that the new feature would do exactly the same thing as the old feature. I would actually argue that continuing to override the java default handler when the user has not actually supplied a Thread.UncaughtExceptionHandler to override it would be more unexpected.

However, I do think it's a bit odd that we would end up invoking the default thread uncaught exception handler at all, since Streams will have already caught and invoked the user-defined handler at that point. The ""bug"" from my perspective is that we rethrow the exception up through run(), vs swallowing it once we reach the outer try in StreamThread#run.

If we think that the user-defined default handler should not be invoked when using the new StreamsUncaughtExceptionHandler (and I think that assumption is not a given, although I don't feel strongly for or against), and should be overriden with a no-op handler, then why continue to throw the exception. And if we don't throw the exception, then why do we need to override with a no-op -- just my line of thinking, again I'm not sure what a realistic user expected behavior is here. But setting a global exception handler and then just blindly calling System.exit in a multi-threaded app where you've consciously implemented a handler that makes it clear threads can die and be replaced...seems like user error to me;;;","21/Apr/21 03:09;wcarlson5;I would not say its a bug per-say but I just wanted anyone surprised by the behavior to know that it was known. Also I am not sure what else to categorize it as

We might be able to handle the EOS corner cases by changing around where the catch block is. I am not sure a no-op would be better, but I do think that since the thread died exceptionally we should have the exception raised again to make sure it it handled properly. Also we should do our best to make sure the exception and thread are paired.  I could see a case where the stream user and the owner of the runtime are not entirely equal so might have different requirements. I do agree calling System.exit might be a bit extreme but they could be tracking threads deaths and exceptions for some reason.;;;","21/Apr/21 23:23;ableegoldman;> we should have the exception raised again to make sure it it handled properly.
I agree with the sentiment here, but if we disable the uncaught exception handler then there's no other handling that is/can be done. I'm also not sure what EOS has to do with this, in both ALOS and EOS we want to (and have to) do a ""dirty close"", otherwise we're committing bad data

> stream user and the owner of the runtime are not entirely equal so might have different requirements
Good point
;;;","21/Apr/21 23:24;ableegoldman;Marking it down for 3.0 so we don't forget;;;","09/Jul/21 03:20;kkonstantine;[~ableegoldman] [~wcarlson5] is this issue a blocker for 3.0. Can we still make it on time before code freeze? Please advise if we should postpone to the next version or wait for a PR;;;","19/Jul/21 21:08;kkonstantine;Postponing to the subsequent release given that this issue is not a blocker and did not make it on time for the 3.0 code freeze. ;;;","15/Nov/21 10:10;dajac;Moving to the next release as we are past the 3.1 release code freeze.;;;","05/Apr/22 15:50;cadonna;Removing from the 3.2.0 release since code freeze has passed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMetadata timeSinceIdlingStarted not reporting correctly,KAFKA-12691,13373708,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson5,wcarlson5,wcarlson5,19/Apr/21 21:39,23/Apr/21 16:40,13/Jul/23 09:17,22/Apr/21 02:43,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,TaskMetadata timeSinceIdlingStarted not reporting correctly. It takes into account suspended but not the call to is processable. To fix this we need to record when the first time it is not processable. ,,ableegoldman,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-19 21:39:50.0,,,,,,,,,,"0|z0q7zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in AlterIsr response handling,KAFKA-12686,13373633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,19/Apr/21 14:17,17/Nov/21 20:28,13/Jul/23 09:17,09/Jul/21 03:24,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In Partition.scala, there is a race condition between the handling of an AlterIsrResponse and a LeaderAndIsrRequest. This is a pretty rare scenario and would involve the AlterIsrResponse being delayed for some time, but it is possible. This was observed in a test environment when lots of ISR and leadership changes were happening due to broker restarts.

When the leader handles the LeaderAndIsr, it calls Partition#makeLeader which overrides the {{isrState}} variable and clears the pending ISR items via {{AlterIsrManager#clearPending(TopicPartition)}}. 

The bug is that AlterIsrManager does not check its inflight state before clearing pending items. The way AlterIsrManager is designed, it retains inflight items in the pending items collection until the response is processed (to allow for retries). The result is that an inflight item is inadvertently removed from this collection.

Since the inflight item is cleared from the collection, AlterIsrManager allows for new AlterIsrItem-s to be enqueued for this partition even though it has an inflight AlterIsrItem. By allowing an update to be enqueued, Partition will transition its {{isrState}} to one of the inflight states (PendingIsrExpand, PendingIsrShrink, etc). Once the inflight partition's response is handled, it will fail to update the {{isrState}} due to detecting changes since the request was sent (which is by design). However, after the response callback is run, AlterIsrManager will clear the partitions that it saw in the response from the unsent items collection. This includes the newly added (and unsent) update.

The result is that Partition has a ""inflight"" isrState but AlterIsrManager does not have an unsent item for this partition. This prevents any further ISR updates on the partition until the next leader election (when {{isrState}} is reset).

If this bug is encountered, the workaround is to force a leader election which will reset the partition's state.",,jack_foy,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12687,,,KAFKA-13227,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-19 14:17:26.0,,,,,,,,,,"0|z0q7io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The valid partition list is incorrectly replaced by the successfully elected partition list,KAFKA-12684,13373436,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,wenbing.shen,wenbing.shen,wenbing.shen,19/Apr/21 02:37,26/Apr/21 06:55,13/Jul/23 09:17,25/Apr/21 13:15,2.6.0,2.7.0,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,tools,,,,,0,,,,,"When using the kafka-election-tool for preferred replica election, if there are partitions in the elected list that are in the preferred replica, the list of partitions already in the preferred replica will be replaced by the successfully elected partition list.

 ",,wenbing.shen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/21 02:37;wenbing.shen;election-preferred-leader.png;https://issues.apache.org/jira/secure/attachment/13024260/election-preferred-leader.png","19/Apr/21 02:37;wenbing.shen;non-preferred-leader.png;https://issues.apache.org/jira/secure/attachment/13024259/non-preferred-leader.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-19 02:37:50.0,,,,,,,,,,"0|z0q6aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kraft MetadataPartitionsBuilder _localChanged and _localRemoved out of order ,KAFKA-12682,13373269,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,jacky0123,jacky0123,18/Apr/21 06:00,03/Dec/21 01:31,13/Jul/23 09:17,03/Dec/21 01:31,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,kip-500,,,,"In version 2.8, MetadataPartitionsBuilder has the field _localChanged and _localRemoved which record the change and delete partition, but we always process _localChanged partitions, and then _localRemoved in the kafka.server.RaftReplicaManager#handleMetadataRecords, not respect the original order, for example, 
1. migrate the partition p1 from b0 to b1;
2. change the leader of p1 
3.migrate p1 from b1 to b0
and the _localRemoved will delete the p1 at last.

and I think MetadataPartition should include topic uuid, and the topic name is optional
for example,
create topic t1, delete topic t1, create topic t1, change leader of p1
and then compact the records 
delete topic t1, change t1, p1

but currently, implementation will be
1. process change t1, p1
2. process delete topic t1

but the MetadataPartition doesn't include topic uuid, it only includes topic name, when to process, it can't find the origin topic uuid, and find the latest the topic id, but it's not right. and delete topic t1 should do before create t1 or change p1.
",,cmccabe,jacky0123,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 03 01:31:25 UTC 2021,,,,,,,,,,"0|z0q59s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/21 01:31;cmccabe;This was fixed in 3.0 when we rewrote the ReplicaManager logic for KRaft.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The raftCluster always send to the wrong active controller and never update,KAFKA-12677,13372959,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,showuon,showuon,16/Apr/21 09:41,12/Jul/21 19:50,13/Jul/23 09:17,12/Jul/21 16:51,2.8.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,kip-500,,,,"We introduce KIP-500 to introduce a Self-Managed Metadata Quorum. We should always have 1 active controller, and all the RPC will send to the active controller. But there's chances that the active controller already changed, but the RPC still send to the old one.

In the attachment log, we can see:
{code:java}
[Controller 3002] Becoming active at controller epoch 1. 
...
[Controller 3000] Becoming active at controller epoch 2. 
{code}
So, the latest active controller should be 3000. But the create topic RPC are all sending to controller 3002:
{code:java}
""errorMessage"":""The active controller appears to be node 3000""
{code}
This bug causes the RaftClusterTests flaky.

 

Debug log while running testCreateClusterAndCreateListDeleteTopic test: https://drive.google.com/file/d/1WVUgy1Erjx8mHyofiP9MVvQGb0LcDYt3/view?usp=sharing",,ableegoldman,cmccabe,dengziming,ijuma,showuon,suriyav,wenbing.shen,,,,,,,,,,,,,,,,,,,,KAFKA-12629,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 30 00:44:52 UTC 2021,,,,,,,,,,"0|z0q3cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 09:45;showuon;[~cmccabe], could you take a look at this bug? If you are busy, you can also guide me where to look into. Thank you.;;;","16/Apr/21 17:15;cmccabe;[~showuon]: thanks for looking at this.  This test failure has been frustrating everyone for a while.  It's great that you reproduced it with DEBUG logging.

bq. The raftCluster always send to the wrong active controller and never update

Hmm... If you do a grep for ""Recorded new controller"" you can see that it does change eventually.

{code}
[cmccabe@zeratul Downloads]$ grep 'Recorded new controller' raftClusterTest_bug.txt 
2021-04-13T13:40:21.708+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,325] INFO [broker2_:BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:21.711+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,376] INFO [broker1_:BrokerToControllerChannelManager broker=1 name=alterIsr]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:21.711+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,376] INFO [broker2_:BrokerToControllerChannelManager broker=2 name=alterIsr]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:21.711+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,376] INFO [broker0_:BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:21.712+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,421] INFO [broker1_:BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
....
2021-04-13T13:40:21.837+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,660] INFO [broker0_:BrokerToControllerChannelManager broker=0 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54228 (id: 3002 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:21.866+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:21,866] INFO [broker2_:BrokerToControllerChannelManager broker=2 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54229 (id: 3000 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:22.324+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:22,013] INFO [broker1_:BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54229 (id: 3000 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
2021-04-13T13:40:22.428+0800 [DEBUG] [TestEventLogger]     [2021-04-13 13:40:22,073] INFO [broker0_:BrokerToControllerChannelManager broker=0 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54229 (id: 3000 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
{code}

Maybe the question is why it takes so long to change our view of the controller on the broker side? That comes directly out of the raft client on the broker. cc [~hachikuji]

Another question is whether we should use the information passed back by the standby controller to fail over quicker.  You can see this in the error that was returned:

{code}
""errorCode"":41,""errorMessage"":""The active controller appears to be node 3000""
{code}

We could potentially try node 3000 after receiving that error (although this might complicate things in other ways...);;;","29/May/21 23:10;showuon;[~cmccabe], I've found the root cause of this issue, and have a fix to it. Do you mind if I take over this ticket? Thank you.;;;","30/May/21 00:44;ijuma;Go for it [~showuon];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running test-kraft-server-start results in error,KAFKA-12672,13372696,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,bbejeck,bbejeck,15/Apr/21 17:14,16/Apr/21 13:47,13/Jul/23 09:17,16/Apr/21 13:47,3.0.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,0,,,,,"Running the {{test-kraft-server-start}} script in the {{raft}} module results in this error

 
{code:java}
ERROR Exiting raft server due to fatal exception (kafka.tools.TestRaftServer$)
java.lang.IllegalArgumentException: No enum constant org.apache.kafka.common.security.auth.SecurityProtocol.
	at java.lang.Enum.valueOf(Enum.java:238)
	at org.apache.kafka.common.security.auth.SecurityProtocol.valueOf(SecurityProtocol.java:26)
	at org.apache.kafka.common.security.auth.SecurityProtocol.forName(SecurityProtocol.java:72)
	at kafka.raft.KafkaRaftManager.$anonfun$buildNetworkClient$1(RaftManager.scala:256)
	at scala.collection.immutable.Map$Map4.getOrElse(Map.scala:530)
	at kafka.raft.KafkaRaftManager.buildNetworkClient(RaftManager.scala:256)
	at kafka.raft.KafkaRaftManager.buildNetworkChannel(RaftManager.scala:234)
	at kafka.raft.KafkaRaftManager.<init>(RaftManager.scala:126)
	at kafka.tools.TestRaftServer.startup(TestRaftServer.scala:88)
	at kafka.tools.TestRaftServer$.main(TestRaftServer.scala:442)
	at kafka.tools.TestRaftServer.main(TestRaftServer.scala)
{code}
Looks like the listener property in the config is not getting picked up as an empty string gets passed to {{SecurityProtocol.forName}}

EDIT: The issue is the properties file needs to have a {{controller.listener.names}} property with just values of the names",,bbejeck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-15 17:14:53.0,,,,,,,,,,"0|z0q1qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect error log on StateDirectory close,KAFKA-12667,13372213,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,antrenta,antrenta,14/Apr/21 07:07,14/Apr/21 20:12,13/Jul/23 09:17,14/Apr/21 20:12,2.6.1,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,2.6.3,2.7.2,2.8.1,3.0.0,,,,,streams,,,,,0,,,,,"{{In StateDirectory.close() an error is logged about unclean shutdown if all locks are in fact released, and nothing is logged in case of an unclean shutdown.}}

 
{code:java}
// all threads should be stopped and cleaned up by now, so none should remain holding a lock
if (locks.isEmpty()) {
 log.error(""Some task directories still locked while closing state, this indicates unclean shutdown: {}"", locks);
}

{code}
 ",,ableegoldman,antrenta,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 17:45:32 UTC 2021,,,,,,,,,,"0|z0pyr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 07:30;cadonna;[~antrenta] Thank you for the ticket! This seems indeed a bug to me. It is too late to submit a bug fix for the two upcoming releases 2.7.1 and 2.6.2. However, I marked this ticket as important for the possible releases 2.7.2 and 2.6.3.

I looked into the code on trunk and there the bug seems to be fixed.;;;","14/Apr/21 17:45;ableegoldman;Thanks [~antrenta]. As Bruno mentioned, we did discover this bug and fixed it in trunk, but unfortunately missed the 2.6.2, 2.7.1, and 2.8.0 releases which have all been in the release process.

I'm happy to backport the fix to the 2.8 branch once 2.8.0 is finally out the door, and maybe to the 2.7 branch as well. It seems unlikely that there will be a 2.6.3 release however.

Sorry for the trouble, I know it's a concerning message to see.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigEntry#equal does not compare other fields when value is NOT null ,KAFKA-12661,13371923,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,13/Apr/21 03:19,02/May/21 05:26,13/Jul/23 09:17,02/May/21 05:26,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"{code:java}
        return this.name.equals(that.name) &&
                this.value != null ? this.value.equals(that.value) : that.value == null &&
                this.isSensitive == that.isSensitive &&
                this.isReadOnly == that.isReadOnly &&
                this.source == that.source &&
                Objects.equals(this.synonyms, that.synonyms);
{code}

the second value of ternary operator is ""that.value == null &&
                this.isSensitive == that.isSensitive &&
                this.isReadOnly == that.isReadOnly &&
                this.source == that.source &&
                Objects.equals(this.synonyms, that.synonyms);"" rather than ""that.value == null"". Hence, it does not compare other fields when value is not null.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-13 03:19:35.0,,,,,,,,,,"0|z0pwyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not update offset commit sensor after append failure,KAFKA-12660,13371913,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,hachikuji,hachikuji,13/Apr/21 01:55,08/Jul/21 17:14,13/Jul/23 09:17,08/Jul/21 17:14,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In the append callback after writing an offset to the log in `GroupMetadataManager`, It seems wrong to update the offset commit sensor prior to checking for errors: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L394. ",,dengziming,hachikuji,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 01:50:40 UTC 2021,,,,,,,,,,"0|z0pwwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/21 21:57;kirktrue;[~dengziming] - can I take a look at this? I have some spare cycles to investigate and hopefully fix.;;;","30/Jun/21 01:50;dengziming;[~kirktrue] Thank you for your attention, but I have already opened a PR for this issue: [https://github.com/apache/kafka/pull/10560,] are you interested in reviewing it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/kafka-metadata-shell.sh cannot find or load main class org.apache.kafka.shell.MetadataShell,KAFKA-12658,13371907,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ijuma,iekpo,iekpo,13/Apr/21 01:24,13/Apr/21 13:14,13/Jul/23 09:17,13/Apr/21 05:33,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"With the latest release candidate for 2.8.0, the binaries from the Scala 2.13 and 2.12 tarballs are not finding the class for the meta data shell from the classpath 
[https://home.apache.org/~vvcephei/kafka-2.8.0-rc1/]
 
kafka-run-class.sh is not able to load it.
 
cd ../kafka_2.12-2.8.0$
 
 bin/kafka-metadata-shell.sh --help
Error: Could not find or load main class org.apache.kafka.shell.MetadataShell
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.shell.MetadataShell

cd ../kafka_2.13-2.8.0/


bin/kafka-metadata-shell.sh --help
Error: Could not find or load main class org.apache.kafka.shell.MetadataShell
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.shell.MetadataShell
!https://ssl.gstatic.com/ui/v1/icons/mail/images/cleardot.gif!
 ","Ubuntu, Java 11",ableegoldman,iekpo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-13 01:24:16.0,,,,,,,,,,"0|z0pwvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2021-28165 - Upgrade jetty to 9.4.39,KAFKA-12655,13371200,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,edwin092,edwin092,12/Apr/21 08:15,15/Jul/21 13:20,13/Jul/23 09:17,13/Apr/21 06:41,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,3.0.0,,,,,,,,,,0,CVE,security,,,"*CVE-2021-28165* vulnerability affects Jetty versions up to *9.4.38*. For more information see [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-28165] 

Upgrading to Jetty version *9.4.39* should address this issue ([https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.39.v20210325)|https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.39.v20210325].",,edwin092,nagarkoti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 15 13:20:10 UTC 2021,,,,,,,,,,"0|z0psio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/21 13:20;nagarkoti;Seems like this was not back-ported to 2.7.1. Can someone confirm? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in InternalTopicManager#cleanUpCreatedTopics,KAFKA-12650,13370759,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,ableegoldman,ableegoldman,10/Apr/21 01:00,14/Apr/21 18:46,13/Jul/23 09:17,14/Apr/21 18:46,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,"{code:java}
java.lang.NullPointerException
	at org.apache.kafka.streams.processor.internals.InternalTopicManager.cleanUpCreatedTopics(InternalTopicManager.java:675)
	at org.apache.kafka.streams.processor.internals.InternalTopicManager.maybeThrowTimeoutExceptionDuringSetup(InternalTopicManager.java:755)
	at org.apache.kafka.streams.processor.internals.InternalTopicManager.processCreateTopicResults(InternalTopicManager.java:652)
	at org.apache.kafka.streams.processor.internals.InternalTopicManager.setup(InternalTopicManager.java:599)
	at org.apache.kafka.streams.processor.internals.InternalTopicManagerTest.shouldOnlyRetryNotSuccessfulFuturesDuringSetup(InternalTopicManagerTest.java:180)
{code}
",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 13 09:21:21 UTC 2021,,,,,,,,,,"0|z0pqgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/21 01:01;ableegoldman;cc [~cadonna] can you take a look at this? ;;;","10/Apr/21 01:06;ableegoldman;This was from InternalTopicManagerTest.shouldOnlyRetryNotSuccessfulFuturesDuringSetup() btw.

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-10411/12/testReport/junit/org.apache.kafka.streams.processor.internals/InternalTopicManagerTest/shouldOnlyRetryNotSuccessfulFuturesDuringSetup/;;;","12/Apr/21 16:27;mjsax;[https://github.com/apache/kafka/pull/10042/checks?check_run_id=2319410471] ;;;","13/Apr/21 09:21;cadonna;[~mjsax] [~ableegoldman] , Thank you for reporting this and providing a PR! I commented on the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractCoordinator ignores backoff timeout when joining the consumer group,KAFKA-12639,13370581,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pnee,matiss.gutmanis,matiss.gutmanis,09/Apr/21 07:44,01/Mar/23 01:45,13/Jul/23 09:17,01/Mar/23 01:37,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.5.0,,,,,,,,clients,consumer,,,,0,,,,,"We observed heavy logging while trying to join consumer group during partial unavailability of Kafka cluster (it's part of our testing process). Seems that {{rebalanceConfig.retryBackoffMs}} used in  {{ org.apache.kafka.clients.consumer.internals.AbstractCoordinator#joinGroupIfNeeded}} is not respected. Debugging revealed that {{Timer}} instance technically is expired thus using sleep of 0 milliseconds which defeats the purpose of backoff timeout.

Minimal backoff timeout should be respected.

 
{code:java}
2021-03-30 08:30:24,488 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] JoinGroup failed: Coordinator 127.0.0.1:9092 (id: 2147483634 rack: null) is loading the group.
2021-03-30 08:30:24,488 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] Rebalance failed.
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: The coordinator is loading and hence can't process requests.
2021-03-30 08:30:24,488 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] (Re-)joining group
2021-03-30 08:30:24,489 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] JoinGroup failed: Coordinator 127.0.0.1:9092 (id: 2147483634 rack: null) is loading the group.
2021-03-30 08:30:24,489 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] Rebalance failed.
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: The coordinator is loading and hence can't process requests.
2021-03-30 08:30:24,489 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] (Re-)joining group
2021-03-30 08:30:24,490 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] JoinGroup failed: Coordinator 127.0.0.1:9092 (id: 2147483634 rack: null) is loading the group.
2021-03-30 08:30:24,490 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] Rebalance failed.
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: The coordinator is loading and hence can't process requests.
2021-03-30 08:30:24,490 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] (Re-)joining group
2021-03-30 08:30:24,491 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] JoinGroup failed: Coordinator 127.0.0.1:9092 (id: 2147483634 rack: null) is loading the group.
2021-03-30 08:30:24,491 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] Rebalance failed.
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: The coordinator is loading and hence can't process requests.
2021-03-30 08:30:24,491 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] (Re-)joining group
2021-03-30 08:30:24,492 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] JoinGroup failed: Coordinator 127.0.0.1:9092 (id: 2147483634 rack: null) is loading the group.
2021-03-30 08:30:24,492 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] Rebalance failed.
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: The coordinator is loading and hence can't process requests.
2021-03-30 08:30:24,492 INFO [fs2-kafka-consumer-41][o.a.k.c.c.i.AbstractCoordinator] [Consumer clientId=app_clientid, groupId=consumer-group] (Re-)joining group

{code}",,guozhang,ijuma,kirktrue,matiss.gutmanis,pnee,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12640,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 03 00:17:18 UTC 2023,,,,,,,,,,"0|z0ppdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/22 18:55;ijuma;Is this related to [https://github.com/apache/kafka/commit/f8f57960c69fe677b9192b841fb7a0361ef2cc83] ?;;;","13/Jun/22 22:27;guozhang;This is a separate issue, as `joinGroupIfNeeded` is not called by the heartbeat thread at all.

I checked the code in trunk and compared with 2.7, I can confirm the situation still persists there. But this may not be a real issue, just in [~matiss.gutmanis]'s testing environment.

The key is that inside the while loop we only check `while (rejoinNeededOrPending()) {` and is only existing due to timed out if the response cannot be received in time. This is relied on the fact that if the timer has already elapsed, after sending the join-group request we should be effectively triggering `select(0)` in which we should rarely receive the response immediately, and hence at that time we would be exiting out of the while loop. I suspect in this testing the cluster failure is shortcut-mocked by returning immediately the response (again, which should not be a common case in practice) which caused the calling loop to never exit.

All that being said, we can still augment the condition as `while (rejoinNeededOrPending() && timer.notElapsed()) {`, but doing so has a potential caveat that we would not send out the request before exiting the while loop --- and this is also the intention of the current code to always make sure we have the inflight request before exiting, to optimize the pipelining latency slightly --- cc  [~pnee] to chime in here.;;;","02/Feb/23 19:11;pnee;Hey [~matiss.gutmanis] - I'm arriving at the same conclusion as what Guozhang previous mentioned, would you mind sharing/briefly describe your test environment?

 

[~guozhang] - should we could exit upon expired timer? maybe something like

```

if (timer.isExpired())

{   return false; }

timer.sleep(rebalanceConfig.retryBackoffMs);

```;;;","03/Feb/23 00:17;guozhang;Yeah that would work better I think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirrormaker 2 offset sync is incorrect if the target partition is empty,KAFKA-12635,13370477,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,fyi,fyi,08/Apr/21 19:56,23/Jan/23 11:15,13/Jul/23 09:17,16/May/22 15:45,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,mirrormaker,,,,,3,,,,,"This bug occurs when using Mirrormaker with ""sync.group.offsets.enabled = true"".

If a source partition is empty, but the source consumer group's offset for that partition is non-zero, then Mirrormaker sets the target consumer group's offset for that partition to the literal, not translated, offset of the source consumer group. This state can be reached if the source consumer group consumed some records that were now deleted (like by a retention policy), or if Mirrormaker replication is set to start at ""latest"". This bug causes the target consumer group's lag for that partition to be negative and breaks offset sync for that partition until lag is positive.

The correct behavior when the source partition is empty would be to set the target offset to the translated offset, not literal offset, which in this case would always be 0. 

Original email thread on this issue: https://lists.apache.org/thread.html/r7c54ee5f57227367b911d4abffa72781772d8dd3b72d75eb65ee19f7%40%3Cusers.kafka.apache.org%3E",,akaltsikis,chetan-pandey,dragotic,fyi,meetpraveen,mimaison,showuon,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13452,,,,,,,,,,,,,,,,,,,"02/Nov/22 06:23;chetan-pandey;image-2022-11-02-11-53-33-329.png;https://issues.apache.org/jira/secure/attachment/13051697/image-2022-11-02-11-53-33-329.png","02/Nov/22 06:26;chetan-pandey;image-2022-11-02-11-56-34-994.png;https://issues.apache.org/jira/secure/attachment/13051699/image-2022-11-02-11-56-34-994.png","02/Nov/22 06:25;chetan-pandey;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13051698/screenshot-1.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 13:33:29 UTC 2022,,,,,,,,,,"0|z0poq8:",9223372036854775807,,yangguo1220,,,,,,,,,,,,,,,,,,"12/Apr/21 08:17;akaltsikis;We are facing the same issue as well.;;;","14/Apr/21 15:53;akaltsikis;[~yangguo1220] [~fyi] is there any workaround to avoid this bug till it's properly fixed? ;;;","14/Apr/21 17:20;yangguo1220;[~akaltsikis] probably try to manually create a consumer group with initial offset = 0. The console consumer command will do that;;;","15/Apr/21 11:04;dragotic;[~yangguo1220] thanks for the reply! We tried your suggestion but unfortunately it didn't work. We created the consumer group with the kafka-console-consumer.sh and offset = 0. However, when the MM2 started the CheckpointConnector the offsets were translated poorly and became negative again.;;;","01/May/21 23:02;yangguo1220;hi [~dragotic] could you please elaborate on how you can re-produce the issue step-by-step? ;;;","01/May/21 23:16;yangguo1220;> This state can be reached if the source consumer group consumed some records that were now deleted (like by a retention policy), or if Mirrormaker replication is set to start at ""latest"". 

[~fyi] I am reading your above statement several times, and could not figure out the reproduce scenario. Do you mind to share the scenario step-by-step from a good state to a bad state? Thanks;;;","04/May/21 16:51;fyi;[~yangguo1220] I was able to repro using the steps below on 2.8.0 brokers and MM2.
 # Create a topic with 1 partition on the source cluster, with `retention.ms` set to something short like 10 seconds.
 ## `./kafka-topics.sh --bootstrap-server $SOURCE --create --topic myTopic --config 'retention.ms=10000'`
 # Create a consumer that consumes from the topic
 ## `./kafka-console-consumer.sh --bootstrap-server $SOURCE --group myConsumer --topic myTopic`
 # Send 100 messages to the topic. These should get consumed by the consumer. Offset for this consumer on source cluster should be 100.
 ## `for i in $(seq 1 100); do echo $i; done | ./kafka-console-producer.sh --bootstrap-server $SOURCE --topic myTopic`
 # Wait until the retention policy deletes the records
 # Start MM2 with `source->target.sync.group.offsets.enabled = true`
 # Observe on the target cluster that log-end-offset is 0, offset is 100, and lag is -100.
 ## `./kafka-consumer-groups.sh --bootstrap-server $TARGET --describe --group myConsumer`;;;","05/May/21 10:53;dragotic;[~yangguo1220] basically the same steps as described by [~fyi] but before Step 5. We create the consumer groups on the target cluster using the kafka-console-consumer.sh command with two extra flags:  --max-messages 1 --timeout-ms 6000;;;","07/May/21 03:02;yangguo1220;pull request: https://github.com/apache/kafka/pull/10644;;;","07/May/21 03:04;yangguo1220;hi [~fyi] [~dragotic] I compiled the latest trunk Kafka version with the above pull request at [https://github.com/ning2008wisc/kafka-trunk-binary/blob/master/kafka_2.13-3.0.0-SNAPSHOT.tgz] It looks solve the bug from my local testing and would like to hear your initial feedback. If good, I will polish and have the pull request ready for review. Thanks for your patience ;;;","07/May/21 15:21;dragotic;Hello [~yangguo1220], thanks for the PR. We built our own image with scala 2.12 and using your PR code and ported it to Strimzi (this is how we run MM2), and following the same steps as before we still get negative lag as seen below.

 
{code:java}
➜  bin ./kafka-consumer-groups.sh --bootstrap-server source-cluster:9092 --describe --group consumer-group-1

GROUP           TOPIC               PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                     HOST            CLIENT-ID
consumer-group-1            topic_1 1          1656            1656            0               consumer-group-1-7584866df4-bgbbq-consumer-group-1-58759d7c-80e4-4ccc-8c52-bd68a6847cc3 /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_2 1          32005209        32005209        0               consumer-group-1-7584866df4-75r7h-consumer-group-1-dc965036-5739-4399-b488-728bde48a026 /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1
consumer-group-1            topic_3 1          445             445             0               consumer-group-1-7584866df4-6bzkd-consumer-group-1-3e68ed59-2316-4168-9186-039cfcad1476 /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_4 0          198311          198311          0               consumer-group-1-7584866df4-75r7h-consumer-group-1-aba1a270-cd9d-45e7-96a9-ac2990755e05 /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1
consumer-group-1            topic_2 0          36517046        36517055        9               consumer-group-1-7584866df4-bgbbq-consumer-group-1-6416e058-c8d6-435b-a14f-583f742ba3a4 /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_1 0          1595            1595            0               consumer-group-1-7584866df4-c4sq6-consumer-group-1-2ec422b2-cc8f-4b0c-adae-59c016cb0666 /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_2 2          35280609        35280613        4               consumer-group-1-7584866df4-6bzkd-consumer-group-1-19d12ff4-b486-4ea9-9eb5-23a037f11f30 /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_2 3          33808349        33808363        14              consumer-group-1-7584866df4-c4sq6-consumer-group-1-57315540-bf27-4d99-bb38-869cd836279f /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_1 2          1652            1652            0               consumer-group-1-7584866df4-6bzkd-consumer-group-1-9349b872-b2a4-44ba-8422-fd8f757ea017 /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_6 1          20311           20311           0               consumer-group-1-7584866df4-c4sq6-consumer-group-1-c56dd4c4-c19d-45ea-9638-d0400f5f59d8 /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_1 3          1720            1720            0               consumer-group-1-7584866df4-75r7h-consumer-group-1-f5b6de01-f2e1-442a-89b7-5730902392f9 /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1
consumer-group-1            topic_6 0          20252           20252           0               consumer-group-1-7584866df4-6bzkd-consumer-group-1-bca10139-17ad-4062-9008-8d3bb7a5b183 /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_5 0          3927            3927            0               consumer-group-1-7584866df4-c4sq6-consumer-group-1-cae382b5-cb6e-48e4-8409-5c2464c6303e /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_3 2          409             409             0               consumer-group-1-7584866df4-bgbbq-consumer-group-1-2cd4551c-5cd1-4264-b397-e3fbf695fb4f /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_5 3          3944            3944            0               consumer-group-1-7584866df4-6bzkd-consumer-group-1-6c4872e1-51dd-46d1-b8a2-68781a6bc95c /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_6 2          20332           20332           0               consumer-group-1-7584866df4-bgbbq-consumer-group-1-5db7872e-82d2-4da5-b888-310eeac36177 /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_4 3          198907          198907          0               consumer-group-1-7584866df4-6bzkd-consumer-group-1-723d3e61-9aae-459d-b8fb-b22012d9e3c7 /host_3  consumer-group-1-7584866df4-6bzkd-consumer-group-1
consumer-group-1            topic_3 3          394             394             0               consumer-group-1-7584866df4-c4sq6-consumer-group-1-556bf8d7-7565-43da-8001-892d0a5c2d92 /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_5 1          3937            3937            0               consumer-group-1-7584866df4-75r7h-consumer-group-1-1c8155fc-9f43-4dee-b788-1c6c897131f1 /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1
consumer-group-1            topic_4 1          198068          198068          0               consumer-group-1-7584866df4-bgbbq-consumer-group-1-b1e30c1b-98af-4cdf-95ea-cbceb7eecffb /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_5 2          3891            3891            0               consumer-group-1-7584866df4-bgbbq-consumer-group-1-4f042970-58a3-4255-ba7c-6ccef11c2d7c /host_1   consumer-group-1-7584866df4-bgbbq-consumer-group-1
consumer-group-1            topic_4 2          198661          198661          0               consumer-group-1-7584866df4-c4sq6-consumer-group-1-51b2805b-9ce2-43c5-9c50-29ec4e35b227 /host_4    consumer-group-1-7584866df4-c4sq6-consumer-group-1
consumer-group-1            topic_3 0          431             431             0               consumer-group-1-7584866df4-75r7h-consumer-group-1-f757eaef-4a0e-4b36-bb6d-38b7220f7c22 /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1
consumer-group-1            topic_6 3          20222           20222           0               consumer-group-1-7584866df4-75r7h-consumer-group-1-e6962bd4-15cf-4322-bc1f-c626d32f64dd /host_2   consumer-group-1-7584866df4-75r7h-consumer-group-1




➜  bin ./kafka-consumer-groups.sh --bootstrap-server target-cluster:9092 --describe --group consumer-group-1

Consumer group 'consumer-group-1' has no active members.

GROUP           TOPIC               PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
consumer-group-1            topic_3 3          75              35              -40             -               -               -
consumer-group-1            topic_2 3          7135020         3202517         -3932503        -               -               -
consumer-group-1            topic_5 2          492             208             -284            -               -               -
consumer-group-1            topic_3 1          82              34              -48             -               -               -
consumer-group-1            topic_2 1          4831093         1472955         -3358138        -               -               -
consumer-group-1            topic_5 0          573             165             -408            -               -               -
consumer-group-1            topic_1 0          1378            1376            -2              -               -               -
consumer-group-1            topic_6 3          4614            1472            -3142           -               -               -
consumer-group-1            topic_1 2          1422            1413            -9              -               -               -
consumer-group-1            topic_4 2          25600           3143            -22457          -               -               -
consumer-group-1            topic_6 1          4847            1857            -2990           -               -               -
consumer-group-1            topic_4 0          25939           3156            -22783          -               -               -
consumer-group-1            topic_5 3          585             170             -415            -               -               -
consumer-group-1            topic_3 2          84              38              -46             -               -               -
consumer-group-1            topic_5 1          520             158             -362            -               -               -
consumer-group-1            topic_3 0          95              41              -54             -               -               -
consumer-group-1            topic_2 2          5508653         1645293         -3863360        -               -               -
consumer-group-1            topic_2 0          6250327         1432524         -4817803        -               -               -
consumer-group-1            topic_6 2          5015            1963            -3052           -               -               -
consumer-group-1            topic_1 1          1460            1453            -7              -               -               -
consumer-group-1            topic_1 3          1467            1467            0               -               -               -
consumer-group-1            topic_4 1          25901           3084            -22817          -               -               -
consumer-group-1            topic_4 3          25988           3227            -22761          -               -               -
consumer-group-1            topic_6 0          4958            1943            -3015           -               -               -
{code}
Also, the following errors are filling up our logs:
{code:java}
2021-05-07 15:13:11,506 ERROR Scheduler for MirrorCheckpointTask caught exception in scheduled task: refreshing end offset of each topic partition pair at target cluster (org.apache.kafka.connect.mirror.Scheduler) [Scheduler for MirrorCheckpointTask-refreshing end offset of each topic partition pair at target cluster]
org.apache.kafka.common.errors.InvalidGroupIdException: To use the group management or offset commit APIs, you must provide a valid group.id in the consumer configuration.

{code}

We are ready to test any other code change on your PR.

Thank you again.

Edit: this topic was not empty, but the behavior was similar for empty topics as well;;;","07/May/21 16:21;yangguo1220;[~dragotic] I see, I probably missed your comment ""but before Step 5. We create the consumer groups on the target cluster using the kafka-console-consumer.sh command with two extra flags:  --max-messages 1 --timeout-ms 6000""

I am updating the PR and testing it again;;;","07/May/21 20:38;fyi;Thanks [~yangguo1220], I will test it this weekend;;;","08/May/21 02:54;yangguo1220;updated and tested the PR;;;","11/May/21 05:34;fyi;fix works for me! Thanks;;;","11/May/21 05:45;yangguo1220;great, thanks for the feedback. I will proceed to finalize the pull request. ;;;","11/May/21 07:32;akaltsikis;Hello [~yangguo1220],
Thanks for your fix.
Yesterday we (with [~dragotic] ) have also tested your PR and fortunately right now we don't have any negative consumer lags in the target cluster
 :tada:
Thus we believe that the fix worked as it should.

BTW can we help you backport this fix to 2.7.2 ( i just saw that 2.7.1 was released 22 hours ago :( ) & to 2.8.1 ?;;;","12/May/21 06:22;yangguo1220;backport is a great point. I guess the review cycle will probably take 3-4 weeks at most, so I will let the committer/reviewer know the backport option and see what is available from their point of view.;;;","15/Jun/21 09:24;dragotic;[~ryannedolan] [~mimaison] Hey there, sorry for pinging but do you believe you have time to take a look at the PR that [~yangguo1220] has opened?? 

thanks in advance! :);;;","15/Jun/21 09:46;mimaison;[~dragotic] Sorry for the delay. I've prioritised other tasks recently but I hope to take a look at it soon. Thanks [~yangguo1220] for raising a PR;;;","15/Jun/21 09:54;dragotic;[~mimaison] No worries, I can totally understand it. Thanks for replying promptly :) ;;;","26/Jul/21 09:12;dragotic;Hey [~mimaison], any chance you took a look at this bug? Thank you :);;;","09/Feb/22 15:17;mimaison;Looking at this again (sorry for the delay).

The offset on the target being negative should not have a functional impact on the consumer. The offset is ""out of range"" so the auto.offset.reset configuration will be used to find a new valid position. As there are no records in the target partition, whether the consumer resets to latest or earliest will have make no difference and it will set its position to 0.

But I understand it may be annoying in terms of metrics. I guess in theory it could also lead to records being skipped if suddenly records are produced to the source cluster and we start a consumer with auto.offset.reset to latest on the target cluster before MirrorMaker is able to emit a new checkpoint/commit offsets.

I think a better alternative than resetting the offset to 0 is to actually not commit any offsets in the target cluster until some records have been mirrored. 

;;;","10/Feb/22 17:58;mimaison;Thinking about this further, one thing I missed yesterday is that the offsets in the target cluster is updated only if the new computed value is larger, see https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java#L277-L284.

This is problematic because until the gap is closed, the checkpoint connector will not update the offsets in the target cluster.;;;","02/Nov/22 06:26;chetan-pandey;[~fyi]  / [~mimaison]  I just used the Kafka 3.3.1 release to run mirror-maker(which should have this fix), to mirror from existing cluster to a new cluster and i can still see  negative lags for various consumer group.  !image-2022-11-02-11-53-33-329.png|width=1067,height=198!;;;","01/Dec/22 17:18;mimaison;[~chetan-pandey] Have these negative offsets persisted ? Are you sure new commits for these partitions have happened?;;;","08/Dec/22 07:15;chetan-pandey;[~mimaison]  No, these negative offsets change over the time as new messages come in the particular partitions.  But this would be a problem for someone doing a new replication setup in an existing cluster. Can't these be set to 0 instead of negative in such scenarios ? ;;;","08/Dec/22 13:33;mimaison;The fix only ensured new offsets will be correct. If you have existing negative offsets, you either have to wait till a new offset is emitted or you can manually update them using the Admin client or the kafka-consumer-groups tool for example.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RaftClusterTest and ClusterTestExtensionTest failures,KAFKA-12626,13369921,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,jolshan,jolshan,07/Apr/21 02:35,07/Apr/21 14:30,13/Jul/23 09:17,07/Apr/21 14:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"RaftClusterTest and ClusterTestExtensionsTest.[Quorum 2] Name=cluster-tests-2, security=PLAINTEXT are failing due to
{noformat}
java.util.concurrent.ExecutionException: java.lang.ClassNotFoundException: org.apache.kafka.controller.NoOpSnapshotWriterBuilder{noformat}
I think it is related to the changes from [https://github.com/apache/kafka/commit/7bc84d6ced71056dbb4cecdc9abbdbd7d8a5aa10#diff-77dc2adb187fd078084644613cff2b53021c8a5fbcdcfa116515734609d1332aR210]
 specifically this part of the code [https://github.com/apache/kafka/blob/33d0445b8408289800352de7822340028782a154/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java#L210]",,chia7712,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 14:22:31 UTC 2021,,,,,,,,,,"0|z0plao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 14:22;chia7712;This should be fixed by https://github.com/apache/kafka/pull/10496;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka setup with Zookeeper- specifying an alternate znode creates the configuration at the wrong znode,KAFKA-12621,13369866,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jibitesh,jibitesh,06/Apr/21 20:27,22/Apr/21 03:11,13/Jul/23 09:17,22/Apr/21 03:11,2.6.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,config,,,,,0,config,newbie,server.properties,,"While configuring kafka with an znode apart from ""/"", the configuration is created in the wrong znode. Fo example, I have the following entry in my server.properties

_zookeeper.connect=10.114.103.207:2181/kafka_secondary_cluster,10.114.103.206:2181/kafka_secondary_cluster,10.114.103.205:2181/kafka_secondary_cluster_

The IPs are the IP addresses of the nodes of zookeeper cluster. I expect the kafka server to use _kafka_secondary_cluster_ as the znode in the zookeeper nodes. But, the znode which is created is actually

_/kafka_secondary_cluster,10.114.103.206:2181/kafka_secondary_cluster,10.114.103.205:2181/kafka_secondary_cluster_

Executing ls on the above path shows me the necessary znodes being created in that path

_[zk: localhost:2181(CONNECTED) 1] ls /kafka_secondary_cluster,10.114.103.206:2181/kafka_secondary_cluster,10.114.103.205:2181/kafka_secondary_cluster_

Output:
 _[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification]_

Shouldn't these configurations be created in _/kafka_secondary_cluster_. It seems the comma separated values are not being split correctly. Or am I doing something wrong?","Linux
OS: 16.04.1-Ubuntu SMP 
Architecture: x86_64
Kernel Version: 4.15.0-1108-azure",cricket007,jibitesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 03:11:20 UTC 2021,,,,,,,,,,"0|z0pkyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 03:36;cricket007;Based on various examples, you only need the znode on the very last entry

This is also stated as such in the docs - https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.connect;;;","22/Apr/21 03:10;jibitesh;Thanks! I completely missed that.;;;","22/Apr/21 03:11;jibitesh;Incorrect configuration on my part;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure LeaderChange message is committed before initializing high watermark,KAFKA-12619,13369650,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,05/Apr/21 20:50,08/Apr/21 18:10,13/Jul/23 09:17,08/Apr/21 18:10,,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,,,,,,0,,,,,"KIP-595 describes an extra condition on commitment here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum#KIP595:ARaftProtocolfortheMetadataQuorum-Fetch. In order to ensure that a newly elected leader's committed entries cannot get lost, it must commit one record from its own epoch. This guarantees that its latest entry is larger (in terms of epoch/offset) than any previously written record which ensures that any future leader must also include it. This is the purpose of the LeaderChange record which is written to the log as soon as the leader gets elected.

We have this check implemented here: https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/LeaderState.java#L122. However, the check needs to be a strict inequality since the epoch start offset does not reflect the LeaderChange record itself. In other words, the check is off by one.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-05 20:50:16.0,,,,,,,,,,"0|z0pjmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistencies between Kafka Config and Log Config,KAFKA-12613,13369460,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,dongjin,dongjin,04/Apr/21 09:21,14/Apr/22 10:00,13/Jul/23 09:17,14/Apr/22 10:00,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,core,,,,,0,,,,,"I found this problem while investigating KAFKA-8926.

Some broker-wide configurations (defined in KafkaConfig) are mapped with log-wide configurations (defined in LogConfig), providing a default value. You can find the complete mapping list in `LogConfig.TopicConfigSynonyms`.

The problem is, *some configuration properties' validation is different between KafkaConfig and LogConfig*:

!20210404-161832.png!

These inconsistencies cause some problems with the dynamic configuration feature. When a user dynamically configures the broker configuration with `AdminClient#alterConfigs`, the submitted config is validated with KafkaConfig, which lacks some validation logic - as a result, they bypasses the correct validation.

For example, a user can set `log.cleaner.min.cleanable.ratio` to -0.5 - which is obviously prohibited in LogConfig.
 * I could not reproduce the situation KAFKA-8926 describes, but fixing this problem also resolves KAFKA-8926.",,dongjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8926,,,,,,"04/Apr/21 09:16;dongjin;20210404-161832.png;https://issues.apache.org/jira/secure/attachment/13023384/20210404-161832.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-04 09:21:46.0,,,,,,,,,,"0|z0pig8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix using random payload in ProducerPerformance incorrectly,KAFKA-12611,13369408,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lamberken,lamberken,lamberken,03/Apr/21 13:37,13/Apr/21 06:29,13/Jul/23 09:17,13/Apr/21 06:29,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"In ProducerPerformance, random payload always same. it has a great impact when use the compression.type option.",,lamberken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-03 13:37:17.0,,,,,,,,,,"0|z0pi4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resources on classpath break PluginClassLoader isolation,KAFKA-12610,13369379,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,nigel.liang,nigel.liang,03/Apr/21 00:55,20/Dec/22 01:51,13/Jul/23 09:17,23/Nov/22 13:39,2.3.1,2.4.1,2.5.1,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,3.4.0,,,,,,,,,,,,,0,connect,,,,"The `PluginClassLoader` does not override `getResource` method it inherits from `ClassLoader` . The default implementation searches for resources in parent loader before calling its own `findResource` method. The result of this is that connectors or dependencies of connectors attempting to read resources in jars that may exist on the classpath get the version of the resource from the jar on classpath instead of from the jar in pluginpath.

I have put together a testcase to demonstrate this issue at https://github.com/ncliang/get-resource-example/blob/classpath-resource-leaks/src/test/java/com/nigelliang/PluginClassLoaderTest.java",,ivanyu,nigel.liang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-04-03 00:55:57.0,,,,,,,,,,"0|z0phy8:",9223372036854775807,,mageshn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The LICENSE and NOTICE files don't list everything they should,KAFKA-12602,13369142,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,01/Apr/21 15:28,06/Apr/21 20:42,13/Jul/23 09:17,06/Apr/21 20:42,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"[~jmclean] raised this on the mailing list: [https://lists.apache.org/thread.html/r2df54c11c10d3d38443054998bc7dd92d34362641733c2fb7c579b50%40%3Cdev.kafka.apache.org%3E]

 

We need to make  the license file match what we are actually shipping in source and binary distributions.",,ableegoldman,jmclean,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12622,KAFKA-12625,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 20:42:44 UTC 2021,,,,,,,,,,"0|z0pghk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 22:58;jmclean;JFYI - Some projects have different license and notice files for their source and binary releases. If you need help assembling them or need a review just ask, I'm happy to help.;;;","05/Apr/21 04:19;vvcephei;Thanks for your kind offer, [~jmclean] !

 

I've gone through all the lib jars that we are shipping in our 2.8.0 binary artifact, and done my best to put together a correct license file here: [https://github.com/apache/kafka/pull/10474] . If you're able to review it, I would greatly appreciate it.

Aside from the binary dependencies, I took a look at the source dependencies that you listed in the mailing list:


1. ./clients/src/main/java/org/apache/kafka/common/utils/PureJavaCrc32C.java
2. ./streams/src/main/java/org/apache/kafka/streams/state/internals/Murmur3.java
3 ./tests/kafkatest/utils/util.py (and a couple of other files)
4 ./gradlew

The first two are licensed as Apache2 in their source locations, and they contain what looks like correct copyright attributions in the files. PureJavaCrc32C came from Hadoop and Murmur3 came from Hive. Given that this code already belongs to the ASF, it doesn't seem like anything needs to be done here. Am I looking at the situation wrong?

The last two were actually just mistakes. Those files had been placed in the repo by a Confluent employee under the regular terms of the CLA, so they shouldn't have had the extra copyright notice. They belong to the ASF now, and there's no special license for them, except the Apache2 license that covers the rest of the code in the repo. I fixed them already in the last PR.

 

Any feedback or guidance you're able to provide is greatly appreciated.

Thanks,

-John

 ;;;","05/Apr/21 08:44;jmclean;Re the first two files - even if something is Apache licensed it's best to mention it in the LICENSE file. There's no need to repeat the license text however. Is if Apache code or 3rd party code? If it come from an ASF project (and there's changes to the original code) then that may have an impact the NOTICE files as well as you need to look at the NOTICE file in the project it come from and copy the relevant parts across.;;;","05/Apr/21 08:46;jmclean;see [1] and [2]

1. https://infra.apache.org/licensing-howto.html#alv2-dep
2. https://infra.apache.org/licensing-howto.html#bundle-asf-product
;;;","05/Apr/21 08:54;jmclean;Looking at the proposed changes, some additions may need to be made to the  binary NOTICE file due to inclusion of ALv2 dependancies. I also see you have included general MIT and BSD licenses. This is probably OK but it would be better to include each individual license text. Both MIT and BSD licenses have copyright lines and the BSD license text can vary a little from license to license.

In short IMO I think it's much better than before. I think this would be OK to merge and make another release candidate. Perhaps consider refining in future releases?;;;","05/Apr/21 17:44;vvcephei;Thank you, [~jmclean] ,

 

There weren't too many BSD or MIT licenses, so I've gone ahead and just copied the exact license files from each one of those dependencies. Thanks for the feedback.

I also added notices regarding the provenance and copyright of PureJavaCrc32C and Murmur3 to our NOTICE file.

I will create two follow-up tickets, one to revisit the NOTICE file, and another to add an automated check to the release script to make sure that the our license file doesn't start to rot again in the future.;;;","06/Apr/21 20:36;vvcephei;I created subtasks for 2.7 and 2.6.

Also created https://issues.apache.org/jira/browse/KAFKA-12622 to automate a check.;;;","06/Apr/21 20:42;vvcephei;Just filed https://issues.apache.org/jira/browse/KAFKA-12625 to fix the NOTICE file.

This is the last loose end, so I'll go ahead and close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Some Scala, Python, and Gradle files contain the wrong license header",KAFKA-12593,13368941,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,31/Mar/21 18:23,02/Apr/21 00:21,13/Jul/23 09:17,01/Apr/21 15:50,,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,,,,,,0,,,,,"Thanks to [~jmclean] for raising this issue in the mailing list thread:

[https://lists.apache.org/thread.html/r2df54c11c10d3d38443054998bc7dd92d34362641733c2fb7c579b50%40%3Cdev.kafka.apache.org%3E]

 ",,ableegoldman,jmclean,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 15:50:23 UTC 2021,,,,,,,,,,"0|z0pf94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/21 18:33;vvcephei;Hi [~jmclean] ,

I don't know if you will get pinged by this comment, but I'd greatly appreciate your input on my proposed fix: [https://github.com/apache/kafka/pull/10452]

I'm not a lawyer, but I've been reading over the ASF documentation. This seems to be in a weird grey area. If we were re-distributing source code, we should clearly maintain any copyright notices. But this code was actually submitted to Kafka by Lightbend under the terms of our CLA, so it seems like it should never have had that copyright notice to begin with. Since then, numerous non-Lightbend contributors have submitted changes to those files; I'm not sure how much of the original code is still present.

It seems wrong to simply drop the notices, though, so I added a mention of it in the NOTICE file.

Do you think this is the right approach?

Thanks for your help.

-John;;;","31/Mar/21 18:39;ableegoldman;Hey [~vvcephei], thanks for the PR -- there are a few other Copyrights which have slipped in besides Lightbend. Are you planning to open separate tickets for them or eventually pull them into the scope of this once we have some confirmation this solution is appropriate? Looks like we have

{code:java}
 Copyright (C) 2017-2018 Alexis Seigneurin
- Copyright (C) 2018 Joan Goyeau.
- Copyright (C) 2018 Lightbend Inc.
- Copyright 2015 Confluent Inc.
- Copyright 2015 the original author or authors
{code}
;;;","31/Mar/21 21:47;vvcephei;Thanks for the reminder, Sophie. I had forgotten to look in the python and gradle files. I believe all the copyright notices are fixed now.;;;","31/Mar/21 21:53;jmclean;In general:
- If the work was done at the ASF it should use ASF headers and not have a copyright line and you should not add anything to NOTICE. [1]
- If the work come in as an SGA then it should be use the ASF header without a copyright line and the copyright line relocated to the NOTICE file.
- If the work is 3rd party file then it should keep the original header including the copyright, unless significant changes are made to it [2]

In this case I think it does make sense to replace the headers with a standard ASF ones and re locate the copyright to the NOTICE file.

1. https://www.apache.org/legal/src-headers.html#headers
2. https://www.apache.org/legal/src-headers.html#3party;;;","01/Apr/21 15:50;vvcephei;Thanks for the confirmation, [~jmclean] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade of netty-codec due to CVE-2021-21295,KAFKA-12583,13368666,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ben.c,dominique,dominique,30/Mar/21 14:35,03/Apr/21 00:48,13/Jul/23 09:17,01/Apr/21 13:19,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,security,,,,,0,,,,,"Our security tool raised the following security flaw on kafka 2.7: [https://nvd.nist.gov/vuln/detail/CVE-2021-21295]

It is a vulnerability related to jar *netty-codec-4.1.51.Final.jar*.

Looking at source code, the netty-codec in trunk and 2.7.0 branches are still vulnerable.

Based on netty issue tracker, the vulnerability is fixed in 4.1.60.Final: https://github.com/netty/netty/security/advisories/GHSA-wm47-8v5p-wjpj",,dominique,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 13:19:39 UTC 2021,,,,,,,,,,"0|z0pdkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 13:19;ijuma;We don't use netty for http2 in Kafka, so I don't think the CVE applies. In any case, we upgraded.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Something wrong with MM2 metrics,KAFKA-12563,13367772,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,minhbt,minhbt,26/Mar/21 05:13,24/Feb/23 20:02,13/Jul/23 09:17,24/Feb/23 20:02,2.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mirrormaker,,,,,0,,,,,"The metric _*`adt_2dc_c1_kafka_connect_mirror_source_connector_replication_latency_ms_avg`*_ shows that value of latency is a very large number but the amount of messages in two DC are the same.

View details in the attachment.",,minhbt,ryannedolan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/21 05:12;minhbt;Screen Shot 2021-03-26 at 12.10.12.png;https://issues.apache.org/jira/secure/attachment/13022984/Screen+Shot+2021-03-26+at+12.10.12.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 15:24:14 UTC 2021,,,,,,,,,,"0|z0p834:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/21 06:22;ryannedolan;The metric is calculated based on the timestamp of each replicated record, which can be set by client code. e.g. a producer can set a timestamp of zero, which would yield the replication latency you observe.;;;","26/Mar/21 09:04;minhbt;Thanks, but as I understand the meaning of this metric, two clusters are in sync if the value is zero. So, my question in this case is how to identify the replication lag betwen 2 clusters correctly?;;;","26/Mar/21 14:01;ryannedolan;You can look at the lag of the internal consumers to get an idea of how far behind the connectors are from real time. But I agree that a metric like you describe would be useful. Are you interested in proposing a KIP? Can you close this ticket?;;;","26/Mar/21 15:24;minhbt;It seems helpful, I will try, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MM2 may not sync partition offsets correctly,KAFKA-12558,13367702,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,askldjd,askldjd,25/Mar/21 20:23,23/Feb/23 14:26,13/Jul/23 09:17,10/Jan/23 14:53,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,,mirrormaker,,,,,6,,,,,"There is a race condition in {{MirrorSourceTask}} where certain partition offsets may never be sent. The bug occurs when the [outstandingOffsetSync semaphore is full|https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java#L207]. In this case, the sendOffsetSync [will silently fail|https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java#L207].

This failure is normally acceptable since offset sync will retry frequently. However, {{maybeSyncOffsets}} has a bug where it will [mutate the partition state|https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java#L199] prior to confirming the result of {{sendOffsetSync}}. The end result is that the partition state is mutated prematurely, and prevent future offset syncs to recover.

Since {{MAX_OUTSTANDING_OFFSET_SYNCS}} is 10, this bug happens when you assign more than 10 partitions to each task.

In my test cases where I had over 100 partitions per task, the majority of the offsets were wrong. Here's an example of such a failure. https://issues.apache.org/jira/browse/KAFKA-12468?focusedCommentId=17308308&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17308308

During my troubleshooting, I customized the {{MirrorSourceTask}} to confirm that all partitions that have the wrong offset were failing to acquire the initial semaphore. The condition [can be trapped here|https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java#L208].

*Possible Fix:*

A possible fix is to create a {{shouldUpdate}} method in {{PartitionState}}. This method should be read-only and return true if {{sendOffsetSync}} is needed. Once {{sendOffsetSync}} is successful, only then {{update}} should be called.

Here's some pseudocode
{code:java}
private void maybeSyncOffsets(TopicPartition topicPartition, long upstreamOffset,
        long downstreamOffset) {
    PartitionState partitionState =
        partitionStates.computeIfAbsent(topicPartition, x -> new PartitionState(maxOffsetLag));
    if (partitionState.shouldUpdate(upstreamOffset, downstreamOffset)) {
        if(sendOffsetSync(topicPartition, upstreamOffset, downstreamOffset)) {
            partitionState.update(upstreamOffset, downstreamOffset)
        }
    }
}
{code}
 

*Workaround:*

For those who are experiencing this issue, the workaround is to make sure you have less than or equal to 10 partitions per task. Set your `tasks.max` value accordingly.",,ableegoldman,akaltsikis,askldjd,axelviii,fvaleri,gharris1727,ryannedolan,scholzj,sinitw,tvainika,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 15:57:26 UTC 2021,,,,,,,,,,"0|z0p7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/21 20:56;ryannedolan;This sounds right. Updating the partition store without sending an offset sync means additional offset syncs are unlikely to arrive for a long time, if ever.

I'm happy to fix but will leave this unassigned for a bit in case someone else wants to take this on.;;;","14/Apr/21 15:57;akaltsikis;Just one clarification:
 The number of *LEADER* partitions / The number of tasks <= 10
Correct?
[~askldjd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.kafka.clients.admin.KafkaAdminClientTest#testClientSideTimeoutAfterFailureToReceiveResponse intermittently hangs indefinitely,KAFKA-12557,13367688,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,25/Mar/21 18:41,30/Mar/21 16:47,13/Jul/23 09:17,30/Mar/21 16:43,,,,,,,,,,,,,,,,,,,,,,,2.8.0,3.0.0,,,,,,,clients,core,,,,0,,,,,"While running tests for [https://github.com/apache/kafka/pull/10397,] I got a test timeout under Java 8.

I ran it locally via `./gradlew clean -PscalaVersion=2.12 :clients:unitTest --profile --no-daemon --continue -PtestLoggingEvents=started,passed,skipped,failed -PignoreFailures=true -PmaxTestRetries=1 -PmaxTestRetryFailures=5` (copied from the Jenkins log) and was able to determine that the hanging test is:

org.apache.kafka.clients.admin.KafkaAdminClientTest#testClientSideTimeoutAfterFailureToReceiveResponse

It's odd, but it hangs most times on my branch, and I haven't seen it hang on trunk, despite the fact that my PR doesn't touch the client or core code at all.

Some debugging reveals that when the client is hanging, it's because the listTopics request is still sitting in its pendingRequests queue, and if I understand the test setup correctly, it would never be completed, since we will never advance time or queue up a metadata response for it.

I figure a reasonable blanket response to this is just to make sure that the test harness will close the admin client eagerly instead of lazily.",,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-25 18:41:10.0,,,,,,,,,,"0|z0p7kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid record error message is not getting sent to application,KAFKA-12548,13367367,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,24/Mar/21 21:56,06/Apr/21 22:09,13/Jul/23 09:17,05/Apr/21 18:51,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The ProduceResponse includes a nice record error message when we return INVALID_RECORD_ERROR. Sadly this is getting discarded by the producer, so the user never gets a chance to see it.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-24 21:56:19.0,,,,,,,,,,"0|z0p5l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Single Threaded applications will not work with SHUTDOWN_APPLICATION,KAFKA-12537,13367087,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson5,wcarlson5,wcarlson5,23/Mar/21 19:29,05/Jan/22 22:23,13/Jul/23 09:17,27/Mar/21 03:34,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"Single Threaded EOS applications will not work with the streams uncaught exception handler option SHUTDOWN_APPLICATION. This is because the EOS thread needs to close and clean up, but to send the shutdown signal it needs to have at least one thread running.",,ableegoldman,guozhang,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 05 22:23:04 UTC 2022,,,,,,,,,,"0|z0p3uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 19:30;wcarlson5;It maybe possible to add a thread, have that send the shutdown signal then the whole things will come down;;;","05/Jan/22 22:23;guozhang;Hey [~wcarlson5] just for my own understanding: why is this only a problem for EOS? I feel for ALOS this is also a problem since in that case the thread would have already died out before it could enforce the rebalance right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to improve handling of TimeoutException when committing offsets,KAFKA-12523,13366870,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,23/Mar/21 03:41,30/Mar/21 17:08,13/Jul/23 09:17,29/Mar/21 21:24,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"Right now, in TaskManager#commitOffsetsOrTransaction if we  catch a TimeoutException then under ALOS we just rethrow it while in EOS we rethrow it as TaskCorruptedException. The problem is that commitOffsetsOrTransaction can be invoked from several places:
# Commit within StreamThread main processing loop (either user requested or commit interval has elapsed: this is presumably the case we had in mind when deciding how to handle the TimeoutException in commitOffsetsOrTransaction , no problem here
# Clean shutdown of application: a bit weird to throw a TaskCorruptedException in this case, but it’ll just end up being caught and forcing a closeDirty, so again no problem here
# From TaskManager#handleRevocation: in this case, it’s possible we hit a TimeoutException on a task that’s actually being revoked. This exception will be saved and rethrown from poll, so under EOS we would catch a TaskCorruptedException and then try to revive this task that we actually no longer own. Pretty sure this will cause an NPE in the TaskManager. Under ALOS, the rethrown TimeoutException will be bubbled up through poll again, but unlike TaskCorruptedException we actually don’t catch TimeoutException anywhere in the StreamThread loop. This will trigger the uncaught exception handler
# From TaskManager#handleTaskCorrupted:  this method is itself invoked from within the catch TaskCorruptedException block of the StreamThread’s runLoop. If we throw TaskCorruptedException again then I believe we won’t even catch this in the safety net catch Throwable block of the runLoop -- it’ll just be thrown directly up through run(). 

",,ableegoldman,lct45,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-23 03:41:02.0,,,,,,,,,,"0|z0p2io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast SMT should allow null value records to pass through ,KAFKA-12522,13366833,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dosvath,dosvath,dosvath,22/Mar/21 22:27,22/Dec/21 13:03,13/Jul/23 09:17,21/May/21 10:35,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The [current Cast SMT|https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java] fails on a null record value (or a null record key), which is problematic for tombstone records. When a tombstone record reaches the transformation the error below is thrown:

{code:java}
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [cast types], found: null
at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
{code}

Null values should instead be allowed to pass through as there is no cast transformation to be done, with the benefit of allowing the connector to handle the tombstone records as intended. ",,dosvath,KunalG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 22 13:03:15 UTC 2021,,,,,,,,,,"0|z0p2ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/21 13:03;KunalG;Can anyone please paste here the final resolved jar file link for this 12522 issue of Kafka.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer state is needlessly rebuilt on startup,KAFKA-12520,13366792,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tson,dhruvilshah,dhruvilshah,22/Mar/21 17:53,29/Jun/21 16:18,13/Jul/23 09:17,29/Jun/21 16:18,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"When we find a {{.swap}} file on startup, we typically want to rename and replace it as {{.log}}, {{.index}}, {{.timeindex}}, etc. as a way to complete any ongoing replace operations. These swap files are usually known to have been flushed to disk before the replace operation begins.

One flaw in the current logic is that when we recover these swap files on startup, we end up truncating the producer state and rebuild it from scratch. This is unneeded as the replace operation does not mutate the producer state by itself. It is only meant to replace the {{.log}} file along with corresponding indices.

Because of this unneeded producer state rebuild operation, we have seen multi-hour startup times for clusters that have large compacted topics.",,dhruvilshah,junrao,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 29 16:18:18 UTC 2021,,,,,,,,,,"0|z0p21c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/21 16:18;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in SubscriptionState,KAFKA-12514,13366546,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,21/Mar/21 02:26,22/Mar/21 18:43,13/Jul/23 09:17,22/Mar/21 18:43,3.0.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,,,,,0,,,,,"In a soak test, we got this exception:

 
{code:java}
java.lang.NullPointerException	at org.apache.kafka.clients.consumer.internals.SubscriptionState.partitionLag(SubscriptionState.java:545)	at org.apache.kafka.clients.consumer.KafkaConsumer.currentLag(KafkaConsumer.java:2241)	at org.apache.kafka.streams.processor.internals.PartitionGroup.readyToProcess(PartitionGroup.java:143)	at org.apache.kafka.streams.processor.internals.StreamTask.isProcessable(StreamTask.java:650)	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:661)	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1114) {code}
This is related to the implementation of:

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-695%3A+Further+Improve+Kafka+Streams+Timestamp+Synchronization]

aka

https://issues.apache.org/jira/browse/KAFKA-10091

 

Luckily, the stack trace is pretty unambiguous. I'll open a PR shortly.",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-21 02:26:36.0,,,,,,,,,,"0|z0p0io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Emit-on-change tables may lose updates on error or restart in at_least_once,KAFKA-12508,13366247,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,nhab,nhab,19/Mar/21 05:06,17/May/21 21:41,13/Jul/23 09:17,25/Mar/21 23:10,2.6.0,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,1,,,,,"[KIP-557|https://cwiki.apache.org/confluence/display/KAFKA/KIP-557%3A+Add+emit+on+change+support+for+Kafka+Streams] added emit-on-change semantics to KTables that suppress updates for duplicate values.

However, this may cause data loss in at_least_once topologies when records are retried from the last commit due to an error / restart / etc.

 

Consider the following example:
{code:java}
streams.table(source, materialized)
.toStream()
.map(mayThrow())
.to(output){code}
 
 # Record A gets read
 # Record A is stored in the table
 # The update for record A is forwarded through the topology
 # Map() throws (or alternatively, any restart while the forwarded update was still being processed and not yet produced to the output topic)
 # The stream is restarted and ""retries"" from the last commit
 # Record A gets read again
 # The table will discard the update for record A because
 ## The value is the same
 ## The timestamp is the same
 # Eventually the stream will commit
 # There is absolutely no output for Record A even though we're running in at_least_once

 

This behaviour does not seem intentional. [The emit-on-change logic explicitly forwards records that have the same value and an older timestamp.|https://github.com/apache/kafka/blob/367eca083b44261d4e5fa8aa61b7990a8b35f8b0/streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java#L50]

This logic should probably be changed to also forward updates that have an older *or equal* timestamp.",,ableegoldman,cadonna,cmaus,jclarke,mjsax,nhab,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8770,KAFKA-10248,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 25 23:10:52 UTC 2021,,,,,,,,,,"0|z0oyo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/21 13:25;cadonna;[~nhab] Thank you for the bug report. I could reproduce the bug and it indeed leads to data loss as you described. I will open an bug fix PR shortly.;;;","19/Mar/21 13:49;cadonna;[~ableegoldman], [~mimaison], and [~vvcephei], I set this bug as a blocker for the ongoing releases since it might lead to data loss (i.e., it breaks at-least-once and exactly-once processing guarantees) and emit-on-change cannot be switched off. Feel free to set it back to major, if you think it is not a blocker. ;;;","19/Mar/21 13:57;nhab;[~cadonna] 

Thanks for the quick feedback!

To clarify, exactly_once should not be affected by this, since EOS would also revert the table-state, so the record wouldn't be classified as duplicate record and dropped;;;","19/Mar/21 13:59;cadonna;You are right! My bad!;;;","22/Mar/21 16:08;cmaus;I'm wondering if emit-on-change tables can be safely used with compacted input topics and at-least-once semantics.

Given the following records in the input topic:
timestamp, key, value

1, 1, 1
2, 1, 1
3, 1, 1


The records with ts 1 and two are read, an exception occurs, causing a restart.

During restart of the consuming process, log compaction removes the records with ts 1 and two, leaving only the record with ts 3 in the input topic.

When the topic is consumed, we will only see a record with ts 3, which is greater than the greatest ts seen for the key.
As the value is the same as stored in the table, the output will be suppressed.

Is there a flaw in my thinking or is this a plausible scenario?;;;","24/Mar/21 17:51;vvcephei;Thanks [~cmaus] , I think you're right.

Since this feature is purely an optimization, and since it doesn't seem like we're going to be able to patch it up to avoid these data loss conditions, I'm just going to disable it entirely.

I'm confident that we can re-introduce it later and avoid these edge cases, but for now I'd like to err on the side of correctness.;;;","25/Mar/21 23:10;vvcephei;Disabled KIP-557 entirely.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resizing the thread cache in a non thread safe way can cause records to be redirected throughout the topology,KAFKA-12503,13366203,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,wcarlson5,wcarlson5,wcarlson5,18/Mar/21 22:35,19/Mar/21 17:37,13/Jul/23 09:17,19/Mar/21 02:38,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"When a thread is added, removed or replaced the cache is resized. When the thread cache was resized it was being done so from the thread initiating these calls. This can cause the record to be redirected to the wrong processor via the call to `evict` in the cache. The evict flushes records downstream to the next processor after the cache. But if this is on the wrong thread the wrong processor receives them. 

This can cause 3 problems.

1) When the owner finishes processing the record it set the current node to null in the processor context a this then causes the other processor to throw an exception `StreamsException: Current node is unknown.`. 

2) Depending on the type it can cause a class cast exception as the record is a different type. Mostly this happened when the value types were different inside of the map node from the toStream method

3) A silent issue is it could cause data to be processed by the wrong node and cause data corruption. We have not been able to confirm this last one but it is the most dangerous in many ways.",,ableegoldman,guozhang,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 17:37:15 UTC 2021,,,,,,,,,,"0|z0oyeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/21 22:59;ableegoldman;Thanks for the writeup! If anyone reading this wants to check out the code for themselves, the methods and classes of note are (1) the ProcessorContext, which tracks a ""current node"" and forwards records to that note as they flow through the topology, (2) StreamTask#process where a StreamThread picks up the next record, sets the current node on the context to the SourceNode, processes the record through the subtopology, and then sets the current node back to null, and (3) TimestampedCacheFlushListener which is tied to a cache and the processor node immediately downstream of that cache. The listener receives records that have been evicted, saves the current node on the context as the `prevNode`, then sets the current node on the context, forwards the record, and resets the current node to `prevNode`.

The `StreamsException: Current node is unknown` must have occurred because the StreamThread owning this task had finished processing the record and set the current node to null when another thread hit the injected exception and tried to resize the cache. When the evicted record ended up in TimestampedCacheFlushListener#apply, the listener saved ""null"" as it's previous node, processed the record with its own node, and then reset it to null. If the other StreamThread had begun processing this task in the meantime, it would suddenly find its current node to be null and throw the exception we see.

The ClassCastException is similar, but instead of the listener setting the current node to null while the StreamTask was in the middle of processing, it must have set the current node to some other node elsewhere in the topology. If this other node has different input/output types, we would run into a ClassCastException as it forwards eg a Long to the downstream node which was expecting a Change<Long>.

Of course these are the race conditions under which we ran into visible symptoms, but as Walker mentioned the more dangerous possibility is all the other times when a foreign processor is inserted into the middle of the subtopology but the data types match and therefore no exception is thrown. In these cases we would be silently corrupting the data.

Obviously one followup question we should ask is why it was so easy for one thread to process records that belong to another in the first place? We should consider some kind of safety mechanism to ensure single-threaded access to the ProcessorContext and task directories -- turns out the locking mechanism doesn't actually protect against multithreaded access unless we explicitly ask it whether we can lock it or not.;;;","18/Mar/21 23:09;guozhang;Thanks for the summary [~wcarlson5] [~ableegoldman]. I'm wondering if we should fix it along with https://issues.apache.org/jira/browse/KAFKA-12500 together by adding a separate function in the cache to just free the space corresponding to a thread that does not trigger eviction --- i.e. just clear the records in the buffer. Wondering what's your current proposal to fix.;;;","19/Mar/21 17:37;wcarlson5;I would also like to note that this was only observed in at least once processing guarantee ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache memory is leaked after removing/replacing a StreamThread,KAFKA-12500,13366167,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,18/Mar/21 19:29,20/Mar/21 01:16,13/Jul/23 09:17,20/Mar/21 01:16,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"We currently leak the memory given to a StreamThread for its cache after it gets shutdown due to being removed or replaced. 

Removal:
If the StreamThread is removing itself, or if a thread is being removed by an external caller but fails to shutdown within the allotted time, it won't be removed from the `threads` list to avoid freeing up it's thread id while still running. If the thread hasn't reached the DEAD state when we resize the cache, the new cache size per thread is computed based on the number of live threads, which includes the removed thread at this point.

Replacement:
When a thread is replaced, we first shutdown that thread but hold off on removing it from the threads list until it's DEAD. Immediately after the shutdown we call addStreamThread to start up a new thread, which resizes the cache according to num-live-threads + 1.

In both of these cases, the cache memory of the shutting down thread won't be made available to the remaining threads.

Note: in general this leak may not be permanent, as a subsequent event once the thread has reached DEAD and been removed from the threads list will cause it's memory to be released. OF course if the subsequent event is another thread removal or replacement, then we just have a new memory leak.",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-18 19:29:10.0,,,,,,,,,,"0|z0oy6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source task offset commits continue even after task has failed,KAFKA-12497,13366125,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,18/Mar/21 15:52,18/Oct/22 16:39,13/Jul/23 09:17,13/Oct/22 14:16,2.0.2,2.1.2,2.2.3,2.3.2,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,3.0.0,,,,,,,,,,,,,3.4.0,,,,,,,,KafkaConnect,,,,,0,,,,,"Source task offset commits take place on a dedicated thread, which periodically triggers offset commits for all of the source tasks on the worker on a user-configurable interval and with a user-configurable timeout for each offset commit.

 

When a task fails, offset commits continue to take place. In the common case where there is no longer any chance for another successful offset commit for the task, this has two negative side-effects:

First, confusing log messages are emitted that some users reasonably interpret as a sign that the source task is still alive:
{noformat}
[2021-03-06 04:30:53,739] INFO WorkerSourceTask{id=Salesforce_PC_Connector_Agency-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2021-03-06 04:30:53,739] INFO WorkerSourceTask{id=Salesforce_PC_Connector_Agency-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask){noformat}
Second, if the task has any source records pending, it will block the shared offset commit thread until the offset commit timeout expires. This will take place repeatedly until the either the task is restarted/deleted, or all of these records are flushed.

 

In some other cases, it's actually somewhat sensible to continue to try to commit offsets. Even if a source task has died, data from it may still be in flight to the broker, and there's no reason not to commit the offsets for that data once it has been ack'd.

 

However, if there is no in-flight data from a source task that is pending an ack from the Kafka cluster, and the task has failed, there is no reason to continue to try to commit offsets. Additionally, if the producer has failed to send a record to Kafka with a non-retriable exception, there is also no reason to continue to try to commit offsets, as the current batch will never complete.

 

We can address one or both of these cases to try to reduce the number of confusing logging messages, and if necessary, alter existing log messages to make it clear to the user that the task may not be alive.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-18 15:52:55.0,,,,,,,,,,"0|z0oxx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbalanced connectors/tasks distribution will happen in Connect's incremental cooperative assignor,KAFKA-12495,13366007,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sagarrao,showuon,showuon,18/Mar/21 07:18,24/Jan/23 14:08,13/Jul/23 09:17,15/Nov/22 21:28,,,,,,,,,,,,,,,,,,,,,,,3.4.0,,,,,,,,KafkaConnect,,,,,6,,,,,"In Kafka Connect, we implement incremental cooperative rebalance algorithm based on KIP-415 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect)|https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect]. However, we have a bad assumption in the algorithm implementation, which is: after revoking rebalance completed, the member(worker) count will be the same as the previous round of reblance.

 

Let's take a look at the example in the KIP-415:

!image-2021-03-18-15-07-27-103.png|width=441,height=556!

It works well for most cases. But what if W4 added after 1st rebalance completed and before 2nd rebalance started? Let's see what will happened? Let's see this example: (we'll use 10 tasks here):

 
{code:java}
Initial group and assignment: W1([AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
Config topic contains: AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5
W1 is current leader
W2 joins with assignment: []
Rebalance is triggered
W3 joins while rebalance is still active with assignment: []
W1 joins with assignment: [AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5]
W1 becomes leader
W1 computes and sends assignments:
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
W2(delay: 0, assigned: [], revoked: [])
W3(delay: 0, assigned: [], revoked: [])

W1 stops revoked resources
W1 rejoins with assignment: [AC0, AT1, AT2, AT3]
Rebalance is triggered
W2 joins with assignment: []
W3 joins with assignment: []

// one more member joined
W4 joins with assignment: []
W1 becomes leader
W1 computes and sends assignments:

// We assigned all the previous revoked Connectors/Tasks to the new member, but we didn't revoke any more C/T in this round, which cause unbalanced distribution
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [])
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: [])
W2(delay: 0, assigned: [BT1, BT2, BT4], revoked: [])
W2(delay: 0, assigned: [BT4, BT5], revoked: [])
{code}
Because we didn't allow to do consecutive revoke in two consecutive rebalances (under the same leader), we will have this uneven distribution under this situation. We should allow consecutive rebalance to have another round of revocation to revoke the C/T to the other members in this case.

expected:
{code:java}
Initial group and assignment: W1([AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
Config topic contains: AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5
W1 is current leader
W2 joins with assignment: []
Rebalance is triggered
W3 joins while rebalance is still active with assignment: []
W1 joins with assignment: [AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5]
W1 becomes leader
W1 computes and sends assignments:
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
W2(delay: 0, assigned: [], revoked: [])
W3(delay: 0, assigned: [], revoked: [])

W1 stops revoked resources
W1 rejoins with assignment: [AC0, AT1, AT2, AT3]
Rebalance is triggered
W2 joins with assignment: []
W3 joins with assignment: []

// one more member joined
W4 joins with assignment: []
W1 becomes leader
W1 computes and sends assignments:

// We assigned all the previous revoked Connectors/Tasks to the new member, **and also revoke some C/T** 
W1(delay: 0, assigned: [AC0, AT1, AT2], revoked: [AT3])
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: [])
W3(delay: 0, assigned: [BT1, BT2, BT4], revoked: [])
W4(delay: 0, assigned: [BT4, BT5], revoked: [])

// another round of rebalance to assign the new revoked C/T to the other members
W1 rejoins with assignment: [AC0, AT1, AT2] 
Rebalance is triggered 
W2 joins with assignment: [AT4, AT5, BC0] 
W3 joins with assignment: [BT1, BT2, BT4]
W4 joins with assignment: [BT4, BT5]

W1 becomes leader 
W1 computes and sends assignments:

// (final) We assigned all the previous revoked Connectors/Tasks to the members
W1(delay: 0, assigned: [AC0, AT1, AT2], revoked: []) 
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: []) 
W2(delay: 0, assigned: [BT1, BT2, BT4], revoked: []) 
W2(delay: 0, assigned: [BT4, BT5, AT3], revoked: [])
{code}
Note: The consumer's cooperative sticky assignor won't have this issue since we re-compute the assignment in each round.

 

Note2: this issue makes KAFKA-12283 test flaky.",,cadonna,ChrisEgerton,dajac,kkonstantine,mcabrera,ramkrish1489,sagarrao,showuon,yazgoo,,,,,,,,,,,,,,,,,,KAFKA-12283,,,,,,,,,,,,,,,,,,KAFKA-13763,KAFKA-13764,KAFKA-8391,,,,,,,"18/Mar/21 07:04;showuon;image-2021-03-18-15-04-57-854.png;https://issues.apache.org/jira/secure/attachment/13022570/image-2021-03-18-15-04-57-854.png","18/Mar/21 07:05;showuon;image-2021-03-18-15-05-52-557.png;https://issues.apache.org/jira/secure/attachment/13022569/image-2021-03-18-15-05-52-557.png","18/Mar/21 07:07;showuon;image-2021-03-18-15-07-27-103.png;https://issues.apache.org/jira/secure/attachment/13022568/image-2021-03-18-15-07-27-103.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 03:25:00 UTC 2022,,,,,,,,,,"0|z0ox6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/21 07:24;ramkrish1489;[~showuon] this is being addressed in https://issues.apache.org/jira/browse/KAFKA-10413 , let me know if you still see any issue with this patch as well;;;","18/Mar/21 07:35;showuon;[~ramkrish1489], thanks for your comments. However, this issue is still there after this patch. As I described, it's that the algorithm didn't consider the member count not the same as previous revocation round. We just evenly distributed the revoked C/T to all the new members. We should consider the member count before doing this. It's not related to the issue in KAFKA-10413.

Or we can say, you found 2 issues in KAFKA-10413, and I found one more issue in this ticket, which all will cause uneven distribution.

Please help review my PR after I completed it. Thank you.;;;","19/Mar/21 03:16;showuon;[~ramkrish1489], I checked your PR in KAFKA-10413 again, and I modified my bug description now. You fixed the issue when member left, and you did a round-robin distribution to all the active members. Nice fix!

The issue I tried to fix here, is when new member joined, we'll have 2 rounds of rebalance for it: 1 for revocation, 1 for re-assignment. And the issue is at the 2nd round of re-assignment. What we did now is re-assign all the previous revoked C/T to the new members, and end the rebalance. However, if there is 1 (or more) members joined during the 1st round and 2nd round of rebalance, at the 2nd round of re-assignment, we'll assign all the revoked C/T to all new members, and end the rebalance. But the revoked C/T was intended for only (numNewMembers - 1), so it causes uneven distribution.

 

You can check the example in the bug description again, you should understand what I meant. Thank you.;;;","03/Apr/21 00:38;showuon;Remember to enable the _testMultipleWorkersRejoining_ test once this is fixed.;;;","13/Jul/21 02:39;showuon;[~kkonstantine], could you take a look the PR? I suddenly found this bug is blocking a V3.0 blocker flaky test. Please help! Thank you.;;;","14/Jul/21 21:36;kkonstantine;This issue corresponds to a corner case that does not seem to appear in practice often. The current suggestion to allow for consecutive revocations carries some risk. I have another fix in mind that I'd like to explore. In the meantime I'm punting this issue to the next release. ;;;","15/Nov/21 10:09;dajac;Moving to the next release as we are past the 3.1 release code freeze.;;;","20/Dec/21 15:23;yazgoo;Hi, any news on this issue ?
I have been testing and it looks like it solves my unbalance issue with autoscaling.


Is there a plan to merge it ?

Thanks;;;","29/Dec/21 03:09;showuon;[~kkonstantine] , could you give some suggestion for the issue?  This issue has been pending for a long time. You mentioned you have fix in mind, do you have time to submit the PR? If no, could you share your thoughts here, so that we can move on to next step. Thank you.;;;","08/Feb/22 03:19;showuon;[~kkonstantine] , there are some users faced this issue and would like to fix this issue soon. Do you have plan when this PR can be merged or provide some comments about what your concern about my fix is, and what's your suggestion to this issue. Please help. Thanks.



cc [~ChrisEgerton] [~rhauch] ;;;","09/Feb/22 20:14;ChrisEgerton;Hi [~showuon]! I'm taking time off right now but should be able to try to get up-to-speed on the state of incremental rebalancing logic in Connect and take a look at your PR and the issue description here sometime next week.

I'll note that the assertion that ""This issue corresponds to a corner case that does not seem to appear in practice often."" seems to be empirically incorrect given the level of attention that this ticket has received, and given some recent conversations on the dev mailing list around timely review of PRs from non-committer contributors, would love to see this get the attention it deserves.

We should all obviously try to maintain a high level of quality and efficacy in the Connect framework and the Kafka code base in general, but at this point it seems like we've let perfect become the enemy of good and have allowed this issue to remain unaddressed for far too long (nearly a year!) despite showing promise as an improvement to Connect. There's only so much I can do as a contributor, but I'd be happy to take the first step and give this a look of my own to try to help move things along.;;;","13/Apr/22 17:52;kkonstantine;Thanks for documenting the issue in detail [~showuon]. Adding here the comment I added to the PR. 

My main concern is indeed related to the proposed change to apply consecutive rebalances that will perform revocations.

The current incremental cooperative rebalancing algorithm, is using two consecutive rebalances in order to move tasks between workers. One rebalance during which revocations are happening and one during which the revoked tasks are reassigned. Although clearly this is not an atomic process (as this issue also demonstrates) I find that it's a good property to maintain and reason about.

Allowing for consecutive revocations that happen immediately when an imbalance is detected might mean that the workers overreact to external circumstances that have caused an imbalanced between the initial calculation of task assignments of the revocation rebalance and the subsequent rebalance for the assignment of revoked tasks. Such circumstances might have to do with rolling upgrades, scaling a cluster up or down or simply might be caused by temporary instability. We were first able to reproduce this issue in integration tests by the test that is currently disabled.

My main thought was that, instead of risking shuffling tasks too aggressively within a short period of time and open the door to bugs that will make workers oscillate between imbalanced task assignments continuously and in a tight loop, we could use the existing mechanism of scheduling delayed rebalances to program workers to perform a pair of rebalanced (revocation + reassignment) soon after an imbalance is detected. Regarding when an imbalance is detected, the good news is that the leader worker sending the assignment during the second rebalance of a pair of rebalances knows that it will send an imbalanced assignment (there's no code to detect right now that but can be easily added just before the assignment is sent). The idea here would be to send this assignment anyways, but also schedule a follow up rebalance that will have the opportunity to balance tasks soon with our standard pair of rebalances that works dependably as long as no new workers are added or removed between the two rebalances. We can discuss what is a good setting for the delay. One obvious possibility is to reuse the existing property. Adding another config just for that seems unwarranted. To shield ourselves from infinite such rebalances the leader should also keep track of how many such attempts have been made and stop attempting to balance out tasks after a certain number of tries. Of course every other normal rebalance should reset both this counter and possibly the delay.

I'd be interested to hear what do you think of this approach that is quite similar to what you have demonstrated already but potentially less risky in terms of changes in the assignor logic and how aggressively the leader attempts to fix an imbalance.;;;","14/Apr/22 01:13;ChrisEgerton;{quote}Allowing for consecutive revocations that happen immediately when an imbalance is detected might mean that the workers overreact to external circumstances that have caused an imbalanced between the initial calculation of task assignments of the revocation rebalance and the subsequent rebalance for the assignment of revoked tasks. Such circumstances might have to do with rolling upgrades, scaling a cluster up or down or simply might be caused by temporary instability.
{quote}
Have you identified a plausible case where this may be an issue? Load-balancing revocations are only necessary when the number of workers has increased (or when the number of connectors/tasks has decreased, although this is not addressed in the current rebalancing algorithm). At least with the case of new workers, is it really an overreaction to find connectors/tasks to allocate to them as soon as possible?

Going over the example cases provided:
 * With a rolling upgrade, the existing delayed rebalancing logic should already apply, preventing excessive revocations from taking place
 * With a cluster scale-down, no load-balancing revocations should be necessary, since the number of connectors/tasks per worker will increase, not decrease
 * With a cluster scale-up, immediate revocation will not only not be harmful, it will actually be advantageous as it will allow the new workers to begin work immediately instead of waiting for the scheduled rebalance delay to elapse. This could be crucial if there's a load burst across the cluster and an external auto-scaling process spins up new workers to try to respond as quickly as possible
 * With temporary instability, if workers fall out of the cluster, the existing delayed rebalancing logic should already apply, preventing excessive revocations from taking place. There may be another interpretation of what this scenario would look like in terms of workers leaving/joining the cluster; let me know if you had something else in mind

The only case I can think of where unconditionally delaying between revocations may be beneficial is if there's a rapid scale-up and then immediate scale-down of a cluster. If we hold off on revoking too many connectors/tasks from the pre-scale-up workers in the cluster, then we'll have to reassign fewer of them once the scale-down takes place. But unless I'm missing something, this is an unlikely edge case and should not be prioritized.
{quote}To shield ourselves from infinite such rebalances the leader should also keep track of how many such attempts have been made and stop attempting to balance out tasks after a certain number of tries. Of course every other normal rebalance should reset both this counter and possibly the delay.
{quote}
This is a great suggestion, especially since it can (and should) be implemented regardless of whether a delay is added between consecutive load-balancing revocations.;;;","14/Apr/22 06:32;showuon;[~kkonstantine] [~ChrisEgerton] , thanks for your comments.

So, in summary, the risk of the current (consecutive revocations) approach:
 # overreact to external circumstances
 # infinite rebalance

I like the idea of (2), to protect ourselves from infinite rebalance, though this should not happen.

For (1), it's really hard to identify if the consecutive revocation is overreact, or quick fix the issue. I think we all agree that's case by case. Then, I think we can follow the current consecutive revocation solution to fix it. The main reason is:
 # the PR is ready for review, and should able to include it in v3.2.0 soon
 # Some users have suffered from this issue and have been waiting for the fix for a long time

 

In my opinion, we will avoid infinite rebalance anyway, we can improve it to use ""delayed revocation"" method if necessary in the future. WDYT?

 

 

 ;;;","14/Apr/22 14:05;cadonna;[~showuon] [~ChrisEgerton] [~kkonstantine] Is this a regression so that it qualifies as a blocker for the 3.2.0 release?;;;","14/Apr/22 17:47;kkonstantine;With respect to release logistics, I think it's worth being clear. This issue, although it's an annoying bug, is not a regression and therefore should not be treated as a blocker for 3.2.0 if we want to be consistent with our release process at this point. Given that we are way into code freeze the fix should target {{trunk}} and be backported to the respective release branches when they go out of the code freeze period. 

[~showuon] regarding the fix itself, it's worth noting that the challenge is with testing rather that the code changes themselves. But irrespective to that, I don't think that being tactical and rushing the existing fix is necessarily the right thing to do in this case (and to be honest in my opinion timing is rarely the most important factor for complicated improvements). 

I believe that it's really worth trying to avoid rebalance storms (that can happen when multiple events that can cause a rebalance happen concurrently). If users want to minimize the time to recover from this type of situation they will still have the ability to do so by setting [scheduled.rebalance.max.delay.ms|https://kafka.apache.org/documentation/#connectconfigs_scheduled.rebalance.max.delay.ms] to a lower value. And if we follow this approach and it works sufficiently, we can consider lowering the default time in the future as an improvement. ;;;","14/Apr/22 18:00;ChrisEgerton;I believe the risks of rebalance storms are being overstated and without a practical example should not affect the fix that is chosen here.;;;","15/Apr/22 02:46;showuon;I agree this is not a regression and is not a blocker for v3.2.0.

[~kkonstantine] , I see what you mean. OK, let's fix it in a delayed rebalance solution.

Speaking of that, I might be busy on daily jobs recently, so if anyone is interested in fixing it and submit a PR, it'll be great! 

 

Thank you.;;;","11/May/22 17:02;mcabrera;I see two different issues when doing deployments with 2.6.2. In my deployments I add 5 Workers at once and then remove 5 other Workers at once (starting with 11, so it goes from 11 to 16 and then back to 11). I say at once but they don't really get started and removed at once, there maybe could be a 30 seconds different between the first one being ready and the last one being ready. Problems I see are:
 # What's mentioned in this ticket when Workers are being added. Basically, there are more workers in the second round than they were in the first round, which leads to unbalanced assignments.
 # When Workers are going away, we sometimes end up with one Worker with ALL the assignments that were in the 5 Workers that went away. The Worker that gets all this assignments is the last one that got started. When this happens, I see that in one generation that Worker had no assignments, and in the next one it has but it also shows the delay that comes from waiting on the Workers that left ({{{}scheduled.rebalance.max.delay.ms{}}}). After that delay expires, all assignments from the 5 Workers that went away go into that one Worker. My theory here by looking at the code is that it may have become the only one in {{candidateWorkersForReassignment}} at the time were it had no assignments, and then remained that way even when we started waiting on the Workers that went away even though this Worker had assignments by now. Either way, I don't really get this {{{}candidateWorkersForReassignment{}}}, because such workers would get all the assignments of the ones that went away, right? What if more Workers went away than the ones that don't have assignments? This is in fact what happens when I test removing 5 Workers and in the waiting period add 1 Worker, everything goes into that single new Worker.

 

 ;;;","12/May/22 07:51;showuon;[~mcabrera] , thanks for the sharing. Looks like you already investigated it and are familiar with the codes. Are you interested in submitting a PR to improve the rebalance algorithm?;;;","16/May/22 15:47;mcabrera;[~showuon] I'm short on time right now, so I won't be able to tackle this.;;;","17/May/22 02:33;showuon;No problem, [~mcabrera] !;;;","03/Aug/22 15:27;sagarrao;[~showuon] , is this still open? I can take a stab at it. Let me know. Thanks!;;;","03/Aug/22 15:42;ChrisEgerton;[~sagarrao] yes, it's unassigned so anyone can feel free to take a shot at it.

I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though. With that change, it would become impossible in some scenarios to scale up a cluster within the rebalance delay, and given that the current default for that delay is five minutes, this may cause some issues for Connect cluster administrators. In fact, I would even argue that it would qualify as a regression since the outcome for users in this scenario would be significantly worse than it is at the moment.

I'm still not convinced that rebalance storms are a serious risk when removing the guard against successive revocation rounds, especially if we add some cycle-detection logic. However, if my colleagues still believe it's necessary to take extra precautions against storms, one possibility is that we can compromise and use exponential backoff between successive revocation rounds. This would be more complex to implement and require some potentially-tricky state tracking in the assignor class, but if done effectively, would allow us to start off by having little to no delay between successive revocation rounds, but, in the event that something goes wrong, eventually work our way up to waiting the full scheduled rebalance delay in between rounds.;;;","04/Aug/22 04:55;sagarrao;Thanks [~ChrisEgerton] . I would take a look. I had another idea as well which is probably more extravagant. I was thinking we let the assignor do whatever it does and we let it stabilise. How about we run some checks afterwards to see if the assignments are balanced or skewed and then trigger a rebalance later on. IIRC, kafka streams does something similar by leveraging probably the Consumer#enforceRebalance. I can dig deep into it if needed. We can take it even a step further by even providing users the ability to define the idea of balanced assignments. Case in point could be running it on cloud v/s running it on prem. This would also alleviate some of the concerns raised in this ticket about rebalance storms(I know you think it's not a risk) or premature successive rebalances etc.

WDYT? Does it make sense?;;;","04/Aug/22 14:35;ChrisEgerton;[~sagarrao] it seems like there are two ideas at play here:
 # Triggering possibly-delayed rebalances independently of the assignor (i.e., without sending out an assignment with a scheduled rebalance delay)
 # Pluggable (i.e., user-supplied) definitions of a ""balanced"" assignment

With regards to item 1, I toyed with this idea too, but I don't really see a practical difference between this and using the scheduled rebalance delay mechanism, as long as we have cycle detection. What would be the advantage here?

With regards to item 2, there's definitely room for improvement of our notion of ""balance"" within a cluster given that the assumption right now is that all tasks and connectors have homogeneous resource utilization. This is obviously not the case, but nobody's put in the (substantial) work yet to try to add more granularity to our rebalance logic, and the existing logic has served us decently well up to this point. Ultimately though, it would require a KIP and is probably out of scope for this ticket since there are alternative solutions that are more lightweight and suitable for backport.;;;","04/Aug/22 16:52;sagarrao;[~ChrisEgerton] You summed it up correctly. Regarding #1, you are right. The real benefit would be seen only when we combine it with idea #2 i.e pluggable notion of balances. It could be load based or equal distribution of connectors/tasks etc. Yeah it might even need a KIP which is out of scope for this. Let me look at the code for this one.;;;","19/Aug/22 15:15;sagarrao;[~ChrisEgerton] , I was looking at this and this comment of yours caught my eye:

`I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though`.  I actually couldn't follow this comment. Today the protocol doesn't allow 2 successive rebalances happening by the same leader and Luke has been saying all along to remove it. I think that's what his PR does as well.

I think to be on the safer side, I would implement the exponential back off logic starting from 0 -> scheduled rebalance delay. But we won't still do it while a delayed rebalance is active. It could be counter based as well i.e we will tolerate w workers to be added before triggering a rebalance. Complexity wise it seems the same to me but how do the 2 compare?

 

Also, you talked about cycle detection. What does that refer to?;;;","22/Aug/22 03:07;showuon;Sorry for the delay. My thoughts are below

 

> `I should note that I don't believe the proposal to use the scheduled rebalance delay in between successive revocation rounds is safe, though`.  I actually couldn't follow this comment. Today the protocol doesn't allow 2 successive rebalances happening by the same leader and Luke has been saying all along to remove it. I think that's what his PR does as well.

 

Yes, my PR is to allow successive rebalances ""without delay"". But the code author, Konstantine, suggested we should have ""delay"" between each rebalance (revocation), to avoid rebalance storm. But Chris doesn't agree that there would cause rebalance storm, and also, the delay might even slow down the normal scale up/down pace. That's why, Chris suggested, with some compromise, we can have a exponential backoff between successive revocation rounds. That is, with current ""scheduled delay"" method, each rebalance needs 5 mins to trigger the rebalance. But with ""exponential backoff"" way, it'll speed up the rebalance, but also keep the concept of ""delay"" rebalance.

 

> I think to be on the safer side, I would implement the exponential back off logic starting from 0 -> scheduled rebalance delay.

 

Sounds good

 

> But we won't still do it while a delayed rebalance is active. It could be counter based as well i.e we will tolerate w workers to be added before triggering a rebalance. Complexity wise it seems the same to me but how do the 2 compare?

 

Yes, I agree complexity wise it seems the same. Again, this way is to keep the original concept of ""rebalance delay"", and also speed up the rebalance pace.

 

[~sagarrao] , I saw you created a KIP for next generation Connect rabalance protocol [here|https://cwiki.apache.org/confluence/display/KAFKA/%5BDRAFT%5DIntegrating+Kafka+Connect+With+New+Consumer+Rebalance+Protocol]. I have a quick scan, and looks like it only integrates the concept of KIP-848 into Kafka Connect, and keep Connect client assignor. I'm wondering that if we can move the client assignor in Connect into group coordinator like consumer does? I don't think Connect needs to have a custom client assignor like Stream does, because Connect has no complicated ""active""/""standBy"" tasks like Stream does. If we can move the algorithm onto group coordinator, I think it'll make our life easier. WDYT?

 

However, I think it still needs a lot of discussion for the new KIP. Before that, we can still have a acceptable fix for this bug. Thank you.;;;","22/Aug/22 03:25;sagarrao;Thanks [~showuon] for your comments. 

Regarding the new draft KIP thank you for giving a pass through it. I am working on a new version based on initial suggestions/feedback. As you pointed out, it keeps the assignment logic at the client side - similar to streams and the current rebalancing protocol. Yeah it could very well be moved to the group coordinator layer. 

Having said that, I decided to keep it in the client layer because in Incremental Cooperative protocol, clients can trigger a rebalance based on the scheduled rebalance delay. As per my understanding of KIP-848, such rebalances which the clients want to trigger, can be done by setting the `reason` field. That kind of rebalance triggering is similar(yet not as complex) as the kind of rebalance that Streams triggers. Also, if we had only Eager assignor to support, then I think moving it to the GC would have made sense as the Group coordinator needed to trigger rebalances only when workers joined or left for example. Also the logic of scheduled rebalance delays is not something the GC really needs to know or keep track of, I felt keeping it at the client side would make more sense and also in line with what KIP-848 is trying to acheive. Let me know if that makes sense.

 

Also, it would be great if this feedback can be shared on the discussion thread! It would evoke further comments from more members in the community which would help in solidifying the design! Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Formatting of example RocksDBConfigSetter is messed up,KAFKA-12492,13365958,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,ben.c,ableegoldman,ableegoldman,17/Mar/21 22:50,09/Apr/21 05:15,13/Jul/23 09:17,09/Apr/21 03:57,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,documentation,streams,,,,0,docs,newbie,,,"See the example implementation class CustomRocksDBConfig in the docs for the rocksdb.config.setter

https://kafka.apache.org/documentation/streams/developer-guide/config-streams.html#rocksdb-config-setter",,ableegoldman,ben.c,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 05:15:47 UTC 2021,,,,,,,,,,"0|z0oww0:",9223372036854775807,,ableegoldman,,,,,,,,,,,,,,,,,,"21/Mar/21 01:27;ben.c;Hi, how can I get assigned for this task? Thanks.;;;","22/Mar/21 00:00;ableegoldman;Hey [~ben.c], I added you as a contributor so you can self-assign tickets from now on. Thanks for picking this up!;;;","22/Mar/21 00:16;ben.c;Thanks so much!;;;","02/Apr/21 05:23;ben.c;We just need to update the spaces within the section in that html.

But I have few questions.
 # There're few folders in this format: kafka-site/\{number}/streams/developer-guide. Are they some history records? I should update all of them, or create a new one like kafka-site/28/streams/developer-guide with the according changes? (I couldn't set kafka-site up. So I cannot try it out.)
 # I tried to setup the local kafka-site by following [https://cwiki.apache.org/confluence/display/KAFKA/Setup+Kafka+Website+on+Local+Apache+Server.] But I still saw 403 as follows and tried to debug by following [https://stackoverflow.com/questions/10873295/error-message-forbidden-you-dont-have-permission-to-access-on-this-server?page=1&tab=votes#tab-top]

          ====================================   
h1.         Forbidden

           You don't have permission to access this resource.

           ====================================          

           Did anyone experience the same issue?

           ;;;","05/Apr/21 22:34;ableegoldman;The kafka-site repo is what the actual, live docs are built from. That's why there are separate folders like 27, 26, etc -- these correspond to the docs for versions 2.7 and 2.6, and so on. You only need to submit a PR to the kafka-site repo if you want your change to show up immediately -- if you don't mind waiting for the next release, you can just open a PR to fix the docs in the kafka repo directly. Then, these will be copied over to the kafka-site repo and made live when the next version is released.

Obviously it would be ideal if we could fix this in all versions, but it's probably sufficient to just fix it going forward. The 2.8 release is actually going on at the moment, so I would recommend submitting a PR to the kafka repo for now. If we can get it merged before 2.8 is released, then we're good -- otherwise you can open a followup PR with the same fix in just the 28 subdirectory of the kafka-site repo.

I'm not sure why you're getting a 403, I was able to setup a local apache server to test some docs but that was a while ago. Since it's just a fix of an existing formatting error, I wouldn't worry about testing it too much. As long as you can figure out why the formatting was messed up to begin with, and feel reasonably confident in your fix, then that's good enough. Remember, once the fix is in kafka-site it'll be live so you can just see what it looks like then. If something is still off, you can always submit a followup PR to fix it right away in kafka-site;;;","05/Apr/21 23:58;ben.c;[~ableegoldman] appreciate for the detailed explanation!

Here is the PR: https://github.com/apache/kafka-site/pull/345/;;;","06/Apr/21 00:44;ableegoldman;Thanks, I'll take a look. But note that you always need to submit a docs PR against the kafka-repo, while the kafka-site repo is optional. The reason being what I said above, ie during a release the docs from kafka get copied over to kafka-site. So if you only fix them in kafka-site but not in kafka, the fix will just get wiped out when the next release comes up. 

In general we usually do the kafka PR before the kafka-site PR, for that reason among others. But that's not a hard rule or anything -- my point is just that you need a PR for the kafka repo as well. ;;;","06/Apr/21 00:51;ben.c;Got it. Thanks. I guess I missed something mentioned here: https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Website+Documentation+Changes;;;","06/Apr/21 00:59;ableegoldman;Yeah, you need to check out the ""*Kafka Code Repository*"" and not just the ""*Kafka Website Repository*"". The whole setup and process with the docs is kind of confusing, I'll see if I can improve the docs/wiki to clear things up a bit. Thanks :);;;","06/Apr/21 06:37;ben.c;https://github.com/apache/kafka/pull/10486;;;","06/Apr/21 06:42;ben.c;Do we need to run [local tests|https://github.com/apache/kafka/blob/trunk/README.md] and build trigger for this task as described here: [https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes] ?;;;","09/Apr/21 03:59;ableegoldman;Merged, thanks for the PR! Unfortunately we just barely missed the 2.8 release, as John cut the RC earlier today. If you want to see this fix in the 2.8 docs then you'll need to submit this exact PR against the kafka-site repo as we discussed, but the 2.8 RC is still under vote at the moment so you'd need to wait for that to be released at which point the docs in kafka/2.8 will be copied over to a new 28 subdirectory in kafka-site. ;;;","09/Apr/21 05:15;ben.c;Sounds good to me. I will do a follow-up PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB not being pulled in as a transitive dependency,KAFKA-12491,13365954,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,ableegoldman,ableegoldman,17/Mar/21 22:09,18/Mar/21 18:08,13/Jul/23 09:17,18/Mar/21 18:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"In [pull/10203|https://github.com/apache/kafka/pull/10203] we stopped pulling in non-api dependencies and listed the rocksdb dependency as an implementation. However users may need to pull in RocksDB classes (such as Options, and various other sub-options such as TableFormatConfig). We should change this dependency to an api so that it gets pulled in as a transitive dependency and users aren't required to pull it in themselves and maintain compatibility",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-17 22:09:01.0,,,,,,,,,,"0|z0owv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink connectors do not work with the cooperative consumer rebalance protocol,KAFKA-12487,13365659,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,16/Mar/21 23:17,22/Nov/21 21:37,13/Jul/23 09:17,10/Nov/21 19:46,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,3.0.0,,,,,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,KafkaConnect,,,,,0,,,,,"The {{ConsumerRebalanceListener}} used by the framework to respond to rebalance events in consumer groups for sink tasks is hard-coded with the assumption that the consumer performs rebalances eagerly. In other words, it assumes that whenever {{onPartitionsRevoked}} is called, all partitions have been revoked from that consumer, and whenever {{onPartitionsAssigned}} is called, the partitions passed in to that method comprise the complete set of topic partitions assigned to that consumer.

See the [WorkerSinkTask.HandleRebalance class|https://github.com/apache/kafka/blob/b96fc7892f1e885239d3290cf509e1d1bb41e7db/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L669-L730] for the specifics.

 

One issue this can cause is silently ignoring to-be-committed offsets provided by sink tasks, since the framework ignores offsets provided by tasks in their {{preCommit}} method if it does not believe that the consumer for that task is currently assigned the topic partition for that offset. See these lines in the [WorkerSinkTask::commitOffsets method|https://github.com/apache/kafka/blob/b96fc7892f1e885239d3290cf509e1d1bb41e7db/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L429-L430] for reference.

 

This may not be the only issue caused by configuring a sink connector's consumer to use cooperative rebalancing. Rigorous unit and integration testing should be added before claiming that the Connect framework supports the use of cooperative consumers with sink connectors.",,ableegoldman,baz33,ChrisEgerton,dajac,kkonstantine,rng,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12857,,,,,,,KAFKA-13472,,,KAFKA-12473,KAFKA-12463,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 04 09:16:20 UTC 2021,,,,,,,,,,"0|z0ov1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/21 12:14;ChrisEgerton;Based on recent [KIP-726|https://lists.apache.org/thread.html/%3CCAFLS_9gOHobj-had=7SVBibJTxfzKCuf7WM+shTL_aTvSeyq=g@mail.gmail.com%3E] discussion, we'll also want to implement the  [ConsumerRebalanceListener::onPartitionsLost|https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html#onPartitionsLost-java.util.Collection-] method in order to avoid task failures due to offset commit failures if/when the consumer rebalance protocol is automatically downgraded from {{COOPERATIVE}} to {{EAGER}}.;;;","07/Jun/21 02:44;showuon;[~ChrisEgerton], is there any update on this ticket? Do you think we can complete it by V3.0? (No push, just want to know the status.) Thank you.;;;","14/Jun/21 13:21;ChrisEgerton;Hi [~showuon]--first off, congrats on the KIP! My PR has been reviewed and approved by a contributor and all that's left is for a committer to take a look. I hope we can get it merged in time for 3.0 but can't make any promises since it's out of my control. Perhaps [~kkonstantine] could take a look, since he voted on the KIP?;;;","09/Jul/21 04:17;kkonstantine;Just a note that for tickets that need to target a specific version, it's highly recommended (if not necessary) to add the fix versions field. Marked the issue as blocker for 3.0 and will be taking a look before code freeze, which approaches quickly. ;;;","19/Jul/21 21:24;kkonstantine;Changing the default consumer protocol to be the cooperative protocol has been postponed for 3.1.0. Given that we are past the code freeze for 3.0, I'm postponing this issue to 3.1.0 while keeping its blocker status for this release. The change is not trivial and would be good to have enough time to test before we release. ;;;","02/Nov/21 13:01;dajac;[~ChrisEgerton] Do we still plan to get this one in the 3.1 release? Based on the PR, it seems that we won't make it. Also, is it really a blocker?;;;","03/Nov/21 19:52;kkonstantine;Hi [~dajac]. I'd like to complete another review pass this week and merge it as a patch that will go in to both 3.0 and 3.1 branches. My initial comments on the PR seem to have been addressed. Would that be ok with you?;;;","04/Nov/21 09:16;dajac;Hi [~kkonstantine]. My understanding of the issue is that we are fixing a bug here. Therefore, merging it to 3.1 and 3.0 is fine for me. Please correct if it is not a bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker can block for longer than scheduled rebalance delay and/or session key TTL,KAFKA-12476,13365394,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,ChrisEgerton,ChrisEgerton,16/Mar/21 03:34,30/Nov/22 21:38,13/Jul/23 09:17,30/Nov/22 21:38,2.3.2,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,3.0.0,,,,,,,,,,,,,,,,3.4.0,,,,,,,,KafkaConnect,,,,,0,,,,,"Near the end of a distributed worker's herder tick loop, it calculates how long it should poll for rebalance activity before beginning a new loop. See [here|https://github.com/apache/kafka/blob/8da65936d7fc53d24c665c0d01893d25a430933b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L399-L409] and [here|https://github.com/apache/kafka/blob/8da65936d7fc53d24c665c0d01893d25a430933b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L459].

In between then and when it begins polling for rebalancing activity, some connector and task (re-)starts take place. While this normally completes in at most a minute or two, an overloaded cluster or one in the midst of garbage collection may take longer. See [here|https://github.com/apache/kafka/blob/8da65936d7fc53d24c665c0d01893d25a430933b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L411-L452].

The worker should calculate the time to poll for rebalance activity as closely as possible to when it actually begins that polling.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-16 03:34:16.0,,,,,,,,,,"0|z0otew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker can die if unable to write new session key,KAFKA-12474,13365391,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,16/Mar/21 03:27,01/Apr/21 23:22,13/Jul/23 09:17,01/Apr/21 19:56,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,3.0.0,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,3.0.0,,,,KafkaConnect,,,,,0,,,,,"If a distributed worker is unable to write (and then read back) a new session key to the config topic, an uncaught exception will be thrown from its herder's tick thread, killing the worker.

See [https://github.com/apache/kafka/blob/8da65936d7fc53d24c665c0d01893d25a430933b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L366-L369]

One way we can handle this case by forcing a read to the end of the config topic whenever an attempt to write a new session key to the config topic fails.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 23:22:12 UTC 2021,,,,,,,,,,"0|z0ote8:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"01/Apr/21 23:22;rhauch;Merged to the `trunk` branch after the `2.8` branch was created and after code freeze, and merged to:
 * the `2.5` branch (for inclusion in upcoming 2.5.2), 
 * the `2.6` branch (for inclusion in upcoming 2.6.2), 
 * the `2.7` branch (for inclusion in upcoming 2.7.1), 
 * the `2.8` branch (for inclusion in upcoming 2.8.0) w/ permission from RM ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initial offsets are copied from source to target cluster,KAFKA-12468,13365345,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,bdeneuter,bdeneuter,15/Mar/21 20:48,05/Jul/23 18:35,13/Jul/23 09:17,17/Feb/23 22:26,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,,,,,mirrormaker,,,,,3,,,,,"We have an active-passive setup where  the 3 connectors from mirror maker 2 (heartbeat, checkpoint and source) are running on a dedicated Kafka connect cluster on the target cluster.

Offset syncing is enabled as specified by KIP-545. But when activated, it seems the offsets from the source cluster are initially copied to the target cluster without translation. This causes a negative lag for all synced consumer groups. Only when we reset the offsets for each topic/partition on the target cluster and produce a record on the topic/partition in the source, the sync starts working correctly. 

I would expect that the consumer groups are synced but that the current offsets of the source cluster are not copied to the target cluster.

This is the configuration we are currently using:

Heartbeat connector

 
{code:xml}
{
  ""name"": ""mm2-mirror-heartbeat"",
  ""config"": {
    ""name"": ""mm2-mirror-heartbeat"",
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"",
    ""source.cluster.alias"": ""eventador"",
    ""target.cluster.alias"": ""msk"",
    ""source.cluster.bootstrap.servers"": ""<SOURCE_CLUSTER>"",
    ""target.cluster.bootstrap.servers"": ""<TARGET_CLUSTER>"",
    ""topics"": "".*"",
    ""groups"": "".*"",
    ""tasks.max"": ""1"",
    ""replication.policy.class"": ""CustomReplicationPolicy"",
    ""sync.group.offsets.enabled"": ""true"",
    ""sync.group.offsets.interval.seconds"": ""5"",
    ""emit.checkpoints.enabled"": ""true"",
    ""emit.checkpoints.interval.seconds"": ""30"",
    ""emit.heartbeats.interval.seconds"": ""30"",
    ""key.converter"": "" org.apache.kafka.connect.converters.ByteArrayConverter"",
    ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter""
  }
}
{code}
Checkpoint connector:
{code:xml}
{
  ""name"": ""mm2-mirror-checkpoint"",
  ""config"": {
    ""name"": ""mm2-mirror-checkpoint"",
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorCheckpointConnector"",
    ""source.cluster.alias"": ""eventador"",
    ""target.cluster.alias"": ""msk"",
    ""source.cluster.bootstrap.servers"": ""<SOURCE_CLUSTER>"",
    ""target.cluster.bootstrap.servers"": ""<TARGET_CLUSTER>"",
    ""topics"": "".*"",
    ""groups"": "".*"",
    ""tasks.max"": ""40"",
    ""replication.policy.class"": ""CustomReplicationPolicy"",
    ""sync.group.offsets.enabled"": ""true"",
    ""sync.group.offsets.interval.seconds"": ""5"",
    ""emit.checkpoints.enabled"": ""true"",
    ""emit.checkpoints.interval.seconds"": ""30"",
    ""emit.heartbeats.interval.seconds"": ""30"",
    ""key.converter"": "" org.apache.kafka.connect.converters.ByteArrayConverter"",
    ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter""
  }
}
{code}
 Source connector:
{code:xml}
{
  ""name"": ""mm2-mirror-source"",
  ""config"": {
    ""name"": ""mm2-mirror-source"",
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
    ""source.cluster.alias"": ""eventador"",
    ""target.cluster.alias"": ""msk"",
    ""source.cluster.bootstrap.servers"": ""<SOURCE_CLUSTER>"",
    ""target.cluster.bootstrap.servers"": ""<TARGET_CLUSTER>"",
    ""topics"": "".*"",
    ""groups"": "".*"",
    ""tasks.max"": ""40"",
    ""replication.policy.class"": ""CustomReplicationPolicy"",
    ""sync.group.offsets.enabled"": ""true"",
    ""sync.group.offsets.interval.seconds"": ""5"",
    ""emit.checkpoints.enabled"": ""true"",
    ""emit.checkpoints.interval.seconds"": ""30"",
    ""emit.heartbeats.interval.seconds"": ""30"",
    ""key.converter"": "" org.apache.kafka.connect.converters.ByteArrayConverter"",
    ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter""
  }
}
{code}
 ",,aaamber,ajosephides,akaltsikis,askldjd,bdeneuter,dragotic,ducminhle,gharris1727,gsavinov,jitesh88,tvainika,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14797,,,KAFKA-14666,KAFKA-13932,KAFKA-15144,,,,,KAFKA-14663,KAFKA-14727,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 09 18:03:07 UTC 2022,,,,,,,,,,"0|z0ot40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/21 14:18;dragotic;We are facing the same issue with MirrorMaker2 that comes with Kafka 2.7.0;;;","23/Mar/21 16:30;bdeneuter;I forgot to mention that we use a custom replication policy to keep the topic names the same in both source and target cluster. ;;;","25/Mar/21 02:42;askldjd;I am seeing similar issue as well. I am also using a custom replication policy to preserve topic name. I would mirror hundreds of topics and CGs. Most of them will mirror correctly except for a few CG:partition.

In this example, group topic 1 has a lag of -16201970 and the offset is determined to be 16764337. This is wrong. The offset 16764337 is copied the original cluster and not translated. 

 
{noformat}
GROUP TOPIC PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
group topic 20         390239          390239          0               -               -               -
group topic 22         494366          494366          0               -               -               -
group topic 16         147241          147241          0               -               -               -
group topic 18         469795          469795          0               -               -               -
group topic 24         835689          835689          0               -               -               -
group topic 3          391505          391505          0               -               -               -
group topic 5          194327          194327          0               -               -               -
group topic 1          16764337        562367          -16201970       -               -               -
group topic 11         913398          913398          0               -               -               -
group topic 13         1245835         1245835         0               -               -               -
group topic 7          52007           52007           0               -               -               -
group topic 9          1001964         1001964         0               -               -               -
group topic 19         1035791         1035791         0               -               -               -
group topic 21         456149          456149          0               -               -               -
group topic 15         696             696             0               -               -               -
group topic 17         225085          225085          0               -               -               -
group topic 23         622744          622744          0               -               -               -
group topic 4          777787          777787          0               -               -               -
group topic 6          286576          286576          0               -               -               -
group topic 0          1233042         1233042         0               -               -               -
group topic 2          1118624         1118624         0               -               -               -
group topic 12         693             693             0               -               -               -
group topic 14         283294          283294          0               -               -               -
group topic 8          924899          924899          0               -               -               -
group topic 10         494636          494636          0               -               -               -
{noformat}
 ;;;","25/Mar/21 20:47;askldjd;I found a race condition in MirrorSourceTask. Referencing here in case this is related.

https://issues.apache.org/jira/browse/KAFKA-12558;;;","31/Mar/21 10:27;akaltsikis;Hey [~askldjd],
I have checked also the race condition you mention.
Using the workaround you mention have you seen the same behavior as in [your previous comment|https://issues.apache.org/jira/browse/KAFKA-12468?focusedCommentId=17308308&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17308308]?
Or it fixed the problem?
{code:java}
For those who are experiencing this issue, the workaround is to make sure you have less than or equal to 10 partitions per task. Set your `tasks.max` value accordingly.
{code};;;","01/Apr/21 03:38;askldjd;Hey Angelos,

By limiting to less than 10 partitions per task, I was able to run 10 consecutive full mirror without any issues. Previously, it would fail almost every time.

The test cluster has about 4500 partitions, and my tasks.max is set to 500.

... Alan;;;","01/Apr/21 15:17;akaltsikis;Hey [~askldjd],
Thanks for your useful comment.
According to the above and due to the fact that I am trying to migrate around 2500 partitions which means that i need around 300 tasks to achieve it.
May i ask where did you run your MM2 (Strimzi MM2, EC2?) and about how much CPU power did the above required?;;;","01/Apr/21 16:01;akaltsikis;By the way, may i ask how you were able to validate that both the data and the consumer offsets have been mirrored correctly in such a big number of partitions (and i guess big number of consumer groups) ?;;;","01/Apr/21 20:20;askldjd;> May i ask where did you run your MM2 (Strimzi MM2, EC2?) and about how much CPU power did the above required?
I am running connect-mirror-maker.sh (standalone mode) on a C5.16xl. It is probably overkill, I am doing a lift-and-shift migration, so MM2 cost isn't a great concern since it is a one-time exercise. If you are doing continuous mirroring (e.g. HA purposes), I would consider running kafka-connect in distributed mode with more smaller nodes.

 

> By the way, may i ask how you were able to validate that both the data and the consumer offsets have been mirrored correctly in such a big number of partitions (and i guess big number of consumer groups) ?

Good question. I can't possibly verify every partition, so I ended up doing a best effort spotcheck
 # Make hello world topic with test consumer in the source cluster to make sure the topic and consumer offset are mirrored correctly in the target cluster
 # Use `kafka-consumer-group.sh`, verify that
 ## CURRENT-OFFSET in the target cluster are all positive
 ## LAG are all zero, or very close to near zero

Just with the basic spotcheck, I was able to find KAFKA-12558.

 ;;;","02/Apr/21 10:53;ducminhle;I got the same issue with Mirror Maker 2.7, consumer group lag < 0, offset can not translate. With topic have retention is delete, old messages are deleted, messages in the topic are 0, lags is always < 0.

I try to work around as [~askldjd] suggested, but it not work with topic have message = 0.;;;","07/Apr/21 14:25;akaltsikis;Hello again,

We have managed to set Mirrormaker2 properly considering [KAFKA-12558 bug|https://issues.apache.org/jira/browse/KAFKA-12558] which requires the SourceConnector tasks.max to be a number that the division of total partition number / tasks.max < 10.
 Also, we have introduced some configurations which offer performance optimizations according to this [blog|https://wmclane.medium.com/how-to-optimize-mirrormaker2-for-high-performance-apache-kafka-replication-697bc5089c64].
 We were trying to prove that the MM2 successfully has copied all the data (every topic message & correct consumer group offsets) from the source cluster to the target, but without great success.
 We tried to compare between source & target clusters the following metrics:
 Log Size per partition on the 2 different clusters but on most of the partitions is different
 Consumer Lag for each Consumer group- topic - partition combination but still all of them have a negative consumer lag. Only when we reset the offsets (as mentioned in this ticket's description) the consumer group lag in the target cluster start being in a better state (not being negative)
 LogEndOffset - LogStartOffset . For topics with cleanup.policy delete the subtraction gives pretty much the same numbers on the different clusters.

Is there *any better way* to verify that MM2 has caught up & continues the mirroring correctly?

What worries us most is that for some topics we observed that in the target cluster for some partitions we have a much smaller log size. We compared the messages in some of those partitions and indeed it seems that they were fewer messages in the target cluster’s topic partition. Our main question is does MM2 offer at least once guarantee = no messages lost?;;;","07/Apr/21 14:34;akaltsikis;We were also considering to add the following configuration in order to enable idempotence from producer side

source-test.producer.enable.idempotence = true
source-test.producer.acks=all
source-test.producer.max.in.flight.requests.per.connection=5
source-test.producer.retries=2147483647
source-test.consumer.isolation.level=read_committed
target-test.producer.enable.idempotence = true
target-test.producer.acks=all
target-test.producer.max.in.flight.requests.per.connection=5
target-test.producer.retries=2147483647
target-test.consumer.isolation.level=read_committed

As it is also mentioned [here|https://stackoverflow.com/a/66917494/2927926]

 ;;;","15/Apr/21 23:50;jitesh88;[~askldjd]

 what do you mean by run 10 consecutive full mirror ? 10 separate processes on the same box?

https://issues.apache.org/jira/browse/KAFKA-12468?focusedCommentId=17312826&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17312826;;;","09/Jun/21 14:42;ajosephides;We are also seeing this issue despite having <10 partitions per task.

[~askldjd] looking at your original configuration you have the `tasks.max` for both the Source and Checkpoint connector to be equal. When you increased the tasks on the Source connector to 500 to overcome the race condition did you find any starvation issues that resulted in a similar effect? Or, did you also increase the `tasks.max` on the Checkpoint connector too?

 ;;;","10/Jun/21 12:50;askldjd;I am running standalone mode, so I am not sure if tasks.max propagate to all connectors. I think it does. Which mode are you running in? If you are in distributed mode, I would consider setting tasks.max on each connector.;;;","10/Jun/21 13:39;ajosephides;We are running in distributed mode and are stipulating a `tasks.max` on each connector (Source, Checkpoint and Heartbeat, 500, 50 and 1 respectively).
We are still seeing this issue with negative offsets on our target cluster.;;;","10/Jun/21 16:54;aaamber;I was using standalone mode with active-passive setup and saw negative offsets in the past as well. One of the reasons I found was due to consumer request timeout, e.g. org.apache.kafka.common.errors.DisconnectException. I increased the request timeout and tasks.max and the offsets are synced correctly now.
{code:java}
# consumer, need to set higher timeout
source.admin.request.timeout.ms = 180000
source.consumer.request.timeout.ms = 180000{code}
 ;;;","04/Aug/21 09:28;ajosephides;Thanks for the suggestions and apologies for the delay in updating how we handled this issue in the end.
Should say from the outset that we did not completely remove this issue but we minimised the occurrences, fixed some and in the remainder - lived with it.
The first step was minimisation. We achieved this via the phasing of turning on our connectors. The first connector we applied was the `Source` connector. For our setup we had a number of source connectors - some set to replicate from `latest` and others from `earliest`. We let this connector run and replicate until we hit a steady state and all replication was confirmed to be at the head of their relevant topic. This soak could be a few days depending on your data volumes, throughputs (client limits) etc.....
Once the soak has completed we then turned on the Checkpoint connector.

If there are negative offsets after this first step we then took steps to manage them. There are 2 categories here. Partitions that have data on them and partitions that have no data on them.
In the first instance (data on partitions) the first thing we try is to `delete` the affected consumer group. This is absolutely fine to do as a) no consumers on the target cluster yet, b) the group is replicated again by MM2.
In 90% of instances the negative offset was corrected.

In the second instance (no data on partitions) the first thing we examined is whether we could publish data (on source cluster) onto the topic to put data onto the partition. This was then followed by a refresh (delete) of the affected consumer group. This was possible only if the downstream consumer handled either dummy garbage messages ok or was fine with a small number of duplicate messages.

What if following the above a negative offset remained?
In the instance where there was zero data on a partition and no new data could be published to it we let the consumer migrate onto the target cluster without much worry. The Kafka consumer behaviour at this point would look at a negative offset and throw a warning that it was out of range. It would then reset it's offset on the cluster to its default setting - either consumer from `latest` or `earliest`. Since there is 0 data on that partition this is one and the same thing.

For instances (rare but did occur) where there remained a negative offset and data on the partition we still migrated and relied on the consumer behaviour to reset its offset to either `earliest` or `latest`. Depending on the consumer and it's use case we picked whichever best suited the scenario.

Hope this is helpful in some way to others that might be experiencing these issues.;;;","09/Feb/22 18:03;gsavinov;[~bdeneuter] there is IdentityReplicationPolicy which can be used to preserve topic names, maybe you don't need to implement your CustomReplicationPolicy.

https://issues.apache.org/jira/browse/KAFKA-9726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Threads in PENDING_SHUTDOWN entering a rebalance can cause an illegal state exception ,KAFKA-12462,13364737,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,wcarlson5,wcarlson5,12/Mar/21 23:32,01/Mar/22 21:09,13/Jul/23 09:17,13/Mar/21 04:10,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,streams,,,,,0,streams,,,,"A thread was removed, sending it to the PENDING_SHUTDOWN state, but went through a rebalance before completing the shutdown.
{code:java}
// [2021-03-07 04:33:39,385] DEBUG [i-07430efc31ad166b7-StreamThread-6] stream-thread [i-07430efc31ad166b7-StreamThread-6] Ignoring request to transit from PENDING_SHUTDOWN to PARTITIONS_REVOKED: only DEAD state is a valid next state (org.apache.kafka.streams.processor.internals.StreamThread)
{code}
Inside StreamsRebalanceListener#onPartitionsRevoked, we have
{code:java}
// 
if (streamThread.setState(State.PARTITIONS_REVOKED) != null && !partitions.isEmpty())
    taskManager.handleRevocation(partitions);
{code}
Since PENDING_SHUTDOWN → PARTITIONS_REVOKED is a disallowed transition, we never invoke TaskManager#handleRevocation. Currently handleRevocation is responsible for preparing any active tasks for close, including committing offsets and writing the checkpoint as well as suspending the task. We can’t close the task in handleRevocation since we still support EAGER rebalancing, which invokes handleRevocation at the beginning of a rebalance on all tasks.

The tasks that are actually revoked will be closed during TaskManager#handleAssignment . The IllegalStateException is specifically because we don’t suspend the task before attempting to close it, and the direct transition from RUNNING → CLOSED is forbidden.",,ableegoldman,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 01 20:18:22 UTC 2022,,,,,,,,,,"0|z0opcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 23:41;ableegoldman;Thanks Walker! This actually seems like a long-lurking bug that was just surfaced by the removeStreamThread() feature, not caused by it. Before we could remove threads this was only possible when shutting down the client, which we don’t test as frequently as we now do removeStreamThread(). It’s also hard to notice that a bug has caused thread(s) to die when the threads were supposed to shut down anyways. But now we might only be removing one thread, and thanks to the new exception handler we’ll shut down the whole application upon hitting this so the thread won’t just quietly die.

We should consider backporting the fix to 2.7, even though the bug isn't going to be as frequent or as bad in earlier versions. I wouldn't cut a new RC for 2.6.2 over this, but we might as well backport to get the fix in 2.7.1 whenever that comes out;;;","12/Mar/21 23:47;wcarlson5;If we were shutting down the whole client the thread would become dead either way. In 2.7 I think the only impact it would have is that the handler would get called after the close call when it shouldn’t. But otherwise it might not have an effect. I suppose there is no harm to back-porting though.

I defiantly don't think it is worth cutting a new RC for anything that does not have removeThread in it

 ;;;","13/Mar/21 00:03;ableegoldman;The only real downside in 2.7 is that we won't properly clean up the task, ie we'll skip committing the offsets and writing the checkpoint. So we'd lose any work we did since the last commit – for EOS this would be a perf hit since we'd probably need to restore the state stores from scratch after starting back up, whereas for ALOS we could get some overcounting. Not the end of the world, but worth fixing if we can;;;","01/Mar/22 20:18;wcarlson5;This also affects 2.6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetValidationTest.test_broker_rolling_bounce failing for Raft quorums,KAFKA-12455,13363940,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,11/Mar/21 21:11,16/Mar/21 21:47,13/Jul/23 09:17,16/Mar/21 21:47,2.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,OffsetValidationTest.test_broker_rolling_bounce in `consumer_test.py` is failing because the consumer group is rebalancing unexpectedly.,,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 20:30:19 UTC 2021,,,,,,,,,,"0|z0okjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/21 18:51;rndgstn;With a 2-broker cluster that undergoes a series of 5 rolling restarts (which is what is happening here), in the Raft case, the consumers sometimes receive a `MetadataResponse` that has only a single broker since the other broker is restarting.  This never happens in the Zookeeper case -- every received `MetadataResponse` in that case always lists both brokers.  I'm not sure why this would be the case in the ZooKeeper configuration, but that is the fundamental difference between the two cases in this test scenario: in the Raft configuration the consumer sometimes sees `METADATA` responses with just a single broker, and in the ZooKeeper scenario this never happens.  The problem with the consumer seeing only a single broker in the `METADATA` response for the Raft configuration is that when that broker that it knows about goes down the consumer suddenly has no available brokers that it knows about, and we see messages in the consumer log saying `Give up sending metadata request since no node is available`.  It then takes a while before the only broker that the consumer knows about restarts, and by that time the consumer group has already moved to the `GroupCoordinator` on the other broker (the one that the consumer didn't know about), and that coordinator fails the consumer due to a lack of a heartbeat -- thus a rebalance happens, and this test is specifically checking to make sure no rebalances occur during the rolling restarts.;;;","15/Mar/21 20:23;rndgstn;I looked at the brokers' metadata caches for the two separate configurations -- ZK vs. Raft -- to find out what percentage of the time they showed showed 1 alive broker instead of 2.  I was expecting the ZooKeeper configuration to show relatively little time with just 1 alive broker since the clients are never seeing that metadata situation, and I was expecting the Raft configuration to show a much higher percentage of time with just 1 alive broker since the clients do see that metadata situation.  I did not find what I was expecting to find.

The amounts of times where the brokers are advertising just 1 alive broker in their metadata cache as follows:

*ZooKeeper Configuration*:
    BrokerId=1: 37 seconds out of 61 seconds of that broker's availability during the test, or 61% of the time with just 1 alive broker in metadata cache
    BrokerId=2: 39 seconds out of 61 seconds of that broker's availability during the test, or 64% of the time with just 1 alive broker in metadata cache

*Raft Configuration*:
    BrokerId=1: 37 seconds out of 88 seconds of that broker's availability during the test, or 42% of the time with just 1 alive broker in metadata cache
    BrokerId=2: 52 seconds out of 88 seconds of that broker's availability during the test, or 59% of the time with just 1 alive broker in metadata cache

So the brokers in the Zookeeper configuration consider just 1 broker to be alive more often than the brokers in the Raft configuration consider just 1 broker to be alive!

It is still not clear why the consumers never see just a single alive broker in the ZooKeeper configuration.  From the above it does not appear to be due to any difference in metadata cache population -- if it were just that then we would see the test failing in the ZooKeeper configuration since that actually advertises a single alive broker more frequently in terms of percentage of test time.

;;;","15/Mar/21 20:30;rndgstn;The test is using the default value of metadata.max.age.ms=300000 (5 minutes).  When I explicitly turn it down to metadata.max.age.ms=5000 (5 seconds) the test passes for Raft but then fails for ZK (2 unexpected group rebalances in that case).

I increased it to 10 seconds and then the Raft configuration failed with 3 unexpected rebalances and the ZK configuration failed with 1 unexpected rebalance.

I decreased it to a very aggressive 1 second -- and they both passed.

We have historically seen some flakiness in the ZooKeeper version of this test, and the fact that the test suddenly failed if we set metadata.max.age.ms to 5 or 10 seconds indicates that the it is just plain luck that the test is passing today.

Given that the current client-side code doesn't fall back to the bootstrap brokers when it sees no brokers available, I think any test really needs to make it *impossible* for the client to see cluster metadata with just a single broker. Decreasing the metadata max age decreases the possibility of it happening but doesn't make it impossible.

Another experiment was to keep metadata.max.age.ms=300000 but define session.timeout.ms = 30000 instead of the 10000 it was setting before -- this is longer tyan the broker roll time, and in fact this change allows both configurations to pass.

A further experiment was to keep metadata.max.age.ms=300000 and session.timeout.ms = 10000 but expand to 3 brokers instead of just 2.  This should fix the issue since there would never be a situation where just 1 broker is available, and a METADATA response would always have at least 2 brokers for the consumer to use.  Both configurations pass.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several streams-test-utils classes missing from javadoc,KAFKA-12435,13362721,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,ijuma,ijuma,05/Mar/21 22:23,30/Mar/21 15:52,13/Jul/23 09:17,30/Mar/21 15:52,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,docs,streams-test-utils,,,,0,,,,,"!image-2021-03-05-14-22-45-891.png!

Only 3 of them show up currently ^. Source: https://kafka.apache.org/27/javadoc/index.html",,ableegoldman,ijuma,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 22:22;ijuma;image-2021-03-05-14-22-45-891.png;https://issues.apache.org/jira/secure/attachment/13021736/image-2021-03-05-14-22-45-891.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 02:21:23 UTC 2021,,,,,,,,,,"0|z0od1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/21 22:24;ijuma;If you run `aggregatedJavadoc` with Java 8, you will see some warnings that are likely related. With Java 11, the same warnings are transformed into errors:
{quote}/home/ijuma/src/kafka/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:44: error: cannot find symbol
import org.apache.kafka.streams.internals.KeyValueStoreFacade;
 ^
 symbol: class KeyValueStoreFacade
 location: package org.apache.kafka.streams.internals
/home/ijuma/src/kafka/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:45: error: cannot find symbol
import org.apache.kafka.streams.internals.WindowStoreFacade;
 ^
 symbol: class WindowStoreFacade
 location: package org.apache.kafka.streams.internals
/home/ijuma/src/kafka/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:69: error: cannot find symbol
import org.apache.kafka.streams.processor.internals.TestDriverProducer;
 ^
 symbol: class TestDriverProducer
 location: package org.apache.kafka.streams.processor.internals
/home/ijuma/src/kafka/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:226: error: cannot find symbol
 private final TestDriverProducer testDriverProducer;
 ^
 symbol: class TestDriverProducer
 location: class TopologyTestDriver
4 errors
{quote};;;","05/Mar/21 22:25;ijuma;cc [~mjsax] [~vvcephei];;;","24/Mar/21 02:21;vvcephei;Thanks for the report, [~ijuma] . The errors are unintuitive, but it seems to be because we're publishing javadocs for public APIs in the test-utils module that import internal classes from the main Streams module, whose javadocs we exclude.

The include/exclude precedence rules are not documented, so it's taking me a little while to work though the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker does not close muted idle connections with buffered data,KAFKA-12427,13362434,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,david.mao,david.mao,david.mao,04/Mar/21 23:26,16/Mar/21 19:19,13/Jul/23 09:17,16/Mar/21 19:19,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,network,,,,0,,,,,,,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-04 23:26:32.0,,,,,,,,,,"0|z0ob9s:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing logic to create partition.metadata files in RaftReplicaManager,KAFKA-12426,13362432,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,hachikuji,hachikuji,04/Mar/21 23:24,06/Apr/21 23:32,13/Jul/23 09:17,01/Apr/21 22:42,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,kip-500,,,,"As part of KIP-516, the broker should create a partition.metadata file for each partition in order to keep track of the topicId. This is done through `Partition.checkOrSetTopicId`. We have the logic for this in `ReplicaManager.becomeLeaderOrFollower` already, but we need to implement analogous logic in `RaftReplicaManager.handleMetadataRecords`.",,hachikuji,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 13 05:06:11 UTC 2021,,,,,,,,,,"0|z0ob9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/21 21:25;jolshan;While working on this bug, I caught another small bug in ReplicaManager due to changing the position of the topic ID check (to prevent handling a request for an inconsistent ID).

I realized that on the first LISR received on a newly created topic, the log will not yet be created when `checkOrSetTopicId` is called. This means the log will not store the topic ID until the second LISR request. This bug means that we lose the benefits of topic IDs on this first LISR pass, but we don't actively break things more than LISR pre-topic IDs.

A similar event will occur when restarting brokers. Logs will not yet be associated with the Partition object, so we won't be able to check the topic ID on the first LISR pass. This means the partition.metadata file is unable to serve its intended purpose in this case, so I think this case is a little worse.

Both cases are not worse than the pre-topic ID behavior we had before.



There is a pretty simple fix to this which I think I can include in this PR 
 [~ijuma] [~vvcephei] do you think this is a blocker for 2.8?;;;","13/Mar/21 05:06;hachikuji;[~jolshan] Thanks, good find. Since it is not a regression, it's tough to call it a blocker. I'd suggest we target 2.8.1 for now, but if the patch is not too crazy, we might still be able to get it in depending on RC progress.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group Coordinator followers are failing with OffsetsOutOfOrderException,KAFKA-12412,13362303,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,Skr379,Skr379,04/Mar/21 13:33,05/Mar/21 04:47,13/Jul/23 09:17,05/Mar/21 04:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Upon failure of group coordinator, the followers of newly elected group coordinator are failing with  OffsetsOutOfOrderException

 

Kafka Broker Version: 2.6.0

Zookeeper version: 3.0.7

consumer API: 1.6.0

producer: libdirkafka: 0.9.1

PFA: follower logs",,Skr379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 13:33;Skr379;replica_logs;https://issues.apache.org/jira/secure/attachment/13021574/replica_logs",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-03-04 13:33:14.0,,,,,,,,,,"0|z0oago:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document multi-tenancy considerations,KAFKA-12393,13361633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,miguno,miguno,miguno,01/Mar/21 15:00,04/Mar/21 15:57,13/Jul/23 09:17,04/Mar/21 15:57,,,,,,,,,,,,,,,,,,,,,,,2.8.0,3.0.0,,,,,,,documentation,,,,,0,,,,,"We should provide an overview of multi-tenancy consideration (e.g., user spaces, security) as the current documentation lacks such information.",,bbejeck,githubbot,miguno,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 15:57:26 UTC 2021,,,,,,,,,,"0|z0o6c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 15:14;githubbot;miguno opened a new pull request #334:
URL: https://github.com/apache/kafka-site/pull/334


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","01/Mar/21 15:14;miguno;PR up at https://github.com/apache/kafka-site/pull/334;;;","01/Mar/21 16:35;githubbot;miguno commented on pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#issuecomment-788089637


   cc to committer @rajinisivaram as the SME on this subject, and who has the most context


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 07:49;githubbot;miguno commented on pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#issuecomment-788696584


   @bbejeck wrote in https://github.com/apache/kafka-site/pull/334#pullrequestreview-600990037:
   > Also, @miguno, can you create an identical PR to go against docs in AK trunk?
   
   Yes, I will do this once the content review of this PR is completed.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 11:53;githubbot;dajac commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585427746



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.

Review comment:
       I am not sure to get your point here. While I do agree that disabling the auto topic creation is a good thing, users/apps can still create topics with the admin client so it does not really help to enforce a topic naming structure. In both cases, the topics would have to respect the ACLs in place and the ""namespace"" if defined.
   
   

##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>
+  </ol>
+
+  <p>
+  When securing a multi-tenant Kafka environment, the most common administrative task is the third category (authorization), i.e., managing the user/client permissions that grant or deny access to certain topics and thus to the data stored by users within a cluster. This task is performed predominantly through the <a href=""#security_authz"">setting of access control lists (ACLs)</a>. Here, administrators of multi-tenant environments in particular benefit from putting a hierarchical topic naming structure in place as described in a previous section, because they can conveniently control access to topics through prefixed ACLs (<code>--resource-pattern-type Prefixed</code>). This significantly minimizes the administrative overhead of securing topics in multi-tenant environments: administrators can make their own trade-offs between higher developer convenience (more lenient permissions, using fewer and broader ACLs) vs. tighter security (more stringent permissions, using more and narrower ACLs).
+  </p>
+
+  <p>
+    In the following example, user Alice—a new member of ACME corporation's InfoSec team—is granted write permissions to all topics whose names start with ""acme.infosec."", such as ""acme.infosec.telemetry.logins"" and ""acme.infosec.syslogs.events"".
+  </p>
+
+<pre class=""line-numbers""><code class=""language-text""># Grant permissions to user Alice
+$ bin/kafka-acls.sh \
+    --bootstrap-server broker1:9092 \
+    --add --allow-principal User:Alice \
+    --producer \
+    --resource-pattern-type prefixed --topic acme.infosec.
+</code></pre>
+
+  <p>
+    You can similarly use this approach to isolate different customers on the same shared cluster.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-isolation"" class=""anchor-link""></a><a href=""#multitenancy-isolation"">Isolating Tenants: Quotas, Rate Limiting, Throttling</a></h4>
+
+  <p>
+  Multi-tenant clusters should generally be configured with <a href=""#design_quotas"">quotas</a>, which protect against users (tenants) eating up too many cluster resources, such as when they attempt to write or read very high volumes of data, or create requests to brokers at an excessively high rate. This may cause network saturation, monopolize broker resources, and impact other clients—all of which you want to avoid in a shared environment.
+  </p>
+
+  <p>
+    <strong>Client quotas:</strong> Kafka supports different types of (per-user principal) client quotas. Because a client's quotas apply irrespective of which topics the client is writing to or reading from, they are a convenient and effective tool to allocate resources in a multi-tenant cluster. <a href=""#design_quotascpu"">Request rate quotas</a>, for example, help to limit a user's impact on broker CPU usage by limiting the time a broker spends on the <a href=""/protocol.html"">request handling path</a> for that user, after which throttling kicks in. In many situations, isolating users with request rate quotas has a bigger impact in multi-tenant clusters than setting incoming/outgoing network bandwidth quotas, because excessive broker CPU usage for processing requests reduces the effective bandwidth the broker can serve. Furthermore, administrators can also define <a href=""#brokerconfigs_controller.quota.window.num"">quotas on topic operations</a> such as create, delete, and alter to prevent Kafka clusters from being overwhelmed by highly concurrent topic operations (see <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations"">KIP-599</a>).

Review comment:
       `#brokerconfigs_controller.quota.window.num"">` is not the correct config to highlight. `controller_mutations_rate` is more appropriate. We don't have an anchor for it though. I suggest to remove it for now.
   
   I need to add more documentation about the controller quota. We can add a link to it here when it is done.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 12:41;githubbot;rajinisivaram commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585530580



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>

Review comment:
       Should we also talk about `policies` like CreateTopicPolicy/AlterConfigPolicy that also support additional restrictions?

##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.

Review comment:
       Yes, the text sounds like you need to disable auto topic creation to enforce topic ACLs which is not the case, we should rewrite that part.

##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>
+  </ol>
+
+  <p>
+  When securing a multi-tenant Kafka environment, the most common administrative task is the third category (authorization), i.e., managing the user/client permissions that grant or deny access to certain topics and thus to the data stored by users within a cluster. This task is performed predominantly through the <a href=""#security_authz"">setting of access control lists (ACLs)</a>. Here, administrators of multi-tenant environments in particular benefit from putting a hierarchical topic naming structure in place as described in a previous section, because they can conveniently control access to topics through prefixed ACLs (<code>--resource-pattern-type Prefixed</code>). This significantly minimizes the administrative overhead of securing topics in multi-tenant environments: administrators can make their own trade-offs between higher developer convenience (more lenient permissions, using fewer and broader ACLs) vs. tighter security (more stringent permissions, using more and narrower ACLs).
+  </p>
+
+  <p>
+    In the following example, user Alice—a new member of ACME corporation's InfoSec team—is granted write permissions to all topics whose names start with ""acme.infosec."", such as ""acme.infosec.telemetry.logins"" and ""acme.infosec.syslogs.events"".
+  </p>
+
+<pre class=""line-numbers""><code class=""language-text""># Grant permissions to user Alice
+$ bin/kafka-acls.sh \
+    --bootstrap-server broker1:9092 \
+    --add --allow-principal User:Alice \
+    --producer \
+    --resource-pattern-type prefixed --topic acme.infosec.
+</code></pre>
+
+  <p>
+    You can similarly use this approach to isolate different customers on the same shared cluster.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-isolation"" class=""anchor-link""></a><a href=""#multitenancy-isolation"">Isolating Tenants: Quotas, Rate Limiting, Throttling</a></h4>
+
+  <p>
+  Multi-tenant clusters should generally be configured with <a href=""#design_quotas"">quotas</a>, which protect against users (tenants) eating up too many cluster resources, such as when they attempt to write or read very high volumes of data, or create requests to brokers at an excessively high rate. This may cause network saturation, monopolize broker resources, and impact other clients—all of which you want to avoid in a shared environment.
+  </p>
+
+  <p>
+    <strong>Client quotas:</strong> Kafka supports different types of (per-user principal) client quotas. Because a client's quotas apply irrespective of which topics the client is writing to or reading from, they are a convenient and effective tool to allocate resources in a multi-tenant cluster. <a href=""#design_quotascpu"">Request rate quotas</a>, for example, help to limit a user's impact on broker CPU usage by limiting the time a broker spends on the <a href=""/protocol.html"">request handling path</a> for that user, after which throttling kicks in. In many situations, isolating users with request rate quotas has a bigger impact in multi-tenant clusters than setting incoming/outgoing network bandwidth quotas, because excessive broker CPU usage for processing requests reduces the effective bandwidth the broker can serve. Furthermore, administrators can also define <a href=""#brokerconfigs_controller.quota.window.num"">quotas on topic operations</a> such as create, delete, and alter to prevent Kafka clusters from being overwhelmed by highly concurrent topic operations (see <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations"">KIP-599</a>).
+  </p>
+
+  <p>
+    <strong>Server quotas:</strong> In addition to client-side quotas, Kafka supports different types of broker-side quotas. For example, administrators can set a limit on the rate with which the <a href=""#brokerconfigs_max.connection.creation.rate"">broker accepts new connections</a>, set the <a href=""#brokerconfigs_max.connections"">maximum number of connections per broker</a>, or set the maximum number of connections allowed <a href=""#brokerconfigs_max.connections.per.ip"">from a specific IP address</a>.

Review comment:
       Don't think we would refer to request rate or bandwidth quotas as `client-side`. They apply to clients, but are protecting broker resources and are enforced by brokers.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 13:40;githubbot;miguno commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585572446



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>
+  </ol>
+
+  <p>
+  When securing a multi-tenant Kafka environment, the most common administrative task is the third category (authorization), i.e., managing the user/client permissions that grant or deny access to certain topics and thus to the data stored by users within a cluster. This task is performed predominantly through the <a href=""#security_authz"">setting of access control lists (ACLs)</a>. Here, administrators of multi-tenant environments in particular benefit from putting a hierarchical topic naming structure in place as described in a previous section, because they can conveniently control access to topics through prefixed ACLs (<code>--resource-pattern-type Prefixed</code>). This significantly minimizes the administrative overhead of securing topics in multi-tenant environments: administrators can make their own trade-offs between higher developer convenience (more lenient permissions, using fewer and broader ACLs) vs. tighter security (more stringent permissions, using more and narrower ACLs).
+  </p>
+
+  <p>
+    In the following example, user Alice—a new member of ACME corporation's InfoSec team—is granted write permissions to all topics whose names start with ""acme.infosec."", such as ""acme.infosec.telemetry.logins"" and ""acme.infosec.syslogs.events"".
+  </p>
+
+<pre class=""line-numbers""><code class=""language-text""># Grant permissions to user Alice
+$ bin/kafka-acls.sh \
+    --bootstrap-server broker1:9092 \
+    --add --allow-principal User:Alice \
+    --producer \
+    --resource-pattern-type prefixed --topic acme.infosec.
+</code></pre>
+
+  <p>
+    You can similarly use this approach to isolate different customers on the same shared cluster.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-isolation"" class=""anchor-link""></a><a href=""#multitenancy-isolation"">Isolating Tenants: Quotas, Rate Limiting, Throttling</a></h4>
+
+  <p>
+  Multi-tenant clusters should generally be configured with <a href=""#design_quotas"">quotas</a>, which protect against users (tenants) eating up too many cluster resources, such as when they attempt to write or read very high volumes of data, or create requests to brokers at an excessively high rate. This may cause network saturation, monopolize broker resources, and impact other clients—all of which you want to avoid in a shared environment.
+  </p>
+
+  <p>
+    <strong>Client quotas:</strong> Kafka supports different types of (per-user principal) client quotas. Because a client's quotas apply irrespective of which topics the client is writing to or reading from, they are a convenient and effective tool to allocate resources in a multi-tenant cluster. <a href=""#design_quotascpu"">Request rate quotas</a>, for example, help to limit a user's impact on broker CPU usage by limiting the time a broker spends on the <a href=""/protocol.html"">request handling path</a> for that user, after which throttling kicks in. In many situations, isolating users with request rate quotas has a bigger impact in multi-tenant clusters than setting incoming/outgoing network bandwidth quotas, because excessive broker CPU usage for processing requests reduces the effective bandwidth the broker can serve. Furthermore, administrators can also define <a href=""#brokerconfigs_controller.quota.window.num"">quotas on topic operations</a> such as create, delete, and alter to prevent Kafka clusters from being overwhelmed by highly concurrent topic operations (see <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations"">KIP-599</a>).
+  </p>
+
+  <p>
+    <strong>Server quotas:</strong> In addition to client-side quotas, Kafka supports different types of broker-side quotas. For example, administrators can set a limit on the rate with which the <a href=""#brokerconfigs_max.connection.creation.rate"">broker accepts new connections</a>, set the <a href=""#brokerconfigs_max.connections"">maximum number of connections per broker</a>, or set the maximum number of connections allowed <a href=""#brokerconfigs_max.connections.per.ip"">from a specific IP address</a>.

Review comment:
       Ack




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 13:45;githubbot;miguno commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585575690



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>

Review comment:
       Yeah, I can add a note. But it's unfortunate that there's essentially zero coverage in the AK docs on how to use these.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 15:03;githubbot;miguno commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585640800



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.

Review comment:
       Ack and updated.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 15:53;githubbot;miguno commented on a change in pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#discussion_r585572446



##########
File path: 27/ops.html
##########
@@ -1090,7 +1090,157 @@ <h4 class=""anchor-heading""><a id=""georeplication-monitoring"" class=""anchor-link""
   </p>
 
 
-  <h3 class=""anchor-heading""><a id=""config"" class=""anchor-link""></a><a href=""#config"">6.4 Kafka Configuration</a></h3>
+  <h3 class=""anchor-heading""><a id=""multitenancy"" class=""anchor-link""></a><a href=""#multitenancy"">6.4 Multi-Tenancy</a></h3>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-overview"" class=""anchor-link""></a><a href=""#multitenancy-overview"">Multi-Tenancy Overview</a></h4>
+
+  <p>
+    As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by ""noisy neighbors"".
+  </p>
+
+  <p>
+    Multi-tenancy is a many-sided subject, including but not limited to:
+  </p>
+
+  <ul>
+    <li>Creating user spaces for tenants (sometimes called namespaces)</li>
+    <li>Configuring topics with data retention policies and more</li>
+    <li>Securing topics and clusters with encryption, authentication, and authorization</li>
+    <li>Isolating tenants with quotas and rate limits</li>
+    <li>Monitoring and metering</li>
+    <li>Inter-cluster data sharing (cf. geo-replication)</li>
+  </ul>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-naming"" class=""anchor-link""></a><a href=""#multitenancy-topic-naming"">Creating User Spaces (Namespaces) For Tenants With Topic Naming</a></h4>
+
+  <p>
+    Kafka administrators operating a multi-tenant cluster typically need to define user spaces for each tenant. For the purpose of this section, ""user spaces"" are a collection of topics, which are grouped together under the management of a single entity or user.
+  </p>
+
+  <p>
+    In Kafka, the main unit of data is the topic. Users can create and name each topic. They can also delete them, but it is not possible to rename a topic directly. Instead, to rename a topic, the user must create a new topic, move the messages from the original topic to the new, and then delete the original. With this in mind, it is recommended to define logical spaces, based on an hierarchical topic naming structure. This setup can then be combined with security features, such as prefixed ACLs, to isolate different spaces and tenants, while also minimizing the administrative overhead for securing the data in the cluster.
+  </p>
+
+  <p>
+    These logical user spaces can be grouped in different ways, and the concrete choice depends on how your organization prefers to use your Kafka clusters. The most common groupings are as follows.
+  </p>
+
+  <p>
+    <em>By team or organizational unit:</em> Here, the team is the main aggregator. In an organization where teams are the main user of the Kafka infrastructure, this might be the best grouping.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />(e.g., ""acme.infosec.telemetry.logins"")</li>
+  </ul>
+
+  <p>
+    <em>By project or product:</em> Here, a team manages more than one project. Their credentials will be different for each project, so all the controls and settings will always be project related.
+  </p>
+
+  <p>
+    Example topic naming structure:
+  </p>
+
+  <ul>
+    <li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />(e.g., ""mobility.payments.suspicious"")</li>
+  </ul>
+
+  <p>
+    Certain information should normally not be put in a topic name, such as information that is likely to change over time (e.g., the name of the intended consumer) or that is a technical detail or metadata that is available elsewhere (e.g., the topic's partition count and other configuration settings).
+  </p>
+
+  <p>
+  To enforce a topic naming structure, it is useful to disable the Kafka feature to auto-create topics on demand by setting <code>auto.create.topics.enable=false</code> in the broker configuration. This stops users and applications from deliberately or inadvertently creating topics with arbitrary names, thus violating the naming structure. Then, you may want to put in place your own organizational process for controlled, yet automated creation of topics according to your naming convention, using scripting or your favorite automation toolkit.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-topic-configs"" class=""anchor-link""></a><a href=""#multitenancy-topic-configs"">Configuring Topics: Data Retention And More</a></h4>
+
+  <p>
+    Kafka's configuration is very flexible due to its fine granularity, and it supports a plethora of <a href=""#topicconfigs"">per-topic configuration settings</a> to help administrators set up multi-tenant clusters. For example, administrators often need to define data retention policies to control how much and/or for how long data will be stored in a topic, with settings such as <a href=""#retention.bytes"">retention.bytes</a> (size) and <a href=""#retention.ms"">retention.ms</a> (time). This limits storage consumption within the cluster, and helps complying with legal requirements such as GDPR.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-security"" class=""anchor-link""></a><a href=""#multitenancy-security"">Securing Clusters and Topics: Authentication, Authorization, Encryption</a></h4>
+
+  <p>
+  Because the documentation has a dedicated chapter on <a href=""#security"">security</a> that applies to any Kafka deployment, this section focuses on additional considerations for multi-tenant environments.
+  </p>
+
+  <p>
+Security settings for Kafka fall into three main categories, which are similar to how administrators would secure other client-server data systems, like relational databases and traditional messaging systems.
+  </p>
+
+  <ol>
+    <li><strong>Encryption</strong> of data transferred between Kafka brokers and Kafka clients, between brokers, between brokers and ZooKeeper nodes, and between brokers and other, optional tools.</li>
+    <li><strong>Authentication</strong> of connections from Kafka clients and applications to Kafka brokers, as well as connections from Kafka brokers to ZooKeeper nodes.</li>
+    <li><strong>Authorization</strong> of client operations such as creating, deleting, and altering the configuration of topics; writing events to or reading events from a topic; creating and deleting ACLs.</li>
+  </ol>
+
+  <p>
+  When securing a multi-tenant Kafka environment, the most common administrative task is the third category (authorization), i.e., managing the user/client permissions that grant or deny access to certain topics and thus to the data stored by users within a cluster. This task is performed predominantly through the <a href=""#security_authz"">setting of access control lists (ACLs)</a>. Here, administrators of multi-tenant environments in particular benefit from putting a hierarchical topic naming structure in place as described in a previous section, because they can conveniently control access to topics through prefixed ACLs (<code>--resource-pattern-type Prefixed</code>). This significantly minimizes the administrative overhead of securing topics in multi-tenant environments: administrators can make their own trade-offs between higher developer convenience (more lenient permissions, using fewer and broader ACLs) vs. tighter security (more stringent permissions, using more and narrower ACLs).
+  </p>
+
+  <p>
+    In the following example, user Alice—a new member of ACME corporation's InfoSec team—is granted write permissions to all topics whose names start with ""acme.infosec."", such as ""acme.infosec.telemetry.logins"" and ""acme.infosec.syslogs.events"".
+  </p>
+
+<pre class=""line-numbers""><code class=""language-text""># Grant permissions to user Alice
+$ bin/kafka-acls.sh \
+    --bootstrap-server broker1:9092 \
+    --add --allow-principal User:Alice \
+    --producer \
+    --resource-pattern-type prefixed --topic acme.infosec.
+</code></pre>
+
+  <p>
+    You can similarly use this approach to isolate different customers on the same shared cluster.
+  </p>
+
+  <h4 class=""anchor-heading""><a id=""multitenancy-isolation"" class=""anchor-link""></a><a href=""#multitenancy-isolation"">Isolating Tenants: Quotas, Rate Limiting, Throttling</a></h4>
+
+  <p>
+  Multi-tenant clusters should generally be configured with <a href=""#design_quotas"">quotas</a>, which protect against users (tenants) eating up too many cluster resources, such as when they attempt to write or read very high volumes of data, or create requests to brokers at an excessively high rate. This may cause network saturation, monopolize broker resources, and impact other clients—all of which you want to avoid in a shared environment.
+  </p>
+
+  <p>
+    <strong>Client quotas:</strong> Kafka supports different types of (per-user principal) client quotas. Because a client's quotas apply irrespective of which topics the client is writing to or reading from, they are a convenient and effective tool to allocate resources in a multi-tenant cluster. <a href=""#design_quotascpu"">Request rate quotas</a>, for example, help to limit a user's impact on broker CPU usage by limiting the time a broker spends on the <a href=""/protocol.html"">request handling path</a> for that user, after which throttling kicks in. In many situations, isolating users with request rate quotas has a bigger impact in multi-tenant clusters than setting incoming/outgoing network bandwidth quotas, because excessive broker CPU usage for processing requests reduces the effective bandwidth the broker can serve. Furthermore, administrators can also define <a href=""#brokerconfigs_controller.quota.window.num"">quotas on topic operations</a> such as create, delete, and alter to prevent Kafka clusters from being overwhelmed by highly concurrent topic operations (see <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations"">KIP-599</a>).
+  </p>
+
+  <p>
+    <strong>Server quotas:</strong> In addition to client-side quotas, Kafka supports different types of broker-side quotas. For example, administrators can set a limit on the rate with which the <a href=""#brokerconfigs_max.connection.creation.rate"">broker accepts new connections</a>, set the <a href=""#brokerconfigs_max.connections"">maximum number of connections per broker</a>, or set the maximum number of connections allowed <a href=""#brokerconfigs_max.connections.per.ip"">from a specific IP address</a>.

Review comment:
       Ack and updated




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","02/Mar/21 15:54;githubbot;miguno commented on pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#issuecomment-789009875


   PR updated with reviewer feedback.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Mar/21 08:06;githubbot;miguno commented on pull request #334:
URL: https://github.com/apache/kafka-site/pull/334#issuecomment-790412618


   Thanks all for reviewing, much appreciated!


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Mar/21 14:40;githubbot;bbejeck merged pull request #334:
URL: https://github.com/apache/kafka-site/pull/334


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Mar/21 15:57;bbejeck;Merged to trunk and cherry-picked to 2.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade of netty-codec due to CVE-2021-21290,KAFKA-12389,13361556,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,dominique,dominique,01/Mar/21 09:34,30/Mar/21 14:35,13/Jul/23 09:17,03/Mar/21 04:28,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,security,,,,,0,,,,,"Our security tool raised the following security flaw on kafka 2.7: [https://nvd.nist.gov/vuln/detail/CVE-2021-21290]

It is a vulnerability related to jar *netty-codec-4.1.51.Final.jar*.

Looking at source code, the netty-codec in trunk and 2.7.0 branches are still vulnerable.

Based on netty issue tracker, the vulnerability is fixed in 4.1.59.Final: https://github.com/netty/netty/security/advisories/GHSA-5mcr-gq6c-3hq2",,dominique,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 04:28:10 UTC 2021,,,,,,,,,,"0|z0o5uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/21 04:28;omkreddy;Issue resolved by pull request 10235
[https://github.com/apache/kafka/pull/10235];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get RaftClusterTest.java and other KIP-500 junit tests working,KAFKA-12383,13361296,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,cmccabe,cmccabe,27/Feb/21 01:31,07/Apr/21 18:57,13/Jul/23 09:17,06/Apr/21 21:02,2.9,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,cmccabe,ijuma,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 18:57:14 UTC 2021,,,,,,,,,,"0|z0o494:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/21 20:31;mjsax;Observed a timeout issue
{quote} {{java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at kafka.server.RaftClusterTest.testCreateClusterAndCreateListDeleteTopic(RaftClusterTest.scala:92)}}{quote};;;","06/Apr/21 21:03;ijuma;The PR for this Jira was merged, so I closed it. We should probably have a different one for the flakiness.;;;","07/Apr/21 18:38;mjsax;Created https://issues.apache.org/jira/browse/KAFKA-12629 as follow up.;;;","07/Apr/21 18:57;ijuma;Thanks [~mjsax]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create KIP-500 README for the 2.8 release,KAFKA-12382,13361295,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cmccabe,cmccabe,cmccabe,27/Feb/21 01:29,18/Mar/21 17:33,13/Jul/23 09:17,18/Mar/21 17:33,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-27 01:29:05.0,,,,,,,,,,"0|z0o48w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible change in verifiable_producer.log in 2.8,KAFKA-12381,13361291,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bchen225242,cmccabe,cmccabe,27/Feb/21 00:50,06/Mar/21 06:46,13/Jul/23 09:17,06/Mar/21 06:46,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,kip-500,,,,"In test_verifiable_producer.py , we used to see this error message in verifiable_producer.log when a topic couldn't be created:

WARN [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {test_topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
The test does a grep LEADER_NOT_AVAILABLE on the log in this case, and it used to pass.

Now we are instead seeing this in the log file:

WARN [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {test_topic=INVALID_REPLICATION_FACTOR} (org.apache.kafka.clients.NetworkClient)
And of course now the test fails.

The INVALID_REPLICATION_FACTOR is coming from the new auto topic creation manager.

It is a simple matter to make the test pass -- I have confirmed that it passes if we grep for INVALID_REPLICATION_FACTOR in the log file instead of LEADER_NOT_AVAILABLE.

I think we just need to decide if this change in behavior is acceptable or not.",,bchen225242,cmccabe,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 02 01:49:55 UTC 2021,,,,,,,,,,"0|z0o480:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/21 00:51;cmccabe;cc [~rndgstn], [~hachikuji], [~ijuma];;;","01/Mar/21 20:29;bchen225242;[~cmccabe] Could you provide more specific details here, such like which test case is failing and where you were fixing the log grep?;;;","02/Mar/21 00:56;hachikuji;[~bchen225242] I would not focus too much on the system test failure. The main thing to verify is whether there is a behavior change here in the `Metadata` response. If we are returning `INVALID_REPLICATION_FACTOR` in some case where we were previously returning `LEADER_NOT_AVAILABLE`, then it is likely a regression. ;;;","02/Mar/21 01:49;bchen225242;I checked 2.7 code, and we only return INVALID_REPLICATION_FACTOR for internal topics:
{code:java}
if (isInternal(topic)) {
  val topicMetadata = createInternalTopic(topic)
  if (topicMetadata.errorCode == Errors.COORDINATOR_NOT_AVAILABLE.code)
    metadataResponseTopic(Errors.INVALID_REPLICATION_FACTOR, topic, true, util.Collections.emptyList())
  else
    topicMetadata
} else if (allowAutoTopicCreation && config.autoCreateTopicsEnable) {
  createTopic(topic, config.numPartitions, config.defaultReplicationFactor)
} else {
  metadataResponseTopic(Errors.UNKNOWN_TOPIC_OR_PARTITION, topic, false, util.Collections.emptyList())
}
{code}
which seems to be lost in the new auto topic creation module. I will try to do a fix there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor in Connect's Worker is not shut down when the worker is,KAFKA-12380,13361284,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,rhauch,rhauch,26/Feb/21 22:41,29/Apr/22 14:36,13/Jul/23 09:17,29/Apr/22 05:37,,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,KafkaConnect,,,,,2,newbie,,,,"The `Worker` class has an [`executor` field|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L100] that the public constructor initializes with a new cached thread pool ([https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127].]).

When the worker is stopped, it does not shutdown this executor. This is normally okay in the Connect runtime and MirrorMaker 2 runtimes, because the worker is stopped only when the JVM is stopped (via the shutdown hook in the herders).

However, we instantiate and stop the herder many times in our integration tests, and this means we're not necessarily shutting down the herder's executor. Normally this won't hurt, as long as all of the runnables that the executor threads run actually do terminate. But it's possible those threads *might* not terminate in all tests. TBH, I don't know that such cases actually exist.

 ",,MilosBog,mr.heisenberg,rajani,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 29 04:52:14 UTC 2022,,,,,,,,,,"0|z0o46g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/21 18:44;MilosBog;HI to All,

I would like to work on this issue, but I cannot assign it to myself. Any chance that somebody contacts the admins as I was sending few emails regarding it but no answer came back;;;","28/Mar/22 13:42;rajani;I raised below pull request to fix this issue

 

[https://github.com/apache/kafka/pull/11955] ;;;","29/Mar/22 04:52;rajani;PR build says test failures

I could not find the exact failed testcases. I can see assertion errors in StreamsUncaughtExceptionHandlerIntegrationTest. If I run it locally, it's taking forever.

Can someone help me find the failed tests?
The change I added is very isolated. Should not cause any failures ideally.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use scheduleAtomicAppend for records that need to be atomic,KAFKA-12376,13361054,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jagsancio,jsancio,jsancio,25/Feb/21 21:37,18/Mar/21 17:34,13/Jul/23 09:17,18/Mar/21 17:34,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,kip-500,,,,,,dengziming,jsancio,rounak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 17:57:43 UTC 2021,,,,,,,,,,"0|z0o2rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/21 17:15;rounak;{quote}records that need to be atomic
{quote}
 

Can you help understand which type of records are these? That is, how do we identify these records and use `scheduleAtomicAppend` for them?;;;","26/Feb/21 17:57;jsancio;Yes [~rounak] . I am working on it at the moment. I'll update the Jira soon with more information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplaceStreamThread creates a new consumer with the same name as the one it's replacing,KAFKA-12375,13361033,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swistak,swistak,swistak,25/Feb/21 19:14,03/Mar/21 00:33,13/Jul/23 09:17,03/Mar/21 00:33,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"I was debugging the kafka-streams soak cluster and noticed that replacing a stream thread was causing the streams application to fail. I have managed to find the following stacktrace:

{code:java}
javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=i-0cdac8830ee1b8f01-StreamThread-1-restore-consumer at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324) at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522) at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:815) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:666) at org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier.getRestoreConsumer(DefaultKafkaClientSupplier.java:56) at org.apache.kafka.streams.processor.internals.StreamThread.create(StreamThread.java:338) at org.apache.kafka.streams.KafkaStreams.createAndAddStreamThread(KafkaStreams.java:896) at org.apache.kafka.streams.KafkaStreams.addStreamThread(KafkaStreams.java:977) at org.apache.kafka.streams.KafkaStreams.replaceStreamThread(KafkaStreams.java:467) at org.apache.kafka.streams.KafkaStreams.handleStreamsUncaughtException(KafkaStreams.java:487) at org.apache.kafka.streams.KafkaStreams.lambda$setUncaughtExceptionHandler$1(KafkaStreams.java:427) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:607) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:555)
{code}


 

followed by:


{code:java}
Exception in thread ""i-0e4d869ffd67ec825-StreamThread-1"" java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2446)	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2430)	at org.apache.kafka.clients.consumer.KafkaConsumer.enforceRebalance(KafkaConsumer.java:2261)	at org.apache.kafka.streams.processor.internals.StreamThread.sendShutdownRequest(StreamThread.java:666)	at org.apache.kafka.streams.KafkaStreams.lambda$handleStreamsUncaughtException$4(KafkaStreams.java:508)	at org.apache.kafka.streams.KafkaStreams.processStreamThread(KafkaStreams.java:1579)	at org.apache.kafka.streams.KafkaStreams.handleStreamsUncaughtException(KafkaStreams.java:508)	at org.apache.kafka.streams.KafkaStreams.lambda$setUncaughtExceptionHandler$1(KafkaStreams.java:427)	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:607)	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:555)
{code}


My understanding so far is that we re-use the consumer name across thread generations which can hit a few flavours of a race condition. My suggestion would be to add the generation-id to the consumer name.

This could be done by adding a thread generation id here
https://github.com/apache/kafka/blob/b35ca4349dabb199411cb6bc4c80ef89f19d9328/streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java#L66


or by adding an overload here: https://github.com/apache/kafka/blob/b35ca4349dabb199411cb6bc4c80ef89f19d9328/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L390
{code:java}
// Some comments here
final Map<String, Object> consumerConfigs = config.getMainConsumerConfigs(applicationId, getConsumerClientId(threadId), threadIdx, generationId);
{code}

I have not yet checked if there are any implications to either of these solutions
",,ableegoldman,cadonna,mjsax,swistak,vvcephei,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 17:08:30 UTC 2021,,,,,,,,,,"0|z0o2mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/21 19:47;ableegoldman;Possible blocker for 2.8? cc [~vvcephei];;;","25/Feb/21 20:46;vvcephei;Thanks, all. The 2.8 branch is not yet stabilized, and it would be a bummer to release this new feature with this bug present, and it doesn't seem too risky to resolve, so I've gone ahead and marked it as a blocker.

A quick note on the proposed ideas: I think those are public APIs, so it might not be ideal to change them if it can be avoided. Just looking at that those stacktraces, I'm not confident that changing the name is the proper solution here:
 * The first one is an error that comes from trying to register the same metric twice. We could fix that by making sure that we properly close the old client and unregister its metrics before creating the new one.
 * The second error looks like we are actually invoking a method on the consumer from outside of the StreamThread that owns it, which is actually not ok. The name is not relevant here, but the actual object instance. We should re-evaluate the implementation to make sure that we won't attempt multi-threaded access to the Consumer.

Thanks for your diligence in tracking this down, [~swistak] !;;;","25/Feb/21 21:04;cadonna;I do not think that the {{java.util.ConcurrentModificationException}} has anything to do with the re-use of the consumer name. I think the issue is that when we shutdown the application, the stream thread that requests the shutdown calls {{mainConsumer.enforceRebalance()}} on all stream threads on the same Streams client (https://github.com/apache/kafka/blob/b35ca4349dabb199411cb6bc4c80ef89f19d9328/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L666). If the {{mainConsumer}} is in the middle of something when this happens, the {{java.util.ConcurrentModificationException}} is thrown.;;;","26/Feb/21 01:20;ableegoldman;Ah, good catch Bruno. We definitely should not be invoking `enforceRebalance`, or any Consumer method, outside of the owning StreamThread. We can just set the error code and then inside the StreamThread's main poll loop, check the assignmentErrorCode and invoke `enforceRebalance` there. Note that this is exactly as efficient, since regardless of when you call `enforceRebalance` the StreamThread wouldn't have actually rejoined the group until it got around to invoking poll() anyways;;;","26/Feb/21 01:25;ableegoldman;Regarding the javax.management.InstanceAlreadyExistsException, this is something I've seen in user logs and tests before. It's definitely a bit confusing/potentially concerning to a user, but I don't think it's a new problem. Maybe the REPLACE_THREAD functionality will have made it slightly more common. Would be nice to fix, but I wouldn't press for that as a blocker of 2.8;;;","26/Feb/21 01:31;ableegoldman;Of course, this still leaves open the question of why we were hitting SHUTDOWN_REQUESTED in the first place. That seems to indicate there was another unexpected exception that we hit first, and the ConcurrentModificationException is only a part of the overall problem. However I don't think the InstanceAlreadyExistsException should have been thrown all the way up to the exception handler. What was the original exception [~swistak]?;;;","26/Feb/21 02:45;ableegoldman;Just to keep everyone up-to-date, the root cause of the application shutting down was due to hitting a RocksDBException: No locks available

This basically means we tried to open rocksdb twice. This can itself mean either that two threads both claimed to own the task directory at the same time, or that we didn't close the rocksdb before trying to re-open it again. In this case, I believe it's actually both: we hit a recoverable exception and fell back on the REPLACE_THREAD functionality. This gave the new thread the same thread.is as the one it's replacing, and therefore the same thread name. This resulted in the RocksDBException because (1) the old thread did not shut down before the new thread started up, and (2) our task directory locking mechanism tracks ownership via the thread name. The new thread _should_ have had to wait for the old thread to clean up and close rocksdb, but Streams can't tell the difference between two threads with the same name and just assumed that the new thread already owned the lock.

There are a few approaches we could take here: ideally our locking mechanism would have been able to distinguish these two threads. But the thread name is really the only practical way to track ownership, and more importantly, the assumption that a thread has a unique id/name is baked into Kafka Streams. I'm not confident that just fixing the locking layer would solve all of our problems here. I think the two realistic solutions here are

1) Don't start up the new thread until the old thread has finished shutting down

2) Give the new thread a different thread.id than the one it's replacing

Imo option 2 makes the most sense, as it doesn't delay the startup of the replacement thread and we can still reuse the old thread id once it does finish shutting down. Also just anecdotally, giving the new thread a fresh id will make it easier to understand and get oriented in the logs across a REPLACE_THREAD event;;;","26/Feb/21 16:15;vvcephei;Thanks [~ableegoldman] .

I couldn't believe that the locking is really just based on the name of the thread, so I had to see for myself:
{code:java}
org.apache.kafka.streams.processor.internals.StateDirectory#lock:

// we already have the lock so bail out here
final LockAndOwner lockAndOwner = locks.get(taskId);
if (lockAndOwner != null && lockAndOwner.owningThread.equals(Thread.currentThread().getName())) {
    log.trace(""{} Found cached state dir lock for task {}"", logPrefix(), taskId);
    return true;
} else if (lockAndOwner != null) {
    // another thread owns the lock
    return false;
} {code}
This is bad, and we can fix it by simply using the Thread reference instead. Is there a reason to prefer working around this bug by being super careful with thread names instead?;;;","26/Feb/21 16:44;ableegoldman;My primary motivation for fixing the thread names was that there are other things (eg the client ids) which are based on the thread.id, and are presumed to be unique. I'm not even sure we could recognize all possible symptoms of temporarily overlapping client.ids.

That said, I would be happy to fix the locking mechanism _in addition_ to ensuring a unique thread.id. I was planning a larger scale cleanup of the locking for sometime after 2.8, but we can take the first steps of tightening it up now. The only reason I didn't propose this initially was because I feel we still need to put in a fix to ensure a unique thread.id, which would happen to be sufficient to solve this particular problem as well. But I don't see any strong reason why we shouldn't do both;;;","26/Feb/21 17:08;cadonna;I agree with [~ableegoldman] that ensuring a unique thread ID solves also other issues like the metrics issue above. We cannot wait for the metrics to be removed from the metrics because that happens during the shutdown that means after the stream thread has been replaced. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing config sasl.mechanism.controller.protocol,KAFKA-12374,13360832,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rndgstn,rndgstn,rndgstn,24/Feb/21 22:27,01/Mar/21 19:51,13/Jul/23 09:17,01/Mar/21 19:51,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,config,,,,,0,,,,,"The config `sasl.mechanism.controller.protocol` from KIP-631 is not implemented.  Furthermore, `KafkaRaftManager` is using inter-broker security information when it connects to the Raft controller quorum.  KafkaRaftClient should use the first entry in `controller.listener.names` to determine the listener name; that listener name's mapped value in the `listener.security.protocol.map` (if such a mapping exists, otherwise the listener name itself) for the security protocol; and the value of `sasl.mechanism.controller.protocol` for the SASL mechanism.  Finally, `RaftControllerNodeProvider` needs to use the value of `sasl.mechanism.controller.protocol` instead of the inter-broker sasl mechanism (it currently determines the listener name and security protocol correctly)",,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-24 22:27:26.0,,,,,,,,,,"0|z0o1e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RaftReplicaManager needs to propagate isr version to Partition,KAFKA-12367,13360441,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,23/Feb/21 21:12,22/Mar/21 17:19,13/Jul/23 09:17,22/Mar/21 17:19,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,kip-500,,,,"The quorum controller verifies the isr version from the `AlterIsr` request. If it does not match, then `INVALID_UPDATE_VERSION` is returned. At the moment, this prevents the leader from being able to shrink or expand the ISR since the version does not get propagated in `RaftReplicaChangeDelegate.makeLeaders`.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-23 21:12:08.0,,,,,,,,,,"0|z0nzh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in stream-table joins on trunk,KAFKA-12366,13360400,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vcrfxia,vcrfxia,23/Feb/21 18:34,23/Mar/21 02:51,13/Jul/23 09:17,23/Mar/21 02:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,Stream-table join benchmarks have revealed a significant performance regression on trunk as compared to the latest release version. We should investigate as a blocker prior to the 3.0 release.,,ableegoldman,mjsax,vcrfxia,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 02:50:59 UTC 2021,,,,,,,,,,"0|z0nzd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/21 02:50;vvcephei;Thanks for the report [~vcrfxia] , I have reviewed the benchmark results offline, and I believe this was caused by the task idling improvements from KIP-695.

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-695%3A+Further+Improve+Kafka+Streams+Timestamp+Synchronization]

That feature was actually pulled from 2.8, so I adjusted the report to target 3.0, just to avoid confusion.

Also, the benchmark in question shows performance for the last few release branches to be about 68k +/- 7k and for trunk to be around 62K +/- 5K, which is certainly a drop, but still in the ballpark.

In contrast, the change is a significant improvement to the semantics of that join. The reason it's slightly slower now is that it is joining the stream and table records in the correct order, which fixes a pretty bad past behavior of producing many missed join results (i.e., dropping stream records because there's no table record to join with, but only because we didn't ingest the table record yet). We were able to compute those results faster, but only because we were missing a lot of the output one would expect.

Since the perf is still pretty close, and because it seems like the slight drop is well worth the improved results, I'll go ahead and close this. Thanks for raising it for review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not inline methods from the scala package by default,KAFKA-12357,13360122,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ijuma,ijuma,ijuma,22/Feb/21 14:28,23/Feb/21 05:32,13/Jul/23 09:17,23/Feb/21 05:32,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"As mentioned in [https://github.com/apache/kafka/pull/9548,] users currently use the kafka jar (`core` module) for integration testing and the current inlining behavior causes problems when the user's classpath contains a different Scala version than the one that was used for compilation.

An example error:
{quote}java.lang.NoClassDefFoundError: scala/math/Ordering$$anon$7
{quote}
We should not inline methods from the scala package by default, but we should make that easy for people who want to build an optimized kafka jar where they can ensure the scala library jar matches the one used for compilation (a reasonably common case).",,dengziming,ijuma,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-22 14:28:12.0,,,,,,,,,,"0|z0nxnk:",9223372036854775807,,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
document  about refresh.topics.interval.seconds default value is not right ,KAFKA-12350,13359757,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,superheizai,superheizai,20/Feb/21 08:50,26/Feb/21 07:06,13/Jul/23 09:17,24/Feb/21 17:53,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,website,,,,,0,,,,,"The config, refresh.topics.interval.seconds, described in document  give the defalut value 6000, ten minutes. 600 seconds or 100 minutes?",,githubbot,showuon,superheizai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 07:06:26 UTC 2021,,,,,,,,,,"0|z0nvew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/21 09:01;showuon;The default values are set to 10 * 60 seconds, which is 10 minutes. I'll update the document. Thanks for reporting [~superheizai];;;","20/Feb/21 09:27;githubbot;showuon opened a new pull request #332:
URL: https://github.com/apache/kafka-site/pull/332


   Correct the wrong default value. ref: https://github.com/apache/kafka/pull/10165


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/21 02:39;githubbot;showuon commented on pull request #332:
URL: https://github.com/apache/kafka-site/pull/332#issuecomment-786370398


   @chia7712 , could you also review this PR? Same as this one: apache/kafka#10165, Just in kafka-site repo. Thanks.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/21 07:06;githubbot;chia7712 merged pull request #332:
URL: https://github.com/apache/kafka-site/pull/332


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/21 07:06;githubbot;chia7712 commented on pull request #332:
URL: https://github.com/apache/kafka-site/pull/332#issuecomment-786457377


   @showuon Thanks for this patch :)


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recent change to use SharedTopicAdmin in KakfkaBasedLog fails with AK 0.10.x brokers,KAFKA-12343,13359440,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,rhauch,rhauch,18/Feb/21 20:15,20/Feb/21 00:35,13/Jul/23 09:17,20/Feb/21 00:35,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,KafkaConnect,,,,,0,,,,,"System test failure ([sample|http://confluent-kafka-2-7-system-test-results.s3-us-west-2.amazonaws.com/2021-02-18--001.1613655226--confluentinc--2.7--54952635e5/report.html]):
{code:java}
Java.lang.Exception: UnsupportedVersionException: MetadataRequest versions older than 4 don't support the allowAutoTopicCreation field
        at org.apache.kafka.clients.admin.KafkaAdminClient$Call.fail(KafkaAdminClient.java:755)
        at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.handleResponses(KafkaAdminClient.java:1136)
        at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1301)
        at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1224)
        at java.lang.Thread.run(Thread.java:748)
[2021-02-16 12:05:11,735] ERROR [Worker clientId=connect-1, groupId=connect-cluster] Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.Di
stributedHerder)
org.apache.kafka.connect.errors.ConnectException: API to get the get the end offsets for topic 'connect-offsets' is unsupported on brokers at worker25:9092
        at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:680)
        at org.apache.kafka.connect.util.KafkaBasedLog.readToLogEnd(KafkaBasedLog.java:338)
        at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:195)
        at org.apache.kafka.connect.storage.KafkaOffsetBackingStore.start(KafkaOffsetBackingStore.java:136)
        at org.apache.kafka.connect.runtime.Worker.start(Worker.java:197)
        at org.apache.kafka.connect.runtime.AbstractHerder.startServices(AbstractHerder.java:128)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:311)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnsupportedVersionException: MetadataRequest versions older than 4 don't support the allowAutoTopicCre
ation field
        at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
        at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
        at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
        at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:668)
        ... 11 more       {code}",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-18 20:15:31.0,,,,,,,,,,"0|z0ntgg:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate javassist library found in broker classpath,KAFKA-12341,13359428,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ijuma,cosming,cosming,18/Feb/21 19:10,23/Feb/21 18:51,13/Jul/23 09:17,23/Feb/21 18:51,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,config,release,,,,1,,,,,"Installed new kafka 2.7.0 with scala 2.13, configured and start broker. Checked the process information and found:
{code:java}
/opt/jdk11/bin/java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/home/kafka/kafka/logs2/kafkaServer-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/kafka/kafka/logs2 -Dlog4j.configuration=file:/home/kafka/kafka/bin/../config/log4j.properties -cp /home/kafka/kafka/bin/../libs/activation-1.1.1.jar:[...]:/home/kafka/kafka/bin/../libs/javassist-3.25.0-GA.jar:/home/kafka/kafka/bin/../libs/javassist-3.26.0-GA.jar:[...] kafka.Kafka /home/kafka/kafka/config/server.properties{code}
There are two versions of javassist library - 3.25.0-GA and 3.26.0-GA.

Should be only one and I believe it should be upgraded to latest version which is 3.27.0-GA.",Linux Ubuntu Server 18.04,cosming,ijuma,kazuki-ma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 13:35:18 UTC 2021,,,,,,,,,,"0|z0ntds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/21 11:28;kazuki-ma;Looks like only tools depends old javassist

 
{noformat}
+--- org.glassfish.jersey.inject:jersey-hk2:2.31
|    +--- org.glassfish.jersey.core:jersey-common:2.31 (*)
|    +--- org.glassfish.hk2:hk2-locator:2.6.1
|    |    +--- org.glassfish.hk2.external:jakarta.inject:2.6.1
|    |    +--- org.glassfish.hk2.external:aopalliance-repackaged:2.6.1
|    |    +--- org.glassfish.hk2:hk2-api:2.6.1
|    |    |    +--- org.glassfish.hk2.external:jakarta.inject:2.6.1
|    |    |    +--- org.glassfish.hk2:hk2-utils:2.6.1
|    |    |    |    \--- org.glassfish.hk2.external:jakarta.inject:2.6.1
|    |    |    \--- org.glassfish.hk2.external:aopalliance-repackaged:2.6.1
|    |    \--- org.glassfish.hk2:hk2-utils:2.6.1 (*)
|    \--- org.javassist:javassist:3.25.0-GA {noformat}
 

All dependencies
{noformat}
% grep javassist **/dependencies.txt
connect/mirror/build/reports/project/dependencies.txt:|    |    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:|    |    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
connect/mirror/build/reports/project/dependencies.txt:|    |    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
connect/mirror/build/reports/project/dependencies.txt:|    |    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.26.0-GA
connect/mirror/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
connect/runtime/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
connect/runtime/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    |    |    +--- org.javassist:javassist:3.27.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
connect/runtime/build/reports/project/dependencies.txt:|    |    \--- org.javassist:javassist:3.25.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:|    |    |    +--- org.javassist:javassist:3.27.0-GA -> 3.26.0-GA
connect/runtime/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.26.0-GA} -> 3.26.0-GA (c)
streams/build/reports/project/dependencies.txt:|    |    |    +--- org.javassist:javassist:3.27.0-GA
streams/build/reports/project/dependencies.txt:|    |    |    +--- org.javassist:javassist:3.27.0-GA
streams/build/reports/project/dependencies.txt:\--- org.javassist:javassist:{strictly 3.27.0-GA} -> 3.27.0-GA (c)
tools/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.25.0-GA
tools/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.25.0-GA
tools/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.25.0-GA} -> 3.25.0-GA (c)
tools/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.25.0-GA
tools/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.25.0-GA} -> 3.25.0-GA (c)
tools/build/reports/project/dependencies.txt:|    \--- org.javassist:javassist:3.25.0-GA
tools/build/reports/project/dependencies.txt:+--- org.javassist:javassist:{strictly 3.25.0-GA} -> 3.25.0-GA (c) {noformat}
 ;;;","23/Feb/21 13:35;ijuma;Thanks for the PR. Submitted a fix here: https://github.com/apache/kafka/pull/10191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recent change to use SharedTopicAdmin results in potential resource leak in deprecated backing store constructors,KAFKA-12340,13359422,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,rhauch,rhauch,18/Feb/21 18:53,22/Feb/21 16:11,13/Jul/23 09:17,22/Feb/21 16:11,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,KafkaConnect,,,,,0,,,,,"When KAFKA-10021 modified the Connect `Kafka*BackingStore` classes, we deprecated the old constructors and changed all uses within AK to use the new constructors that take a `Supplier<TopicAdmin>`.

If the old deprecated constructors are used (outside of AK), then they will not close the Admin clients that are created by the ""default"" supplier.",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10021,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 16:11:21 UTC 2021,,,,,,,,,,"0|z0ntcg:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"22/Feb/21 16:11;rhauch;Merged to `trunk`, and cherry-picked to:
 * `2.8` for inclusion in 2.8.0 (with release manager approval)
 * `2.7` for inclusion in 2.7.1
 * `2.6` for inclusion in 2.6.2 (with release manager approval)
 * `2.5` for inclusion in 2.5.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add retry to admin client's listOffsets,KAFKA-12339,13359421,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,chia7712,chia7712,chia7712,18/Feb/21 18:50,10/Mar/22 21:21,13/Jul/23 09:17,22/Feb/21 16:11,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,0,,,,,"After upgrading our connector env to 2.9.0-SNAPSHOT, sometimes the connect cluster encounters following error.
{quote}Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:324)

org.apache.kafka.connect.errors.ConnectException: Error while getting end offsets for topic 'connect-storage-topic-connect-cluster-1'

at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:689)

at org.apache.kafka.connect.util.KafkaBasedLog.readToLogEnd(KafkaBasedLog.java:338)

at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:195)

at org.apache.kafka.connect.storage.KafkaStatusBackingStore.start(KafkaStatusBackingStore.java:216)

at org.apache.kafka.connect.runtime.AbstractHerder.startServices(AbstractHerder.java:129)

at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:310)

at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)

at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)

at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.

at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)

at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)

at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)

at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)

at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:668)

... 10 more
{quote}
[https://github.com/apache/kafka/pull/9780] added shared admin to get end offsets. KafkaAdmin#listOffsets does not handle topic-level error, hence the UnknownTopicOrPartitionException on topic-level can obstruct worker from running when the new internal topic is NOT synced to all brokers.",,chia7712,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10021,,KAFKA-12879,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 16:11:24 UTC 2021,,,,,,,,,,"0|z0ntc8:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"22/Feb/21 16:11;rhauch;Merged to `trunk`, and cherry-picked to:
 * `2.8` for inclusion in 2.8.0 (with release manager approval)
 * `2.7` for inclusion in 2.7.1
 * `2.6` for inclusion in 2.6.2 (with release manager approval)
 * `2.5` for inclusion in 2.5.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"custom stream naming does not work while calling stream[K, V](topicPattern: Pattern) API with named Consumed parameter ",KAFKA-12336,13359324,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,Geordie,ramazanyich,ramazanyich,18/Feb/21 13:37,24/Jun/21 16:56,13/Jul/23 09:17,24/Jun/21 16:09,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.0,,,,,,,streams,,,,,0,easy-fix,newbie,,,"In our Scala application I am trying to implement custom naming for Kafka Streams application nodes.

We are using topicPattern for our stream source.

Here is an API which I am calling:

 
{code:java}
val topicsPattern=""t-[A-Za-z0-9-].suffix""
val operations: KStream[MyKey, MyValue] =
  builder.stream[MyKey, MyValue](Pattern.compile(topicsPattern))(
    Consumed.`with`[MyKey, MyValue].withName(""my-fancy-name"")
  )
{code}
 Despite the fact that I am providing Consumed with custom name the topology describe still show ""KSTREAM-SOURCE-0000000000"" as name for our stream source.

It is not a problem if I just use a name for topic. But our application needs to get messages from set of topics based on topicname pattern matching.

After checking the kakfa code I see that

org.apache.kafka.streams.kstream.internals.InternalStreamBuilder (on line 103) has a bug:
{code:java}
public <K, V> KStream<K, V> stream(final Pattern topicPattern,
                                   final ConsumedInternal<K, V> consumed) {
    final String name = newProcessorName(KStreamImpl.SOURCE_NAME);
    final StreamSourceNode<K, V> streamPatternSourceNode = new StreamSourceNode<>(name, topicPattern, consumed);
{code}
node name construction does not take into account the name of consumed parameter.

For example code for another stream api call with topic name does it correctly:
{code:java}
final String name = new NamedInternal(consumed.name()).orElseGenerateWithPrefix(this, KStreamImpl.SOURCE_NAME);
{code}
 ",,ableegoldman,bbejeck,mjsax,ramazanyich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 16:56:01 UTC 2021,,,,,,,,,,"0|z0nsqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/21 16:10;bbejeck;merged into trunk;;;","24/Jun/21 16:56;bbejeck;cherry-picked to 2.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error partitions from topics with invalid IDs in LISR requests,KAFKA-12332,13359119,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jolshan,jolshan,jolshan,17/Feb/21 18:18,19/Feb/21 19:08,13/Jul/23 09:17,19/Feb/21 19:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"In a situation where topics are deleted and recreated in a short amount of time, LeaderAndIsr requests can contain topics with invalid IDs, but correct epochs. In this case, we will incorrectly handle the request and simply log an error message on the broker. It will be more useful to not handle the request for the partition with an invalid ID and send and error back to the controller.",,hachikuji,jolshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-17 18:18:39.0,,,,,,,,,,"0|z0nrh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchSessionCache may cause starvation for partitions when FetchResponse is full,KAFKA-12330,13358953,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,lucasbradstreet,lucasbradstreet,16/Feb/21 22:13,16/Mar/21 19:24,13/Jul/23 09:17,16/Mar/21 19:24,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"The incremental FetchSessionCache sessions deprioritizes partitions where a response is returned. This may happen if log metadata such as log start offset, hwm, etc is returned, or if data for that partition is returned.

When a fetch response fills to maxBytes, data may not be returned for partitions even if the fetch offset is lower than the fetch upper bound. However, the fetch response will still contain updates to metadata such as hwm if that metadata has changed. This can lead to degenerate behavior where a partition's hwm or log start offset is updated resulting in the next fetch being unnecessarily skipped for that partition. At first this appeared to be worse, as hwm updates occur frequently, but starvation should result in hwm movement becoming blocked, allowing a fetch to go through and then becoming unstuck. However, it'll still require one more fetch request than necessary to do so. Consumers may be affected more than replica fetchers, however they often remove partitions with fetched data from the next fetch request and this may be helping prevent starvation.

I believe we should only reorder the partition fetch priority if data is actually returned for a partition.
{noformat}
private class PartitionIterator(val iter: FetchSession.RESP_MAP_ITER,
                                val updateFetchContextAndRemoveUnselected: Boolean)
  extends FetchSession.RESP_MAP_ITER {
  var nextElement: util.Map.Entry[TopicPartition, FetchResponse.PartitionData[Records]] = null

  override def hasNext: Boolean = {
    while ((nextElement == null) && iter.hasNext) {
      val element = iter.next()
      val topicPart = element.getKey
      val respData = element.getValue
      val cachedPart = session.partitionMap.find(new CachedPartition(topicPart))
      val mustRespond = cachedPart.maybeUpdateResponseData(respData, updateFetchContextAndRemoveUnselected)
      if (mustRespond) {
        nextElement = element
        // Example POC change:
        // Don't move partition to end of queue if we didn't actually fetch data
        // This should help avoid starvation even when we are filling the fetch response fully while returning metadata for these partitions
        if (updateFetchContextAndRemoveUnselected && respData.records != null && respData.records.sizeInBytes > 0) {
          session.partitionMap.remove(cachedPart)
          session.partitionMap.mustAdd(cachedPart)
        }
      } else {
        if (updateFetchContextAndRemoveUnselected) {
          iter.remove()
        }
      }
    }
    nextElement != null
  }{noformat}
 ",,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-16 22:13:06.0,,,,,,,,,,"0|z0nqg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MM2 fails to start to due missing required configuration ""bootstrap.servers""",KAFKA-12326,13358371,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,dajac,dajac,12/Feb/21 16:10,14/Feb/21 03:34,13/Jul/23 09:17,12/Feb/21 23:40,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,,,,,,0,,,,,"I just tried to mirror a topic with MM2 between two clusters with a really simple configuration. Basically, the one provided in the repo with the correct `bootstrap.servers`.

MM2 fails with the following error:
{noformat}
[2021-02-12 17:05:50,389] ERROR [Worker clientId=connect-1, groupId=B-mm2] Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:324)
org.apache.kafka.common.config.ConfigException: Missing required configuration ""bootstrap.servers"" which has no default value.
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:493)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:483)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:108)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:142)
	at org.apache.kafka.clients.admin.AdminClientConfig.<init>(AdminClientConfig.java:233)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:145)
	at org.apache.kafka.connect.util.TopicAdmin.<init>(TopicAdmin.java:277)
	at org.apache.kafka.connect.util.SharedTopicAdmin.createAdmin(SharedTopicAdmin.java:143)
	at java.util.concurrent.atomic.AtomicReference.updateAndGet(AtomicReference.java:179)
	at org.apache.kafka.connect.util.SharedTopicAdmin.topicAdmin(SharedTopicAdmin.java:82)
	at org.apache.kafka.connect.util.SharedTopicAdmin.get(SharedTopicAdmin.java:72)
	at org.apache.kafka.connect.util.SharedTopicAdmin.get(SharedTopicAdmin.java:44)
	at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:163)
	at org.apache.kafka.connect.storage.KafkaOffsetBackingStore.start(KafkaOffsetBackingStore.java:136)
	at org.apache.kafka.connect.runtime.Worker.start(Worker.java:197)
	at org.apache.kafka.connect.runtime.AbstractHerder.startServices(AbstractHerder.java:128)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:310)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
It seems that a regression was introduced by [https://github.com/apache/kafka/commit/982ea2f6a471c217c7400a725c9504d9d8348d02]. MM2 starts when the commit is reverted.",,dajac,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10021,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 12 23:40:10 UTC 2021,,,,,,,,,,"0|z0nmuw:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"12/Feb/21 16:13;dajac;cc [~rhauch] ;;;","12/Feb/21 18:06;rhauch;Yeah, the fix in KAFKA-10021 definitely caused this problem with the MirrorMaker 2 executable. I've verified that the *MirrorMaker 2 connector and Connect are unaffected*.

I've replicated the problem building a local 2.6.2 snapshot archive, installing it, and then running the Kafka quickstart to create and populate a `quickstart` topic:
{code}
$ bin/zookeeper-server-start.sh config/zookeeper.properties
$ bin/kafka-server-start.sh config/server.properties
$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event
{code}
I then changed the local `config/connect-mirror-maker.properties` configuration file the local installation to reference the local Kafka broker and to replicate the `quickstart` topic:
{code}
clusters = A, B
A.bootstrap.servers = localhost:9092
B.bootstrap.servers = localhost:9092

A->B.enabled = true
A->B.topics = quickstart-events
B->A.enabled = false
...
{code}
and then running MirrorMaker 2 executable:
{code}
$ bin/connect-mirror-maker.sh config/connect-mirror-maker.properties
{code}

Doing this with the current `2.6` branch resulted in the exception that [~dajac] mentioned.

There is a simple one-line fix for the `MirrorMaker` class that will pass the correct worker properties to the new `SharedTopicAdmin` instance.

;;;","12/Feb/21 18:26;rhauch;Created a PR to fix the issue. This is a one-line change to the `MirrorMaker` class that is the MM2 executable.;;;","12/Feb/21 23:40;rhauch;Merged to `trunk`, cherry-picked to the `2.8`, `2.7`, `2.6`, and `2.5` branches (the same as KAFKA-10021).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jetty to fix CVE-2020-27218,KAFKA-12324,13358232,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dongjin,jrstacy,jrstacy,11/Feb/21 20:32,22/Feb/21 17:51,13/Jul/23 09:17,22/Feb/21 17:51,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,,,,,,0,,,,,"h3. CVE-2020-27218 Detail

In Eclipse Jetty version 9.4.0.RC0 to 9.4.34.v20201102, 10.0.0.alpha0 to 10.0.0.beta2, and 11.0.0.alpha0 to 11.0.0.beta2, if GZIP request body inflation is enabled and requests from different clients are multiplexed onto a single connection, and if an attacker can send a request with a body that is received entirely but not consumed by the application, then a subsequent request on the same connection will see that body prepended to its body. The attacker will not see any data but may inject data into the body of the subsequent request.",,dongjin,jrstacy,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 17:51:07 UTC 2021,,,,,,,,,,"0|z0nm00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/21 10:16;dongjin;I am now working on this issue. But the Jetty's upgrade is a little bit complicated than expected for API changes.;;;","22/Feb/21 17:51;omkreddy;Issue resolved by pull request 10177
[https://github.com/apache/kafka/pull/10177];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Record timestamps not populated in event,KAFKA-12323,13358225,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,abellemare,abellemare,11/Feb/21 19:59,12/Apr/21 23:04,13/Jul/23 09:17,24/Feb/21 04:42,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,streams,,,,,0,,,,,"Upgraded a kafka streams application from 2.6.0 to 2.7.0. Noticed that the events being produced had a ""CreatedAt"" timestamp = 0, causing downstream failures as we depend on those timestamps. Reverting back to 2.6.0/2.6.1 fixed this issue. This was the only change to the Kafka Streams application.

Consuming the event stream produced by 2.6.0 results in events that, when consumed using the `kafka-avro-console-consumer` and `--property print.timestamp=true` result in events prepended with the event times, such as:
{code:java}
CreateTime:1613072202271 <key> <value>
CreateTime:1613072203412 <key> <value>
CreateTime:1613072205431 <key> <value>
{code}
etc.

However, when those events are produced by the Kafka Streams app using 2.7.0, we get:
{code:java}
CreateTime:0 <key> <value>
CreateTime:0 <key> <value>
CreateTime:0 <key> <value> 
{code}
I don't know if these is a default value somewhere that changed, but this is actually a blocker for our use-cases as we now need to circumnavigate this limitation (or roll back to 2.6.1, though there are other issues we must deal with then). I am not sure which unit tests in the code base to look at to validate this, but I wanted to log this bug now in case someone else has already seen this or an open one exists (I didn't see one though).",,abellemare,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12643,,,,,,,,,,,,,,,,,,,"12/Feb/21 17:50;abellemare;PunctuateTimestampZeroTest.java;https://issues.apache.org/jira/secure/attachment/13020411/PunctuateTimestampZeroTest.java",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 19 14:46:25 UTC 2021,,,,,,,,,,"0|z0nlyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/21 21:06;ableegoldman;Hey [~abellemare], thanks for reporting. I'm not personally aware of any changes that might have caused this but maybe it will ring a bell for someone else. But I did want to ask:

1) can you include a minimal example to reproduce?
2) can you try reading the events produced by 2.7.0 Streams using a 2.6.1 kafka-avro-console-consumer, so we can rule out a regression there?;;;","11/Feb/21 22:04;abellemare;Hi [~ableegoldman] 

I am trying to reproduce it all on Kafka 2.7.0 using the basic producer and consumer, writing from the console without any keys. At the moment I cannot reproduce it. 

My environment has different versions for the broker, the consumer, and the producer, so I'm going to have to take a closer look at that. Additionally, the producer is using Kafka Streams (2.6.1 and 2.7.0), whereas the consumer is just Confluent 5.5.0 console clients (both Avro client and non-Avro client result in the missing value). I will add more information when I can either successfully reproduce it, or give up trying.;;;","12/Feb/21 18:49;abellemare;Alright, I think I found source of the 0 timestamp assignment:

This is returning a 0 timestamp for a punctuation-created event. 
[https://github.com/apache/kafka/blob/2.7/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java#L169]

It calls through to AbstractProcessorContextImpl, which executes this and returns 0L for every punctuated event.
[https://github.com/apache/kafka/blob/2.7/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java#L176]
I have observed that this will always assign 0L for the timestamp, because *recordContext* is always null for this processor.



I noticed that I am adding the processor through a now-deprecated *addProcessor* API call. I suspect that the deprecation change is responsible for the bug:
{code:java}
topology
  //This addProcessor call is deprecated.
 .addProcessor(progressProcessorName,
               punctuationProcessor(...),
               sinkEventsName)
 .addSink(""progressSink"",
                ""topicName"",
                 serdeFactory.serde(true).serializer(),
                 serdeFactory.progressEventSerde().serializer(),
                 progressProcessorName);
{code}

I have attached the code ([^PunctuateTimestampZeroTest.java] that illustrates the error. You can run it as an integration test in Kafka (you will have to disable warnings-as-errors (Werror) from build.gradle, as it will fail because I am using a deprecated function).

I don't think this is a blocker because I suspect I can work around it with the non-deprecated function. It is, however, quite major, as a person can upgrade their code and have this happen without noticing. It's particularly insidious as it may cause downstream tasks that rely on the timestamp to silently fail (as is what happened to me). 

 

{color:#cc7832} {color};;;","15/Feb/21 22:41;mjsax;To me, setting zero as default timestamp is highly questionable... We should either throw an exception to force people to set a timestamp, or maybe use `stream-time` as default? \cc [~vvcephei] [~guozhang];;;","17/Feb/21 17:35;guozhang;I checked the source code between 2.6 and trunk, and I think there's indeed a regression: for old API, the behavior is not actually ""For timestamp, the dummy value is `0L`."" Instead, it would just keep the processor context passed in through the punctuator, and that would carry the timestamp of either stream time or system time depending on the punctuation type. What's worse, is that even if using the new API this regression seems still persist.

I think we should fix `StreamTask.punctuate` that when calling `updateProcessorContext`, we should create a recordContext with the timestamp passed into it instead of passing null.;;;","17/Feb/21 18:20;mjsax;Thanks [~guozhang]. SGTM. – Are we sure that we want to use current wall-clock time as default timestamp for output records of wall-clock time punctuations? I am ok with it, just wanted to double check. Using current stream-time as default timestamp for output records for both punctuation types seems like a sound alternative approach.

As it is a regression, I am tagging this a blocker for 2.8.0 and 2.7.1 release. [~abellemare] are you interested picking it up?;;;","19/Feb/21 14:46;abellemare;[~mjsax]Sure, I can pick it up - though I won't likely be able to start work on it until the beginning of next week (eg: Feb 22-23).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the comparison function for uuid type should be 'equals' rather than '==',KAFKA-12321,13357956,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,10/Feb/21 15:31,12/Feb/21 17:15,13/Jul/23 09:17,12/Feb/21 17:15,,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,,,,,,0,,,,,"trunk:
{code:java}
if (this.taggedUuid != Uuid.fromString(""H3KKO4NTRPaCWtEmm3vW7A""))
{code}

expected:
{code:java}
if (!this.taggedUuid.equals(Uuid.fromString(""H3KKO4NTRPaCWtEmm3vW7A"")))
{code}",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-10 15:31:12.0,,,,,,,,,,"0|z0nkaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigDef.parseType deadlock,KAFKA-12308,13357362,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,cosmozhu,cosmozhu,07/Feb/21 07:57,14/Jul/21 17:55,13/Jul/23 09:17,14/Jul/21 17:55,2.5.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,config,KafkaConnect,,,,0,,,,,"hi,
 the problem was found, when I restarted *ConnectDistributed*

I restart ConnectDistributed in the single node for the test, with not delete connectors.
 sometimes the process stopped when creating connectors.

I add some logger and found it had a deadlock in `ConfigDef.parseType`.My connectors always have the same transforms. I guess when connector startup (in startAndStopExecutor which default 8 threads) and load the same class file it has something wrong.

I attached the jstack log file.

thanks for any help.","kafka 2.5.0
centos7
java version ""1.8.0_231""",cosmozhu,kkonstantine,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7421,,,,,,,,,,,,,,,"07/Feb/21 07:57;cosmozhu;deadlock.log;https://issues.apache.org/jira/secure/attachment/13020132/deadlock.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 14 17:53:28 UTC 2021,,,,,,,,,,"0|z0ngmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/21 15:11;tombentley;I think this is caused by the fact the {{DelegatingClassLoader}} is not registered as parallel capable, but should be. It should be because, according to https://docs.oracle.com/javase/7/docs/technotes/guides/lang/cl-mt.html, to qualify for the acyclic delegation model ""If the class is not found, the class loader asks its parent to locate the class. If the parent cannot find the class, the class loader attempts to locate the class itself."", but {{DelegatingClassLoader}} may actually ask the {{PluginClassLoader}} to load a class before it's tried {{super}}.

From the stack dump provided
{noformat}
""StartAndStopExecutor-connect-1-5"":
	at java.lang.ClassLoader.loadClass(ClassLoader.java:398)                                              // wait for DCL getClassLoadingLock
	- waiting to lock <0x00000006c222db00> (a org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.loadClass(DelegatingClassLoader.java:397) // deletate to super
	at java.lang.ClassLoader.loadClass(ClassLoader.java:405)                                              // super delegates to parent (DCL)
	- locked <0x000000077b9bf3c0> (a java.lang.Object)                                                    // lock PCLY+name (super's getClassLoadingLock)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:104)
	- locked <0x000000077b9bf3c0> (a java.lang.Object)                                                    // lock PCLY+name (getClassLoadingLock)
	- locked <0x00000006c25b4e38> (a org.apache.kafka.connect.runtime.isolation.PluginClassLoader)        // lock PCLY (synchronized)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
{noformat}

and 

{noformat}
""StartAndStopExecutor-connect-1-6"":
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:91) // lock PCLX (synchronized)
	- waiting to lock <0x00000006c25b4e38> (a org.apache.kafka.connect.runtime.isolation.PluginClassLoader)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.loadClass(DelegatingClassLoader.java:394) // delegated to PCL
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)                                             // ClassLoader.loadClass(String name) calling PCL.loadClass(String,
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
{noformat}

It also says 

{noformat}
""StartAndStopExecutor-connect-1-5"":
  waiting to lock monitor 0x00000203a553b6f8 (object 0x00000006c222db00, a org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader),
  which is held by ""StartAndStopExecutor-connect-1-6""
{noformat}

the {{0x00000006c222db00}} doesn't appear in the stacktrace, I think that's because it's [held by the JVM itself|https://github.com/openjdk/jdk/blob/06170b7cbf6129274747b4406562184802d4ff07/src/hotspot/share/classfile/systemDictionary.cpp#L695]. 

If DelegatingClassloader is registered as parallel capable this won't happen

{noformat}
	at java.lang.ClassLoader.loadClass(ClassLoader.java:398)                                              // wait for DCL getClassLoadingLock
	- waiting to lock <0x00000006c222db00> (a org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
{noformat}

Because {{DCL.getClassLoadingLock}} will return an object specific to the class being loaded, rather than the DCL instance itself, which is locked by the JVM.

Does this seem plausible to you [~kkonstantine] [~ChrisEgerton]?;;;","25/Feb/21 07:57;kkonstantine;[~tombentley] I actually think that the initial suggestion in https://issues.apache.org/jira/browse/KAFKA-7421 regarding the removal of the method lock is correct. 

The `DelegatingClassLoader` doesn't seem to need to be parallel because it delegates loading to either `PluginClassLoader` instances that are parallel capable or the parent which normally is the system classloader and should also be parallel. 

Note, that the loading sequence that you mention above, is inverted on purpose to actually implement classloading isolation. First we attempt loading the class from the ""child"" `PluginClassLoader` of the designated plugin and if not found then the parent classloader of the `DelegatingClassLoader` is consulted. 

I have updated the PR that had added a test for this type of deadlock originally submitted by [~gharris1727] in: 
 [https://github.com/apache/kafka/pull/8259]

cc [~rhauch];;;","25/Feb/21 09:10;tombentley;[~kkonstantine] I'm not an expert in classloaders but I'm still not sure that DCL shouldn't be considered parallel. The referred class loader guide explicitly says that an acyclic CL should delegate to {{super}} _first_. I understand that delegating to PCL first is intentional, but it doesn't fit with the definition given AFAICS. The fact that the CLs it delegates to are both parallel doesn't seem to be relevant. Also, the parent of the PCL is the DCL, which looks like a cycle to me (but, as I said, I'm no expert, so happy to be corrected). 

Assuming the {{synchronized}} was removed from PCL {{loadClass}}, then 
{noformat}
""StartAndStopExecutor-connect-1-6"":
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:91) // lock PCLX (synchronized)
{noformat}
wouldn't get blocked, but there would still be two threads contenting two locks when racing to load the same class, those locks would be the {{getClassLoadingLock()}} on the PCL and the monitor of the DCL instance itself, so I think perhaps a deadlock would still be possible, just on different monitors. ;;;","14/Jul/21 17:53;kkonstantine;Adding the comment that I added in the PR here as well: 



The idea that the {{DelegatingClassLoader}} did not have to be parallel capable originated to the fact that it doesn't load classes directly. It delegates loading either to the appropriate PluginClassLoader directly via composition, or to the parent by calling {{super.loadClass}}.

The latter is the key point of why we need to make the {{DelegatingClassLoader}} also parallel capable even though it doesn't load a class. Because inheritance is used (via a call to {{super.loadClass}}) and not composition (via a hypothetical call to {{parent.loadClass}}, which is not possible because {{parent}} is a private member of the base abstract class {{ClassLoader}}) when {{getClassLoadingLock}} is called in {{super.loadClass}} it checks that actually the derived class (here an instance of {{DelegatingClassLoader}}) is not parallel capable and therefore ends up not applying fine-grain locking during classloading even though the parent clasloader is used actually load the classes.

Based on the above, the {{DelegatingClassLoader}} needs to be parallel capable too in order for the parent loader to load classes in parallel. 

I've tested both classloader types being parallel capable in a variety of scenarios with multiple connectors, SMTs and converters and a deadlock did not reproduce. Of course reproducing the issue is difficult without the specifics of the jar layout to begin with. The possibility of a deadlock is still not zero, but also probably not exacerbated compared to the current code. The plugin that depends on other plugins to be loaded while it's loading its classes is the connector type plugin only and there are no inter-connector dependencies (a connector requiring another connector's classes to be loaded while loading its own). With that in mind, a deadlock should be even less possible now. In the future we could consider introducing deadlock recovery methods to get out of this type of situation if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flatten SMT fails on arrays,KAFKA-12305,13357207,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,06/Feb/21 02:53,23/Jun/22 23:12,13/Jul/23 09:17,23/Jun/22 23:12,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,2.5.1,2.6.1,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,0,,,,,"The {{Flatten}} SMT fails for array types. A sophisticated approach that tries to flatten arrays might be desirable in some cases, and may have been punted during the early design phase of the transform, but in the interim, it's probably not worth it to make array data and the SMT mutually exclusive.

A naive approach that preserves arrays as-are and doesn't attempt to flatten them seems fair for now, but one alternative could be to traverse array elements and, if any are maps or structs, flatten those as well.

Adding behavior to fully flatten arrays by essentially transforming them into maps whose elements are the elements of the array and whose keys are the indices of each element is likely out of scope for a bug fix and, although useful, might have to wait for a KIP.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 14:47:18 UTC 2021,,,,,,,,,,"0|z0nfog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/21 17:34;rhauch;[~ChrisEgerton] wrote:
{quote}
A naive approach that preserves arrays as-are and doesn't attempt to flatten them seems fair for now, but one alternative could be to traverse array elements and, if any are maps or structs, flatten those as well.
{quote}
+1 for this behavior. Here's my thought process:

The `Flatten` transform documentation ([source|https://github.com/apache/kafka/blob/8bd5ceb3d297bd6cd06ccec52978315898719e6d/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java#L43-L45]) says:
{quote}
Flatten a nested data structure, generating names for each field by concatenating the field names at each level with a configurable delimiter character. Applies to a Struct when a schema is present, or a Map in the case of schemaless data.
{quote}
IMO, this explains the intention: flatten nested `Struct` instances if using a schema(e.g., flatten the Struct fields of a record's key/value Struct) or nested `Map` instances if using a schemaless key/value. 

Nowhere does it mention that arrays are flattened into a separate field for each element in the array.

For example, consider this record key or value (with a top-level schema that has 4 fields, one of which is a struct with 3 fields):
{code}
{
  ""f1"": ""field 1 value"",           // field with string schema
  ""f2"": {                          // field with struct schema containing 3 fields
    ""nestedInt"": 0,                // field with int32 schema
    ""nestedString"": ""nested"",      // field with string schema
    ""nestedArray"": [ ""v1"", ""v2"" ]  // field with array(string) schema
  },
  ""f3"": [ ""e1"", ""e2"", ""e3"" ],      // field with array(string) schema
  ""f4"": <null>      // field with optional float32 schema
}
{code}

Using what you mention as the ""naive approach"", Flatten applied to the record key/value described above should produce the following key or value (with a schema that has 5 fields):
{code}
  ""f1"": ""field 1 value"",            // field with string schema
  ""f2.nestedInt"": 0,                // field with int32 schema
  ""f2.nestedString"": ""nested"",      // field with string schema
  ""f2.nestedArray"": [ ""v1"", ""v2"" ]  // field with array(string) schema
  ""f3"": [ ""e1"", ""e2"", ""e3"" ],       // field with array(string) schema
  ""f4"": <null>                      // field with optional float32 schema
{code}

Note that the arrays are *_not_* expanded into separate fields for each element. This makes sense to me and aligns with the previously mentioned documentation for the Flatten SMT.;;;","08/Feb/21 17:49;ChrisEgerton;Thanks [~rhauch]. Do you have thoughts on whether elements inside arrays should themselves be flattened? For example, with this input:

 

 
{code:java}
{
  ""f1"": [
    {
      ""f2"": {
        ""f3"": ""v1""
      }
    }
  ]
}
{code}
should the output be this (option A):
{code:java}
{
  ""f1"": [
    {""f2.f3"": ""v1""}
  ]
}{code}
or this (option B):
{code:java}
{
  ""f1"": [
    {
      ""f2"": {
        ""f3"": ""v1""
      }
    }
  ]
}
{code}
I'm leaning toward option A since, as you noted, the SMT definition deals explicitly with structs and maps, and makes no distinction about whether they're in arrays or top-level.;;;","08/Feb/21 18:47;rhauch;{quote}
Do you have thoughts on whether elements inside arrays should themselves be flattened? 
{quote}

I think it's arguable whether Flatten should flatten structs/maps *with* arrays. My initial thought is that it should not because the documentation does not mention arrays, and because it's not clear to me how often that case will come up. 

I'm definitely open to suggestions from others, though!
;;;","10/Feb/21 14:47;ChrisEgerton;I think the fact that the documentation doesn't call out arrays could go both ways, though–maps and structs are still maps and structs, regardless of whether they're top-level or contained in an array.

However, the wording in the docs is that the SMT will ""Flatten *a* nested data structure"" (emphasis mine). The singular ""a"" there implies only one such nested data structure. Considering that the SMT only accepts top-level map/struct values, I think that gives enough weight to the position that {{Flatten}} should only concern itself with that top-level nested map/struct and, after encountering something it cannot flatten (i.e., an array or a primitive), stop descending further.

In other words, I'm also satisfied with the naive approach (option B outlined above).

 

If this causes anyone difficulty down the road, they have plenty of options, including forking the open-source {{Flatten}} SMT, writing their own SMT from scratch, and publishing a KIP to alter the behavior of {{Flatten}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flatten SMT drops some fields when null values are present,KAFKA-12303,13357204,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,06/Feb/21 02:25,18/Feb/21 15:30,13/Jul/23 09:17,18/Feb/21 15:03,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,2.5.1,2.6.1,2.7.0,2.8.0,,,,,,,,,,,,,,3.0.0,,,,,,,,KafkaConnect,,,,,0,,,,,"[This line|https://github.com/apache/kafka/blob/0bc394cc1d19f1e41dd6646e9ac0e09b91fb1398/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java#L109] should be {{continue}} instead of {{return}}; otherwise, the rest of the entries in the currently-being-iterated map are skipped unnecessarily.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-06 02:25:12.0,,,,,,,,,,"0|z0nfns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation of MockProducer contradicts documentation of Callback for async send,KAFKA-12297,13357066,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,Zordid,Zordid,05/Feb/21 12:46,13/Feb/21 17:53,13/Jul/23 09:17,13/Feb/21 17:53,2.7.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,producer ,unit tests,,,,0,,,,,"In Unit tests, a MockProducer is used to imitate a real producer.

Using the errorNext(RuntimeException e) method, it is possible to indicate failures.

BUT: the asynchronous send method with a callback has a clear documentation of that callback interface, stating that Metadata will always be set, and never null.

{{The metadata for the record that was sent (i.e. the partition and offset). An empty metadata with -1 value for all fields except for topicPartition will be returned if an error occurred.}}

 

The bug is, that in MockProducer's Completion implementation the following happens:

{{if (e == null)}}
 {{    callback.onCompletion(metadata, null);}}
 {{else}}
 {{    callback.onCompletion(null, e);}}

 

Behaving against the own documentation leads to very subtle bugs: tests that implement the error condition checking metadata != null will be fine, but in real life fail horribly.

 

A MockProducer should at all times behave exactly like the real thing and adhere to the documentation of the Callback!",,Zordid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-05 12:46:02.0,,,,,,,,,,"0|z0netc:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a new topic or removing an existing topic to/from --whitelist in mirrormaker config file stops all replication,KAFKA-12290,13356805,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,chris.cairns,chris.cairns,04/Feb/21 16:21,24/Feb/21 09:52,13/Jul/23 09:17,24/Feb/21 09:51,0.10.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mirrormaker,,,,,0,mirrormaker,,,,"Looking for some assistance, not too familiar with mirrormaker.

In our mirrormaker, adding a new topic or removing an existing topic from the --whitelist in the config service file for mirrormaker stops all replication.
 # Could I get advise on why this might be & what to check?
 # Mirrormaker is at the target cluster
 # Topics are manually created in both source and destination",,chris.cairns,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-04 16:21:22.0,,,,,,,,,,"0|z0nd7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams metric commit-latency-max and commit-latency-avg is always 0,KAFKA-12272,13356263,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,Dabz,Dabz,02/Feb/21 18:08,12/Feb/21 01:00,13/Jul/23 09:17,12/Feb/21 01:00,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,,,,,"After upgrading to Kafka Streams 2.7.0, the JMX metrics commit-latency-max and commit-latency-avg is always equal to 0.


For the same application, with Kafka Streams 2.6.0 and bellow, I can observe:
 !KS-2.6.0.png! 


With Kafka Streams 2.7.0:
 !KS-2.7.0.png! 


By quickly looking at the issue, I got the feeling it's a drawback from: https://github.com/apache/kafka/pull/9634.

We are setting _now_ to the current Time in the _maybeCommit()_ function: https://github.com/apache/kafka/blob/2.7/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L930. 

And just after we do a _Time.millisecond() - now_ (that we just updated) to compute the latency: https://github.com/apache/kafka/blob/2.7/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L692",,ableegoldman,Dabz,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10755,,,,,,,,,,,,,,"02/Feb/21 18:05;Dabz;KS-2.6.0.png;https://issues.apache.org/jira/secure/attachment/13019870/KS-2.6.0.png","02/Feb/21 18:05;Dabz;KS-2.7.0.png;https://issues.apache.org/jira/secure/attachment/13019869/KS-2.7.0.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 01:09:19 UTC 2021,,,,,,,,,,"0|z0n9uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/21 01:09;mjsax;Bumping this to ""blocker"" as it's a regression.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connect may fail a task when racing to create topic,KAFKA-12270,13356247,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rhauch,rhauch,rhauch,02/Feb/21 16:40,04/Feb/21 00:41,13/Jul/23 09:17,04/Feb/21 00:41,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,KafkaConnect,,,,,0,,,,,"When a source connector configured with many tasks and to use the new topic creation feature is run, it is possible that multiple tasks will attempt to write to the same topic, will see that the topic does not exist, and then race to create the topic. The topic is only created once, but some tasks might fail with:
{code:java}
org.apache.kafka.connect.errors.ConnectException: Task failed to create new topic (name=TOPICX, numPartitions=8, replicationFactor=3, replicasAssignments=null, configs={cleanup.policy=delete}). Ensure that the task is authorized to create topics or that the topic exists and restart the task
  at org.apache.kafka.connect.runtime.WorkerSourceTask.maybeCreateTopic(WorkerSourceTask.java:436)
  at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:364)
  at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:264)
  at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
  at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
... {code}
The reason appears to be that the WorkerSourceTask throws an exception if the topic creation failed, and does not account for the fact that the topic may have been created between the time the WorkerSourceTask lists existing topics and tries to create the topic.

 

See in particular: [https://github.com/apache/kafka/blob/5c562efb2d76407011ea88c1ca1b2355079935bc/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L415-L423]

 

This is only an issue when using topic creation settings in the source connector configuration, and when running multiple tasks that write to the same topic.

The workaround is to create the topics manually before starting the connector, or to simply restart the failed tasks using the REST API.",,kkonstantine,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 00:41:12 UTC 2021,,,,,,,,,,"0|z0n9rc:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"02/Feb/21 20:12;kkonstantine;[~rhauch] I remember considering this use case. 

But maybe the issue here has to do with the fact that the exception is not considered retriable. Not sure yet if something changed or this has been an issue since the KIP was merged. 
Thanks for reporting this;;;","02/Feb/21 23:09;rhauch;I don't think it's because the exception is retriable, because the `TopicAdmin.createTopics(...)` method explicitly catches a `TopicExistsException` and simply returns a response without the topic name, signaling that the topic was not created. I think the response is adequate to know whether my request created the topic (or topics), but it's less than ideal if the caller simply wants to know whether the topic ""was created or already existed"".

We can't easily change the response, but we could potentially add an overloaded method that takes a flag as to whether existing topics should be included in the response. I'm just not sure that's worth it. WDYT?

BTW, I've added a PR that keeps the TopicAdmin methods the same and instead just re-describes the topic. This should be an infrequent occurrence, but I'm happy to eliminate the re-describe if you think that's a better approach.;;;","03/Feb/21 02:02;rhauch;Ok, I didn't really like that the first PR added another admin client call to (re)describe the topic in question when the `createTopic(...)` method returned false, meaning the topic was not created (because it already existed by the time the create topic request was made/received by Kafka).

So, I created an alternative PR that changes how the `TopicAdmin` creates a topic and returns precisely which topic names were created AND which were found to already exist. This allows the `WorkerSourceTask` to know exactly what happend and to log it accordingly.

This new method in `TopicAdmin` is called `createOrFindTopics(...)`, and it is the old implementation of `createTopics(...)` with only slight modification; the previously-existing `createTopics(...)` and `createTopic(...)` methods were changed to delegate to the new method. Thus the behavior of the existing methods remains unchanged (for the multiple places where they are called), but we get the more precise results in `WorkerSourceTask`.;;;","04/Feb/21 00:41;rhauch;Merged to `trunk` for the upcoming 2.8.0, and cherrypicked to the 2.7 branch for the next 2.7.1 and to the 2.6 branch for the next 2.6.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System tests broken because consumer returns early without records ,KAFKA-12268,13356197,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,vvcephei,rsivaram,rsivaram,02/Feb/21 12:48,22/Mar/21 17:10,13/Jul/23 09:17,22/Mar/21 17:10,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,consumer,,,,,0,,,,,"https://issues.apache.org/jira/browse/KAFKA-10866 added metadata to ConsumerRecords. We add metadata even when there are no records. As a result, we sometimes return early from KafkaConsumer#poll() with no records because FetchedRecords.isEmpty returns false if either metadata or records are available. This breaks system tests which rely on poll timeout, expecting records to be returned on every poll when there is no timeout.",,dengziming,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-02 12:48:54.0,,,,,,,,,,"0|z0n9g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New session keys are never distributed when follower with key becomes leader,KAFKA-12262,13355950,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,01/Feb/21 13:52,18/Jun/21 17:10,13/Jul/23 09:17,18/Jun/21 17:10,,,,,,,,,,,,,,,,,,,,,,,2.6.3,2.7.2,2.8.1,3.0.0,,,,,KafkaConnect,,,,,0,,,,,"The expiration time for session keys [starts at {{Long.MAX_VALUE}}|https://github.com/apache/kafka/blob/1c00d9dfa50570b4554a123bfe91ccc5e9ad1ca9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L258] and [is updated when a new session key is read if and only if the worker is the leader|https://github.com/apache/kafka/blob/1c00d9dfa50570b4554a123bfe91ccc5e9ad1ca9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1579-L1581]. If a follower reads a session key from the config topic, the expiration time will remain {{Long.MAX_VALUE}} and, even if the worker becomes the leader, will not be updated.

Once this happens, all key rotation will cease until and unless the former leader of the cluster becomes the leader again without being restarted in the meantime, or all workers in the cluster are shut down at the same time and then the cluster is brought back up again.",,ChrisEgerton,paulrbrown,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 17:10:51 UTC 2021,,,,,,,,,,"0|z0n7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/21 16:19;rhauch;[https://github.com/apache/kafka/pull/10014] fixes this issue and KAFKA-12252.;;;","18/Jun/21 17:10;rhauch;Merged and backported for KAFKA-12252.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consolidated Status endpoint returns 500 when config provider can't find a config,KAFKA-12259,13355567,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,mageshn,mageshn,30/Jan/21 02:42,04/Feb/21 00:12,13/Jul/23 09:17,04/Feb/21 00:12,,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,,,,,,0,,,,,"The consolidated connectors endpoint connectors?expand=status return a `500` error when any of the connector's has an exception from the config provider.  [https://github.com/apache/kafka/blob/e9edf104866822d9e6c3b637ffbf338767b5bf27/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java#L287]

 The status endpoint doesn't need the complete list of configs and just requires the Connector Class to infer if it's a `Source` or a `Sink`. The failed connector status should be returned as `Failed` instead of the endpoint returning a 500.",,mageshn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-30 02:42:36.0,,,,,,,,,,"0|z0n5ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Consumer mishandles topics deleted and recreated with the same name,KAFKA-12257,13355561,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,Jack-Lee,rleslie,rleslie,30/Jan/21 00:38,17/Nov/21 17:57,13/Jul/23 09:17,17/Nov/21 17:57,2.2.2,2.3.1,2.4.1,2.5.1,2.6.1,2.7.1,2.8.1,,,,,,,,,,,,,,,,2.8.2,3.0.0,3.1.0,,,,,,consumer,,,,,0,,,,,"In KAFKA-7738, caching of leader epochs (KIP-320) was added to o.a.k.c.Metadata to ignore metadata responses with epochs smaller than the last seen epoch.

The current implementation can cause problems in cases where a consumer is subscribed to a topic that has been deleted and then recreated with the same name. This is something seen more often in consumers that subscribe to a multitude of topics using a wildcard.

Currently, when a topic is deleted and the Fetcher receives UNKNOWN_TOPIC_OR_PARTITION, the leader epoch is not cleared. If at a later time while the consumer is still running a topic is created with the same name, the leader epochs are set to 0 for the new topics partitions, and are likely smaller than those for the previous topic. For example, if a broker had restarted during the lifespan of the previous topic, the leader epoch would be at least 1 or 2. In this case the metadata will be ignored since it is incorrectly considered stale. Of course, the user will sometimes get lucky, and if a topic was only recently created so that the epoch is still 0, no problem will occur on recreation. The issue is also not seen when consumers happen to have been restarted in between deletion and recreation.

The most common side effect of the new metadata being disregarded is that the new partitions end up assigned but the Fetcher is unable to fetch data because it does not know the leaders. When recreating a topic with the same name it is likely that the partition leaders are not the same as for the previous topic, and the number of partitions may even be different. Besides not being able to retrieve data for the new topic, there is a more sinister side effect of the Fetcher triggering a metadata update after the fetch fails. The subsequent update will again ignore the topic's metadata if the leader epoch is still smaller than the cached value. This metadata refresh loop can continue indefinitely and with a sufficient number of consumers may even put a strain on a cluster since the requests are occurring in a tight loop. This can also be hard for clients to identify since there is nothing logged by default that would indicate what's happening. Both the Metadata class's logging of ""_Not replacing existing epoch_"", and the Fetcher's logging of ""_Leader for partition <T-P> is unknown_"" are at DEBUG level.

A second possible side effect was observed where if the consumer is acting as leader of the group and happens to not have any current data for the previous topic, e.g. it was cleared due to a metadata error from a broker failure, then the new topic's partitions may simply end up unassigned within the group. This is because while the subscription list contains the recreated topic the metadata for it was previously ignored due to the leader epochs. In this case the user would see logs such as:
{noformat}
WARN o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=myClientId, groupId=myGroup] The following subscribed topics are not assigned to any members: [myTopic]{noformat}
Interestingly, I believe the Producer is less affected by this problem since o.a.k.c.p.i.ProducerMetadata explicitly clears knowledge of its topics in retainTopics() after each metadata expiration. ConsumerMetadata does no such thing.

To reproduce this issue:
 # Turn on DEBUG logging, e.g. for org.apache.kafka.clients.consumer and org.apache.kafka.clients.Metadata
 # Begin a consumer for a topic (or multiple topics)
 # Restart a broker that happens to be a leader for one of the topic's partitions
 # Delete the topic
 # Create another topic with the same name
 # Publish data for the new topic
 # The consumer will not receive data for the new topic, and there will be a high rate of metadata requests.
 # The issue can be corrected by restarting the consumer or restarting brokers until leader epochs are large enough

I believe KIP-516 (unique topic ids) will likely fix this problem, since after those changes the leader epoch map should be keyed off of the topic id, rather than the name.

One possible workaround with the current version of Kafka is to add code to onPartitionsRevoked() to manually clear leader epochs before each rebalance, e.g.
{code:java}
Map<TopicPartition, Integer> emptyLeaderEpochs = new HashMap<>();
ConsumerMetadata metadata = (ConsumerMetadata)FieldUtils.readField(consumer, ""metadata"", 
true);
FieldUtils.writeField(metadata, ""lastSeenLeaderEpochs"", emptyLeaderEpochs, true);{code}
This is not really recommended of course, since besides modifying private consumer state, it defeats the purpose of epochs! It does in a sense revert the consumer to pre-2.2 behavior before leader epochs existed.",,agencer,calohmn,jolshan,rleslie,satish.duggana,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 09:01;Jack-Lee;KAFKA-12257-1.patch;https://issues.apache.org/jira/secure/attachment/13019984/KAFKA-12257-1.patch","02/Feb/21 08:44;Jack-Lee;KAFKA-12257.patch;https://issues.apache.org/jira/secure/attachment/13019821/KAFKA-12257.patch",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 09 16:40:51 UTC 2021,,,,,,,,,,"0|z0n5jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/21 16:03;jolshan;With the changes from [KIP-516|https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers] there some options to fix this bug. This ticket mentions using the topic ID as a key for the leader epoch map, but another option that might be a bit simpler to implement is checking for a new topic ID in the request.

There is already work ongoing to update the Fetch path to use topic IDs ([https://github.com/apache/kafka/pull/9944]) and this will include storing the topic ID in the consumer's metadata cache.  Upon receiving a new metadata response, we can check if the topic ID matches the ID already stored in the cache and set a flag if it has changed. Then, in `updateLatestMetadata`, we have code that checks the epoch:
{code:java}
if (currentEpoch == null || newEpoch >= currentEpoch) {
    log.debug(""Updating last seen epoch for partition {} from {} to epoch {} from new metadata"", tp, currentEpoch, newEpoch);
    lastSeenLeaderEpochs.put(tp, newEpoch);
    return Optional.of(partitionMetadata);
{code}
 

If we include an or ( || ) for the changed topic ID, I believe this will achieve the behavior this ticket is looking for. The lastSeenEpoch will be reset to the epoch of the new topic and we will use the new partitionMetadata.;;;","02/Nov/21 22:44;agencer;We were able to reproduce this issue in Kafka 2.4.;;;","09/Nov/21 16:40;jolshan;We should merge the change for 3.1. [~hachikuji] I can take another look at the PR and update it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto commit causes delays due to retriable UNKNOWN_TOPIC_OR_PARTITION,KAFKA-12256,13355557,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,rleslie,rleslie,29/Jan/21 23:58,11/Feb/22 17:59,13/Jul/23 09:17,11/Feb/22 17:59,2.0.0,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,,,,consumer,,,,,1,new-consumer-threading-should-fix,,,,"In KAFKA-6829 a change was made to the consumer to internally retry commits upon receiving UNKNOWN_TOPIC_OR_PARTITION.

Though this helped mitigate issues around stale broker metadata, there were some valid concerns around the negative effects for routine topic deletion:

https://github.com/apache/kafka/pull/4948

In particular, if a commit is issued for a deleted topic, retries can block the consumer for up to max.poll.interval.ms. This is tunable of course, but any amount of stalling in a consumer can lead to unnecessary lag.

One of the assumptions while permitting the change was that in practice it should be rare for commits to occur for deleted topics, since that would imply messages were being read or published at the time of deletion. It's fair to expect users to not delete topics that are actively published to. But this assumption is false in cases where auto commit is enabled.

With the current implementation of auto commit, the consumer will regularly issue commits for all topics being fetched from, regardless of whether or not messages were actually received. The fetch positions are simply flushed, even when they are 0. This is simple and generally efficient, though it does mean commits are often redundant. Besides the auto commit interval, commits are also issued at the time of rebalance, which is often precisely at the time topics are deleted.

This means that in practice commits for deleted topics are not really rare. This is particularly an issue when the consumer is subscribed to a multitude of topics using a wildcard. For example, a consumer might subscribe to a particular ""flavor"" of topic with the aim of auditing all such data, and these topics might dynamically come and go. The consumer's metadata and rebalance mechanisms are meant to handle this gracefully, but the end result is that such groups are often blocked in a commit for several seconds or minutes (the default is 5 minutes) whenever a delete occurs. This can sometimes result in significant lag.

Besides having users abandon auto commit in the face of topic deletes, there are probably multiple ways to deal with this, including reconsidering if commits still truly need to be retried here, or if this behavior should be more configurable; e.g. having a separate commit timeout or policy. In some cases the loss of a commit and subsequent message duplication is still preferred to processing delays. And having an artificially low max.poll.interval.ms or rebalance.timeout.ms comes with its own set of concerns.

In the very least the current behavior and pitfalls around delete with active consumers should be documented.",,calohmn,guozhang,rleslie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13310,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 11 17:58:46 UTC 2022,,,,,,,,,,"0|z0n5ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 08:54;calohmn;I would vote for raising the priority here.
The continuous and needless commit retries in the case of deleted topics make it very hard to work with scenarios in which topics get deleted.

Even when implementing your own auto-commit mechanism (skipping commits of offsets that were already committed before), you have to take extra care that topics don't get deleted before having done the last offset commits for contained partitions.

If you do a commit on a deleted partition, it's not only the delay because of the continuous retries, it's also that this delay then may exceed 'max.poll.interval.ms' causing a LeaveGroup request of the consumer.;;;","09/Feb/22 20:22;rleslie;[~RivenSun] [~guozhang] Hey guys, I see that [https://github.com/apache/kafka/pull/11340] was just recently merged for KAFKA-13310. Thank you [~RivenSun] for your work on this! It looks like this ticket should be the same issue. Please let me know if you guys agree and we can close it out.

I also wanted to see how you guys felt about potentially backporting the fix to earlier branches like 2.8, or maybe 3.0 if 2.8 is considered dead already. It looks like the 3.0.1 will happen soon at least.;;;","11/Feb/22 17:58;guozhang;Hello [~rleslie] yeah I think we can close this ticket along with KAFKA-13310.

Regarding the backporting, I think it makes sense to backport to 3.0, I will see if it has any cherry-picking conflicts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker 2.0 creates destination topic with default configs,KAFKA-12254,13355520,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dhruvilshah,dhruvilshah,dhruvilshah,29/Jan/21 20:06,02/Mar/21 22:37,13/Jul/23 09:17,01/Mar/21 09:33,2.4.0,2.4.1,2.5.0,2.5.1,2.6.0,2.6.1,2.7.0,2.8.0,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"`MirrorSourceConnector` implements the logic for replicating data, configurations, and other metadata between the source and destination clusters. This includes the tasks below:
 # `refreshTopicPartitions` for syncing topics / partitions from source to destination.
 # `syncTopicConfigs` for syncing topic configurations from source to destination.

A limitation is that `computeAndCreateTopicPartitions` creates topics with default configurations on the destination cluster. A separate async task `syncTopicConfigs` is responsible for syncing the topic configs. Before that sync happens, topic configurations could be out of sync between the two clusters.

In the worst case, this could lead to data loss eg. when we have a compacted topic being mirrored between clusters which is incorrectly created with the default configuration of `cleanup.policy = delete` on the destination before the configurations are sync'd via `syncTopicConfigs`.

Here is an example of the divergence:

Source Topic:

```

Topic: foobar PartitionCount: 1 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824

```

Destination Topic:

```

Topic: A.foobar PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824

```

A safer approach is to ensure that the right configurations are set on the destination cluster before data is replicated to it.",,akaltsikis,dhruvilshah,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-29 20:06:34.0,,,,,,,,,,"0|z0n5a8:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distributed herder tick thread loops rapidly when worker loses leadership,KAFKA-12252,13355459,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,29/Jan/21 15:25,18/Jun/21 17:09,13/Jul/23 09:17,06/May/21 21:00,,,,,,,,,,,,,,,,,,,,,,,2.6.3,2.7.2,2.8.1,3.0.0,,,,,KafkaConnect,,,,,0,,,,,"When a new session key is read from the config topic, if the worker is the leader, it [schedules a new key rotation|https://github.com/apache/kafka/blob/5cf9cfcaba67cffa2435b07ade58365449c60bd9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1579-L1581]. The time between key rotations is configurable but defaults to an hour.

The herder then continues its tick loop, which usually ends with a long poll for rebalance activity. However, when a key rotation is scheduled, it will [limit the time spent polling|https://github.com/apache/kafka/blob/5cf9cfcaba67cffa2435b07ade58365449c60bd9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L384-L388] at the end of the tick loop in order to be able to perform the rotation.

Once woken up, the worker checks to see if a key rotation is necessary and, if so, [sets the expected key rotation time to Long.MAX_VALUE|https://github.com/apache/kafka/blob/bf4afae8f53471ab6403cbbfcd2c4e427bdd4568/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L344], then [writes a new session key to the config topic|https://github.com/apache/kafka/blob/bf4afae8f53471ab6403cbbfcd2c4e427bdd4568/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L345-L348]. The problem is, [the worker only ever decides a key rotation is necessary if it is still the leader|https://github.com/apache/kafka/blob/5cf9cfcaba67cffa2435b07ade58365449c60bd9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L456-L469]. If the worker is no longer the leader at the time of the key rotation (likely due to falling out of the cluster after losing contact with the group coordinator), its key expiration time won’t be reset, and the long poll for rebalance activity at the end of the tick loop will be given a timeout of 0 ms and result in the tick loop being immediately restarted. Even if the worker reads a new session key from the config topic, it’ll continue looping like this since its scheduled key rotation won’t be updated. At this point, the only thing that would help the worker get back into a healthy state would be if it were made the leader of the cluster again.

One possible fix could be to add a conditional check in the tick thread to only limit the time spent on rebalance polling if the worker is currently the leader.",,ChrisEgerton,paulrbrown,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 17:09:46 UTC 2021,,,,,,,,,,"0|z0n4wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/21 21:00;rhauch;I'm still working on backporting this to the 2.7 and 2.6 branches. When I'm able to do that, I'll update the fix versions on this issue.;;;","18/Jun/21 16:34;rhauch;Backported to 2.7 for inclusion in any subsequent 2.7.2 patch release.;;;","18/Jun/21 17:09;rhauch;Backported to 2.6 for inclusion in any subsequent 2.6.3 patch release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZkAdminManager.describeConfigs returns no config when 2+ configuration keys are specified,KAFKA-12235,13354464,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ivanyu,ivanyu,ivanyu,25/Jan/21 15:15,26/Feb/21 20:09,13/Jul/23 09:17,26/Feb/21 20:09,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,core,,,,,0,regression,,,,"When {{ZkAdminManager.describeConfigs}} receives {{DescribeConfigsResource}} with 2 or more {{configurationKeys}} specified, it returns an empty configuration.

Here's a test for {{ZkAdminManagerTest}} that reproduces this issue:
  
{code:scala}
@Test
def testDescribeConfigsWithConfigurationKeys(): Unit = {
  EasyMock.expect(zkClient.getEntityConfigs(ConfigType.Topic, topic)).andReturn(TestUtils.createBrokerConfig(brokerId, ""zk""))
  EasyMock.expect(metadataCache.contains(topic)).andReturn(true)

  EasyMock.replay(zkClient, metadataCache)

  val resources = List(new DescribeConfigsRequestData.DescribeConfigsResource()
    .setResourceName(topic)
    .setResourceType(ConfigResource.Type.TOPIC.id)
    .setConfigurationKeys(List(""retention.ms"", ""retention.bytes"", ""segment.bytes"").asJava)
  )

  val adminManager = createAdminManager()
  val results: List[DescribeConfigsResponseData.DescribeConfigsResult] = adminManager.describeConfigs(resources, true, true)
  assertEquals(Errors.NONE.code, results.head.errorCode())
  val resultConfigKeys = results.head.configs().asScala.map(r => r.name()).toSet
  assertEquals(Set(""retention.ms"", ""retention.bytes"", ""segment.bytes""), resultConfigKeys)
}

{code}
Works fine with one configuration key, though.

The patch is following shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 15:32:10 UTC 2021,,,,,,,,,,"0|z0myrs:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,"28/Jan/21 15:32;ivanyu;https://github.com/apache/kafka/pull/9990;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the length passed to FileChannel by `FileRecords.writeTo`,KAFKA-12233,13354266,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,dengziming,dengziming,25/Jan/21 05:40,27/Jan/21 04:51,13/Jul/23 09:17,27/Jan/21 04:51,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"[https://github.com/apache/kafka/pull/2140/files#r563471404]

we set `int count = Math.min(length, oldSize)`, but we are expected to write from `offset`, so the count should be `Math.min(length, oldSize - offset)`.",,chia7712,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 07:57:28 UTC 2021,,,,,,,,,,"0|z0mxjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/21 07:04;chia7712;{quote}
`FileRecords.writeTo` set length incorrectly
{quote}

Pardon me, I don't observe the error. ""oldSize"" is NOT an (end) index of file. Hence, we do get the correct new offset (start + input offset) and new length (min(input length, oldSize)).;;;","25/Jan/21 07:57;dengziming;[~chia7712] Thank you, I inspected it again, and below is my understanding.
 # FileRecords.writeTo is called my `RecordSend` and the method signature is `writeTo(TransferableChannel channel, long previouslyWritten, int remaining)`
 # We assume the log file isn't truncated (oldSize=sizeInBytes), so we set length=(min(remaining, sizeInBytes)), but remaining < sizeInBytes is always true
 # So the real size should be (min(remaining, sizeInBytes - previouslyWritten)), (sizeInBytes - previouslyWritten) represents remaining size to be written. 

Also ping [~hachikuji] to have a look, he set `count = min(length, size - offset)` in KAFKA-2066 [https://github.com/apache/kafka/pull/2069] , but he overwrite `count=min(length, size)` in KAFKA-4390 [https://github.com/apache/kafka/pull/2140] , I think the previous is more reasonable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High-throughput source tasks fail to commit offsets,KAFKA-12226,13353363,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,19/Jan/21 20:39,22/Nov/21 16:03,13/Jul/23 09:17,15/Nov/21 23:40,,,,,,,,,,,,,,,,,,,,,,,3.0.1,3.1.0,3.2.0,,,,,,KafkaConnect,,,,,0,,,,,"The current source task thread has the following workflow:
 # Poll messages from the source task
 # Queue these messages to the producer and send them to Kafka asynchronously.
 # Add the message to outstandingMessages, or if a flush is currently active, outstandingMessagesBacklog
 # When the producer completes the send of a record, remove it from outstandingMessages

The commit offsets thread has the following workflow:
 # Wait a flat timeout for outstandingMessages to flush completely
 # If this times out, add all of the outstandingMessagesBacklog to the outstandingMessages and reset
 # If it succeeds, commit the source task offsets to the backing store.
 # Retry the above on a fixed schedule

If the source task is producing records quickly (faster than the producer can send), then the producer will throttle the task thread by blocking in its {{send}} method, waiting at most {{max.block.ms}} for space in the {{buffer.memory}} to be available. This means that the number of records in {{outstandingMessages}} + {{outstandingMessagesBacklog}} is proportional to the size of the producer memory buffer.

This amount of data might take more than {{offset.flush.timeout.ms}} to flush, and thus the flush will never succeed while the source task is rate-limited by the producer memory. This means that we may write multiple hours of data to Kafka and not ever commit source offsets for the connector. When the task is lost due to a worker failure, hours of data will be re-processed that otherwise were successfully written to Kafka.",,ChrisEgerton,dajac,fvaleri,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13469,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 15 23:40:13 UTC 2021,,,,,,,,,,"0|z0mryw:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"07/Nov/21 17:41;rhauch;Merged the PR to the `trunk` branch, which will be included in the upcoming 3.1.0 branch. ;;;","15/Nov/21 10:07;dajac;[~rhauch] I don't see this commit in the 3.1.0 branch. Note that the 3.1.0 branch was cut on November 2nd.;;;","15/Nov/21 17:21;rhauch;Thanks for catching this, [~dajac]. I did miss merging this to the `3.1` branch -- I recall at the time looking for the branch and not seeing it. I'm in the process or building the branch after merging locally, so you should see this a bit later today.
;;;","15/Nov/21 23:40;rhauch;Merged to branches:
* `3.0` for upcoming 3.0.1 release
* `3.1` for upcoming 3.1.0 release
* `trunk` for upcoming 3.2 (or 4.0) release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential race condition in InMemoryKeyValueStore,KAFKA-12219,13353022,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dongjin,dongjin,dongjin,18/Jan/21 08:53,19/Jan/21 19:50,13/Jul/23 09:17,19/Jan/21 18:35,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,streams,,,,,0,,,,,"With KAFKA-8802 (included in [2.4.0 release|https://downloads.apache.org/kafka/2.4.0/RELEASE_NOTES.html]), {{ConcurrentSkipListMap}} in {{InMemoryKeyValueStore}} was reverted into {{TreeMap}} for performance issues. However, the {{synchronized}} keyword for {{reverseRange}}, {{reverseAll}} methods were omitted when adding the reverse range methods in 2.7, leaving possibility of race condition.",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-18 08:53:33.0,,,,,,,,,,"0|z0mpvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated code does not include UUID or struct fields in its toString output,KAFKA-12214,13352566,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,15/Jan/21 21:27,04/Feb/22 18:36,13/Jul/23 09:17,04/Feb/22 18:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,generator,,,,,0,kip-500,,,,The generated code does not include UUID or struct fields in its toString output.,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-15 21:27:52.0,,,,,,,,,,"0|z0mn28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchFileException will be thrown if hasPersistentStores is false when creating stateDir,KAFKA-12211,13352434,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,showuon,showuon,15/Jan/21 08:33,25/Feb/21 00:30,13/Jul/23 09:17,25/Feb/21 00:30,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,,,,,"We improved the state directory folder/file permission setting in KAFKA-10705. But we forgot to consider one situation: if user doesn't have PersistentStores, we won't need to create base dir and state dir. And if there's no such dir/file, and we tried to set permission to them, we'll have *NoSuchFileException*

 


{code:java}
ERROR Error changing permissions for the state or base directory /var/folders/4l/393lkmzx3zvftwynjngzdsfw0000gn/T/kafka-11254487964259813330/appId_AdjustStreamThreadCountTestshouldAddStreamThread  (org.apache.kafka.streams.processor.internals.StateDirectory:117)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]     java.nio.file.NoSuchFileException: /var/folders/4l/393lkmzx3zvftwynjngzdsfw0000gn/T/kafka-11254487964259813330/appId_AdjustStreamThreadCountTestshouldAddStreamThread
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/sun.nio.fs.UnixFileAttributeViews$Posix.setMode(UnixFileAttributeViews.java:254)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/sun.nio.fs.UnixFileAttributeViews$Posix.setPermissions(UnixFileAttributeViews.java:276)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at java.base/java.nio.file.Files.setPosixFilePermissions(Files.java:2079)
2021-01-15T16:10:44.570+0800 [DEBUG] [TestEventLogger]          at org.apache.kafka.streams.processor.internals.StateDirectory.<init>(StateDirectory.java:115)
{code}",,ableegoldman,marcin.kuthan,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 19:56:46 UTC 2021,,,,,,,,,,"0|z0mm8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/21 19:56;ableegoldman;Just re-opening this ticket while we get the fix merged to 2.6 for the 2.6.2 release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix synchronization issue happening in KafkaStreams (related to flaky AdjustStreamThreadCountTest),KAFKA-12195,13352187,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,14/Jan/21 08:53,19/Jan/21 09:54,13/Jul/23 09:17,19/Jan/21 09:54,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"{code}
threads = Collections.synchronizedList(new LinkedList<>());
{code}

The synchronization list requires us to manually synchronize the iterator. non-synchronizing the list results in inconsistent results and consequently unstabilize the AdjustStreamThreadCountTest.",,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-14 08:53:25.0,,,,,,,,,,"0|z0mkq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-resolve IPs when a client is disconnected,KAFKA-12193,13352073,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bob-barrett,bob-barrett,bob-barrett,13/Jan/21 18:37,08/Dec/21 06:51,13/Jul/23 09:17,04/Feb/21 11:43,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,clients,,,,,0,,,,,"If `client.dns.lookup` is set to `use_all_dns_ips` or `resolve_canonical_bootstrap_servers_only`, the NetworkClient can store multiple IPs for each node, and currently it tries each IP in the list when connecting before re-resolving the IPs. This is useful when first establishing a connection because it ensures that the client exhausts all possible IPs. However, in the case where the IPs changed after a connection was already established, this would cause a reconnecting client to try several invalid IPs before re-resolving and trying a valid one. Instead, we should re-resolve DNS when a client disconnects from an established connection, rather than assuming the all previously-resolved IPs are still valid.",,ableegoldman,bob-barrett,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-13 18:37:12.0,,,,,,,,,,"0|z0mk0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure on Windows due to an UnsupportedOperationException when StateDirectory sets file permissions,KAFKA-12190,13352055,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,awilkinson,awilkinson,awilkinson,13/Jan/21 17:15,29/Sep/22 15:18,13/Jul/23 09:17,25/Jan/21 19:54,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,bug,,,,"There appears to be a regression in Kafka 2.6.1 due to [the changes|https://github.com/apache/kafka/pull/9583] made for KAFKA-10705 that causes a failure on Windows. After upgrading to 2.6.1 from 2.6.0, we're seeing failures in Spring Boot's CI on Windows such as the following:

{noformat}
Caused by: java.lang.UnsupportedOperationException: (No message provided)    
        at java.nio.file.Files.setPosixFilePermissions(Files.java:2044)    
        at org.apache.kafka.streams.processor.internals.StateDirectory.<init>(StateDirectory.java:115)    
        at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:745)    
        at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:585)    
        at org.springframework.kafka.config.StreamsBuilderFactoryBean.start(StreamsBuilderFactoryBean.java:316)    
        at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:178)    
        at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:54)    
        at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:356)    
        at java.lang.Iterable.forEach(Iterable.java:75)    
        at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:155)    
        at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:123)    
        at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:940)    
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:591)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.configureContext(AbstractApplicationContextRunner.java:447)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.createAndLoadContext(AbstractApplicationContextRunner.java:423)    
        at org.springframework.boot.test.context.assertj.AssertProviderApplicationContextInvocationHandler.getContextOrStartupFailure(AssertProviderApplicationContextInvocationHandler.java:61)    
        at org.springframework.boot.test.context.assertj.AssertProviderApplicationContextInvocationHandler.<init>(AssertProviderApplicationContextInvocationHandler.java:48)    
        at org.springframework.boot.test.context.assertj.ApplicationContextAssertProvider.get(ApplicationContextAssertProvider.java:112)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.createAssertableContext(AbstractApplicationContextRunner.java:412)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.lambda$null$0(AbstractApplicationContextRunner.java:382)    
        at org.springframework.boot.test.util.TestPropertyValues.applyToSystemProperties(TestPropertyValues.java:175)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.lambda$run$1(AbstractApplicationContextRunner.java:381)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.withContextClassLoader(AbstractApplicationContextRunner.java:392)    
        at org.springframework.boot.test.context.runner.AbstractApplicationContextRunner.run(AbstractApplicationContextRunner.java:381)    
        at org.springframework.boot.actuate.autoconfigure.metrics.KafkaMetricsAutoConfigurationTests.whenKafkaStreamsIsEnabledAndThereIsNoMeterRegistryThenListenerCustomizationBacksOff(KafkaMetricsAutoConfigurationTests.java:92)
{noformat}

The same code worked without changes using Kafka 2.6.0.",,ableegoldman,awilkinson,chia7712,davispw,grussell,iponomarev,lct45,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12448,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 06:32:57 UTC 2021,,,,,,,,,,"0|z0mjwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/21 20:13;ableegoldman;Hey [~awilkinson], thanks for the bug report. Technically Kafka Streams is not ""officially supported"" on Windows and we encourage users to run on Linux based systems if at all possible.

But I recognize that sometimes it really isn't possible, so I'm sorry that this slipped through. We've had kind of a ""best effort"" approach to supporting Windows based on user reports in the past. Unfortunately I doubt many (if any) Kafka Streams devs even have a Windows environment set up for testing, much less run those tests on a regular basis. I'm not sure what the best approach would be to prevent this sort of thing going forward – perhaps we need to reach out to Windows users during the release process and ask them to participate in the pre-release testing.

For now, would you be interested in submitting a patch for this issue? We should try to get this fixed before the next release;;;","14/Jan/21 08:20;awilkinson;Yes, I'd be happy to provide a patch. What's the timeline for the next release? It may take me a little while to get my employer's approval to contribute. The SLA for approval is 3 days but it has taken longer in the past. I can work on the changes in the meantime. To that end, what branch should I base the changes on? Should it be trunk with the changes then potentially being back-ported to the 2.6 and 2.7 branches?;;;","14/Jan/21 14:37;lct45;Hey [~awilkinson], the 2.8 release plan is here: [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=173081737.] Code freeze is February 17th;;;","14/Jan/21 17:58;grussell;Given that this is a critical bug, and a regression in a point release (2.6.1), doesn't this warrant a fairly swift 2.6.2 release?;;;","15/Jan/21 17:30;mjsax;Note that `2.7.0` release is not affected by this issue, but I don't think we need to aggressively release `2.6.2`.

However, KAFKA-10705 was cherry-picked for `2.7.1` release and thus a fix should be cherry-picked to `2.7` to avoid that we release this regression in `2.7.1`.;;;","15/Jan/21 21:55;mjsax;[~awilkinson] PR should first always be against `trunk` branch and we would cherry-pick the commit after merging to older branches. Only if there are non-resolvable conflicts while cherry-picking, we would do a dedicated PR against older branches for back-porting.;;;","19/Jan/21 09:22;awilkinson;Thanks, Matthias. I've made the changes against trunk and tested them in a Windows VM. I'm ready to submit a PR once I've received approval to do so from my employer. To that end, can you please let me know if contributors to Kafka are required to sign the Apache CLA?;;;","19/Jan/21 15:19;grussell;Thanks [~awilkinson].

{quote}
but I don't think we need to aggressively release `2.6.2`.
{quote}

I couldn't disagree more. In many enterprises, devs are only allowed to upgrade to patch releases. In order to upgrade to a new version (even a minor version), the library has to go through some formal approval process. Not exactly my preference but reality, unfortunately.

{quote}
we encourage users to run on Linux based systems if at all possible.
{quote}

Furthermore, while deployments are probably more often on Linux, developers are often forced to develop on Windows.

Please reconsider accelerating 2.6.2 to resolve this critical regression, for these reasons.;;;","20/Jan/21 23:10;mjsax;\cc [~ijuma] [~guozhang] – what do you think? Should we do a `2.6.2` release quickly because of this bug?;;;","21/Jan/21 18:15;iponomarev;Hello, I came across the same issue. While I'm running Kafka Streams on Linux, I'm developing (and debugging Kafka itself) on Windows, so I need to be able to run TopologyTestDriver-based tests at least. But the fix must be trivial and safe (see [PR9946|https://github.com/apache/kafka/pull/9946]);;;","21/Jan/21 19:30;awilkinson;As promised, here's a pull request for the changes: https://github.com/apache/kafka/pull/9947. The permissions are set as well as they can be on Windows and the tests have been updated to verify the behaviour.;;;","21/Jan/21 20:24;mjsax;[~iponomarev] Thanks for chiming it – [~awilkinson] was working on this issue already and also did a PR. – I would prefer to review and merge Andy's PR, because he was the first one who reported the issue and started to look into it. Would this be ok for you?;;;","21/Jan/21 20:26;mjsax;[~awilkinson] Thanks for the PR. I added you to the list of contributors and assigned this ticket to you. You can now also self-assign tickets.;;;","22/Jan/21 06:32;iponomarev;Hi [~mjsax] sure! I searched for the issue on Jira, but didn't find it, that's why the duplicated Jira ticket and the PR. Looking forward for the [~awilkinson] 's fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreQueryIntegrationTest#shouldQueryAllStalePartitionStores,KAFKA-12185,13351895,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,mjsax,mjsax,mjsax,13/Jan/21 00:46,22/Jan/21 06:29,13/Jul/23 09:17,21/Jan/21 16:53,2.8.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"{{{{java.lang.AssertionError: Application did not reach a RUNNING state for all streams instances. Non-running instances: \{org.apache.kafka.streams.KafkaStreams@651720d3=NOT_RUNNING}
	at org.junit.Assert.fail(Assert.java:89)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning(IntegrationTestUtils.java:892)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQueryAllStalePartitionStores(StoreQueryIntegrationTest.java:270)}}}}

 

{{https://github.com/apache/kafka/pull/9835/checks?check_run_id=1666639314}}",,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 19:11:23 UTC 2021,,,,,,,,,,"0|z0mix4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/21 19:11;vvcephei;I just saw this fail locally and happened to notice this in the logs:

 
{code:java}

[2021-01-20 13:03:55,375] ERROR stream-client [app-StoreQueryIntegrationTestshouldQueryAllStalePartitionStores-ba1cebd9-bccc-4d54-af8a-2f6a51200612] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now.  (org.apache.kafka.streams.KafkaStreams:469)
org.apache.kafka.common.KafkaException: User rebalance callback throws an error
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:436)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:451)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:367)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:508)
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1262)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1231)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:882)
	at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:839)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:679)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:567)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:547)
Caused by: java.lang.RuntimeException: Unexpected failure to close 1 task(s) [[0_1]]. First unexpected exception (for task 0_1) follows.
	at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:311)
	at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1484)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:279)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:421)
	... 11 more
Caused by: java.util.ConcurrentModificationException
	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1704)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.apache.kafka.streams.processor.internals.Tasks.convertActiveToStandby(Tasks.java:133)
	at org.apache.kafka.streams.processor.internals.TaskManager.handleCloseAndRecycle(TaskManager.java:394)
	at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:291)
	... 14 more {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Connect Cast cannot deal with fields of type ""bytes"" correctly",KAFKA-12170,13351295,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,seknop,seknop,seknop,10/Jan/21 11:02,03/Mar/21 22:18,13/Jul/23 09:17,03/Mar/21 22:18,2.6.1,2.7.0,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,KafkaConnect,,,,,0,pull-request-available,,,,"Cast operation on a field of type bytes to a string returns the default ByteBuffer representation, not the content. For example, a field called ""payload"" of type bytes transformed in this way:

payload:string

Returns the result 

""payload"":\{""string"":""java.nio.HeapByteBuffer[pos=0 lim=4 cap=4]""}

We propose instead to represent the byte array as a hex string

""payload"":\{""string"":""FFEEDDAABB123F""}",,chia7712,seknop,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6684,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Fri Jan 22 14:20:14 UTC 2021,,,,,,,,,,"0|z0mf80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/21 14:39;chia7712;[~seknop] Are you working for this issue? If so, I can assign this issue to you :);;;","11/Jan/21 15:23;seknop;Yes please - and yes I am working on this issue. The PR is ready, but I am just reading through the steps that have to be taken since this is my first PR for Apache Kafka.;;;","11/Jan/21 15:29;chia7712;[~seknop] welcome to Kafka :)

I have assigned this issue to you.;;;","11/Jan/21 16:53;seknop;Thanks a lot. I work for Confluent, somewhat familiar to Kafka :);;;","22/Jan/21 14:20;seknop;https://github.com/apache/kafka/pull/9950;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.kafka.common.quota classes omitted from Javadoc,KAFKA-12165,13351058,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,08/Jan/21 10:18,10/Jan/21 04:12,13/Jul/23 09:17,08/Jan/21 19:57,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,clients,,,,,0,,,,,"The public API classes in `org.apache.kafka.common.quota` should be included in the javadoc, but are currently omitted. E.g. see https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/admin/Admin.html#alterClientQuotas-java.util.Collection-",,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-08 10:18:53.0,,,,,,,,,,"0|z0mdrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStreams configs are documented incorrectly,KAFKA-12160,13350998,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,JimGalasyn,mjsax,mjsax,08/Jan/21 02:44,23/Feb/21 01:08,13/Jul/23 09:17,23/Feb/21 01:08,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,docs,streams,,,,0,,,,,"In version 2.3, we removed the KafkaStreams default of `max.poll.interval.ms` and fall-back to the consumer default. However, the docs still contain `Integer.MAX_VALUE` as default.

Because we rely on the consumer default, we should actually remove `max.poll.interval.ms` from the Kafka Streams docs completely. We might want to fix this is some older versions, too. Not sure how far back we want to go.

Furhtermore, in 2.7 docs, the section of ""Default Values"" and ""Parameters controlled by Kafka Streams"" contain incorrect information.

cf https://kafka.apache.org/27/documentation/streams/developer-guide/config-streams.html#default-values

 ",,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-08 02:44:25.0,,,,,,,,,,"0|z0mde8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
follower can hit OffsetOutOfRangeException during truncation,KAFKA-12153,13350752,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,junrao,junrao,06/Jan/21 21:32,10/Jan/21 04:21,13/Jul/23 09:17,09/Jan/21 01:41,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,core,,,,,0,,,,,"Currently, we have the following code path.

log.truncateTo() => updateLogEndOffset() => updateHighWatermarkMetadata() => maybeIncrementFirstUnstableOffset() => convertToOffsetMetadataOrThrow() => read()

This path seems problematic. The issue is that updateLogEndOffset() is called before loadProducerState() in log.truncateTo(). At that point, the producerState is not reflecting the truncated state yet and producerStateManager.firstUnstableOffset(called in maybeIncrementFirstUnstableOffset() to feed read()) could return an offset larger than the truncated logEndOffset, which will lead to OffsetOutOfRangeException.

 

This issue is relatively rare since it requires truncation below the high watermark.",,dengziming,junrao,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-06 21:32:06.0,,,,,,,,,,"0|z0mbvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idempotent Producer does not reset the sequence number of partitions without in-flight batches,KAFKA-12152,13350671,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,06/Jan/21 13:07,22/Jan/21 06:38,13/Jul/23 09:17,21/Jan/21 09:19,2.5.0,2.6.0,2.7.0,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,,,,,,0,,,,,"When a `OutOfOrderSequenceException` error is received by an idempotent producer for a partition, the producer bumps its epoch, adjusts the sequence number and the epoch of the in-flight batches of the partitions affected by the `OutOfOrderSequenceException` error. This happens in `TransactionManager#bumpIdempotentProducerEpoch`.

The remaining partitions are treated separately. When the last in-flight batch of a given partition is completed, the sequence number is reset. This happens in `TransactionManager#handleCompletedBatch`.

However, when a given partition does not have in-flight batches when the producer epoch is bumped, its sequence number is not reset. It results in having subsequent producer request to use the new producer epoch with the old sequence number and to be rejected by the broker.",,calohmn,dajac,dengziming,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-06 13:07:16.0,,,,,,,,,,"0|z0mbdk:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic auth extension's JAAS config can be corrupted by other plugins,KAFKA-10895,13348611,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,31/Dec/20 16:49,03/Feb/21 23:33,13/Jul/23 09:17,03/Feb/21 23:33,2.0.0,2.0.1,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,2.5.1,2.6.0,2.7.0,,,,,,,,2.3.2,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,,,KafkaConnect,,,,,0,,,,,"The Connect [BasicAuthSecurityRestExtension|https://github.com/apache/kafka/blob/trunk/connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/BasicAuthSecurityRestExtension.java]'s doc states that ""An entry with the name {{KafkaConnect}} is expected in the JAAS config file configured in the JVM.""

This is technically accurate, as the [JaasBasicAuthFilter|https://github.com/apache/kafka/blob/afa5423356d3d2a2135a51200573b45d097f6d60/connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilter.java#L61-L63] that the extension installs creates a {{LoginContext}} using a [constructor|https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/LoginContext.html#LoginContext-java.lang.String-javax.security.auth.callback.CallbackHandler-] that does not include a [Configuration|https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/Configuration.html] to be passed in, which causes [Configuration::getConfiguration|https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/Configuration.html#getConfiguration--] to be used under the hood by the {{LoginContext}} to fetch the JAAS configuration to use for authentication.

Unfortunately, other plugins (connectors, converters, even other REST extensions, etc.) may invoke [Configuration::setConfiguration|https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/Configuration.html#setConfiguration-javax.security.auth.login.Configuration-] and install a completely different JAAS configuration onto the JVM. If the user starts their JVM with a JAAS config set via the {{-Djava.security.auth.login.config}} property, that JAAS config can then be completely overwritten, and if the basic auth extension depends on the JAAS config that's installed at startup (as opposed to at runtime by a plugin), it will break.

It's debatable whether this can or should be addressed with a code fix. One possibility is to cache the current JVM's configuration as soon as the basic auth extension is loaded by invoking {{Configuration::getConfiguration}} and saving the resulting configuration for future {{LoginContext}} instantiations. However, it may be possible that users actually rely on runtime plugins being able to install custom configurations at runtime for their basic auth extension, in which case this change would actually be harmful.

Regardless, it's worth noting this odd behavior here in the hopes that it can save some time for others who encounter the same issue.",,ableegoldman,ChrisEgerton,ifconfig,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 23:33:23 UTC 2021,,,,,,,,,,"0|z0lyhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/20 16:51;ChrisEgerton;At the moment, I think that it's fine to cache the earliest-available {{Configuration}} and use that exclusively in the basic auth extension. It seems highly unlikely that a runtime-installed JAAS config is desirable by users, especially since it would have to be installed by a completely different plugin. This won't catch all cases (as it's always possible that some plugin class that gets loaded before the basic auth extension manages to install a custom configuration before it can be read by the extension), but it should catch most of them.;;;","02/Feb/21 00:29;ChrisEgerton;Reopening as the original fix included a small regression.;;;","03/Feb/21 23:33;kkonstantine;The regression has been fixed and the fix has been applied to the same target branches. No release took place in the meantime;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null replica nodes included in client quota callback Cluster,KAFKA-10894,13348515,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,30/Dec/20 21:30,05/Jan/21 21:03,13/Jul/23 09:14,05/Jan/21 21:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"I noticed an NPE in the client quota callback `updateClusterMetadata` due to the presence of null nodes inside a `PartitionInfo` instance. Here is the trace:
{code}
java.lang.NullPointerException
	at org.apache.kafka.common.PartitionInfo.formatNodeIds(PartitionInfo.java:143)
	at org.apache.kafka.common.PartitionInfo.toString(PartitionInfo.java:132)
	at java.base/java.lang.String.valueOf(String.java:3388)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:167)
	at java.base/java.util.AbstractCollection.toString(AbstractCollection.java:457)
	at java.base/java.util.Collections$UnmodifiableCollection.toString(Collections.java:1042)
	at java.base/java.lang.String.valueOf(String.java:3388)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:167)
	at org.apache.kafka.common.Cluster.toString(Cluster.java:348)
{code}

After some debugging, I found that `PartitionInfo.replicas` had a null value. The javadoc for this field is the following:
{code}
    /**
     * The complete set of replicas for this partition regardless of whether they are alive or up-to-date
     */
    public Node[] replicas() {
        return replicas;
    }
{code}

It's pretty clear that the expectation is that arrays do not contain null values. On the client in `MetadataResponse`, we use the following logic to deal with nodes which are not alive:
{code}
    private static Node[] convertToNodeArray(List<Integer> replicaIds, Map<Integer, Node> nodesById) {
        return replicaIds.stream().map(replicaId -> {
            Node node = nodesById.get(replicaId);
            if (node == null)
                return new Node(replicaId, """", -1);
            return node;
        }).toArray(Node[]::new);
    }
{code}

However, inside `MetadataCache.getClusterMetadata` (which is used in the quota callback), we have the following logic:
{code}
    val nodes = snapshot.aliveNodes.map { case (id, nodes) => (id, nodes.get(listenerName).orNull) }
    def node(id: Integer): Node = nodes.get(id.toLong).orNull
    val partitions = getAllPartitions(snapshot)
      .filter { case (_, state) => state.leader != LeaderAndIsr.LeaderDuringDelete }
      .map { case (tp, state) =>
        new PartitionInfo(tp.topic, tp.partition, node(state.leader),
          state.replicas.asScala.map(node).toArray,
          state.isr.asScala.map(node).toArray,
          state.offlineReplicas.asScala.map(node).toArray)
      }
{code}
Note specifically that the nested `node` method returns null if the replica is not alive. It looks like we need to mimic the same logic from `MetadataResponse` here.


",,dengziming,hachikuji,iBlackeyes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-30 21:30:16.0,,,,,,,,,,"0|z0lxw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" Sticky partition leads to uneven product msg, resulting in abnormal delays in some partitions",KAFKA-10888,13347755,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,alivshits,jr981008,jr981008,24/Dec/20 13:12,12/Jul/22 21:07,13/Jul/23 09:17,06/May/22 18:33,2.4.1,,,,,,,,,,,,,,,,,,,,,,3.3.0,,,,,,,,clients,producer ,,,,0,,,,,"  110 producers ,550 partitions ,550 consumers , 5 nodes Kafka cluster
  The producer uses the nullkey+stick partitioner, the total production rate is about 100w tps
Observed partition delay is abnormal and message distribution is uneven, which leads to the maximum production and consumption delay of the partition with more messages 
abnormal.

  I cannot find reason that stick will make the message distribution uneven at this production rate.
  I can't switch to the round-robin partitioner, which will increase the delay and cpu cost. Is thathe stick partationer design cause uneven message distribution, or this is abnormal. How to solve it?

  !image-2020-12-24-21-09-47-692.png!

As shown in the picture, the uneven distribution is concentrated on some partitions and some brokers, there seems to be some rules.

This problem does not only occur in one cluster, but in many high tps clusters,

The problem is more obvious on the test cluster we built.

!image-2020-12-24-21-10-24-407.png!",,ableegoldman,dengziming,erdody,hachikuji,jolshan,jr981008,junrao,kirktrue,MarkC0x,nk2242696@gmail.com,rng,showuon,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13540,,,KAFKA-14020,,,,,,,,,,,,"24/Dec/20 13:06;jr981008;image-2020-12-24-21-05-02-800.png;https://issues.apache.org/jira/secure/attachment/13017615/image-2020-12-24-21-05-02-800.png","24/Dec/20 13:11;jr981008;image-2020-12-24-21-09-47-692.png;https://issues.apache.org/jira/secure/attachment/13017614/image-2020-12-24-21-09-47-692.png","24/Dec/20 13:12;jr981008;image-2020-12-24-21-10-24-407.png;https://issues.apache.org/jira/secure/attachment/13017613/image-2020-12-24-21-10-24-407.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 01 03:41:45 UTC 2022,,,,,,,,,,"0|z0lt7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/20 17:11;junrao;[~jr981008]: Thanks for reporting this. There are a couple of threads in the dev mailing list that might be related to what's described in this jira.

[https://lists.apache.org/thread.html/rae8d2d5587dae57ad9093a85181e0cb4256f10d1e57138ecdb3ef287%40%3Cdev.kafka.apache.org%3E]

[https://lists.apache.org/list.html?dev@kafka.apache.org:lte=1M:kip-693]

 ;;;","16/Feb/21 17:59;hachikuji;We have found one cause of imbalance when the sticky partitioner is used. Basically the intuition behind the sticky partitioner breaks down a little bit when a small `linger.ms` is in use (sadly this is the default). The user is opting out of batching with this setting which means there is often little opportunity to fill batches before they get drained and sent. That leaves the door open to sustained imbalance in some cases.

To see why, suppose that we have a producer writing to 3 partitions with linger.ms=0 and one partition slows down a little bit for some reason. It could be a leader change or some transient network issue. The producer will have to hold onto the batches for that partition until it becomes available. While it is holding onto those batches, additional batches will begin piling up. Each of these batches is likely to get filled because the producer is not ready to send to this partition yet.

Consider this from the perspective of the sticky partitioner. Every time the slow partition gets selected, the producer will fill the batches completely. On the other hand, the remaining ""fast"" partitions will likely not get their batches filled because of the `linger.ms=0` setting. As soon as a single record is available, it might get sent. So more data ends up getting written to the partition that has already started to build a backlog. And even after the cause of the original slowness (e.g. leader change) gets resolved, it might take some time for this imbalance to recover. We believe this can even create a runaway effect if the partition cannot catch up with the handicap of the additional load.

We analyzed one case where we thought this might be going on. Below I've summarized the writes over a period of one hour to 3 partitions. Partition 0 here is the ""slow"" partition. All partitions get roughly the same number of batches, but the slow partition has much bigger batch sizes.

{code}
Partition TotalBatches TotalBytes TotalRecords BytesPerBatch RecordsPerBatch
0         1683         25953200   25228        15420.80      14.99        
1         1713         7836878    4622         4574.94       2.70
2         1711         7546212    4381         4410.41       2.56
{code}

After restarting the application, the producer was healthy again. It just was not able to recover with the imbalanced workload.;;;","17/Feb/21 18:03;junrao;[~hachikuji]: Thanks for the analysis. One thing that led to this behavior is that sticky partitioner tries to distribute the data to different partitions in batches and the batches sometimes are not even in size as you pointed out. So, one way of fixing this is to have the sticky partitioner distribute the data in units that are always equal (e.g., a fixed amount of bytes based on batch size).;;;","17/Feb/21 19:03;hachikuji;[~junrao] That's a good observation. The partitioner knows the approximate size of each batch that gets sent, so it could take that into account before rotating partitions. We were thinking of approaches which involved the partitioner being aware of how many bytes were inflight, but propagating this information from the accumulator to the partitioner is kind of messy. 

Perhaps it is simpler for the partitioner to track at the level of total bytes sent to each partition. Maybe we could see this analogously to the tcp window size. For example, say you start with an expected size of 10 records (or 1k bytes or whatever). The partitioner keeps writing to the partition until this many records/bytes have been sent regardless of the batching. If the batch is not filled when the limit is reached, then we move onto the next partition, but we increase the batch size. On the other hand, if a batch gets sent before the limit is reached, we continue writing to the partition until the limit is reached and we decrease it when we move on to the next partition. In this way, we can keep a better balance between partitions.;;;","02/Aug/21 08:46;showuon;[~hachikuji], I was thinking about your suggestion above, to be analogous to the tcp window size implementation. If I understand it correctly, this example should explain what your suggestion:

*topic A with partition 3:*

Suppose we use default `batch.size` and `linger.ms` setting (16K bytes, 0ms), and on average, we can send 2k bytes for each batch, and we set the default window size of 1k bytes. So,

1st batch, we'll have: (window size = 1k)

partition A-0, 1k bytes (reach the limit, so move to next partition, and increase to 2k bytes window size (suppose +/- 1k each time))

partition A-1, 1k bytes

 

2nd batch, we'll have: (window size = 2k)

partition A-2, 2k bytes

 

===

so far, we'll have

partition A-0: 1k bytes

partition A-1: 1k bytes

partition A-2: 2k bytes

and keep going.

===

Suppose partition A-0 slows down in next batch, with 6k bytes be sent:

4th batch (window size = 2k) ,

partition A-0, 2k bytes (reach the limit, so move to next partition, and increase to 2k bytes window size, so 4k bytes now )

partition A-1, 4k bytes

 

5th batch (window size = 4k)

partition A-2, 2k bytes (not reach the limit, keep sending to partition A-2 in next batch, decrease the window size to 2k bytes )

 

===

so far, we'll have

partition A-0: 3k bytes

partition A-1: 5k bytes

partition A-2: 4k bytes

and keep going.

===

I think this proposal might still cause uneven distribution as above example showed. Also, it will send to 2 or more batches in some cases(ex: 1st batch in above example) , which was originally sent to 1 batch only (and it's the spirit of the sticky partitioner, to stick to a partition before batch full to improve throughput).

 

So, I'm proposing a way to make it evenly distribution and still keep the original sticky partitioner spirit: check the distribution status when all partitions are sent 1 batch. And have a threshold to see if we want to skip the exceeding partition in the following rounds, and how many rounds it should be skipped. Using the above example:

==1st round, same result as using original sticky partitioner==

1st batch

partition A-0, 2k bytes

 

2nd batch

partition A-1, 2k bytes

 

3rd batch:

partition A-2, 2k bytes

===

so far, we'll have

partition A-0: 2k bytes

partition A-1: 2k bytes

partition A-2: 2k bytes

After all partitions sent 1 batch, we check if there's any partition batch size in this round is exceeding other partitions more than the threshold (ex: 70%), here, no, so keeps going

===

===2nd round, still have the same result as using original sticky partitioner===

Suppose partition A-0 slows down in next batch, with 6k bytes be sent:

4th batch

partition A-0, 6k bytes

 

5th batch

partition A-1, 2k bytes

 

6th batch

partition A-2, 2k bytes

===

in this round, we'll have

partition A-0: 6k bytes

partition A-1: 2k bytes

partition A-2: 2k bytes

After this round, we check if there's any partition batch size in this round is exceeding other partitions more than the threshold (ex: 70%), here, we have partition A-0 exceeding with 4k bytes, and compute how many rounds it should be skipped: 4k / (2k * 0.7) = 2.8 => only care the integer part, 2. So, partition A-0 should be skipped 2 rounds

===

So, we can imagine, after 3rd and 4th rounds, partition A-0 is skipped, we'll have balanced messages sent

partition A-0: 6k bytes

partition A-1: 6k bytes

partition A-2: 6k bytes

 

What do you think?

(Sorry for the long response) Thank you.

 ;;;","13/Dec/21 10:25;nk2242696@gmail.com;[~showuon] [~hachikuji] 

I propose a solution based on keeping the  track of number of offsets/messages written to each partition. 
 # At partitioner level, Store the number of offsets/messages written to each partition. (sizePerPartitionMap) and total(total offsets for a topic)
 # Before choosing next batch to write(onNextBatch()) . Use (sizePerPartitionMap) to blacklisting the available partitions which causes skewness(USING a configurable THRESHOLD %) . Choose next partition from list of available whitelisted partitions. 
 # To configure sizePerPartitionMap. Use the callback method of producer.send() to update the sizePerPartitionMap and total .

This way, we can skip slower partitions(blacklisted) for few rounds and ensure all partitions are roughly of equal size. ;;;","15/Dec/21 06:26;showuon;[~nk2242696@gmail.com] , for your information, [~alivshits] is working on  [https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner] to fix this issue. Thanks.;;;","06/May/22 18:33;junrao;merged the PR to trunk. Thanks [~alivshits] for the design, implementation and the testing.;;;","01/Jul/22 03:41;zhangzs;goog job;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer should handle REBALANCE_IN_PROGRESS from JoinGroup,KAFKA-10870,13346879,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,hachikuji,hachikuji,18/Dec/20 23:55,04/Jan/21 22:20,13/Jul/23 09:17,04/Jan/21 22:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"We hit a timeout when persisting group metadata to the __consumer_offsets topic:
{code}
[2020-12-18 18:06:08,209] DEBUG [GroupMetadataManager brokerId=1] Metadata from group test_group_id with generation 1 failed when appending to log due to org.apache.kafka.common.errors.TimeoutException (kafka.coordinator.group.GroupMetadataManager)
[2020-12-18 18:06:08,210] WARN [GroupCoordinator 1]: Failed to persist metadata for group test_group_id: The group is rebalancing, so a rejoin is needed. (kafka.coordinator.group.GroupCoordinator)
{code}

This in turn resulted in a REBALANCE_IN_PROGRESS being returned from the JoinGroup:

{code}
[2020-12-18 18:06:08,211] INFO Completed request:RequestHeader(apiKey=JOIN_GROUP, apiVersion=7, clientId=consumer-test_group_id-test_group_id-instance-1, correlationId=3) -- {group_id=test_group_id,session_timeout_ms=60000,rebalance_timeout_ms=300000,member_id=,group_instance_id=test_group_id-instance-1,protocol_type=consumer,protocols=[{name=range,metadata=java.nio.HeapByteBuffer[pos=0 lim=26 cap=26],_tagged_fields={}}],_tagged_fields={}},response:{throttle_time_ms=0,error_code=27,generation_id=1,protocol_type=consumer,protocol_name=range,leader=test_group_id-instance-2-32e72316-2c3f-40d6-bc34-8ec23d633d34,member_id=,members=[],_tagged_fields={}} from connection 172.31.46.222:9092-172.31.44.169:41310-6;totalTime:5014.825,requestQueueTime:0.193,localTime:11.575,remoteTime:5002.195,throttleTime:0.66,responseQueueTime:0.105,sendTime:0.094,sendIoTime:0.038,securityProtocol:PLAINTEXT,principal:User:ANONYMOUS,listener:PLAINTEXT,clientInformation:ClientInformation(softwareName=apache-kafka-java, softwareVersion=5.5.3-ce) (kafka.request.logger)      
{code}

The consumer has no logic to handle REBALANCE_IN_PROGRESS from JoinGroup.

{code}
[2020-12-18 18:06:08,210] ERROR [Consumer instanceId=test_group_id-instance-1, clientId=consumer-test_group_id-test_group_id-instance-1, groupId=test_group_id] Attempt to join group failed due to unexpected error
: The group is rebalancing, so a rejoin is needed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2020-12-18 18:06:08,211] INFO [Consumer instanceId=test_group_id-instance-1, clientId=consumer-test_group_id-test_group_id-instance-1, groupId=test_group_id] Join group failed with org.apache.kafka.common.KafkaE
xception: Unexpected error in join group response: The group is rebalancing, so a rejoin is needed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2020-12-18 18:06:08,211] ERROR Error during processing, terminating consumer process:  (org.apache.kafka.tools.VerifiableConsumer)
org.apache.kafka.common.KafkaException: Unexpected error in join group response: The group is rebalancing, so a rejoin is needed.
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:653)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:574)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1096)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1076)
        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
{code}",,ableegoldman,dengziming,hachikuji,sql_consulting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-18 23:55:13.0,,,,,,,,,,"0|z0lnsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test `TransactionsTest.testFencingOnSendOffsets`,KAFKA-10861,13346460,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,16/Dec/20 23:46,17/Dec/20 02:31,13/Jul/23 09:17,17/Dec/20 02:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"{code}
org.scalatest.exceptions.TestFailedException: Got an unexpected exception from a fenced producer.
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
	at org.scalatest.Assertions.fail(Assertions.scala:1107)
	at org.scalatest.Assertions.fail$(Assertions.scala:1103)
	at org.scalatest.Assertions$.fail(Assertions.scala:1389)
	at kafka.api.TransactionsTest.testFencingOnSendOffsets(TransactionsTest.scala:373)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at Caused by: org.apache.kafka.common.errors.InvalidProducerEpochException: Producer attempted to produce with an old epoch (producerId=0, epoch=0)
{code}",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-16 23:46:29.0,,,,,,,,,,"0|z0ll7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-local return in envelope validation in causes unexpected response send,KAFKA-10855,13346034,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,15/Dec/20 05:19,15/Dec/20 21:08,13/Jul/23 09:17,15/Dec/20 21:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"`KafkaApis.handle` uses the following logic to validate the envelope:
```
      request.envelope.foreach { envelope =>
        if (maybeHandleInvalidEnvelope(envelope, request.header.apiKey)) {
          return
        }
      }
```
Since this is a non-local return, scala raises an exception, which then causes `handleError` to get invoked, which sends an error response.",,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-15 05:19:00.0,,,,,,,,,,"0|z0lil4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileStreamSourceTask buffer can grow without bound,KAFKA-10846,13345544,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,tombentley,tombentley,11/Dec/20 15:16,25/May/21 10:56,13/Jul/23 09:17,18/Dec/20 04:01,,,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.0,,,,,,,KafkaConnect,,,,,0,,,,,"When reading a large file the buffer used by {{FileStreamSourceTask}} can grow without bound. Even in the unit test org.apache.kafka.connect.file.FileStreamSourceTaskTest#testBatchSize the buffer grows from 1,024 to 524,288 bytes just reading 10,000 copies of a line of <100 chars.

The problem is that the condition for growing the buffer is incorrect. The buffer is doubled whenever some bytes were read and the used space in the buffer == the buffer length.
The requirement to increase the buffer size should be related to whether {{extractLine()}} actually managed to read any lines. It's only when no complete lines were read since the last call to {{read()}} that we need to increase the buffer size (to cope with the large line).
",,iBlackeyes,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-11 15:16:46.0,,,,,,,,,,"0|z0lfk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recovery logic is using incorrect ProducerStateManager instance when updating producers ,KAFKA-10832,13345137,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kprakasam,kprakasam,kprakasam,09/Dec/20 19:53,12/Dec/20 00:37,13/Jul/23 09:17,12/Dec/20 00:37,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"The bug is that from within {{Log.updateProducers(…)}}, the code operates on the {{producerStateManager}} attribute of the {{Log}} instance instead of operating on an input parameter. Please see [this|https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L1464] LOC where it calls {{producerStateManager.prepareUpdate}} thus accessing the attribute from the {{Log}} object (see [this|https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L251]). This looks unusual particularly for {{Log.loadProducersFromLog(...)}} [path|https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L956]. Here I believe we should be using the instance passed to the method, rather than the attribute from the {{Log}} instance.",,junrao,kprakasam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 12 00:37:06 UTC 2020,,,,,,,,,,"0|z0ld20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/20 00:37;junrao;Merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamsProducer should catch InvalidProducerEpoch and throw TaskMigrated in all cases,KAFKA-10813,13344313,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bchen225242,bchen225242,bchen225242,05/Dec/20 00:30,10/Dec/20 17:15,13/Jul/23 09:17,10/Dec/20 17:14,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"We fixed the error code handling on producer in https://issues.apache.org/jira/browse/KAFKA-10687, however the newly thrown `InvalidProducerEpoch` exception was not properly handled on Streams side in all cases. We should catch it and rethrow as TaskMigrated to trigger exception, similar to ProducerFenced.",,ableegoldman,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-05 00:30:33.0,,,,,,,,,,"0|z0l7zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System exit from MirrorConnectorsIntegrationTest#testReplication,KAFKA-10811,13344308,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,rhauch,rhauch,04/Dec/20 23:46,20/Sep/22 14:25,13/Jul/23 09:17,07/Dec/20 19:51,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,2.8.0,,,,,KafkaConnect,mirrormaker,,,,0,,,,,"The MirrorConnectorsIntegrationTest::testReplication has been very frequently causing the build to fail with:

{noformat}
FAILURE: Build failed with an exception.
13:50:17  
13:50:17  * What went wrong:
13:50:17  Execution failed for task ':connect:mirror:integrationTest'.
13:50:17  > Process 'Gradle Test Executor 52' finished with non-zero exit value 1
13:50:17    This problem might be caused by incorrect test process configuration.
13:50:17    Please refer to the test execution section in the User Manual at https://docs.gradle.org/6.7.1/userguide/java_testing.html#sec:test_execution
{noformat}

Even running this locally resulted in mostly failures, and specifically the `MirrorConnectorsIntegrationTest::testReplication` test method reliably fails due to the process being exited.

[~ChrisEgerton] traced this to the fact that these integration tests are creating multiple EmbeddedConnectCluster instances, each of which by default:
* mask the Exit procedures upon startup
* reset the Exit procedures upon stop

But since *each* cluster does this, then {{Exit.resetExitProcedure()}} is called when the first Connect cluster is stopped, and if any problems occur while the second Connect cluster is being stopped (e.g., the KafkaBasedLog produce thread is interrupted) then the Exit called by the Connect worker results in the termination of the JVM.

The solution is to change the MirrorConnectorsIntegrationTest to own the overriding of the exit procedures, and to tell the EmbeddedConnectCluster instances to not mask the exit procedures.

With these changes, running these tests locally made the tests always pass locally for me.",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14244,,,KAFKA-9013,KAFKA-10812,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 19:51:28 UTC 2020,,,,,,,,,,"0|z0l7yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/20 19:51;rhauch;Merged to `trunk`, backported to `2.7` (with agreement from [~bbejeck]) to avoid build failures. 

Also backported to `2.6` and `2.5`, though this included a few more lines from the `MirrorConnectorsIntegrationTest` class to make it more consistent with newer branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip improper dynamic configs while initialization and include the rest correct ones,KAFKA-10803,13344055,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prat0318,prat0318,prat0318,03/Dec/20 16:26,04/Dec/20 03:59,13/Jul/23 09:17,04/Dec/20 03:59,1.1.1,2.5.1,2.6.0,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"There is [a bug|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala#L470] in how incorrect dynamic config keys are removed from the original Properties list, resulting in persisting the improper configs in the properties list.

This eventually results in exception being thrown while parsing the list by [KafkaConfig ctor|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala#L531], resulting in skipping of the complete dynamic list (including the correct ones).",,iBlackeyes,prat0318,sql_consulting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 16:31:34 UTC 2020,,,,,,,,,,"0|z0l6e8:",9223372036854775807,,rajinisivaram@gmail.com,,,,,,,,,,,,,,,,,,"03/Dec/20 16:31;prat0318;PR: https://github.com/apache/kafka/pull/9682;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AlterIsr path does not update ISR shrink/expand meters,KAFKA-10799,13343836,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,hachikuji,hachikuji,02/Dec/20 18:20,04/Dec/20 14:58,13/Jul/23 09:17,04/Dec/20 14:58,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,We forgot to update the ISR change metrics when we added support for AlterIsr. These are currently only updated when ISR changes are made through Zk.,,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-02 18:20:54.0,,,,,,,,,,"0|z0l51k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed authentication delay doesn't work with some SASL authentication failures,KAFKA-10798,13343793,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,02/Dec/20 14:42,18/Apr/23 02:52,13/Jul/23 09:17,07/Dec/20 16:12,,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,security,,,,,0,,,,,"KIP-306 introduced the config `connection.failed.authentication.delay.ms` to delay connection closing on brokers for failed authentication to limit the rate of retried authentications from clients in order to avoid excessive authentication load on brokers from failed clients. We rely on authentication failure response to be delayed in this case to prevent clients from detecting the failure and retrying sooner.

SaslServerAuthenticator delays response for SaslAuthenticationException, but not for SaslException, even though SaslException is also converted into SaslAuthenticationException and processed as an authentication failure by both server and clients. As a result, connection delay is not applied in many scenarios like SCRAM authentication failures.",,iBlackeyes,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-02 14:42:10.0,,,,,,,,,,"0|z0l4s0:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in FindCoordinatorFuture permanently severs connection to group coordinator,KAFKA-10793,13343668,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ableegoldman,ableegoldman,ableegoldman,02/Dec/20 01:10,11/Feb/22 07:31,13/Jul/23 09:17,27/Jan/21 03:08,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,consumer,streams,,,,0,new-consumer-threading-should-fix,,,,"Pretty much as soon as we started actively monitoring the _last-rebalance-seconds-ago_ metric in our Kafka Streams test environment, we started seeing something weird. Every so often one of the StreamThreads (ie a single Consumer instance) would appear to permanently fall out of the group, as evidenced by a monotonically increasing _last-rebalance-seconds-ago._ We inject artificial network failures every few hours at most, so the group rebalances quite often. But the one consumer never rejoins, with no other symptoms (besides a slight drop in throughput since the remaining threads had to take over this member's work). We're confident that the problem exists in the client layer, since the logs confirmed that the unhealthy consumer was still calling poll. It was also calling Consumer#committed in its main poll loop, which was consistently failing with a TimeoutException.

When I attached a remote debugger to an instance experiencing this issue, the network client's connection to the group coordinator (the one that uses MAX_VALUE - node.id as the coordinator id) was in the DISCONNECTED state. But for some reason it never tried to re-establish this connection, although it did successfully connect to that same broker through the ""normal"" connection (ie the one that juts uses node.id).

The tl;dr is that the AbstractCoordinator's FindCoordinatorRequest has failed (presumably due to a disconnect), but the _findCoordinatorFuture_ is non-null so a new request is never sent. This shouldn't be possible since the FindCoordinatorResponseHandler is supposed to clear the _findCoordinatorFuture_ when the future is completed. But somehow that didn't happen, so the consumer continues to assume there's still a FindCoordinator request in flight and never even notices that it's dropped out of the group.

These are the only confirmed findings so far, however we have some guesses which I'll leave in the comments. Note that we only noticed this due to the newly added _last-rebalance-seconds-ago_ __metric, and there's no reason to believe this bug hasn't been flying under the radar since the Consumer's inception",,ableegoldman,guozhang,Jack-Lee,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13563,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 06:56:23 UTC 2021,,,,,,,,,,"0|z0l408:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/20 01:36;ableegoldman;At this point we can only guess, but all signs point to a race condition between the main consumer thread and the heartbeat thread. One possibility is that when the future failed it just didn't trigger the `onFailure` callback, but [~guozhang] & I have both looked through the source code and don't see any way for this to occur. Another possibility is that the `onFailure` callback was triggered, but it was invoked too soon. If the future was completed before we ever assigned it to the _findCoordinatorFuture_ field, then we would never actually clear the latest future (we would just set an already-null field to null again).

Is this possible? Here's how the AbstractCoordinator builds the request and assigns the future:
{code:java}
protected synchronized RequestFuture<Void> lookupCoordinator() {
    ...
    findCoordinatorFuture = sendFindCoordinatorRequest(node);
}
{code}
{code:java}
private RequestFuture<Void> sendFindCoordinatorRequest(Node node) {
    ...
    return client.send(node, requestBuilder)
       .compose(new FindCoordinatorResponseHandler());{code}
{code:java}
}{code}
Inside #compose we call #addListener, which contains this snippet:
{code:java}
if (failed()) 
    fireFailure(); 
{code}
If the request has already failed by the time we reach this, then we'll trigger the `onFailure` callback before #compose ever returns – ie before we've assigned the future to _findCoordinatorFuture_.

The obvious question now is whether it's possible for the request to be failed in another thread while one thread is in the middle of the synchronized lookupCoordinator(). The request can be failed by the ConsumerNetworkClient when polled, during checkDisconnects(). The heartbeat thread actually synchronizes the entire run loop, so it doesn't seem possible for the hb thread to fail this request in the background of the main thread during a lookupCoordinator().

But the inverse is not true: it's possible for the main consumer thread to fail the request while the hb thread is inside of lookupCoordinator(). The AbstractCoordinator will poll the network client inside of joinGroupIfNeeded(), which in not itself synchronized and may be invoked without any locking through a Consumer#poll. ;;;","02/Dec/20 01:47;Jack-Lee;[~ableegoldman] I want to check whether the heartbeat Thread will check the status , thanks. ;;;","02/Dec/20 01:50;ableegoldman;So that's our best guess. We should discuss the solution on the PR, but I'll lay out the two possibilities I see here in case anyone has any better ideas.

1) synchronize joinGroupIfNeeded()
2) clear the _findCoordinatorFuture_ when handling the result, rather than in the listener callbacks. For example in the main loop of ensureCoordinatorReady()

Personally I think option 2 is better, since it makes a lot more sense to me to begin with. By clearing the future in the listener callbacks, we might clear it before we ever even get to check on the result, eg the exception if failed. We actually seem to already anticipate this particular problem and recently implemented a workaround by adding an extra listener which saves the exception to a class _findCoordinatorException_ field. If we just wait to clear the future then presumably we could remove this workaround as well, and just save the exception when we check ""if (future.failed())"" inside of lookupCoordinator().

All that said, I'm not intimately familiar with the technical details of the ConsumerNetworkClient and how it handles its RequestFutures, so it's possible I'm missing something important.;;;","29/Jan/21 06:56;guozhang;cc [~hachikuji] [~ijuma] Hopefully we nailed it this time! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source tasks can block herder thread by hanging during stop,KAFKA-10792,13343642,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,01/Dec/20 19:53,14/Jun/23 16:06,13/Jul/23 09:17,04/Dec/20 22:53,2.4.0,2.4.1,2.4.2,2.5.0,2.5.1,2.6.0,2.7.0,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,KafkaConnect,,,,,0,,,,,"If a source task blocks during its {{stop}} method, the herder thread will also block, which can cause issues with detecting rebalances, reconfiguring connectors, and other vital functions of a Connect worker.

This occurs because the call to {{SourceTask::stop}} occurs on the herder's thread, instead of on the source task's own dedicated thread. This can be fixed by moving the call to {{SourceTask::stop}} onto the source task's dedicated thread and aligning with the current approach for {{Connector}} instances and {{SinkTask}} instances.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12726,,,,,,,KAFKA-15090,,,KAFKA-14670,,,KAFKA-6566,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 18:33:10 UTC 2021,,,,,,,,,,"0|z0l3ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/21 23:15;rhauch;Backported/merged to the `2.7` branch for inclusion in the next patch release (2.7.1).;;;","12/Jan/21 23:37;rhauch;Backported/merged to the `2.6` branch for inclusion in the next patch release (2.6.2).;;;","13/Jan/21 17:30;ChrisEgerton;I've opened [https://github.com/apache/kafka/pull/9880] to backport to 2.5.;;;","14/Jan/21 18:33;rhauch;Thanks for the backport PR for the `2.5` branch, [~ChrisEgerton]. I've merged it to the `2.5` branch for inclusion in the next patch release (2.5.2).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reassignment tool sets throttles incorrectly when overriding a reassignment,KAFKA-10779,13343433,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dengziming,hachikuji,hachikuji,01/Dec/20 01:28,07/Jan/21 23:11,13/Jul/23 09:17,07/Jan/21 23:11,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"The logic in `ReassignPartitionsCommand.calculateProposedMoveMap` assumes that adding replicas are not included in the replica set returned from `Metadata` or `ListPartitionReassignments`.  This is evident in the test case `ReassignPartitionsUnitTest.testMoveMap`. Because of this incorrect assumption, the move map is computed incorrectly which can result in the wrong throttles being applied. As far as I can tell, this is only an issue when overriding an existing reassignment. ",,chia7712,dengziming,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-01 01:28:05.0,,,,,,,,,,"0|z0l2k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stronger log fencing after write failure,KAFKA-10778,13343398,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,hachikuji,hachikuji,30/Nov/20 19:50,06/Jan/21 18:07,13/Jul/23 09:17,06/Jan/21 18:07,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"If a log append operation fails with an IO error, the broker attempts to fail the log dir that it resides in. Currently this is done asynchronously, which means there is no guarantee that additional appends won't be attempted before the log is fenced. This can be a problem for EOS because of the need to maintain consistent producer state.

1. Iterate through batches to build producer state and collect completed transactions
2. Append the batches to the log 
3. Update the offset/timestamp indexes
4. Update log end offset
5. Apply individual producer state to `ProducerStateManager`
6. Update the transaction index
7. Update completed transactions and advance LSO

One example of how this process can go wrong is if the index updates in step 3 fail. In this case, the log will contain updated producer state which has not been reflected in `ProducerStateManager`. If the append is retried before the log is fenced, then we can have duplicates. There are probably other potential failures that are possible as well.

I'm sure we can come up with some way to fix this specific case, but the general fencing approach is slippery enough that we'll have a hard time convincing ourselves that it handles all potential cases. It would be simpler to add synchronous fencing logic for the case when an append fails due to an IO error. For example, we can mark a flag to indicate that the log is closed for additional read/write operations.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-30 19:50:52.0,,,,,,,,,,"0|z0l2c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX metric RequestsPerSec requires API version to access,KAFKA-10776,13343201,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,badai,badai,30/Nov/20 01:44,25/Dec/20 02:24,13/Jul/23 09:17,25/Dec/20 02:24,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"JMX metric for ""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"" seems to require the API version by adding ""version=8"" at the end of JMX metric name.
{noformat}
badai@Badai-Aqrandista-MBP15 % bin/kafka-run-class kafka.tools.JmxTool --jmx-url service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi -object-name kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce           Trying to connect to JMX url: service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi. No matched attributes for the queried objects ArrayBuffer(kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce).
{noformat}
 
{noformat}
badai@Badai-Aqrandista-MBP15 % bin/kafka-run-class kafka.tools.JmxTool --jmx-url service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi -object-name kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8
Trying to connect to JMX url: service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi.
""time"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:Count"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:EventType"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:FifteenMinuteRate"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:FiveMinuteRate"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:MeanRate"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:OneMinuteRate"",""kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce,version=8:RateUnit""
1606699872861,6,requests,0.1989162534702867,0.19690572377187665,0.15294943373697226,0.1883600179350904,SECONDS
1606699874859,6,requests,0.19781422719176484,0.19365115842372535,0.14552410136055885,0.1732995824406591,SECONDS
1606699876860,8,requests,0.19781422719176484,0.19365115842372535,0.18505354392836157,0.1732995824406591,SECONDS
1606699878863,8,requests,0.19893436710436987,0.19706180478057453,0.1768573689590503,0.19142554703039305,SECONDS
^C{noformat}
 

While other JMX metric under RequestMetrics do not require to specify API version:
{noformat}
badai@Badai-Aqrandista-MBP15 confluent-5.5.1 % bin/kafka-run-class kafka.tools.JmxTool --jmx-url service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi -object-name kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce    Trying to connect to JMX url: service:jmx:rmi:///jndi/rmi://127.0.0.1:9999/jmxrmi. ""time"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:50thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:75thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:95thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:98thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:999thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:99thPercentile"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:Count"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:Max"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:Mean"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:Min"",""kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce:StdDev"" 1606700344429,2.0,2.0,4.699999999999989,31.279999999999745,46.0,46.0,72,46.0,2.638888888888889,1.0,5.436718033280366 1606700346433,2.0,2.0,4.699999999999989,31.279999999999745,46.0,46.0,72,46.0,2.638888888888889,1.0,5.436718033280366 
^C                                             
{noformat}
 

This is definitely not documented here: [https://kafka.apache.org/documentation/#monitoring]

 

I think ""version=X"" part in  ""RequestsPerSec"" is a mistake and should be removed.

 ",,badai,ijuma,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 03:11:01 UTC 2020,,,,,,,,,,"0|z0l14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 02:34;ijuma;See https://cwiki.apache.org/confluence/display/KAFKA/KIP-272%3A+Add+API+version+tag+to+broker%27s+RequestsPerSec+metric;;;","30/Nov/20 02:42;showuon;[~ijuma], you're right! This is actually a documentation miss, the version attribute is required to match the Kafka version. Thanks.;;;","30/Nov/20 03:11;badai;Ok. Thanks for pointing that out Ismael.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task already exists error on same worker due to skip removal of tasks,KAFKA-10763,13342302,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,barten,barten,24/Nov/20 08:37,26/Jan/21 20:30,13/Jul/23 09:17,26/Jan/21 20:30,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.2,2.6.2,2.7.1,2.8.0,,,KafkaConnect,,,,,0,,,,,"In our production environment, upon start two KafkaConnect workers, during the first couple of minutes, the leader bounces between worker1 and worker2. And a lot of tasks throw Task already exists in this worker exception on worker2.

The sequence of events:

worker2(hostname:sinkdp2)

gen3 assign
 Start task 1

gen4 assign task 1

gen5 assign task 1

gen6 skip stopping task 1 and removal due to rebalance unresolved
 revoke

gen7 assign task 1
 Start task 1(Task already exists eror)

 

Worker1(hostname: sinkdp1)
{code:java}
03:36:07,340 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:36:10,460 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 1 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-a5790b31-6890-4958-905d-d44be9e18842', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:36:10,694 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Starting task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-5][DistributedHerder.java:1073]
03:36:10,979 [INFO ] Instantiated task dp-hive-sink-connector-dptask_475_22-0 with version 0.14.0-SNAPSHOT of type com.datapipeline.sink.connector.hive.HiveConnectorTask   [pool-9-thread-5][Worker.java:426]
03:36:37,692 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:36:37,806 [INFO ] Stopping task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-5][Worker.java:702]
03:40:09,721 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Finished stopping tasks in preparation for rebalance   [DistributedHerder-connect-1][DistributedHerder.java:1502]
03:40:09,722 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 2 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-a5790b31-6890-4958-905d-d44be9e18842', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_599_20, dp-tidb-connector-dptask_580_13, dp-hive-sink-connector-dpta
03:40:09,722 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:41:10,650 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks   [DistributedHerder-connect-1][DistributedHerder.java:1517]
03:41:10,650 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 4 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-ff52e9fd-c96c-4e50-93cf-25cf2ae430a4', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[], taskIds=[], revokedConnectorIds=[dp-hive-sink-connector-dptask_599_20, dp-tidb-connector-dptask
03:41:10,651 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:42:10,815 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 5 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-5a0dbfd9-10d8-42dd-8efd-52ac6ba81e17', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:42:10,953 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Starting task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-8][DistributedHerder.java:1073]
03:42:10,953 [INFO ] Instantiated task dp-hive-sink-connector-dptask_475_22-0 with version 0.14.0-SNAPSHOT of type com.datapipeline.sink.connector.hive.HiveConnectorTask   [pool-9-thread-8][Worker.java:426]
03:42:29,429 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:43:28,336 [INFO ] Stopping task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-2][Worker.java:702]
03:46:05,804 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Finished stopping tasks in preparation for rebalance   [DistributedHerder-connect-1][DistributedHerder.java:1502]
03:46:05,806 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 6 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-5a0dbfd9-10d8-42dd-8efd-52ac6ba81e17', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_599_20, dp-tidb-connector-dptask_580_13, dp-hive-sink-connector-dpta
03:46:05,806 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:47:06,564 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks   [DistributedHerder-connect-1][DistributedHerder.java:1517]
{code}
Worker2 (hostname: sinkdp2)
{code:java}
03:36:35,984 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:36:37,780 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 2 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-a5790b31-6890-4958-905d-d44be9e18842', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [Dist
03:37:40,789 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:37:40,916 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 3 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-ff52e9fd-c96c-4e50-93cf-25cf2ae430a4', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:37:41,151 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Starting task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-1][DistributedHerder.java:1073]
03:37:41,507 [INFO ] Instantiated task dp-hive-sink-connector-dptask_475_22-0 with version 0.14.0-SNAPSHOT of type com.datapipeline.sink.connector.hive.HiveConnectorTask   [pool-9-thread-1][Worker.java:426]
03:40:13,254 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:42:27,376 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Finished stopping tasks in preparation for rebalance   [DistributedHerder-connect-1][DistributedHerder.java:1502]
03:42:27,377 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 4 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-ff52e9fd-c96c-4e50-93cf-25cf2ae430a4', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:42:27,378 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:43:28,190 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks   [DistributedHerder-connect-1][DistributedHerder.java:1517]
03:43:28,191 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 6 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-5a0dbfd9-10d8-42dd-8efd-52ac6ba81e17', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[], taskIds=[], revokedConnectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-d
03:43:28,191 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:44:28,358 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 7 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-8494a329-d5a8-443c-8c90-ba710cc44c2d', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:44:28,692 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Starting task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-7][DistributedHerder.java:1073]
kafka.connect.errors.ConnectException: Task already exists in this worker: dp-hive-sink-connector-dptask_475_22-0
03:46:07,401 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:48:07,024 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Finished stopping tasks in preparation for rebalance   [DistributedHerder-connect-1][DistributedHerder.java:1502]
03:48:07,246 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 8 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-8494a329-d5a8-443c-8c90-ba710cc44c2d', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpta
03:48:07,246 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:49:07,446 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks   [DistributedHerder-connect-1][DistributedHerder.java:1517]
03:49:07,446 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 10 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-2a498edc-a517-457b-b982-c84cc9ff4521', leaderUrl='http://sinkdp1:8083/', offset=6457, connectorIds=[], taskIds=[], revokedConnectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-
03:49:07,447 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Rebalance started   [DistributedHerder-connect-1][WorkerCoordinator.java:233]
03:50:07,677 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Joined group at generation 11 with protocol version 1 and got assignment: Assignment{error=0, leader='connect-1-d0be6a23-8dfa-4289-a818-8a655effa48d', leaderUrl='http://sinkdp2:8083/', offset=6457, connectorIds=[dp-hive-sink-connector-dptask_475_22, dp-hive-sink-connector-dptask_366_20, dp-tidb-connector-dpt
03:50:08,079 [INFO ] [Worker clientId=connect-1, groupId=group_connect_sink_dp] Starting task dp-hive-sink-connector-dptask_475_22-0   [pool-9-thread-3][DistributedHerder.java:1073]
kafka.connect.errors.ConnectException: Task already exists in this worker: dp-hive-sink-connector-dptask_475_22-0
{code}",,barten,iBlackeyes,rng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-24 08:37:54.0,,,,,,,,,,"0|z0kvlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams consuming from a pattern goes to PENDING_SHUTDOWN when adding a new topic,KAFKA-10758,13341941,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,davideicardi,davideicardi,21/Nov/20 21:36,30/Nov/20 19:13,13/Jul/23 09:17,25/Nov/20 02:25,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"I have a simple Kafka Stream app that consumes from multiple input topics using the _stream_ function that accepts a Pattern ([link|https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/StreamsBuilder.html#stream-java.util.regex.Pattern-]).
 
Whenever I add a new topic that matches the pattern the kafka stream state goes to REBALANCING -> ERROR -> PENDING_SHUTDOWN .
If I restart the app it correctly starts reading again without problems.
It is by design? Should I handle this and simply restart the app?
 
Kafka Stream version is 2.6.0.
The error is the following:
{code:java}
ERROR o.a.k.s.p.i.ProcessorTopology - Set of source nodes do not match:
sourceNodesByName = [KSTREAM-SOURCE-0000000003, KSTREAM-SOURCE-0000000002]
sourceTopicsByName = [KSTREAM-SOURCE-0000000000, KSTREAM-SOURCE-0000000014, KSTREAM-SOURCE-0000000003, KSTREAM-SOURCE-0000000002]
org.apache.kafka.common.KafkaException: User rebalance callback throws an error
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:436)
  at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:440)
  at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:513)
  at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1268)
  at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1230)
  at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210)
  at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:766)
  at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:624)
  at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:551)
  at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:510)
 Caused by: java.lang.IllegalStateException: Tried to update source topics but source nodes did not match
  at org.apache.kafka.streams.processor.internals.ProcessorTopology.updateSourceTopics(ProcessorTopology.java:151)
  at org.apache.kafka.streams.processor.internals.AbstractTask.update(AbstractTask.java:109)
  at org.apache.kafka.streams.processor.internals.StreamTask.update(StreamTask.java:514)
  at org.apache.kafka.streams.processor.internals.TaskManager.updateInputPartitionsAndResume(TaskManager.java:397)
  at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:261)
  at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1428)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:279)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:421)
  ... 10 common frames omitted
 KafkaStream state is ERROR
 17:28:53.200 [datalake-StreamThread-1] ERROR o.apache.kafka.streams.KafkaStreams - stream-client [datalake] All stream threads have died. The instance will be in error state and should be closed.
 ============> User rebalance callback throws an error
 KafkaStream state is PENDING_SHUTDOWN
{code}
 
 ",,ableegoldman,cadonna,davideicardi,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 18:16:46 UTC 2020,,,,,,,,,,"0|z0kte0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 10:21;cadonna;[~davideicardi] Thank you for the report!

Could you please also post your call to {{StreamsBuilder#stream(Pattern)}} and your topology description that you can get with {{topology.describe().toString()}}?;;;","23/Nov/20 14:34;davideicardi;[~cadonna] here the details requested:

I have this code:

 
{code:java}
private val inputCommandsStream =
    streamsBuilder.stream[Key, Envelop[RawDataCommand]](Pattern.compile(""^ingestion\\.datalake\\..+\\..+\\.commands$""))
private val inputEventStream =
    streamsBuilder.stream[Key, Envelop[RawDataEvent]](Pattern.compile(""^ingestion\\.datalake\\..+\\..+\\.events""))
{code}
And here the topology:

 
{code:java}
Topologies:
   Sub-topology: 0
    Source: KSTREAM-SOURCE-0000000000 (topics: [ingestion.datasources.events])
      --> KSTREAM-PROCESSOR-0000000001
    Processor: KSTREAM-PROCESSOR-0000000001 (stores: [])
      --> none
      <-- KSTREAM-SOURCE-0000000000  Sub-topology: 1
    Source: KSTREAM-SOURCE-0000000002 (topics: ^ingestion\.datalake\..+\..+\.commands$)
      --> KSTREAM-LEFTJOIN-0000000006
    Processor: KSTREAM-LEFTJOIN-0000000006 (stores: [ingestion.datalake.store.snapshots])
      --> KSTREAM-MAP-0000000010, KSTREAM-SINK-0000000007
      <-- KSTREAM-SOURCE-0000000002
    Source: KSTREAM-SOURCE-0000000003 (topics: ^ingestion\.datalake\..+\..+\.events)
      --> KSTREAM-FILTER-0000000004
    Processor: KSTREAM-FILTER-0000000004 (stores: [])
      --> KSTREAM-AGGREGATE-0000000005
      <-- KSTREAM-SOURCE-0000000003
    Processor: KSTREAM-AGGREGATE-0000000005 (stores: [ingestion.datalake.store.snapshots])
      --> KTABLE-TOSTREAM-0000000008
      <-- KSTREAM-FILTER-0000000004
    Processor: KSTREAM-MAP-0000000010 (stores: [])
      --> KSTREAM-FILTER-0000000013
      <-- KSTREAM-LEFTJOIN-0000000006
    Processor: KSTREAM-FILTER-0000000013 (stores: [])
      --> KSTREAM-SINK-0000000012
      <-- KSTREAM-MAP-0000000010
    Processor: KTABLE-TOSTREAM-0000000008 (stores: [])
      --> KSTREAM-SINK-0000000009
      <-- KSTREAM-AGGREGATE-0000000005
    Sink: KSTREAM-SINK-0000000007 (extractor class: service.streaming.EventStreamTopicNameExtractor@20801cbb)
      <-- KSTREAM-LEFTJOIN-0000000006
    Sink: KSTREAM-SINK-0000000009 (extractor class: service.streaming.SnapshotStreamTopicNameExtractor@1c240cf2)
      <-- KTABLE-TOSTREAM-0000000008
    Sink: KSTREAM-SINK-0000000012 (topic: KSTREAM-TOTABLE-0000000011-repartition)
      <-- KSTREAM-FILTER-0000000013  Sub-topology: 2
    Source: KSTREAM-SOURCE-0000000014 (topics: [KSTREAM-TOTABLE-0000000011-repartition])
      --> KSTREAM-TOTABLE-0000000011
    Processor: KSTREAM-TOTABLE-0000000011 (stores: [ingestion.datalake.store.eventsByMsgId])
      --> none
      <-- KSTREAM-SOURCE-0000000014

{code}
 

It is a work in progress, for sure to be optimized, but I don't understand the reason for the error.

 

thanks!;;;","23/Nov/20 14:36;davideicardi;Also, to be more precise, the error doesn't happen exactly when I add the topic. But I think it happens when the app try to refresh the topics information. I think every 5 mins by default. Right?;;;","24/Nov/20 01:33;ableegoldman;Hey [~davideicardi], thanks for submitting this ticket. Looks like there's a bug in the topic update logic. I've opened a PR which we should be able to get into the 2.6.1 and 2.7.0 releases.

Just to answer some of your other questions: yes, this error wouldn't appear right after the topic was created but only once the Streams app refreshed its topic metadata (5min default as you said). Yes, it should be safe to just restart the application after hitting this error as a workaround. And no, this was not by design :) 

Sorry for the trouble. Obviously I'd recommend upgrading to 2.6.1 to get the fix once it's been released, but for now you should be ok to just ignore it and start up the application again. You'll only hit this error once, since after the restart there's no need to update anything;;;","24/Nov/20 18:16;davideicardi;[~ableegoldman] Thank you very much!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KAFKA-10755 brings a compile error ,KAFKA-10757,13341884,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dengziming,dengziming,dengziming,21/Nov/20 04:18,21/Nov/20 19:32,13/Jul/23 09:17,21/Nov/20 10:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,The `new TaskManager` has 10 params but StreamThreadTest call a `new StreamThreadTest` with 9 params.,,dengziming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-21 04:18:52.0,,,,,,,,,,"0|z0kt1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should consider commit latency when computing next commit timestamp,KAFKA-10755,13341871,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,mjsax,mjsax,20/Nov/20 22:55,03/Feb/21 01:34,13/Jul/23 09:17,21/Nov/20 03:16,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"In 2.6, we reworked the main processing/commit loop in `StreamThread` and introduced a regression, by _not_ updating the current time after committing. This implies that we compute the next commit timestamp too low (ie, too early).

For small commit intervals and high commit latency (like in EOS), this big may lead to an increased commit frequency and fewer processed records between two commits, and thus to reduced throughput.

For example, assume that the commit interval is 100ms and the commit latency is 50ms, and we start the commit at timestamp 10000. The commit finishes at 10050, and the next commit should happen at 10150. However, if we don't update the current timestamp, we incorrectly compute the next commit time as 10100, ie, 50ms too early, and we have only 50ms to process data instead of the intended 100ms.

In the worst case, if the commit latency is larger than the commit interval, it would imply that we commit after processing a single record per task.",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12272,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 19:32:34 UTC 2020,,,,,,,,,,"0|z0ksyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/20 03:16;mjsax;Cherry-picked to `2.6` branch, but don't set fix version `2.6.1` yet, as we don't know if there will be a new RC that will contain the fix. \cc [~mimaison]

This must be cherry-picked to `2.7` after the `2.7.0` release is finished, and fixed version should be updated to include `2.7.1`. \cc [~bbejeck] (just FYI; I try to take of it myself);;;","23/Nov/20 19:32;mjsax;After reconsideration, I think we should get this into 2.7.0 release, too. \cc [~bbejeck];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka clients throw AuthenticationException during Kerberos re-login,KAFKA-10727,13340833,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,16/Nov/20 13:05,05/May/21 15:39,13/Jul/23 09:17,23/Nov/20 09:07,,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.3,2.7.2,2.8.0,,,,,,,,,,0,,,,,"During Kerberos re-login, we log out and login again. There is a timing issue where the principal in the Subject has been cleared, but a new one hasn't been populated yet. We need to ensure that we don't throw AuthenticationException in this case to avoid Kafka clients (consumer/producer etc.) failing instead of retrying.",,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-16 13:05:08.0,,,,,,,,,,"0|z0kmk0:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Command to run single quorum in raft is missing ""--config"" parameters.",KAFKA-10724,13340642,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,huldar,huldar,14/Nov/20 15:21,04/Feb/22 18:35,13/Jul/23 09:17,04/Feb/22 18:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,docs,,,,0,kip-500,,,,"When I run ""bin/test-raft-server-start.sh config/raft.properties"", I get an error：
[2020-11-14 23:00:38,742] ERROR Exiting Kafka due to fatal exception (kafka.tools.TestRaftServer$)
org.apache.kafka.common.config.ConfigException: Missing required configuration ""zookeeper.connect"" which has no default value.
 at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:478)
 at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:468)
 at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:108)
 at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:142)
 at kafka.server.KafkaConfig.<init>(KafkaConfig.scala:1314)
 at kafka.server.KafkaConfig.<init>(KafkaConfig.scala:1317)
 at kafka.tools.TestRaftServer$.main(TestRaftServer.scala:607)
 at kafka.tools.TestRaftServer.main(TestRaftServer.scala)

The correct command is “ ./bin/test-raft-server-start.sh --config ./config/raft.properties”",,cmccabe,huldar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 04 18:34:55 UTC 2022,,,,,,,,,,"0|z0kldk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/22 18:34;cmccabe;The command has been renamed to {{raft/bin/test-kraft-server-start.sh}}, and it does have a {{--config}} flag now:

{code}
raft/bin/test-kraft-server-start.sh
Standalone raft server for performance testing
Option                                  Description                           
------                                  -----------                           
--config <String: filename>             Required configured file              
--help                                  Print usage information.              
--record-size <Integer: size in bytes>  The size of each record (default: 256)
--throughput <Integer: records/sec>     The number of records per second the  
                                          leader will write to the metadata   
                                          topic (default: 5000)               
--version                               Display Kafka version.                
{code}

(To be clear, this is just an internal testing tool, not something that end-users should use)

Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogManager leaks internal thread pool activity during shutdown,KAFKA-10723,13340608,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kprakasam,kprakasam,kprakasam,13/Nov/20 23:53,19/Nov/20 18:55,13/Jul/23 09:17,19/Nov/20 18:55,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"*TL;DR:*

The asynchronous shutdown in {{LogManager}} has the shortcoming that if during shutdown any of the internal futures fail, then we do not always ensure that all futures are completed before {{LogManager.shutdown}} returns. As a result, despite the shut down completed message from KafkaServer is seen in the error logs, some futures continue to run from inside LogManager attempting to close the logs. This is misleading and it could possibly break the general rule of avoiding post-shutdown activity in the Broker.

*Description:*

When LogManager is shutting down, exceptions in log closure are handled [here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogManager.scala#L497-L501]. However, this [line|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogManager.scala#L502] in the finally clause shuts down the thread pools *asynchronously*. The code: _threadPools.foreach(.shutdown())_ initiates an orderly shutdown (for each thread pool) in which previously submitted tasks are executed, but no new tasks will be accepted (see javadoc link [here|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ExecutorService.html#shutdown()])_._ As a result, if there is an exception during log closure, some of the thread pools which are closing logs could be leaked and continue to run in the background, after the control returns to the caller (i.e. {{KafkaServer}}). As a result, even after the ""shut down completed"" message is seen in the error logs (originating from {{KafkaServer}} shutdown sequence), log closures continue to happen in the background, which is misleading.
  

*Proposed options for fixes:*

It seems useful that we maintain the contract with {{KafkaServer}} that after {{LogManager.shutdown}} is called once, all tasks that close the logs are guaranteed to have completed before the call returns. There are probably couple different ways to fix this:
 # Replace {{_threadPools.foreach(.shutdown())_ with _threadPools.foreach(.awaitTermination())_}}{{.}} This ensures that we wait for all threads to be shutdown before returning the {{_LogManager.shutdown_}} call.
 # Skip creating of checkpoint and clean shutdown file only for the affected directory if any of its futures throw an error. We continue to wait for all futures to complete for all directories. This can require some changes to [this for loop|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogManager.scala#L481-L496], so that we wait for all futures to complete regardless of whether one of them threw an error.",,Hangleton,junrao,kprakasam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 18:55:00 UTC 2020,,,,,,,,,,"0|z0kl60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 18:55;junrao;merged to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamped store is used even if not desired,KAFKA-10722,13340602,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,fml2,fml2,fml2,13/Nov/20 22:26,23/Dec/20 23:53,13/Jul/23 09:17,23/Dec/20 23:53,2.4.1,2.6.0,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"I have a stream which I then group and aggregate (this results in a KTable). When aggregating, I explicitly tell to materialize the result table using a usual (not timestamped) store.

After that, the KTable is filtered and streamed. This stream is processed by a processor that accesses the store.

The problem/bug is that even if I tell to use a non-timestamped store, a timestamped one is used, which leads to a ClassCastException in the processor (it iterates over the store and expects the items to be of type ""KeyValue"" but they are of type ""ValueAndTimestamp"").

Here is the code (schematically).

First, I define the topology:
{code:java}
KTable table = ...aggregate(
  initializer, // initializer for the KTable row
  aggregator, // aggregator
  Materialized.as(Stores.persistentKeyValueStore(""MyStore"")) // <-- Non-Timestamped!
    .withKeySerde(...).withValueSerde(...));

table.toStream().process(theProcessor);
{code}
In the class for the processor:
{code:java}
public void init(ProcessorContext context) {
   var store = context.getStateStore(""MyStore""); // Returns a TimestampedKeyValueStore!
}
{code}
A timestamped store is returned even if I explicitly told to use a non-timestamped one!

 

I tried to find the cause for this behaviour and think that I've found it. It lies in this line: [https://github.com/apache/kafka/blob/cfc813537e955c267106eea989f6aec4879e14d7/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java#L241]

There, TimestampedKeyValueStoreMaterializer is used regardless of whether materialization supplier is a timestamped one or not.

I think this is a bug.

 ",,ableegoldman,fml2,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 23:53:16 UTC 2020,,,,,,,,,,"0|z0kl4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/20 00:57;mjsax;[~fml2], what you describe is not a bug but it's by design. Note, that technically, the `aggregate()` operator requires a timestamped key-value store. The only reason why we allow you to pass in a plain key-value store is for backward compatibility reasons. Before we introduced timestamped key-value store, users might have written code like yours, and we needed to make sure to not break their code when they upgrade.

If we would have designed the API ""from scratch"" your code would not be valid and we would enforce that you pass a timestamped key-value store into `aggregate()`.

Does this make sense?;;;","14/Nov/20 09:17;fml2;Hello, thank you [~mjsax] for the quick response! This perfectly makes sense. Could you please explain why the aggregate operation requires a timestamped store? The operation is not windowed if I understand correctly. It it was windowed (by time), then I'd understand it. Could you please explain? Why is the timestamp needed?

And one more thing (proposal): if such usage is discouraged, wouldn't it make sense to log some warning that such usage is not good anymore? Or make the API deprecated so that IDEs warn the developers.

Because, as of now, it mesleads to wrong usage.;;;","14/Nov/20 10:29;fml2;Matthias, does your response mean that we *always* get a timestamped store? Now, since I'm not sure what I get, I wrote the code that can handle both (unwrap the ValueAndTimestamp to get the data). WIth that knowledge I could simplify the code. Tahnk you.;;;","16/Nov/20 18:42;mjsax;Every record in Kafka Streams has a timestamp, and aggregate() needs to set a timestamp for its output records. It computes the output record timestamps as ""max"" over all input records. That is why it needs a timestamped key-value store to track the maximum timestamp.

Unfortunately, we cannot deprecate the API easily because of Java type erasure... I guess we could log a warn message thought... Feel free to do a PR for it. We could log when we create the `KeyValueToTimestampedKeyValueByteStoreAdapter`.

And yes, you always get a timestamped key-value store and you can simplify your code accordingly. (Note thought, that if you provide a non-timestamped store, the timestamp won't really be stored, because the above mentioned adapter will just drop the timestamp before storing the data in the provided store – on read, the adapter will just set `-1`, ie, unknown, as timestamp.)

Thus, I would actually recommend to pass in a timestamped key-value store to begin with.  On the other hand, why do you pass in a store at all? I seem you actually only want to set a name (to be able to access the store from the other `Processor`) what you can do via `Materialized.as(""MyStore"")` – passing in a `StoreSupplier` should be used if you want to pass in your own custom store implementation. As you create the store using `Stores` anyway, you can just let KS DSL create the store for you.

I am also open to improve our docs, to point out this issue better. Atm, it seem we only documented in the upgrade guide when the feature was added: [https://kafka.apache.org/26/documentation/streams/upgrade-guide#streams_api_changes_230];;;","16/Nov/20 21:58;fml2;OK, I accept that Kafka Streams needs timestamps for the internal processing. But I still fail to see why all the users (clients) are imposed to use it. It adds an additional (I assume, rarely needed) wrapping layer (`ValueAndTimestamp` with the value within in being a `KeyValues` vs. just `KeyValue`). But OK, I accepti it.

I can't see why a method can't be deprecated. Deprecation does not change the API, the code will still work. Just the IDE will issue a warning alert if the method is used.

And you are of course right that I can just use `Materialized.as(""MyStore"")`. Actually this is what I did first. And got a `ClassCastException`. And started to investigate the case. And wanted to guarantee that I get a non-timestamped store – but could not get it. And hence this ticket :).

I also saw the upgrade note for 2.3.0. I understood the words ""Some DSL operators (for example KTables) are using those new stores."" as ""they can use them"" or ""they use them internally"" – but not as ""you will always get the new store type and should unwrap timestamped values"". And ""you might need to update your code to cast to the correct type"" did not sound very obligatory too.

Would it make sense to introduce the method ""value()"" (or similar) that would return the real data – both for a `KeyValue` and a `ValueAndTimestamp`? This would be confusing for `ValueAndTimestamp` though since `value` (the field) would return a `KeyValue` but `value()` (the method) would return the value part of the KeyValue.

Another note is that I could not find the explanation of the values of timestamps used in Kafka Streams. I found out this is a millis epoch. But, judging just by the type, it could have been the nano epoch. Using e.g. `Instance` would eliminate the question. But this is spread over so many places that I assume a change is not possible. Besides, this is another topic.

Thank you for your replies!;;;","17/Nov/20 16:05;mjsax;{quote}But I still fail to see why all the users (clients) are imposed to use it.
{quote}
Not sure why you say this? Note, that for your case, you try to get access to an ""existing"" (ie, automatically added) store by a different processor. If you would add a new store (that is not used by – and not added by – the aggergate() operator) you can also use a plain key-value store.
{quote}I can't see why a method can't be deprecated. Deprecation does not change the API, the code will still work. Just the IDE will issue a warning alert if the method is used.
{quote}
Well, of course a method could be deprecated, however, we would need to add a new one _with different name_. We cannot overload the method due to type erasure. And renaming the method is tricky... (naming is hard... :))
{quote}I also saw the upgrade note for 2.3.0. I understood the words ""Some DSL operators (for example KTables) are using those new stores."" as ""they can use them"" or ""they use them internally"" – but not as ""you will always get the new store type and should unwrap timestamped values"". And ""you might need to update your code to cast to the correct type"" did not sound very obligatory too.
{quote}
Thanks for the feedback! Can you help us improve the docs and open a PR?
{quote}Would it make sense to introduce the method ""value()"" (or similar) that would return the real data – both for a `KeyValue` and a `ValueAndTimestamp`? This would be confusing for `ValueAndTimestamp` though since `value` (the field) would return a `KeyValue` but `value()` (the method) would return the value part of the KeyValue.
{quote}
Not sure. But this would be public API change and would require a KIP. Thus, we would discuss on the KIP.
{quote}Another note is that I could not find the explanation of the values of timestamps used in Kafka Streams. I found out this is a millis epoch. But, judging just by the type, it could have been the nano epoch. Using e.g. `Instance` would eliminate the question. But this is spread over so many places that I assume a change is not possible. Besides, this is another topic.
{quote}
We use plain `long` for performance reasons. Many public APIs that are not at the critical code path got already migrated from `long` to `Duration` and `Instant`. However, for the ""runtime code"" that is on the hot code path, there are not plans to move off `long`.;;;","17/Nov/20 16:47;fml2;{quote}If you would add a new store (that is not used by – and not added by – the aggergate() operator) you can also use a plain key-value store.
{quote}
Ah, that's the clue! I use something the system provides and uses for its internal purposes, and hence have to take what's there. This is a good explanation, I'm fully satisfied now :)

Where do you think the note about timestamped stores should be placed? I found two candidates:
 * [https://kafka.apache.org/26/documentation/streams/core-concepts#streams_state]
 * [https://kafka.apache.org/26/documentation/streams/developer-guide/processor-api.html#id3]

I'd write something like this: Note that some stream operations, e.g. `aggregate()`, need a state store for doing their work. You can define the name of the store, but not its type. It will always be a `TimestampedKeyValueStore`.

You can close the ticket if you wish.;;;","17/Nov/20 17:06;mjsax;{quote}I'm fully satisfied now 
{quote}
Glad to hear :)

About the docs: I would not put it into the ""core concepts"" sections, as this section talks about state stores only in a more abstract/conceptual way.

For the ""Processor API"" section, I am not sure either. In general, the Processor API allows you to add any state store you want... It's the DSL that uses the timestamped store. Thus, it seem the best place would be [https://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#stateful-transformations]? (1) non-windowed aggregations and plain KTables use timestamped kv-store (2) time-windowed aggregations and kstream-kstream joins use timestamped window stores (3) session windowed aggregations use plain session stores (there is no timestamped session store atm).

If you do a PR, please use this ticket as reference. We can close the ticket after we merged the PR. Thanks a lot!;;;","17/Nov/20 21:50;fml2;I've created [https://github.com/apache/kafka/pull/9606] and [https://github.com/apache/kafka/pull/9607].;;;","23/Dec/20 23:53;mjsax;Added you to the list of contributors and assigned the ticket to you. You can now also self-assign tickets.

Thanks for reporting the issue and for helping to improve the JavaDocs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams processId is unstable across restarts resulting in task mass migration,KAFKA-10716,13340393,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ableegoldman,ableegoldman,ableegoldman,13/Nov/20 01:41,05/Feb/21 20:20,13/Jul/23 09:17,05/Feb/21 20:20,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,,,,,"The new high availability feature of KIP-441 relies on deterministic assignment to produce an eventually-stable assignment. The HighAvailabilityTaskAssignor assigns tasks based on the unique processId assigned to each client, so if the same set of Kafka Streams applications participate in a rebalance it should generate the same task assignment every time.

Unfortunately the processIds aren't stable across restarts. We generate a random UUID in the KafkaStreams constructor, so each time the process starts up it would be assigned a completely different processId. Unless this new processId happens to be in exactly the same order as the previous one, a single bounce or crash/restart can result in a large scale shuffling of tasks based on a completely different eventual assignment.

Ultimately we should fix this via KAFKA-10121, but that's a nontrivial undertaking and this bug merits some immediate relief if we don't intend to tackle the larger problem in the upcoming releases ",,ableegoldman,mjsax,o0oxid,thebearmayor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10678,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 02:45:18 UTC 2020,,,,,,,,,,"0|z0kju8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 02:45;ableegoldman;There are a few possible ways forward here:

1) generate the processId from the client.id config, if specified. This requires users to set this config and ensure that it's unique to the instance
2) generate the processId from the group.instance.id, if specified. This would only work for static membership users
3) write/load the processId from the checkpoint file in task directories
4) write/load the processId from a single file in the top-level application directory

Both 1 & 2 would be simple for us to implement, but somewhat obnoxious to require of a user just for basic functionality of their app. That said, if a user already has specified either the client.id or group.instance.id, I don't see any reason _not_ to generate the processId from that. This might be a good stop-gap measure, but not a good permanent solution. However if we plan to implement KAFKA-10121 right away then maybe it's best not to mess around with options 3 or 4

Options 3 and 4 would be a bit trickier. Option 3 in particular seems to open up a lot of nasty possibilities, like the processId differing from one task directory to another, or even between threads in the same app. But Option 4 seems pretty clean: we load the processId file within the KafkaStreams constructor, and if it's not found we generate a random UUID like we do now. This would all happen before any threads are created so no need to worry about them synchronizing at all;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker 2 creates all combinations of herders,KAFKA-10710,13340163,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,Twobeeb,Twobeeb,12/Nov/20 09:39,29/Jan/21 02:35,13/Jul/23 09:17,28/Jan/21 22:53,2.5.1,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,mirrormaker,,,,,0,,,,,"We are using MM2 distributed to synchronize topics from a ""Central"" broker down to multiple ""Local"" brokers. 
{quote}replica_CENTRAL->replica_OLS.enabled = true
 replica_CENTRAL->replica_OLS.topics = _schemas
 replica_CENTRAL->replica_OLS.replication.factor = 3

replica_CENTRAL->replica_HBG.enabled = true
 replica_CENTRAL->replica_HBG.topics = _schemas
 replica_CENTRAL->replica_HBG.replication.factor = 3

...

many more

...

replica_CENTRAL->replica_VIT.enabled = true
 replica_CENTRAL->replica_VIT.topics = _schemas
 replica_CENTRAL->replica_VIT.replication.factor = 3

replica_CENTRAL->replica_UGO.enabled = true
 replica_CENTRAL->replica_UGO.topics = _schemas
 replica_CENTRAL->replica_UGO.replication.factor = 3
{quote}
 

When looking into the Mirror Maker logs, we discover that a herder is created for each combination even if we specifically don't describe a link between 2 clusters

 

Exemples:
{quote}[2020-11-12 08:43:30,351] INFO creating herder for replica_VIT->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:33,697] INFO creating herder for replica_CNO->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:36,020] INFO creating herder for replica_CENTRAL->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:38,508] INFO creating herder for replica_UMO->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:40,438] INFO creating herder for replica_CNO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:42,898] INFO creating herder for replica_ARA->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:45,457] INFO creating herder for replica_TST->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:47,860] INFO creating herder for replica_UMO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:50,248] INFO creating herder for replica_OLS->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:52,817] INFO creating herder for replica_UGO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:54,903] INFO creating herder for replica_HBG->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:57,081] INFO creating herder for replica_OLS->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:43:59,215] INFO creating herder for replica_TST->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:01,481] INFO creating herder for replica_OLS->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:03,320] INFO creating herder for replica_CNO->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:05,337] INFO creating herder for replica_VIT->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:07,494] INFO creating herder for replica_CENTRAL->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:09,595] INFO creating herder for replica_ARA->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:12,201] INFO creating herder for replica_VLD->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:14,360] INFO creating herder for replica_HBG->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:16,490] INFO creating herder for replica_OLS->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:18,680] INFO creating herder for replica_ARA->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:20,779] INFO creating herder for replica_VLD->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:22,596] INFO creating herder for replica_CENTRAL->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:24,579] INFO creating herder for replica_VIT->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:26,076] INFO creating herder for replica_TST->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:27,868] INFO creating herder for replica_OLS->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:30,047] INFO creating herder for replica_TST->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:32,295] INFO creating herder for replica_OLS->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:33,777] INFO creating herder for replica_CNO->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:36,083] INFO creating herder for replica_CENTRAL->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:38,857] INFO creating herder for replica_ARA->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:40,629] INFO creating herder for replica_CNO->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:43,170] INFO creating herder for replica_UGO->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:45,154] INFO creating herder for replica_HBG->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:46,981] INFO creating herder for replica_UMO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:49,042] INFO creating herder for replica_UGO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:51,881] INFO creating herder for replica_HBG->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:54,406] INFO creating herder for replica_HBG->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:56,378] INFO creating herder for replica_TST->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:44:58,825] INFO creating herder for replica_ARA->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:00,394] INFO creating herder for replica_CNO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:02,060] INFO creating herder for replica_UGO->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:04,513] INFO creating herder for replica_TST->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:06,071] INFO creating herder for replica_VLD->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:08,700] INFO creating herder for replica_CNO->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:10,533] INFO creating herder for replica_VIT->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:12,539] INFO creating herder for replica_CNO->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:14,749] INFO creating herder for replica_HBG->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:16,222] INFO creating herder for replica_UMO->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:18,308] INFO creating herder for replica_VIT->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:20,212] INFO creating herder for replica_HBG->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:22,618] INFO creating herder for replica_VLD->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:24,663] INFO creating herder for replica_UGO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:26,333] INFO creating herder for replica_ARA->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:28,408] INFO creating herder for replica_CNO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:30,480] INFO creating herder for replica_HBG->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:32,810] INFO creating herder for replica_VIT->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:34,904] INFO creating herder for replica_OLS->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:36,893] INFO creating herder for replica_UMO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:38,364] INFO creating herder for replica_UMO->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:40,781] INFO creating herder for replica_OLS->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:43,168] INFO creating herder for replica_VIT->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:45,215] INFO creating herder for replica_CENTRAL->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:47,305] INFO creating herder for replica_UMO->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 08:45:49,331] INFO creating herder for replica_VLD->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:16,537] INFO creating herder for replica_VIT->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:20,193] INFO creating herder for replica_CNO->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:22,410] INFO creating herder for replica_CENTRAL->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:25,009] INFO creating herder for replica_UMO->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:26,949] INFO creating herder for replica_CNO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:29,604] INFO creating herder for replica_ARA->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:32,121] INFO creating herder for replica_TST->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:34,883] INFO creating herder for replica_UMO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:37,323] INFO creating herder for replica_OLS->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:39,755] INFO creating herder for replica_UGO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:42,106] INFO creating herder for replica_HBG->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:44,425] INFO creating herder for replica_OLS->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:46,484] INFO creating herder for replica_TST->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:49,388] INFO creating herder for replica_OLS->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:51,300] INFO creating herder for replica_CNO->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:54,560] INFO creating herder for replica_VIT->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:00:57,861] INFO creating herder for replica_CENTRAL->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:00,673] INFO creating herder for replica_ARA->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:03,222] INFO creating herder for replica_VLD->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:05,627] INFO creating herder for replica_HBG->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:07,752] INFO creating herder for replica_OLS->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:09,947] INFO creating herder for replica_ARA->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:12,492] INFO creating herder for replica_VLD->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:14,265] INFO creating herder for replica_CENTRAL->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:16,102] INFO creating herder for replica_VIT->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:17,661] INFO creating herder for replica_TST->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:19,440] INFO creating herder for replica_OLS->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:22,479] INFO creating herder for replica_TST->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:24,830] INFO creating herder for replica_OLS->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:26,478] INFO creating herder for replica_CNO->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:29,179] INFO creating herder for replica_CENTRAL->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:31,616] INFO creating herder for replica_ARA->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:33,467] INFO creating herder for replica_CNO->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:35,816] INFO creating herder for replica_UGO->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:37,937] INFO creating herder for replica_HBG->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:40,195] INFO creating herder for replica_UMO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:42,318] INFO creating herder for replica_UGO->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:44,688] INFO creating herder for replica_HBG->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:47,083] INFO creating herder for replica_HBG->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:49,199] INFO creating herder for replica_TST->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:51,516] INFO creating herder for replica_ARA->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:53,042] INFO creating herder for replica_CNO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:54,575] INFO creating herder for replica_UGO->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:57,081] INFO creating herder for replica_TST->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:01:58,540] INFO creating herder for replica_VLD->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:01,584] INFO creating herder for replica_CNO->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:03,401] INFO creating herder for replica_VIT->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:05,714] INFO creating herder for replica_CNO->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:07,664] INFO creating herder for replica_HBG->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:09,246] INFO creating herder for replica_UMO->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:11,565] INFO creating herder for replica_VIT->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:13,451] INFO creating herder for replica_HBG->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:15,872] INFO creating herder for replica_VLD->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:18,094] INFO creating herder for replica_UGO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:19,597] INFO creating herder for replica_ARA->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:21,576] INFO creating herder for replica_CNO->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:23,834] INFO creating herder for replica_HBG->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:26,410] INFO creating herder for replica_VIT->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:28,516] INFO creating herder for replica_OLS->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:30,344] INFO creating herder for replica_UMO->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:31,850] INFO creating herder for replica_UMO->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:34,094] INFO creating herder for replica_OLS->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:36,743] INFO creating herder for replica_VIT->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:38,778] INFO creating herder for replica_CENTRAL->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:40,785] INFO creating herder for replica_UMO->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:42,816] INFO creating herder for replica_VLD->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:45,225] INFO creating herder for replica_VLD->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:46,659] INFO creating herder for replica_ARA->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:48,729] INFO creating herder for replica_TST->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:50,722] INFO creating herder for replica_TST->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:52,387] INFO creating herder for replica_UGO->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:54,326] INFO creating herder for replica_CENTRAL->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:56,085] INFO creating herder for replica_ARA->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:57,914] INFO creating herder for replica_VLD->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:02:59,665] INFO creating herder for replica_ARA->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:01,717] INFO creating herder for replica_OLS->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:03,832] INFO creating herder for replica_UGO->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:06,278] INFO creating herder for replica_VLD->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:08,417] INFO creating herder for replica_CENTRAL->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:10,731] INFO creating herder for replica_UGO->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:12,712] INFO creating herder for replica_TST->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:15,082] INFO creating herder for replica_VIT->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:17,439] INFO creating herder for replica_VLD->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:19,800] INFO creating herder for replica_HBG->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:21,554] INFO creating herder for replica_UMO->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:23,536] INFO creating herder for replica_VIT->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:26,041] INFO creating herder for replica_CENTRAL->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:28,090] INFO creating herder for replica_UGO->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:30,353] INFO creating herder for replica_CENTRAL->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-12 09:03:32,826] INFO creating herder for replica_UMO->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker)
{quote}
So much that we reached the limit of our user property LimitNOFILE recently when trying to add a new ""Local"" cluster.

I believe this behavior leads to unecessary connections and resource usage that could be easily avoided by just limiting the herder creation only to elements specifically described in the mirrormaker.properties file

[https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMaker.java#L130-L136]

 ",,Twobeeb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 17:04:42 UTC 2020,,,,,,,,,,"0|z0kif4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 17:04;Twobeeb;Results with fix provided in PR:
{code:java}
[2020-11-13 15:53:53,935] INFO creating herder for replica_CENTRAL->replica_ZAR (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:53:58,597] INFO creating herder for replica_CENTRAL->replica_UGO (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:01,180] INFO creating herder for replica_CENTRAL->replica_HBG (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:03,656] INFO creating herder for replica_CENTRAL->replica_OLS (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:06,097] INFO creating herder for replica_CENTRAL->replica_UMO (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:08,186] INFO creating herder for replica_OLS->replica_CENTRAL (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:09,774] INFO creating herder for replica_CENTRAL->replica_CNO (org.apache.kafka.connect.mirror.MirrorMaker)
[2020-11-13 15:54:12,253] INFO creating herder for replica_CENTRAL->replica_TST (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:14,407] INFO creating herder for replica_CENTRAL->replica_VLD (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:16,599] INFO creating herder for replica_CENTRAL->replica_ARA (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:19,005] INFO creating herder for replica_CENTRAL->replica_VIT (org.apache.kafka.connect.mirror.MirrorMaker) 
[2020-11-13 15:54:21,789] INFO Kafka MirrorMaker starting with 11 herders. (org.apache.kafka.connect.mirror.MirrorMaker){code}

 Before fix
 Main PID: 880 (java) Tasks: 2204 Memory: 2.9G
  
 Afterfix :
 Main PID: 48881 (java) Tasks: 224 Memory: 1.0G;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Liveness bug in truncation protocol can lead to indefinite URP,KAFKA-10706,13339892,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,11/Nov/20 05:54,21/Nov/20 17:56,13/Jul/23 09:17,21/Nov/20 17:56,,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.2,2.6.1,2.7.1,,,,,,,,,,0,,,,,"We hit an interesting liveness condition in the truncation protocol. Broker A was leader in epoch 7, broker B was leader in epoch 8, and then broker A was leader in epoch 9 again.

On broker A, we had the following state in the epoch cache:
{code}
epoch 4, start offset 3953
epoch 7, start offset 3983
epoch 9, start offset 3988
{code}

On broker B, we had the following:
{code}
epoch 4, start offset 3953
epoch 8, start offset 3983
{code}

After A was elected, broker B sent epoch 8 in OffsetsForLeaderEpoch. Broker A correctly responded with epoch 7 ending at offset 3988. The end offset on broker B was in fact 3983, so this truncation had no effect. Broker B then retried with epoch 8 again and replication was stuck. 

When a replica becomes leader, it first inserts an entry into the epoch cache with the current log end offset. This ensures that that it has a larger epoch in the cache than any epoch that could be requested by a valid replica. However, I think it is incorrect to turn around and use this epoch when becoming a follower. It seems like we need symmetric logic after becoming a follower to remove this epoch entry.",,hachikuji,Hangleton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-11 05:54:18.0,,,,,,,,,,"0|z0kgqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid World Readable RocksDB,KAFKA-10705,13339851,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lthomas,wcarlson5,wcarlson5,10/Nov/20 22:35,10/Jan/21 04:24,13/Jul/23 09:17,13/Nov/20 18:51,,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.1,2.8.0,,,,,,,,,,,0,streams,,,,The state directory could be protected more restrictive by preventing access to state directory for group and others. At least other should have no readable access,,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-10 22:35:31.0,,,,,,,,,,"0|z0kghs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirror maker with TLS at target,KAFKA-10704,13339747,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,yangguo1220,tbhasme,tbhasme,10/Nov/20 13:05,24/Feb/23 20:01,13/Jul/23 09:17,24/Feb/23 20:01,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mirrormaker,,,,,0,,,,,"We need to setup mirror maker from a single node kafka cluster to a three node Strimzi cluster. There is no SSL setup at source, however the target cluster is configured with MTLS.

With below config, commands from source like listing topics etc are working:
{code:java}
cat client-ssl.properties
security.protocol=SSL
ssl.truststore.location=my.truststore
ssl.truststore.password=123456
ssl.keystore.location=my.keystore
ssl.keystore.password=123456
ssl.key.password=password{code}

However, we are not able to get mirror maker working with the similar configs:
{code:java}
source.security.protocol=PLAINTEXT
target.security.protocol=SSL
target.ssl.truststore.location=my.truststore
target.ssl.truststore.password=123456
target.ssl.keystore.location=my.keystore
target.ssl.keystore.password=123456
target.ssl.key.password=password{code}
Errors while running mirror maker:
{code:java}
org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1605011994642, tries=1, nextAllowedTryMs=1605011994743) timed out at 1605011994643 after 1 attempt(s)


Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata


[2020-11-10 12:40:24,642] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)


[2020-11-10 12:40:24,643] INFO [AdminClient clientId=adminclient-8] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager:235)


org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1605012024643, tries=1, nextAllowedTryMs=-9223372036854775709) timed out at 9223372036854775807 after 1attempt(s)


Caused by: org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata


[2020-11-10 12:40:24,644] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)


[2020-11-10 12:40:24,644] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)


[2020-11-10 12:40:24,644] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)


[2020-11-10 12:40:24,645] ERROR Stopping due to error (org.apache.kafka.connect.mirror.MirrorMaker:304)


org.apache.kafka.connect.errors.ConnectException: Failed to connect to and describe Kafka cluster. Check worker's broker connection and security properties.


        at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:70)


        at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:51)


        at org.apache.kafka.connect.mirror.MirrorMaker.addHerder(MirrorMaker.java:235)


        at org.apache.kafka.connect.mirror.MirrorMaker.lambda$new$1(MirrorMaker.java:136)


        at java.lang.Iterable.forEach(Iterable.java:75)


        at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:136)


        at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:148)


        at org.apache.kafka.connect.mirror.MirrorMaker.main(MirrorMaker.java:291)


Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listNodes, deadlineMs=1605012024641, tries=1, nextAllowedTryMs=1605012024742)timed out at 1605012024642 after 1 attempt(s)


        at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)


        at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)


        at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)


        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)


        at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:64)


        ... 7 more


Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=listNodes, deadlineMs=1605012024641, tries=1, nextAllowedTryMs=1605012024742) timed out at 1605012024642 after 1 attempt(s)


Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
{code}",,tbhasme,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 05:33:12 UTC 2020,,,,,,,,,,"0|z0kfuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/20 16:34;yangguo1220;This PR [https://github.com/apache/kafka/pull/9224] actually tested out SSL case (unencrypted source cluster, but encrypted target cluster). In real world, there is use case of mirroring from unencrypted cluster to AWS hosted Kafka (always encrypted). So I believe current MirrorMaker 2 can support encryption at source or target out-of-the-box ;;;","16/Nov/20 18:10;tbhasme;Sorry, I forgot to update this bug. We were able to find the correct configuration to make it work. It would have been great if all the configurations for mm2 were well documented, maybe it is but I don't know where. I had to debug through the code to get the correct nomenclature. Correct configuration that worked for us was:
{code:java}
clusters = A, B

B.security.protocol=SSL
B.ssl.truststore.location=client.truststore
B.ssl.truststore.password=123456
B.ssl.keystore.location=client.keystore
B.ssl.keystore.password=123456
B.ssl.key.password=123456 {code};;;","18/Nov/20 05:33;yangguo1220;Indeed, we should clearly document how to produce to SSL-enabled cluster. I will create a PR against [https://github.com/apache/kafka/blob/trunk/connect/mirror/README.md] to add a section about SSL-enabled cluster;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First line of detailed stats from consumer-perf-test.sh incorrect,KAFKA-10701,13339543,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lijubjohn,mumrah,mumrah,09/Nov/20 14:51,04/Jan/21 17:24,13/Jul/23 09:17,04/Jan/21 17:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tools,,,,,0,newbie,,,,"When running the console perf test with {{--show-detailed-stats}}, the first line out of output has incorrect results

{code}
$ ./bin/kafka-consumer-perf-test.sh --bootstrap-server localhost:9092 --topic test --messages 10000000 --reporting-interval 1000 --show-detailed-stats
time, threadId, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec
2020-11-06 11:57:01:420, 0, 275.3878, 275.3878, 288765, 288765.0000, 1604681820723, -1604681819723, 0.0000, 0.0000
2020-11-06 11:57:02:420, 0, 952.1456, 676.7578, 998397, 709632.0000, 0, 1000, 676.7578, 709632.0000
2020-11-06 11:57:03:420, 0, 1654.2940, 702.1484, 1734653, 736256.0000, 0, 1000, 702.1484, 736256.0000
2020-11-06 11:57:04:420, 0, 2492.1389, 837.8448, 2613197, 878544.0000, 0, 1000, 837.8448, 878544.0000
2020-11-06 11:57:05:420, 0, 3403.2993, 911.1605, 3568618, 955421.0000, 0, 1000, 911.1605, 955421.0000
2020-11-06 11:57:06:420, 0, 4204.1540, 800.8547, 4408375, 839757.0000, 0, 1000, 800.8547, 839757.0000
2020-11-06 11:57:07:420, 0, 4747.1275, 542.9735, 4977724, 569349.0000, 0, 1000, 542.9735, 569349.0000
2020-11-06 11:57:08:420, 0, 5282.2266, 535.0990, 5538816, 561092.0000, 0, 1000, 535.0990, 561092.0000
2020-11-06 11:57:09:420, 0, 5824.3732, 542.1467, 6107298, 568482.0000, 0, 1000, 542.1467, 568482.0000
{code}

This seems to be due to incorrect initialization of the {{joinStart}} variable in the consumer perf test code.",,cuyan,lijubjohn,mumrah,quanuw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 17:24:07 UTC 2021,,,,,,,,,,"0|z0kelc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/20 06:32;quanuw;Hi, I can work on this bug if it hasn't been assigned yet.;;;","16/Nov/20 11:40;lijubjohn;Opened below pr for the fix

https://github.com/apache/kafka/pull/9598;;;","04/Jan/21 14:21;cuyan;It maybe totally the case that I'm missing something , but how come the issue is still Open, when the above PR has been merged to trunk ? Cheers.;;;","04/Jan/21 17:24;lijubjohn;PR is merged to the trunk , marking the issue resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests instantiate QuotaManagers without closing the managers in teardown,KAFKA-10693,13339246,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,david.mao,david.mao,david.mao,06/Nov/20 22:04,11/Nov/20 10:48,13/Jul/23 09:17,11/Nov/20 10:48,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,,,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-06 22:04:08.0,,,,,,,,,,"0|z0kcrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assignor can't determine number of partitions on FJK with upstream windowed repartition,KAFKA-10689,13339094,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ableegoldman,ableegoldman,ableegoldman,05/Nov/20 23:26,03/Feb/21 01:33,13/Jul/23 09:17,18/Nov/20 01:00,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,,,,,"Due to a minor logical gap in how windowed repartition sink nodes are written to the topology, they are never added to the official map of sink topics tracked by the InternalTopologyBuilder. This makes it impossible to determine the number of partitions of downstream repartition topics in StreamsPartitionAssignor#setRepartitionTopicMetadataNumberOfPartitions, causing the assignor to loop infinitely in this method. 

 ",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 23:28:27 UTC 2020,,,,,,,,,,"0|z0kbtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 23:28;ableegoldman;It's a pretty obnoxious bug, since the application stays stuck in REBALANCING while StreamThreads slowly drop out of the group one-by-one as the current group leader gets stuck and a new rebalance has to be triggered. Meanwhile we don't log anything within this loop so it's impossible to know what happened based on the logs.

Ideally we would just limit the number of iterations and shut down the application if we can't seem to figure out the number of partitions for some reason. Unfortunately, given the random way that setRepartitionTopicMetadataNumberOfPartitions walks through the topology and the lack of a ceiling on topological cycles/complexity, it's not immediately obvious how (or if) we can pick a limit on the number of necessary iterations. 

Still, we can probably improve the current situation and do better than just silently looping forever. One simple option would be to just start logging a warning once we're past some large iteration number.

Another option is to keep track of the set of repartition topics whose partitions are still unknown, and if this set fails to change over one full iteration of the outer `topicGroups.values()` loop, then break out and shut down the application. This seems pretty airtight, although obviously a bit more complicated than just logging a warning at high iteration count. The logging is probably more than sufficient for a user to debug their application, but also a worse user experience.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Produce request should be bumped for new error code PRODUCE_FENCED,KAFKA-10687,13339090,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bchen225242,bchen225242,bchen225242,05/Nov/20 22:23,19/Dec/20 01:07,13/Jul/23 09:17,18/Nov/20 22:06,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"In https://issues.apache.org/jira/browse/KAFKA-9911, we missed a case where the ProduceRequest needs to be bumped to return the new error code PRODUCE_FENCED. This gap needs to be addressed as a blocker since it is shipping in 2.7.",,bbejeck,bchen225242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 22:06:32 UTC 2020,,,,,,,,,,"0|z0kbso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/20 22:06;bbejeck;Fixed for 2.7 via https://github.com/apache/kafka/pull/9613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
--to-datetime passed to kafka-consumer-groups interpreting microseconds wrong,KAFKA-10685,13338937,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,russ_sayers,russ_sayers,05/Nov/20 05:51,17/Nov/20 19:33,13/Jul/23 09:17,17/Nov/20 19:32,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"If you pass more than 3 decimal places for the fractional seconds of the datetime, the microseconds get interpreted as milliseconds.

{{kafka-consumer-groups --bootstrap-server kafka:9092 }}
{{--reset-offsets }}
{{--group webserver-avro }}
{{--topic driver-positions-avro }}
{{ {{--to-datetime ""}}{{2020-11-05T00:46:48.002237400}}"" }}
{{ {{--dry-run}}}}

Relevant code [here|https://github.com/apache/kafka/blob/2.7/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L1304]. The datetime is being turned into Nov 5, 2020 1:24:05.400 because SimpleDateFormat is adding 2237400 milliseconds to Nov 5, 2020 00:46:48.

Experimenting with getDateTime:
 * getDateTime(""2020-11-05T00:46:48.000"") -> 1604537208000
 * getDateTime(""2020-11-05T00:46:48.000+0800"") -> 1604508408000 - correct the formatting string allows for ZZZ timezones
 * getDateTime(""2020-11-05T00:46:48.000123"") -> 1604537208123 - note this ends with 123 milliseconds.

The pattern string is ""yyyy-MM-dd'T'HH:mm:ss.SSS"".  So SimpleDateFormat interprets ""000123"" as 123 milliseconds. See the stackoverflow answer [here|https://stackoverflow.com/a/21235602/109102].

The fix?  Remove any digits after more than 3 characters after the decimal point, or raise an exception. The code would still need to allow the RFC822 timezone, i.e Sign TwoDigitHours Minutes.

 ",,mikebin,russ_sayers,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 12:04:48 UTC 2020,,,,,,,,,,"0|z0kauw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 08:29;mikebin;This issue seems to occur with Java 11, but not with Java 8. Code currently uses {{SimpleDateFormat}}. {{DateTimeFormatter}} behaved consistently across Java 8 and 11 (throwing a parse exception if milliseconds is more than 3 digits) - maybe worth considering changing to:
{code:java}
DateTimeFormatter.ofPattern(""yyyy-MM-dd'T'HH:mm:ss.SSSXXX"")
{code}
 

 ;;;","09/Nov/20 12:04;showuon;Thanks for the suggestion. It turns out that the *SimpleDateFormat.setLenient(false)* can also strictly parse the timestamp and throw parseException if milliseconds is more than 3 digits. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows Kafka cluster not reachable via Azure data Bricks,KAFKA-10682,13338886,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,sneevand,sneevand,04/Nov/20 18:32,04/Nov/20 18:53,13/Jul/23 09:17,04/Nov/20 18:53,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,consumer,,,,,0,,,,,"We have windows Kafka cluster,
 * We enabled inbound and outbound for port 9092/9093
 * Topic return results on local windows cmd used
 ** ./kafka-console-consumer.bat --topic SIP.SIP.SHIPMENT --from-beginning --bootstrap-server 10.53.56.140:9092
 * We trying to consume the topic from Azure data bricks
 ** Simple ping and telnet works fine and connects to underlying server 
 *** %sh telnet 10.53.56.140 9092
 *** %sh ping 10.53.56.140
 ** df = spark \
 .readStream \
 .format(""kafka"") \
 .option(""kafka.bootstrap.servers"", ""10.53.56.140:9092"") \
 .option(""subscribe"", ""SIP.SIP.SHIPMENT"") \
 .option(""minPartitions"", ""10"") \
 .option(""startingOffsets"", ""earliest"") \
 .load()
 #df.isStreaming() # Returns True for DataFrames that have streaming sources

df.printSchema()
 * 
 ** Display(df)

On using display command after before amount of time we got below error:

Lost connection to cluster. The notebook may have been detached or the cluster may have been terminated due to an error in the driver such as an OutOfMemoryError.

  What we see in Logs is below error

20/11/04 18:23:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-515ba67c-f265-4577-935b-5c7ba954a31d--1012371861-driver-0-5, groupId=spark-kafka-source-515ba67c-f265-4577-935b-5c7ba954a31d--1012371861-driver-0] Error connecting to node Navin.us.corp.tim.com:9092 (id: 0 rack: null)20/11/04 18:23:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-515ba67c-f265-4577-935b-5c7ba954a31d--1012371861-driver-0-5, groupId=spark-kafka-source-515ba67c-f265-4577-935b-5c7ba954a31d--1012371861-driver-0] Error connecting to node Navin.us.corp.tim.com:9092 (id: 0 rack: null)java.net.UnknownHostException: Navin.us.corp.tim.com at java.net.InetAddress.getAllByName0(InetAddress.java:1281) at java.net.InetAddress.getAllByName(InetAddress.java:1193) at java.net.InetAddress.getAllByName(InetAddress.java:1127) at kafkashaded.org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:104) at kafkashaded.org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:403) at kafkashaded.org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:363) at kafkashaded.org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:151) at kafkashaded.org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:949) at kafkashaded.org.apache.kafka.clients.NetworkClient.access$500(NetworkClient.java:71) at kafkashaded.org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1122) at kafkashaded.org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1010) at kafkashaded.org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:545) at kafkashaded.org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) at kafkashaded.org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) at kafkashaded.org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) at kafkashaded.org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161) at kafkashaded.org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:240) at kafkashaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:444) at kafkashaded.org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1267) at kafkashaded.org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1235) at kafkashaded.org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168) at org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReader.scala:540) at org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReader.scala:602) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77) at org.apache.spark.sql.kafka010.KafkaOffsetReader.withRetriesWithoutInterrupt(KafkaOffsetReader.scala:601) at org.apache.spark.sql.kafka010.KafkaOffsetReader.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReader.scala:538) at org.apache.spark.sql.kafka010.KafkaOffsetReader.runUninterruptibly(KafkaOffsetReader.scala:569) at org.apache.spark.sql.kafka010.KafkaOffsetReader.partitionsAssignedToConsumer(KafkaOffsetReader.scala:538) at org.apache.spark.sql.kafka010.KafkaOffsetReader.fetchEarliestOffsets(KafkaOffsetReader.scala:300) at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:151) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:148) at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:76) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:398) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:398) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:276) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:274) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:71) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:391) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) at scala.collection.immutable.Map$Map1.foreach(Map.scala:128) at scala.collection.TraversableLike.map(TraversableLike.scala:238) at scala.collection.TraversableLike.map$(TraversableLike.scala:231) at scala.collection.AbstractTraversable.map(Traversable.scala:108) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:388) at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:619) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:384) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:216) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:276) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:274) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:71) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:199) at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:193) at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:346) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:259)

 ",,sneevand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 18:53:12 UTC 2020,,,,,,,,,,"0|z0kajk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 18:52;sneevand;uncommented below command  in server-properties located at kafka\config; Issue got resolved

listeners = PLAINTEXT://10.53.56.140:9092;;;","04/Nov/20 18:53;sneevand;kafka\config

Add 

listeners = PLAINTEXT://10.53.56.140:9092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AK site docs changes need to get ported to Kafka/docs,KAFKA-10679,13338693,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,bbejeck,bbejeck,03/Nov/20 17:04,04/Nov/20 16:43,13/Jul/23 09:17,04/Nov/20 16:43,2.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,docs,,,,,0,,,,,"During the update of the Apache Kafka website, changes made to the kafka-site repo were not made to the kafka/docs directory.

All the changes made need to get migrated to kafka/docs to keep the website in sync. ",,bbejeck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 16:43:13 UTC 2020,,,,,,,,,,"0|z0k9cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 16:43;bbejeck;Resolved via [https://github.com/apache/kafka/pull/9551]

and https://github.com/apache/kafka/pull/9554;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-deploying Streams app causes rebalance and task migration,KAFKA-10678,13338683,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,thebearmayor,thebearmayor,03/Nov/20 16:25,05/Feb/21 22:58,13/Jul/23 09:17,05/Feb/21 22:58,2.6.0,2.6.1,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,streams,,,,,0,,,,,"Re-deploying our Streams app causes a rebalance, even when using static group membership. Worse, the rebalance creates standby tasks, even when the previous task assignment was balanced and stable.

Our app is currently using Streams 2.6.1-SNAPSHOT (due to [KAFKA-10633]) but we saw the same behavior in 2.6.0. The app runs on 4 EC2 instances, each with 4 streams threads, and data stored on persistent EBS volumes.. During a redeploy, all EC2 instances are stopped, new instances are launched, and the EBS volumes are attached to the new instances. We do not use interactive queries. {{session.timeout.ms}} is set to 30 minutes, and the deployment finishes well under that. {{num.standby.replicas}} is 0.

h2. Expected Behavior
Given a stable and balanced task assignment prior to deploying, we expect to see the same task assignment after deploying. Even if a rebalance is triggered, we do not expect to see new standby tasks.

h2. Observed Behavior
Attached are the ""Assigned tasks to clients"" log lines from before and after deploying. The ""before"" is from over 24 hours ago, the task assignment is well balanced and ""Finished stable assignment of tasks, no followup rebalances required."" is logged. The ""after"" log lines show the same assignment of active tasks, but some additional standby tasks. There are additional log lines about adding and removing active tasks, which I don't quite understand.

I've also included logs from the broker showing the rebalance was triggered for ""Updating metadata"".",,ableegoldman,eran-levy,gyammine,mjsax,o0oxid,thebearmayor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10716,,,,,,,,KAFKA-10633,,,,,,"03/Nov/20 16:25;thebearmayor;after;https://issues.apache.org/jira/secure/attachment/13014657/after","03/Nov/20 16:25;thebearmayor;before;https://issues.apache.org/jira/secure/attachment/13014658/before","03/Nov/20 16:25;thebearmayor;broker;https://issues.apache.org/jira/secure/attachment/13014659/broker",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 22:58:56 UTC 2021,,,,,,,,,,"0|z0k9ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 22:59;ableegoldman;Thanks for opening a separate ticket for this. There seem to be two main problems/unanswered questions here:

1) Why was there a rebalance at all if static membership was enabled?
2) Why did the rebalance result in a large shuffling of tasks?

For 1) it's difficult to say with only the broker side logs, since they won't tell us _why_ the client triggered a new rebalance after it was bounced. Would it be possible to collect logs from the client covering the period immediately after it was bounced, when it apparently tried to trigger a rebalance?

I was discussing question 2) with [~cadonna] and it seems to be a combination of a few things: first, the ""eventual"" assignment is currently performed without regard to the previous placement of tasks. It just tries to distribute tasks as evenly as possible, using intermediate assignments and probing rebalances as needed. [~vvcephei] wrote up some thoughts on this in KAFKA-10121. We're aware of this limitation but haven't addressed it since the assignor is deterministic and therefore no-op group changes – such as an existing member being bounced – shouldn't result in a different eventual assignment than the stable one pre-bounce.

Unfortunately this assignment identifies clients based on the encoded processId, which is actually randomly generated during StreamThread startup. So the processId identifier would change after a bounce, meaning different initial conditions to the assignor function and therefore a different final result :/

I think if the shuffling of tasks wasn't so bad then even if you did still get a rebalance even with static membership, then it would hardly be noticeable (given that it can continue to actively process during a cooperative rebalance). We could probably improve a majority of cases just by fixing the processId thing, but I feel like we might as well skip that and just go ahead with implementing KAFKA-10121 at that point to improve it for all cases.;;;","06/Nov/20 16:53;thebearmayor;Sophie, thank you again for your thorough answers. Our logs are busy, but I was able to find the line you described, ""Requested to schedule immediate rebalance to update group with new host endpoint = ..."". I said we don't use IQ, but we used to, and neglected to remove the {{application.server}} configuration. So, that explains question 1).

We may want to use IQ again (we removed it while we waited on the rebalance changes and [KAFKA-9568]). So, I'd still like to fix question 2). I think I understand what you're saying about [KAFKA-10121]. The current assignment, after bouncing, is based on the caught-up state stores, but there is an ""eventual"" assignment which is different. Is it then true that bouncing a single instance would cause it to have a new processId and potentially cause a different eventual assignment? That's unfortunate, because we have instances come and go all the time, and task migration of large state stores is noticeable to us, due to the increased network transfer and disk usage.

For now, I think removing {{application.server}} should be sufficient for us.;;;","13/Nov/20 01:54;ableegoldman;Yeah, it seems like anytime a member is restarted and the randomly generated UUID places it in a different order relative to all the other clients, you can get this task migration. I filed KAFKA-10716 so we can look into this right away rather than wait on KAFKA-10121

I can't think of a true workaround for the meantime, but you could set the ""max.warmup.replicas"" config to 1 to slow down the movement of tasks (at the cost of some speed when scaling out, etc). It's also possible to revert to using the old assignor with an internal backdoor for emergencies. Obviously that means sacrificing the new HA guarantees, but it may work well enough for example if you have frequent restarts but the group membership is generally stable (and state isn't lost, etc);;;","05/Feb/21 22:58;ableegoldman;Resolved via [https://github.com/apache/kafka/pull/9978]

[~thebearmayor] this should be fixed in the upcoming 2.8.0 release and 2.6.2 releases which are currently in progress (and in 2.7.1 but I'm not sure of the schedule for that yet). If/when you're able to upgrade to one of these, please verify that the task shuffling due to redeployment has been mitigated. And obviously, reopen this ticket if not – thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partition.assignment.strategy documentation does not include all options,KAFKA-10671,13338304,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,alpscycler,alpscycler,01/Nov/20 15:31,21/Jan/21 08:38,13/Jul/23 09:17,21/Jan/21 08:38,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,documentation,,,,,0,,,,,"The current documentation for partition.assignment.strategy does not mention the following options:

org.apache.kafka.clients.consumer.StickyAssignor or
org.apache.kafka.clients.consumer.CooperativeStickyAssignor",,alpscycler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-01 15:31:40.0,,,,,,,,,,"0|z0k6yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StreamTableJoinTopologyOptimizationIntegrationTest.shouldDoStreamTableJoinWithDifferentNumberOfPartitions[Optimization = all],KAFKA-10665,13338138,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,ableegoldman,ableegoldman,30/Oct/20 20:34,23/Feb/21 02:50,13/Jul/23 09:17,23/Feb/21 02:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,flaky-test,,,,"{code:java}
java.nio.file.DirectoryNotEmptyException: /tmp/kafka-13241964730537515637/app-StreamTableJoinTopologyOptimizationIntegrationTestshouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___all_
	at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:246)
	at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105)
	at java.base/java.nio.file.Files.delete(Files.java:1146)
	at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:869)
	at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:839)
	at java.base/java.nio.file.Files.walkFileTree(Files.java:2822)
	at java.base/java.nio.file.Files.walkFileTree(Files.java:2876)
	at org.apache.kafka.common.utils.Utils.delete(Utils.java:839)
	at org.apache.kafka.common.utils.Utils.delete(Utils.java:825)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:151)
	at org.apache.kafka.streams.integration.StreamTableJoinTopologyOptimizationIntegrationTest.whenShuttingDown(StreamTableJoinTopologyOptimizationIntegrationTest.java:122)
{code}
https://github.com/apache/kafka/pull/9515/checks?check_run_id=1333753280",,ableegoldman,mimaison,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 05:46:51 UTC 2020,,,,,,,,,,"0|z0k5xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 00:07;mimaison;Failure while building 2.6.1: https://ci-builds.apache.org/job/Kafka/job/kafka-2.6-jdk8/50/testReport/junit/org.apache.kafka.streams.integration/StreamTableJoinTopologyOptimizationIntegrationTest/shouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___none_/;;;","02/Dec/20 05:46;showuon;org.apache.kafka.streams.integration.StreamTableJoinTopologyOptimizationIntegrationTest.shouldDoStreamTableJoinWithDifferentNumberOfPartitions[Optimization = none]
Failing for the past 1 build (Since [!https://ci-builds.apache.org/static/23d98232/images/16x16/red.png! #294|https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/294/] )
[Took 4.5 sec.|https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/294/testReport/junit/org.apache.kafka.streams.integration/StreamTableJoinTopologyOptimizationIntegrationTest/shouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___none_/history]
 
h3. Error Message

java.nio.file.NoSuchFileException: /tmp/kafka-3251174604229833116/app-StreamTableJoinTopologyOptimizationIntegrationTestshouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___none_/1_1/.checkpoint.tmp
h3. Stacktrace

java.nio.file.NoSuchFileException: /tmp/kafka-3251174604229833116/app-StreamTableJoinTopologyOptimizationIntegrationTestshouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___none_/1_1/.checkpoint.tmp at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148) at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) at java.base/java.nio.file.Files.readAttributes(Files.java:1843) at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219) at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276) at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373) at java.base/java.nio.file.Files.walkFileTree(Files.java:2840) at java.base/java.nio.file.Files.walkFileTree(Files.java:2876) at org.apache.kafka.common.utils.Utils.delete(Utils.java:841) at org.apache.kafka.common.utils.Utils.delete(Utils.java:827) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:151) at org.apache.kafka.streams.integration.StreamTableJoinTopologyOptimizationIntegrationTest.whenShuttingDown(StreamTableJoinTopologyOptimizationIntegrationTest.java:122) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor15.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at jdk.internal.reflect.GeneratedMethodAccessor14.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:832)

 

 

https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/294/testReport/junit/org.apache.kafka.streams.integration/StreamTableJoinTopologyOptimizationIntegrationTest/shouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___none_/;;;","02/Dec/20 05:46;showuon;org.apache.kafka.streams.integration.StreamTableJoinTopologyOptimizationIntegrationTest.shouldDoStreamTableJoinWithDifferentNumberOfPartitions[Optimization = all]
Failing for the past 1 build (Since [!https://ci-builds.apache.org/static/23d98232/images/16x16/red.png! #291|https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/291/] )
[Took 4.5 sec.|https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/291/testReport/junit/org.apache.kafka.streams.integration/StreamTableJoinTopologyOptimizationIntegrationTest/shouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___all_/history]
 
h3. Error Message

java.nio.file.DirectoryNotEmptyException: /tmp/kafka-12480882490717231293/app-StreamTableJoinTopologyOptimizationIntegrationTestshouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___all_
h3. Stacktrace

java.nio.file.DirectoryNotEmptyException: /tmp/kafka-12480882490717231293/app-StreamTableJoinTopologyOptimizationIntegrationTestshouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___all_ at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:246) at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105) at java.base/java.nio.file.Files.delete(Files.java:1146) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:871) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:841) at java.base/java.nio.file.Files.walkFileTree(Files.java:2822) at java.base/java.nio.file.Files.walkFileTree(Files.java:2876) at org.apache.kafka.common.utils.Utils.delete(Utils.java:841) at org.apache.kafka.common.utils.Utils.delete(Utils.java:827) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:151) at org.apache.kafka.streams.integration.StreamTableJoinTopologyOptimizationIntegrationTest.whenShuttingDown(StreamTableJoinTopologyOptimizationIntegrationTest.java:122) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor15.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at jdk.internal.reflect.GeneratedMethodAccessor14.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:832) 



 

 

https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk15/291/testReport/junit/org.apache.kafka.streams.integration/StreamTableJoinTopologyOptimizationIntegrationTest/shouldDoStreamTableJoinWithDifferentNumberOfPartitions_Optimization___all_/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams fails to overwrite corrupted offsets leading to infinite OffsetOutOfRangeException loop,KAFKA-10664,13337978,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,30/Oct/20 01:48,03/Nov/20 14:06,13/Jul/23 09:17,30/Oct/20 23:57,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"In KAFKA-10391 we fixed an issue where Streams could get stuck in an infinite loop of  OffsetOutOfRangeException/TaskCorruptedException due to re-initializing the corrupted offsets from the checkpoint after each revival. The fix we applied was to remove the corrupted offsets from the state manager and then force it to write a new checkpoint file without those offsets during revival.

Unfortunately we missed that there's an optimization in OffsetCheckpoint#write to just return without writing anything when there's no offsets. So if a task doesn't have any offsets that _aren't_ corrupted, it will skip overwriting the corrupted checkpoint.

Probably we should just fix the optimization in OffsetCheckpoint so that it deletes the current checkpoint in the case there are no offsets to write",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-30 01:48:48.0,,,,,,,,,,"0|z0k4y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ErrantRecordReporter.report always return completed future even though the record is not sent to DLQ topic yet ,KAFKA-10658,13337789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,29/Oct/20 03:28,28/Jan/21 06:24,13/Jul/23 09:17,28/Jan/21 06:24,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.0,,,,,,,,,,,0,,,,,"This issue happens when both DLQ and error log are enabled. There is a incorrect filter in handling multiple reports and it results in the uncompleted future is filtered out. Hence, users always receive a completed future even though the record is still in producer buffer.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-29 03:28:59.0,,,,,,,,,,"0|z0k3s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NetworkClient.java: print out the feature flags received at DEBUG level, as well as the other version information",KAFKA-10656,13337758,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,cmccabe,cmccabe,28/Oct/20 22:36,16/Dec/20 14:10,13/Jul/23 09:17,16/Dec/20 13:50,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,,,cmccabe,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 09:25:11 UTC 2020,,,,,,,,,,"0|z0k3lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 09:25;tombentley;[~cmccabe] I opened a PR for this, please could you take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assignor reports offsets from uninitialized task,KAFKA-10651,13337528,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,27/Oct/20 22:56,03/Nov/20 13:48,13/Jul/23 09:17,30/Oct/20 23:57,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"In KIP-441, the new HA assignor makes an informed decision about stateful task placement based on the offset sums reported by each instance. Offset sums are computed one of two ways: for assigned tasks (ie those in the TaskManager's ""tasks"" map), it will just sum up the tasks' changelog offsets directly. For tasks that are not assigned but whose directory remains on disk, it reads the changelog offsets from the checkpoint file. This is encoded with the subscription userdata sent during the JoinGroup phase of a rebalance.

The problem here is that it's possible for the instance to rejoin the group after having been assigned a new task, but before that task is initialized. In this case it would not compute the offset sum from the checkpoint file but instead from the uninitialized task, causing it to skip reporting any offsets for that task whatsoever.

This results in a particularly nefarious interaction between HA and cooperative rebalancing. An instance may read from the checkpoint file of a caught-up (but unassigned) task and report this in its subscription, leading the assignor to compute a small lag and place this task on the instance. After placing all stateful tasks in this way, it will distribute the stateless tasks across the group to balance the overall workload. It does this without considering the previous owner of the stateless tasks, so odds are good that moving the stateful task to this instance will result in a different assortment of stateless tasks in this rebalance.

Any time owned tasks are moved around, the current owner will have to revoke them and trigger a followup cooperative rebalance. Within the Consumer client, this actually happens immediately: that is, within an invocation of poll() it will loop inside joinGroupIfNeeded() as long as a rejoin is needed. And at the end of the last rebalance, if any partitions are revoked then a rejoin will indeed be needed. So the Consumer will send out it's next JoinGroup – including the userdata with computed task offset sums – without first exiting from the current poll(). Streams never gets the chance to initialize its new tasks, and ends up excluding them from the offset sums it reports in the following rebalance.

And since it doesn't report any offsets for this task, the assignor now believes the instance does _not_ have any caught up state for this task, and assigns the task elsewhere. This causes a shuffling of stateless tasks once more, which in turn results in another cooperative rebalance. This time the task is no longer assigned so the instance reports offsets based on the checkpoint file again, and we're back at the beginning.

Given the deterministic assignment, once a group is caught up in this cycle it will be impossible to escape it without manual intervention (ie deleting some or all of the task directories and forcing it to restore from scratch)",,ableegoldman,cadonna,mingaliu,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-27 22:56:13.0,,,,,,,,,,"0|z0k268:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only serialize owned partition when consumer protocol version >= 0 ,KAFKA-10647,13337286,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dajac,dajac,dajac,26/Oct/20 20:39,27/Oct/20 10:14,13/Jul/23 09:17,27/Oct/20 10:14,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,A regression got introduced by https://github.com/apache/kafka/pull/8897. The owned partition field must be ignored for version < 1 otherwise the serialization fails with an unsupported version exception.,,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-26 20:39:52.0,,,,,,,,,,"0|z0k0o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryableStateIntegrationTest fails due to stricter store checking,KAFKA-10638,13336973,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,23/Oct/20 18:49,29/Oct/20 17:10,13/Jul/23 09:17,29/Oct/20 17:10,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"Observed:
{code:java}
org.apache.kafka.streams.errors.InvalidStateStoreException: Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING
	at org.apache.kafka.streams.state.internals.StreamThreadStateStoreProvider.stores(StreamThreadStateStoreProvider.java:81)
	at org.apache.kafka.streams.state.internals.WrappingStoreProvider.stores(WrappingStoreProvider.java:50)
	at org.apache.kafka.streams.state.internals.CompositeReadOnlyKeyValueStore.get(CompositeReadOnlyKeyValueStore.java:52)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores(StoreQueryIntegrationTest.java:200)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
 {code}
and also
{code:java}
org.apache.kafka.streams.errors.InvalidStateStoreException: Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING
	at org.apache.kafka.streams.state.internals.StreamThreadStateStoreProvider.stores(StreamThreadStateStoreProvider.java:81)
	at org.apache.kafka.streams.state.internals.WrappingStoreProvider.stores(WrappingStoreProvider.java:50)
	at org.apache.kafka.streams.state.internals.CompositeReadOnlyKeyValueStore.get(CompositeReadOnlyKeyValueStore.java:52)
	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQueryOnlyActivePartitionStoresByDefault(StoreQueryIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
 {code}
 

These are both due to:

KAFKA-10598: Improve IQ name and type checks (#9408)

 ",,ableegoldman,bbejeck,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 17:43:48 UTC 2020,,,,,,,,,,"0|z0jyqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 18:50;vvcephei;Hey [~bbejeck] , I've marked this as a 2.7.0 blocker. It's a regression (due to KAFKA-10598), but fortunately, only a regression in the store query test. Still, we should get the fix in.;;;","27/Oct/20 17:47;bbejeck;Hey [~vvcephei] I appreciate you wanting to get a fix in for the test, but I'm not sure it meets the criteria for a blocker.

Going back over the previous [2.7 builds|https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-2.7-jdk8/activity] [32-41], it's only failed once in [build 40|https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-2.7-jdk8/detail/kafka-2.7-jdk8/40/tests/]

For the 2.7.0 release process, I'm going to reduce the severity and move the fix version to 2.7.1 and 2.8.0;;;","28/Oct/20 17:43;vvcephei;Thanks, [~bbejeck] , I agree. When I filed this ticket, I mistakenly thought the test would fail every time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant probing rebalances in Streams 2.6,KAFKA-10633,13336789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,thebearmayor,thebearmayor,22/Oct/20 19:21,07/Jan/21 19:51,13/Jul/23 09:17,07/Jan/21 19:51,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,,,,,,,,streams,,,,,0,,,,,"We are seeing a few issues with the new rebalancing behavior in Streams 2.6. This ticket is for constant probing rebalances on one StreamThread, but I'll mention the other issues, as they may be related.

First, when we redeploy the application we see tasks being moved, even though the task assignment was stable before redeploying. We would expect to see tasks assigned back to the same instances and no movement. The application is in EC2, with persistent EBS volumes, and we use static group membership to avoid rebalancing. To redeploy the app we terminate all EC2 instances. The new instances will reattach the EBS volumes and use the same group member id.

After redeploying, we sometimes see the group leader go into a tight probing rebalance loop. This doesn't happen immediately, it could be several hours later. Because the redeploy caused task movement, we see expected probing rebalances every 10 minutes. But, then one thread will go into a tight loop logging messages like ""Triggering the followup rebalance scheduled for 1603323868771 ms."", handling the partition assignment (which doesn't change), then ""Requested to schedule probing rebalance for 1603323868771 ms."" This repeats several times a second until the app is restarted again. I'll attach a log export from one such incident.

",,ableegoldman,cadonna,eran-levy,gyammine,mjsax,o0oxid,thebearmayor,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10455,,,,,,,,,,,,,,,KAFKA-10678,,,,,,,,,"22/Oct/20 19:20;thebearmayor;Discover 2020-10-21T23 34 03.867Z - 2020-10-21T23 44 46.409Z.csv;https://issues.apache.org/jira/secure/attachment/13014003/Discover+2020-10-21T23+34+03.867Z+-+2020-10-21T23+44+46.409Z.csv",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 16:28:57 UTC 2020,,,,,,,,,,"0|z0jxm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/20 19:58;thebearmayor;/cc [~vvcephei];;;","23/Oct/20 18:16;ableegoldman;I'm like 99% sure this will turn out to be KAFKA-10455;;;","23/Oct/20 18:22;ableegoldman;A thread should always reset its scheduled rebalance after triggering one, and it will only set the rebalance schedule when it receives its assignment after a rebalance. And the probing rebalance is always scheduled for 10min past the current time, so the fact that you see the same time printed in the ""Triggering the follow rebalance"" and ""Requested to schedule probing rebalance"" messages indicates that the member did not actually go through a rebalance, it just received its previous assignment directly from the broker.

Also, a rebalance will never be completed in under a second, so seeing `Triggering the followup rebalance scheduled for 1603323868771 ms` followed immediately by `Requested to schedule probing rebalance for 1603323868771 ms`   seems to verify that it did not in fact go through a rebalance.

[~thebearmayor] this issue is fixed in 2.7.0 and 2.6.1 – not sure when 2.6.1 will be released but the 2.7.0 release is currently in progress so it should hopefully be available soon. Apologies for our oversight in designing KIP-441;;;","23/Oct/20 18:32;thebearmayor;Thank you, Sophie! I'll do a build with the PR for KAFKA-10455 and confirm that it fixes this issue.;;;","23/Oct/20 22:20;thebearmayor;Sophie (et al.), do you have any thoughts about what would cause rebalances (and task movements) after redeploying? If we could fix that, then this would be less of a problem, because we would rarely have probing rebalances. We do have another problem where state directories are not deleted, but their contents are (almost like KAFKA-6647, but not quite the same). Is it possible that is confusing the task assignor?;;;","23/Oct/20 23:02;ableegoldman;The issue with directory contents being deleted but not the directories themselves sounds like KAFKA-10564 (also fixed in 2.7.0/2.6.1). I don't believe that particular bug has any real implications, other than being annoying/misleading in the logs – it should still delete everything in the task directory, including the checkpoint file which is how the assignor determines which persistent state is/isn't on an instance.

I'm kind of surprised that you would still get a rebalance after redeploying like that when using static membership. Unless it takes longer than the static group membership timeout I guess. To be honest, I'm not that familiar with the specifics of static membership in general – maybe [~bchen225242] can chime in here. But I suppose I would start by checking out the logs on the Streams side after it comes back up, and see if it's logged any reason for triggering a rebalance explicitly.

(There are a few reasons to trigger a rebalance after a static member is bounced, for example if it's hostname changed for IQ. If anything like that happened it should be logged clearly);;;","28/Oct/20 17:30;thebearmayor;It's possible that the task assignment has never been balanced, and that this issue was hiding that fact. After deploying the fix for [KAFKA-10455], I haven't seen this issue recur, but I have seen steady task movement. So, it may be that will have to wait a while before tasks are balanced.;;;","01/Nov/20 10:52;eran-levy;[~thebearmayor]  same happens here, didnt deploy the fix yet - just to make sure, do you still see any issues?

I see that it hasn't been released yet - [https://github.com/apache/kafka/releases]

Whats the best way to get that fix?

 ;;;","01/Nov/20 18:38;thebearmayor;[~eran-levy] The fix did seem to work for us, we've been running it for a few days. We ended up using the latest commit for 2.6.1-SNAPSHOT. We just built it locally from https://github.com/apache/kafka/tree/2.6. As you said, 2.6.1 is not released, and doesn't have a planned release date yet.;;;","02/Nov/20 10:09;cadonna;[~thebearmayor] [~eran-levy] FYI: The 2.6.1 release is currently ongoing. Here the plan: 

[https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+2.6.1]

It is planned to be released this month.;;;","02/Nov/20 12:52;eran-levy;thanks [~cadonna] [~thebearmayor];;;","03/Nov/20 16:28;thebearmayor;Closing this ticket as the issue is fixed in 2.6.1. I've opened [KAFKA-10678] for our other problem with unexpected rebalancing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerFencedException is not Handled on Offest Commit,KAFKA-10631,13336778,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cadonna,cadonna,cadonna,22/Oct/20 17:16,23/Oct/20 17:05,13/Jul/23 09:17,22/Oct/20 19:25,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,clients,,,,,0,,,,,"The transaction manager does currently not handle producer fenced errors returned from a offset commit request.

We found this bug because we encountered the following exception in our soak cluster:
{code:java}
org.apache.kafka.streams.errors.StreamsException: Error encountered trying to commit a transaction [stream-thread [i-037c09b3c48522d8d-StreamThread-3] task [0_0]]
        at org.apache.kafka.streams.processor.internals.StreamsProducer.commitTransaction(StreamsProducer.java:256)
        at org.apache.kafka.streams.processor.internals.TaskManager.commitOffsetsOrTransaction(TaskManager.java:1050)
        at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1013)
        at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:886)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:677)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:553)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:512)
[2020-10-22T04:09:54+02:00] (streams-soak-2-7-eos-alpha_soak_i-037c09b3c48522d8d_streamslog) Caused by: org.apache.kafka.common.KafkaException: Unexpected error in TxnOffsetCommitResponse: There is a newer producer with the same transactionalId which fences the current one.
        at org.apache.kafka.clients.producer.internals.TransactionManager$TxnOffsetCommitHandler.handleResponse(TransactionManager.java:1726)
        at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1278)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:584)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:576)
        at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:415)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:313)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240)
        at java.lang.Thread.run(Thread.java:748)
{code}",,cadonna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-22 17:16:12.0,,,,,,,,,,"0|z0jxjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamThread killed by ""IllegalStateException: The processor is already closed""",KAFKA-10616,13335844,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,ableegoldman,ableegoldman,17/Oct/20 04:55,26/Oct/20 22:33,13/Jul/23 09:17,26/Oct/20 22:33,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"Application is hitting ""java.lang.IllegalStateException: The processor is already closed"". Over the course of about a day, this exception killed 21/100 of the queries (StreamThreads). The (slightly trimmed) stacktrace:

 
{code:java}
java.lang.RuntimeException: Caught an exception while closing caching window store for store Aggregate-Aggregate-Materialize at org.apache.kafka.streams.state.internals.ExceptionUtils.throwSuppressed(ExceptionUtils.java:39) at org.apache.kafka.streams.state.internals.CachingWindowStore.close(CachingWindowStore.java:432) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.close(ProcessorStateManager.java:527) at org.apache.kafka.streams.processor.internals.StreamTask.closeDirty(StreamTask.java:499) at org.apache.kafka.streams.processor.internals.TaskManager.handleLostAll(TaskManager.java:626) … Caused by: java.lang.IllegalStateException: The processor is already closed at org.apache.kafka.streams.processor.internals.ProcessorNode.throwIfClosed(ProcessorNode.java:172) at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:178) at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:273) at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:252) at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:214) at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:45) at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:28) at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$setFlushListener$1(MeteredWindowStore.java:110) at org.apache.kafka.streams.state.internals.CachingWindowStore.putAndMaybeForward(CachingWindowStore.java:118) at org.apache.kafka.streams.state.internals.CachingWindowStore.lambda$initInternal$0(CachingWindowStore.java:93) at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:151) at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:109) at org.apache.kafka.streams.state.internals.ThreadCache.flush(ThreadCache.java:116) at org.apache.kafka.streams.state.internals.CachingWindowStore.lambda$close$1(CachingWindowStore.java:427) at org.apache.kafka.streams.state.internals.ExceptionUtils.executeAll(ExceptionUtils.java:28) at org.apache.kafka.streams.state.internals.CachingWindowStore.close(CachingWindowStore.java:426)
{code}
 

I'm guessing we close the topology before closing the state states, so records that get flushed during the caching store's close() will run into an already-closed processor. During a clean close we should always flush before closing anything (during prepareCommit()), but since this was a handleLostAll() we would just skip right to suspend() and close the topology.

Presumably the right thing to do here is to flush the caches before closing anything during a dirty close.",,ableegoldman,guozhang,mjsax,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 17:29:48 UTC 2020,,,,,,,,,,"0|z0jrsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 02:09;sagarrao;hey [~ableegoldman], Can I take this one up?;;;","20/Oct/20 04:29;ableegoldman;I spoke with [~guozhang]  earlier and he said he already has a fix for this that he had to implement in some other work when he ran into the same issue. He's just going to cherrypick it into a new PR for this – sorry, forgot to assign the ticket to him ;;;","20/Oct/20 04:39;ableegoldman;By the way, we should actually fix this in 2.6 as well. There's no IllegalStateException killing the thread in that branch, but the underlying issue is still there: flushing records downstream to closed processors. AFAICT the only effect this has in the DSL operators is that these records would not be recorded in any metrics, but there could be more severe consequences if a PAPI user does something more interesting in the processor's close() method.

It's possible we'll need a separate PR for the 2.6 branch, depending on what the fix for 2.7/trunk looks like. [~sagarrao] if we do need a separate PR then feel free to pick it up;;;","20/Oct/20 05:08;sagarrao;Alright thanks [~ableegoldman], Let me know if we would need a separate PR for this.. We might need another issue to be create for that?;;;","20/Oct/20 17:29;guozhang;I think this is a long lurking bug caused by https://github.com/apache/kafka/pull/9083.

Generally speaking, before we close the topology we should always try to flush the state store manager (we actually do not really need to flush the whole store, but just the cache). I have this fix piggy-backed in my other PR https://github.com/apache/kafka/pull/8988 but it is dragging very long and every 3-4 days I'd have to rebase it again, so I do not feel that it could be merged soon.

What I can do is to extract that fix along from the PR and merge it for 2.7 / 2.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group coordinator onElection/onResignation should guard against leader epoch,KAFKA-10614,13335548,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,guozhang,guozhang,15/Oct/20 04:41,13/Jul/21 21:04,13/Jul/23 09:17,07/Jun/21 16:16,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,,,,,"When there are a sequence of LeaderAndISR or StopReplica requests sent from different controllers causing the group coordinator to elect / resign, we may re-order the events due to race condition. For example:

1) First LeaderAndISR request received from old controller to resign as the group coordinator.
2) Second LeaderAndISR request received from new controller to elect as the group coordinator.
3) Although threads handling the 1/2) requests are synchronized on the replica manager, their callback {{onLeadershipChange}} would trigger {{onElection/onResignation}} which would schedule the loading / unloading on background threads, and are not synchronized.
4) As a result, the {{onElection}} maybe triggered by the thread first, and then {{onResignation}}. As a result, the coordinator would not recognize it self as the coordinator and hence would respond any coordinator request with {{NOT_COORDINATOR}}.

Here are two proposals on top of my head:

1) Let the scheduled load / unload function to keep the passed in leader epoch, and also materialize the epoch in memory. Then when execute the unloading check against the leader epoch.

2) This may be a bit simpler: using a single background thread working on a FIFO queue of loading / unloading jobs, since the caller are actually synchronized on replica manager and order preserved, the enqueued loading / unloading job would be correctly ordered as well. In that case we would avoid the reordering. ",,ableegoldman,cadonna,guozhang,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 15:57:55 UTC 2020,,,,,,,,,,"0|z0jpz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 22:24;cadonna;FYI: This bug makes system test {{StreamsBrokerBounceTest.test_broker_type_bounce}} fail once in a while when the group coordinator (i.e., leader of partition 2 of the {{__consumer_offset}} topic) is bounced. Unfortunately, I haven't been able to reproduce the failure locally.;;;","23/Nov/20 15:57;tombentley;[~guozhang] [~hachikuji] any thoughts about the PR I opened for this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker should not set leader epoch if the list-offset request version < 4,KAFKA-10613,13335501,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,14/Oct/20 21:41,15/Oct/20 17:07,13/Jul/23 09:17,15/Oct/20 17:04,2.1.0,2.2.0,2.3.0,2.4.0,2.5.0,2.6.0,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,,,,,0,,,,,"The list-offset response added a new field in version 4:

{code}
        { ""name"": ""LeaderEpoch"", ""type"": ""int32"", ""versions"": ""4+"" }
{code}

And the compiled code would throw UnsupportedVersionException if that field is not default (-1) with version < 4. However, on the broker side we forget to add the logic to not setting this field based on the request version. This would cause old versioned clients' list-offset call to always get UnsupportedVersionException and an empty result would be returned.
",,ableegoldman,guozhang,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 17:03:57 UTC 2020,,,,,,,,,,"0|z0jpoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 21:45;ijuma;When did we regress here?;;;","15/Oct/20 17:03;guozhang;When we introduce the new version 4 in list-offsets in https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation#KIP320:Allowfetcherstodetectandhandlelogtruncation-ListOffsets I think, but we did not get it since it would only affect old clients.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto create non-existent topics when fetching metadata for all topics,KAFKA-10606,13335292,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,andrewlincong@gmail.com,andrewlincong@gmail.com,andrewlincong@gmail.com,13/Oct/20 22:28,09/Dec/20 15:32,13/Jul/23 09:17,09/Dec/20 15:31,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"The ""allow auto topic creation"" flag is hardcoded to be true for the fetch-all-topic metadata request:

https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java#L37

In the below code, annotation claims that ""*This never causes auto-creation*"". It it NOT true and auto topic creation still gets triggered under some circumstances. So, this is a bug that needs to be fixed.

https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java#L68


For example, the bug could be manifested in the below situation:

A topic T is being deleted and a request to fetch metadata for all topics gets sent to one broker. The broker reads names of all topics from its metadata cache (shown below).

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/KafkaApis.scala#L1196

Then the broker authorizes all topics and makes sure that they are allowed to be described. Then the broker tries to get metadata for every authorized topic by reading the metadata cache again, once for every topic (show below).

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/KafkaApis.scala#L1240

However, the metadata cache could have been updated while the broker was authorizing all topics and topic T and its metadata no longer exist in the cache since the topic got deleted and metadata update requests eventually got propagated from the controller to all brokers. So, at this point, when the broker tries to get metadata for topic T from its cache, it realizes that it does not exist and the broker tries to ""auto create"" topic T since the allow-auto-topic-creation flag was set to true in all the fetch-all-topic metadata requests.

I think this bug exists since ""*metadataRequest.allowAutoTopicCreation*"" was introduced.",,andrewlincong@gmail.com,huxi_2b,ijuma,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 18:20:10 UTC 2020,,,,,,,,,,"0|z0joe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 10:43;huxi_2b;I am wondering if we could change the default value for _AllowAutoTopicCreation_ to _false_ in MetadataRequest.json. In doing so could we have ALL_TOPICS_REQUEST_DATA actually disable auto-creation.;;;","14/Oct/20 16:58;andrewlincong@gmail.com;[~huxi_2b] Thanks for your comment. However, clients set this to true explicitly on all paths, and even if they didn't this breaks behavior when getting MD for particular topics (where auto creation may be expected because the topics were named. 

For example, 
https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java#L37

Also changing default values for RPC objects changes the behavior for older clients. Thus it is a breaking API change. So even if we want to change the default value of that field (for hypothetical clients that don't explicitly set it), it needs to be in a new RPC version.

A simple way to fix it on the server side is:

https://github.com/linkedin/kafka/pull/94/commits/95ad9add181db980914a13a6ffe1a88cd5636a6d

I prefer the change that does not require Kafka client version upgrade. Because in some cases (e.g. LinkedIn), client version upgrade means hundreds or even thousands of Kafka users will have to bump Kafka client version in their project dependency and it makes this approach less feasible compared to the server side fix.

There are definitely other options with more complex (or fancier) way to fix this issue on the server side as well.
;;;","14/Oct/20 17:19;ijuma;Seems like the way to fix this is to pass `false` as the `allowAutoTopicCreation` parameter of `getTopicMetadata` when it's an all topics request.;;;","14/Oct/20 17:20;tombentley;[~andrewlincong@gmail.com] agreed that a broker-side fix is necessary. Are you going to open a PR for Apache Kafka too? I'd assigned this to myself, since you didn't assign it to yourself, but if you're intending to open a PR then go ahead.;;;","14/Oct/20 17:30;andrewlincong@gmail.com;Hi [~tombentley]. Thanks for your comment. I will open a PR with the same change in https://github.com/linkedin/kafka/pull/94/commits/95ad9add181db980914a13a6ffe1a88cd5636a6d

Thanks for your comment as well [~ijuma], I prefer broker-side fix instead of client-side fix since the client-side fix requires Kafka users' client version upgrade and it is much more difficult to do compared to deploy a new version of Kafka server. Let me know whether you have any particular concerns regarding the broker-side fix.;;;","14/Oct/20 18:20;andrewlincong@gmail.com;Hi [~tombentley] [~ijuma] I just created a PR. Let me know if you have any feedback or suggestions. Thanks!

https://github.com/apache/kafka/pull/9435;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The StreamsConfig.STATE_DIR_CONFIG's default value does not reflect the JVM parameter or OS-specific settings,KAFKA-10604,13335155,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,dongjin,dongjin,dongjin,13/Oct/20 08:01,02/Feb/21 07:58,13/Jul/23 09:17,02/Feb/21 07:58,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,,,,,"I found this problem working for [KAFKA-10585|https://issues.apache.org/jira/browse/KAFKA-10585].

The JVM's temporary directory location is different per OS, and JVM allows to change it with `java.io.tmpdir` system property. In Linux, it defaults to `/tmp`.

The problem is the default value of StreamsConfig.STATE_DIR_CONFIG (`state.dir`) is fixed to `/tmp/kafka-streams`. For this reason, it does not change if the runs on OS other than Linux or the user specifies `java.io.tmpdir` system property.

It should be `\{temp-directory}/kafka-streams`, not `/tmp/kafka-streams`.",,ableegoldman,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 07:58:40 UTC 2021,,,,,,,,,,"0|z0jnk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/20 08:04;dongjin;[~mjsax] Perhaps we need a KIP discussion for this issue, like [KIP-676|https://cwiki.apache.org/confluence/display/KAFKA/KIP-676%3A+Respect+logging+hierarchy]. Isn't it?;;;","13/Oct/20 18:33;mjsax;Not sure if we need a KIP. Our focus is on UNIX based systems (even if we try via best effort to support other OS, too) and for UNIX based systems nothing change... Personally, I would be fine without a KIP.

Curious to hear what [~guozhang] or [~vvcephei] think?;;;","05/Nov/20 23:55;guozhang;I personally would prefer not a KIP for this straight-forward fix.;;;","06/Nov/20 03:49;vvcephei;I agree; I do not think this needs a KIP. It seems clear that the intent was to use ""the temporary directory"", and it was simply a bug/oversight to hard-code ""/tmp"" instead of using the platform-indendent ""java.io.tmpdir"".

In fact, I've just reclassified this as a bug.;;;","06/Nov/20 04:53;mjsax;Thanks. Good to see that we are in agreement. Will review the PR soon. Thanks for the ticket and PR [~dongjin]!;;;","05/Dec/20 06:50;dongjin;ALL // Great. Let's continue the discussion on the PR! :D;;;","02/Feb/21 07:58;mjsax;Thanks for reporting the issue and the PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DLQ Reporter throws NPE when reporting from different thread,KAFKA-10602,13335082,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,levzemlyanov,levzemlyanov,12/Oct/20 21:12,15/Oct/20 19:27,13/Jul/23 09:17,15/Oct/20 19:27,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,,,,,,0,,,,,"If a connector uses separate threads to report errant records using the ErrantRecordReporter from KIP-610, it  can sometimes hit this race condition and throw an NPE when reporting both serialization errors from a converter and errors from a connector on separate thread because both use the same [RetryWithToleranceOperator|https://github.com/apache/kafka/blob/24290de82821d16f7d163d086f5cfa88cec2b976/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java#L92-L94] in the [reporters|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L635], which results in the NPE because the error can get reset to `null`. 

The `WorkerErrantRecordReporter::report` needs to be made threadsafe otherwise it can conflict with the DLQ reporter. 
{code:java}
java.lang.NullPointerException
 at org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter.stacktrace(DeadLetterQueueReporter.java:187)
 at org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter.populateContextHeaders(DeadLetterQueueReporter.java:177)
 at org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter.report(DeadLetterQueueReporter.java:149)
 at org.apache.kafka.connect.runtime.errors.ProcessingContext.lambda$report$0(ProcessingContext.java:151)
 at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
 at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1624)
 at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
 at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
 at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
 at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
 at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
 at org.apache.kafka.connect.runtime.errors.ProcessingContext.report(ProcessingContext.java:153)
 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:126)
 at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:498)
 at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:475)
 at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:325)
 at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:229)
 at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
 at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
 at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
 at java.base/java.lang.Thread.run(Thread.java:832) {code}
 ",,levzemlyanov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-12 21:12:40.0,,,,,,,,,,"0|z0jn3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect adds error to property in validation result if connector does not define the property,KAFKA-10600,13335051,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,rhauch,rhauch,12/Oct/20 16:29,07/Jul/21 15:50,13/Jul/23 09:17,16/Oct/20 14:17,0.10.0.0,0.11.0.0,1.0.0,1.1.0,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,2.5.0,2.6.0,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,KafkaConnect,,,,,0,,,,,"Kafka Connect's {{AbstractHerder.generateResult(...)}} method is responsible for taking the result of a {{Connector.validate(...)}} call and constructing the {{ConfigInfos}} object that is then mapped to the JSON representation.

As this method (see [code|https://github.com/apache/kafka/blob/1f8ac6e6fee3aa404fc1a4c01ac2e0c48429a306/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java#L504-L507]) iterates over the {{ConfigKey}} objects in the connector's {{ConfigDef}} and the {{ConfigValue}} objects returned by the {{Connector.validate(...)}} method, this method adds an error message to any {{ConfigValue}} whose {{configValue.name()}} does not correspond to a {{ConfigKey}} in the connector's {{ConfigDef}}. 

{code}
            if (!configKeys.containsKey(configName)) {
                configValue.addErrorMessage(""Configuration is not defined: "" + configName);
                configInfoList.add(new ConfigInfo(null, convertConfigValue(configValue, null)));
            }
{code}

Interestingly, these errors are not included in the total error count of the response. Is that intentional??

This behavior does not allow connectors to report validation errors against extra properties not defined in the connector's {{ConfigDef}}. 

Consider a connector that allows arbitrary properties with some prefix (e.g., {{connection.*}}) to be included and used in the connector properties. One example is to supply additional properties to a JDBC connection, where the connector may not be able to know these ""additional properties"" in advance because the connector either works with multiple JDBC drivers or the connection properties allowed by a JDBC driver are many and/or vary over different JDBC driver versions or server versions.

Such ""additional properties"" are not prohibited by Connect API, yet if a connector implementation chooses to include any such additional properties in the {{Connector.validate(...)}} result (whether or not the corresponding {{ConfigValue}} has an error) then Connect will always add the following error to that property. 

{quote}
Configuration is not defined: <additionalPropertyName>
{quote}

This code was in the 0.10.0.0 release of Kafka via the [PR|https://github.com/apache/kafka/pull/964] for KAFKA-3315, which is one of the tasks that implemented [KIP-26|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=58851767] for Kafka Connect (approved and partially added in 0.9.0.0). There is no mention of ""validation"" in KIP-26 nor any followup KIP (that I can find).

I can kind of imagine the original thought process: any user-supplied property that is not defined by a {{ConfigDef}} is inherently an error. However, this assumption is not matched by any mention in the Connect API, documentation, or one of Connect's KIP.
IMO, this is a bug in the {{AbstractHerder}} that over-constrains the connector properties to be only those defined in the connector's {{ConfigDef}}.

Quite a few connectors already support additional properties, and it's perhaps only by chance that this happens to work: 
* If a connector does not override {{Connector.validate(...)}}, extra properties are not validated and therefore are not included in the resulting {{Config}} response with one {{ConfigValue}} per property defined in the connector's {{ConfigDef}}.
* If a connector does override {{Connector.validate(...)}} and includes in the {{Config}} response a {{ConfigValue}} for the any additional properties, the {{AbstractHerder.generateResults(...)}} method does add the error but does not include this error in the error count, which is actually used to determine if there are any validation problems before starting/updating the connector.

I propose that the {{AbstractHerder.generateResult(...)}} method be changed to not add any additional error message to the validation result, and to properly handle all {{ConfigValue}} objects regardless of whether there is a corresponding {{ConfigKey}} in the connector's {{ConfigDef}}.",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 14:17:13 UTC 2020,,,,,,,,,,"0|z0jmww:",9223372036854775807,,kkarantasis,,,,,,,,,,,,,,,,,,"13/Oct/20 17:54;rhauch;See PR https://github.com/apache/kafka/pull/9425;;;","16/Oct/20 14:17;rhauch;Connect should not always add an error to configuration values in validation results that don't have a `ConfigKey` defined in the connector's `ConfigDef`, and any errors on such configuration values included by the connector should be counted in the total number of errors. Added more unit tests for `AbstractHerder.generateResult(...)`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStreams reports inappropriate error message for IQ,KAFKA-10598,13334844,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,11/Oct/20 03:28,12/Oct/20 17:43,13/Jul/23 09:17,12/Oct/20 17:43,,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"Presently, KafkaStreams#store , or calling methods on the returned store, will throw an InvalidStateStoreException if the store name or type is wrong. However, the message in the exception is ""The state store may have migrated to another instance"", which is incorrect.",,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-11 03:28:36.0,,,,,,,,,,"0|z0jlmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Still dealing issue with KAFKA-9066,KAFKA-10597,13334808,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,blcksrx,blcksrx,10/Oct/20 13:29,26/Oct/20 20:52,13/Jul/23 09:17,14/Oct/20 06:33,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,0,,,,,I upgraded the Kafka cluster to 2.6.0. The version that seems to the is #KAFKA-9066 not resolved yet. beside the *UNASGINED* connectors and tasks has the same issue,,blcksrx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-10 13:29:38.0,,,,,,,,,,"0|z0jlew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite loop in SimpleHeaderConverter and Values classes,KAFKA-10574,13333758,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,05/Oct/20 16:17,18/May/22 01:32,13/Jul/23 09:17,12/Oct/20 20:00,1.1.0,1.1.1,1.1.2,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,2.5.1,2.6.0,,,,,2.3.2,2.4.2,2.5.2,2.6.1,2.7.0,,,,KafkaConnect,,,,,0,,,,,"A header value with the byte sequence {{0xEF, 0xBF, 0xBF}} will cause an infinite loop in the {{Values::parseString}} method. Since that method is invoked by the default header converter ({{SimpleHeaderConverter}}), any sink record with that byte array will, by default, cause a sink task reading that record to stall forever.

This occurs because that byte sequence, when parsed as a UTF-8 string and then read by a [StringCharacterIterator|https://docs.oracle.com/javase/8/docs/api/java/text/StringCharacterIterator.html], causes the [CharacterIterator.DONE|https://docs.oracle.com/javase/8/docs/api/java/text/CharacterIterator.html#DONE] character to be returned from [StringCharacterIterator::current|https://docs.oracle.com/javase/8/docs/api/java/text/StringCharacterIterator.html#current--], [StringCharacterIterator::next|https://docs.oracle.com/javase/8/docs/api/java/text/StringCharacterIterator.html#next--], etc., and a check for that character is used by the {{Values}} class for its parsing logic.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 20:11:14 UTC 2020,,,,,,,,,,"0|z0jfds:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"05/Oct/20 16:18;ChrisEgerton;This can be reproduced by adding the following test case to the {{ValuesTest}} class (beware that running this test will cause an infinite loop and it will need to be terminated manually):

{code:java}
    @Test
    public void shouldNotEncounterInfiniteLoop() {
        byte[] bytes = new byte[] { -17, -65,  -65 };
        String str = new String(bytes, StandardCharsets.UTF_8);
        Values.parseString(str);
    }
{code}
;;;","12/Oct/20 20:00;rhauch;Thanks, [~ChrisEgerton].

Merged to the `trunk` branch for inclusion in the next release after 2.7, and cherry-picked to the following branches:
* `2.7` for inclusion in the upcoming 2.7.0 release
* `2.6` for inclusion in the next 2.6.1 release if/when that happens
* `2.5` for inclusion in the next 2.5.2 release if/when that happens
* `2.4` for inclusion in the next 2.4.2 release if/when that happens
* `2.3` for inclusion in the next 2.3.2 release if/when that happens
;;;","12/Oct/20 20:11;ChrisEgerton;Thanks [~rhauch]. Really appreciate the backports; it's a one-in-16-million chance for this bug to show up but when it does it can be pretty nasty. Nice to know that those future releases will be safe.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Console producer displays interactive prompt even when input is not interactive,KAFKA-10565,13330464,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,morozov,morozov,01/Oct/20 23:05,26/Nov/20 03:57,13/Jul/23 09:17,26/Nov/20 03:57,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,tools,,,,,0,,,,,"The prompt introduced in KAFKA-2955 may be indeed helpful when a user enters messages manually but when the messages are read from a file, it's not helpful and may be really annoying.
h5. Steps to reproduce
 # Create a file with a decent number of messages (e.g. 80,000 in my case)
 # Start console producer and forward the file contents to its STDIN:

{noformat}
$ kafka-console-producer --broker-list b1,b2,b3 --topic test < messages.txt
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ... >>>>>>>>>>>>>>
{noformat}
For each message the producer reads from the file, there's one > displayed polluting the output.
h5. Expected behavior:
 # If the producer can detect that the input stream is a TTY, it should not display the prompt.
 # Ideally, there should be a configuration parameter to disable this explicitly.

 ",,morozov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2955,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-01 23:05:18.0,,,,,,,,,,"0|z0j3k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Continuous logging about deleting obsolete state directories,KAFKA-10564,13330427,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mikebin,mikebin,01/Oct/20 18:47,21/Oct/20 02:19,13/Jul/23 09:17,12/Oct/20 18:23,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,beginner,newbie,,,"The internal process which automatically cleans up obsolete task state directories was modified in https://issues.apache.org/jira/browse/KAFKA-6647. The current logic in 2.6 is to remove all files from the task directory except the {{.lock}} file:

[https://github.com/apache/kafka/blob/2.6/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java#L335]

However, the directory is only removed in its entirely for a manual cleanup:

[https://github.com/apache/kafka/blob/2.6/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java#L349-L353]

The result of this is that Streams will continue revisiting this directory and trying to clean it up, since it determines what to clean based on last-modification time of the task directory (which is now no longer deleted during the automatic cleanup). So a user will see log messages like:
stream-thread [app-c2d773e6-8ac3-4435-9777-378e0ec0ab82-CleanupThread] Deleting obsolete state directory 0_8 for task 0_8 as 60600001ms has elapsed (cleanup delay is 600000ms)
repeated again and again.

This issue doesn't seem to break anything - it's more about avoiding unnecessary logging and cleaning up empty task directories.",,ableegoldman,mikebin,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-01 18:47:56.0,,,,,,,,,,"0|z0j3c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't shutdown the entire app upon TimeoutException during internal topic validation,KAFKA-10559,13330237,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sagarrao,ableegoldman,ableegoldman,30/Sep/20 18:43,20/Oct/20 17:33,13/Jul/23 09:17,15/Oct/20 22:18,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"During some of the KIP-572 work, we made things pretty brittle by changing the StreamsPartitionAssignor to send the `INCOMPLETE_SOURCE_TOPIC_METADATA` error code and shut down the entire application if a TimeoutException is hit during the internal topic creation/validation.

Internal topic validation occurs during every rebalance, and we have seen it time out on topic discovery in unstable environments. So shutting down the entire application seems like a step in the wrong direction, and antithetical to the goal of KIP-572 (improving the resiliency of Streams in the face of TimeoutExceptions)

I'm not totally sure what the previous behavior was, but it seems to me we have three options:
 # Rethrow the TimeoutException and allow it to kill the thread
 # Swallow the TimeoutException and retry the rebalance indefinitely
 # Some combination of the above: swallow the TimeoutException but don't retry indefinitely:
 ## Start a timer and allow retrying rebalances for up the configured task.timeout.ms, the timeout config introduced in KIP-572
 ## Retry for some constant number of rebalances

I think if we go with option 3, then shutting down the entire application is relatively more palatable, as we have given the environment a chance to stabilize.

But, killing the thread still seems preferable, given the two new features that are coming out soon: the ability to start up new threads, and the improved exception handler that allows the user to choose to shut down the entire application if that's really what they want. Once users have this level of control over the application, we should allow them to decide how they want to handle exceptional cases like this, rather than forcing an option on them (eg shutdown everything) 

 

Imo we should fix this before 2.7 comes out, even if it's just a partial fix (eg we do option 1 in 2.7, but plan to implement option 3 eventually)",,ableegoldman,bbejeck,mjsax,sagarrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 18:43:42 UTC 2020,,,,,,,,,,"0|z0j260:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/20 17:08;sagarrao;hey [~ableegoldman], I can pick this one if needed? Is there anything more that you would want to add apart ffrom the nicely worded description? ;;;","05/Oct/20 18:53;ableegoldman;[~sagarrao] Yeah, go ahead! This should be a pretty small PR so it would be great if we could knock it out in the next week or two. Just ping me when it's ready.

For the PR itself, I think it sounds reasonable to just rethrow the TimeoutException to kill the thread. The ""add/recover stream thread"" functionality will probably slip 2.7, but it'll be implemented soon. So we don't really need to go out of our way to save a single thread from death in rare circumstances imo;;;","07/Oct/20 16:06;sagarrao;[~ableegoldman], i looked at the code today.. I see that, INCOMPLETE_SOURCE_TOPIC_METADATA error code is being thrown in 2 places:

[https://github.com/confluentinc/kafka/blob/master/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java#L379-L383]

and 

[https://github.com/confluentinc/kafka/blob/master/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java#L345-L351]

 

Both these places, it is being thrown for both TaskAssignmentException and TimeoutException. As per your suggestion, for the case of TimeoutException, i just need to rethrow TimeoutException and that's all I need to do?;;;","07/Oct/20 23:40;ableegoldman;Exactly, just rethrow the TimeoutException. Or actually we can just not catch it in those two blocks in the first place.;;;","08/Oct/20 03:18;sagarrao;PR available here: https://github.com/confluentinc/kafka/pull/422;;;","09/Oct/20 16:43;bbejeck;[~sagarrao] can you close the above PR and open it against [https://github.com/apache/kafka] ?

 

Thanks,

Bill;;;","09/Oct/20 18:43;ableegoldman;Ah thanks Bill, yeah please open it against the apache/kafka repo. And can you name it in the form ""KAFKA-10559: description of PR"" so that it'll be automatically linked to this ticket?

Thanks [~sagarrao]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing docs when describing topic configs including documentation,KAFKA-10557,13330208,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ecomar,mimaison,mimaison,30/Sep/20 16:34,02/Oct/20 11:09,13/Jul/23 09:17,02/Oct/20 11:09,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,,,,,,0,,,,,"When describing topic or broker configs with the AdminClient, we can request the documentation of configuration settings to be included.

This does not work with topic configs. The issue lies in this line:
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/AdminManager.scala#L767

This uses a KafkaConfig object to look up topic configs. Hence for configurations that have different names between KafkaConfig and LogConfig, no documentation is found!",,ecomar,mimaison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-30 16:34:19.0,,,,,,,,,,"0|z0j1zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaBasedLog can sleep for negative values,KAFKA-10531,13329804,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vikasconfluent,vikasconfluent,vikasconfluent,28/Sep/20 20:01,05/Oct/20 19:45,13/Jul/23 09:17,05/Oct/20 19:45,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,core,,,,,0,,,,,"{{time.milliseconds}} is not monotonic, so this code can throw :

{{java.lang.IllegalArgumentException: timeout value is negative}}

 
{code:java}
        long started = time.milliseconds();
        while (partitionInfos == null && time.milliseconds() - started < CREATE_TOPIC_TIMEOUT_MS) {
            partitionInfos = consumer.partitionsFor(topic);
            Utils.sleep(Math.min(time.milliseconds() - started, 1000));
        }
{code}

We need to check for negative value before sleeping.",,rhauch,vikasconfluent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 05 19:45:15 UTC 2020,,,,,,,,,,"0|z0izhs:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"05/Oct/20 19:45;rhauch;Merged to `trunk` and cherry-picked to the `2.6` and `2.5` branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InitProducerId may be blocked if least loaded node is not ready to send,KAFKA-10520,13329185,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,24/Sep/20 10:53,26/Jan/21 20:19,13/Jul/23 09:17,21/Oct/20 19:19,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,producer ,,,,,1,,,,,"From the logs of a failing producer that shows InitProducerId timing out after request timeout, it looks like we don't poll while waiting for transactional producer to be initialized and FindCoordinator request cannot be sent. The producer configuration used one bootstrap server and `max.in.flight.requests.per.connection=1`. The failing sequence:

 # Producer sends MetadataRequest to least loaded node (bootstrap server)
 # Producer is ready to send InitProducerId, needs to find transaction coordinator
 # Producer creates FindCoordinator request, but the only node known is the bootstrap server. Producer cannot send to this node since there is already the Metadata request in flight and max.inflight is 1.
 # Producer waits without polling, so Metadata response is not processed. InitProducerId times out eventually.
  
 
 We need to update the condition used to determine whether Sender should poll() to fix this issue.",,ableegoldman,philipp94831,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8803,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 07 17:56:43 UTC 2020,,,,,,,,,,"0|z0ivog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 17:46;ableegoldman;Hey [~rsivaram], is this something we can get fixed for 2.7? I'm just asking because the freeze deadlines are approaching, and this seems like it might be a simple fix for a pretty much fatal error (although workarounds do exist);;;","07/Oct/20 17:56;rsivaram;[~ableegoldman] Yes, will try and get this done in time for 2.7.0 code freeze.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE: Foreign key join serde may not be initialized with default serde if application is distributed,KAFKA-10515,13328945,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,thorsten.hake,thorsten.hake,thorsten.hake,23/Sep/20 08:50,12/Nov/20 10:35,13/Jul/23 09:17,20/Oct/20 23:46,2.5.1,2.6.0,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"The fix of KAFKA-9517 fixed the initialization of the foreign key joins serdes for KStream applications that do not run distributed over multiple instances.

However, if an application runs distributed over multiple instances, the foreign key join serdes may still not be initialized leading to the following NPE:
{noformat}
Encountered the following error during 
processing:java.lang.NullPointerException: null
	at 
org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde$SubscriptionWrapperSerializer.serialize(SubscriptionWrapperSerde.java:85)
	at 
org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde$SubscriptionWrapperSerializer.serialize(SubscriptionWrapperSerde.java:52)
	at 
org.apache.kafka.streams.state.internals.ValueAndTimestampSerializer.serialize(ValueAndTimestampSerializer.java:59)
	at 
org.apache.kafka.streams.state.internals.ValueAndTimestampSerializer.serialize(ValueAndTimestampSerializer.java:50)
	at 
org.apache.kafka.streams.state.internals.ValueAndTimestampSerializer.serialize(ValueAndTimestampSerializer.java:27)
	at 
org.apache.kafka.streams.state.StateSerdes.rawValue(StateSerdes.java:192)
	at 
org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$put$3(MeteredKeyValueStore.java:144)
	at 
org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)
	at 
org.apache.kafka.streams.state.internals.MeteredKeyValueStore.put(MeteredKeyValueStore.java:144)
	at 
org.apache.kafka.streams.processor.internals.ProcessorContextImpl$KeyValueStoreReadWriteDecorator.put(ProcessorContextImpl.java:487)
	at 
org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionStoreReceiveProcessorSupplier$1.process(SubscriptionStoreReceiveProcessorSupplier.java:102)
	at 
org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionStoreReceiveProcessorSupplier$1.process(SubscriptionStoreReceiveProcessorSupplier.java:55)
	at 
org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142)
	at 
org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)
	at 
org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142)
	at 
org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201)
	at 
org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180)
	at 
org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:133)
	at 
org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:104)
	at 
org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$3(StreamTask.java:383)
	at 
org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)
	at 
org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:383)
	at 
org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.process(AssignedStreamsTasks.java:475)
	at 
org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:550)
	at 
org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:802)
	at 
org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)
	at 
org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670){noformat}

This happens because the processors for foreign key joins will be distributed across multiple tasks. The serde will only be initialized with the default serde during the initialization of the task containing the sink node (""subscription-registration-sink""). So if the task containing the SubscriptionStoreReceiveProcessor (""subscription-receive"") is not assigned to the same instance as the task containing the sink node, a NPE will be thrown because the Serde of the state store used within the SubscriptionStoreReceiveProcessor is not initialized.",,ableegoldman,bbejeck,mjsax,thorsten.hake,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9517,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 16:33:03 UTC 2020,,,,,,,,,,"0|z0iu7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/20 15:43;vvcephei;Thanks for this report, [~thorsten.hake] !

I've just taken a look at the code and the stacktrace, and I see that your explanation is spot on.

I've gone ahead and marked the ticket as a blocker for the next release in each reported version, although the release managers may push back, since this is not a regression. Still, I think we should get it fixed asap.

Would you like to submit a patch for this? No pressure; I just wanted to check if you were planning on claiming this one.

Thanks,

-John;;;","24/Sep/20 19:17;thorsten.hake;@[~vvcephei] I would like to submit a patch. Thanks for asking :);;;","25/Sep/20 05:10;thorsten.hake;I just submitted a PR for the trunk branch: https://github.com/apache/kafka/pull/9338;;;","09/Oct/20 16:55;vvcephei;Thanks for the patch [~thorsten.hake] ! I'll review it as soon as I can.

By the way, [~bbejeck] is working on the release now and is following up on all blockers. I downgraded this to ""critical"" just because it doesn't meet the criteria of either being a regression in 2.7.0 or having a really destructive effect.

I absolutely want to get it in to 2.7.0, though.

Thanks!;;;","20/Oct/20 23:46;bbejeck;Resolved via [https://github.com/apache/kafka/pull/9338]

 

Merged to trunk and cherry-picked to 2.7;;;","21/Oct/20 15:25;vvcephei;Removed the 2.6 and 2.5 fix versions until [https://github.com/apache/kafka/pull/9467] is merged and picked.;;;","03/Nov/20 16:33;vvcephei;[https://github.com/apache/kafka/pull/9467] is merged to 2.6. There were also merge conflicts on 2.5, but there's no bugfix release proposed for 2.5 yet, so I decided just to leave it at 2.6/2.7/trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed test StateDirectoryTest.shouldLogStateDirCleanerMessage,KAFKA-10514,13328926,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,chia7712,chia7712,23/Sep/20 06:34,05/Oct/20 20:23,13/Jul/23 09:17,24/Sep/20 06:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,unit tests,,,,,0,flaky,,,,"{quote}
java.lang.AssertionError: 
Expected: a collection containing a string ending with ""ms has elapsed (cleanup delay is 0ms).""
     but: was empty
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldLogStateDirCleanerMessage(StateDirectoryTest.java:569)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at jdk.internal.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119)
	at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834)
{quote}",,cadonna,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 08:22:31 UTC 2020,,,,,,,,,,"0|z0iu34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 07:09;chia7712;more details: https://github.com/apache/kafka/pull/9262#issuecomment-697177819;;;","23/Sep/20 08:22;cadonna;Thank you for catching this! This test seems to be flaky for the reason, you pointed out. I will submit a PR shortly!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Threadlocal  may can not set null,because it may create a memory leak",KAFKA-10502,13328285,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,huangyiminghappy@163.com,huangyiminghappy@163.com,18/Sep/20 17:04,28/Sep/20 08:30,13/Jul/23 09:17,28/Sep/20 02:16,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"When setting Threadlocal to null it may create a memory leak, you can see the link:
[https://stackoverflow.com/questions/12424838/threadlocal-remove], so I think we should invoke its remove instead ",,huangyiminghappy@163.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-18 17:04:35.0,,,,,,,,,,"0|z0iq4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"4 Unit Tests are breaking after addition of a new A record to ""apache.org""",KAFKA-10499,13328122,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,prat0318,prat0318,17/Sep/20 19:53,19/Oct/20 07:47,13/Jul/23 09:17,19/Oct/20 07:47,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,unit tests,,,,,0,,,,,"{{apache.org}} earlier used to resolve only to 2 A records: 95.216.24.32 and 40.79.78.1

 

With addition of a new A record 95.216.26.30, 4 unit tests have started failing, which expect the count of DNS resolution to be 2, but instead it is now 3.

 
{code:java}
org.apache.kafka.clients.ClusterConnectionStatesTest > testMultipleIPsWithUseAll FAILED
    java.lang.AssertionError: expected:<2> but was:<3>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at org.apache.kafka.clients.ClusterConnectionStatesTest.testMultipleIPsWithUseAll(ClusterConnectionStatesTest.java:241)


org.apache.kafka.clients.ClusterConnectionStatesTest > testHostResolveChange FAILED
    java.lang.AssertionError: expected:<2> but was:<3>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at org.apache.kafka.clients.ClusterConnectionStatesTest.testHostResolveChange(ClusterConnectionStatesTest.java:256)

org.apache.kafka.clients.ClusterConnectionStatesTest > testMultipleIPsWithDefault FAILED
    java.lang.AssertionError: expected:<2> but was:<3>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at org.apache.kafka.clients.ClusterConnectionStatesTest.testMultipleIPsWithDefault(ClusterConnectionStatesTest.java:231)

org.apache.kafka.clients.ClientUtilsTest > testResolveDnsLookupAllIps FAILED
    java.lang.AssertionError: expected:<2> but was:<3>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:834)
        at org.junit.Assert.assertEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:631)
        at org.apache.kafka.clients.ClientUtilsTest.testResolveDnsLookupAllIps(ClientUtilsTest.java:87)
 {code}",,prat0318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 19 07:47:16 UTC 2020,,,,,,,,,,"0|z0ip4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 07:47;dongjin;Resolved with commit {{1443f24}} (see: https://github.com/apache/kafka/pull/9294);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception if users try to update configs of existent listeners,KAFKA-10479,13327450,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,14/Sep/20 14:07,29/Sep/20 19:15,13/Jul/23 09:17,29/Sep/20 19:15,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"{code}
    def immutableListenerConfigs(kafkaConfig: KafkaConfig, prefix: String): Map[String, AnyRef] = {
      newConfig.originals.asScala.filter { case (key, _) =>
        key.startsWith(prefix) && !DynamicSecurityConfigs.contains(key)
      }
    }
{code}

We don't actually compare new configs to origin configs so the suitable exception is not thrown.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-14 14:07:37.0,,,,,,,,,,"0|z0il00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink Connector fails with DataException when trying to convert Kafka record with empty key to Connect Record,KAFKA-10477,13327151,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,shaikzakir.iitm,shaikzakir.iitm,shaikzakir.iitm,11/Sep/20 20:59,12/Oct/20 12:41,13/Jul/23 09:17,02/Oct/20 16:49,2.3.1,2.4.0,2.4.1,2.5.0,2.5.1,2.6.0,,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.2,2.5.2,2.6.1,2.7.0,,,KafkaConnect,,,,,0,,,,,"Sink connector is facing a DataException when trying to convert a kafka record with empty key to Connect data format. 

Kafka's trunk branch currently depends on *jackson v2.10.5* 

A short unit test (shared below) in `org.apache.kafka.connect.json.JsonConverterTest` class reproduces the issue.  
{code:java}
@Test
    public void testToConnectDataEmptyKey() throws IOException {
        Map<String, Boolean> props = Collections.singletonMap(""schemas.enable"", false);
        converter.configure(props, true);
        String str = """";
        SchemaAndValue schemaAndValue = converter.toConnectData(""testTopic"", str.getBytes());
        System.out.println(schemaAndValue);
    }
{code}
This test code snippet fails with the following exception:
{noformat}
org.apache.kafka.connect.errors.DataException: Unknown schema type: null

	at org.apache.kafka.connect.json.JsonConverter.convertToConnect(JsonConverter.java:764)
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:385)
	at org.apache.kafka.connect.json.JsonConverterTest.testToConnectDataEmptyKey(JsonConverterTest.java:792)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{noformat}
 

This seems related to the issue [https://github.com/FasterXML/jackson-databind/issues/2211] , where jackson lib started returning `MissingNode` for empty input in `ObjectMapper.readTree(input)` method invocation. Precise code change can be observed here: [https://github.com/FasterXML/jackson-databind/commit/f0abe41b54b36f43f96f05ab224f6e6f364fbe7a#diff-0d472011dea2aac97f0381097cd1a0bfR4094] 

 

This causes an exception to throw up in our JsonConverter class : [https://github.com/apache/kafka/blob/8260d7cdfbe30250e8bf4079c8f0734e1b5a203b/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L764] 

 

In my opinion, when the `jsonValue.getNodeType()` is `MISSING` ([https://github.com/apache/kafka/blob/8260d7cdfbe30250e8bf4079c8f0734e1b5a203b/connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java#L754] ), we need to fall back to the behaviour of the case `NULL` (i.e. return null), although not sure of any further repercussions this might bring in.

 

Things were working fine when the dependency on *jackson* lib was of version  *v2.9.10.3* or lesser as the `ObjectMapper` returned null in that case.

 

Thanks,

Zakir",,rhauch,shaikzakir.iitm,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 12:41:20 UTC 2020,,,,,,,,,,"0|z0ij5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/20 21:29;shaikzakir.iitm;Observed that the issue doesn't occur in Kafka *v2.3.0* (where jackson dependency resolves to *v2.9.9*).;;;","23/Sep/20 18:18;shaikzakir.iitm;All versions up and above *v2.3.1* are affected by this issue. ;;;","01/Oct/20 15:39;shaikzakir.iitm;Following are the version dependencies of Apache Kafka (ak) on com.fasterxml.jackson.core:jackson-databind (j)
 * ak v2.1.0 -> j v2.9.7
 * ak v2.1.1 -> j v2.9.8
 * ak v2.2.0 -> j v2.9.8
 * ak v2.2.1 -> j v2.9.8
 * ak v2.2.2 -> j v2.10.0
 * ak v2.3.0 -> j v2.9.9 [*Looks like an outlier, where ak version increased with a decrease in jackson-databind version*]
 * ak v2.3.1 -> j v2.10.0
 * ak v2.4.0 -> j v2.10.0
 * ak v2.4.1 -> j v2.10.0
 * ak v2.5.1 -> j v2.10.2
 * ak v2.6 -> j v2.10.2

The behavior of the Jackson ObjectMapper#readTree() method when called with a blank string/empty-input changed as follows (based on the information from [FasterXML/jackson-databind#2211|https://github.com/FasterXML/jackson-databind/issues/2211]) :
 * For Jackson v2.x-2.8.x, return NullNode
 * For Jackson v2.9.x return null
 * For Jackson v2.10.0 and beyond return MissingNode

However, in all AK versions JsonConverter throws an error when it gets a MissingNode. This is only a problem for AK versions (v2.2.2, v2.3.1, v2.4.0, v2.4.1, v2.5.1) that were released with Jackson 2.10.0 or later:
 * All versions of AK 2.1.x-2.2.1, ObjectMapper.readTree() returns null on empty input, which AK populates in an envelope, and the payload in the envelope which is null is treated as json node of JsonNodeType NULL. And this NULL case is handled in {{JsonConverter#convertToConnect()}}.
 * AK version 2.2.2, ObjectMapper.readTree() returns MissingNode on empty input, which AK populates as payload in envelope; this payload is a jsonNode of JsonNodeType MISSING. And {{JsonConverter#convertToConnect()}} considers {{schemaType}} to be null for MISSING case, which results in {{DataException}}.
 * AK 2.3.0, (uses jackson-databind version 2.9.9), ObjectMapper.readTree() returns null on empty input, which AK populates as payload in envelope; this payload when retrieved in to be used in JsonConverter.convertToConnect(), is treated as NullNode, of JsonNodeType NULL. (as in the case of AK 2.1.x-2.2.1).
 * AK 2.3.1-2.6, ObjectMapper.readTree() returns MissingNode on empty input, which AK populates as payload in envelope; this payload is a jsonNode of type MISSING which causes the DataException.

So, this PR aims to resolve the DataException that gets thrown when jackson-databind's {{ObjectMapper.readTree()}} returns MissingNode upon encountering an empty input, so that JsonConverter treats MissingNode in a similar behavior as that of null.;;;","01/Oct/20 23:27;rhauch;Thanks for putting together this comprehensive list of AK versions, Jackson versions, and behavior.

You mentioned:
{quote}
* ak v2.3.0 -> j v2.9.9 [Looks like an outlier, where ak version increased with a decrease in jackson-databind version]
{quote}

I don't think this was an outlier. If you sort them by release dates, things look a bit more logical and you can see how the Jackson versions only continually increase over time:
* ak v2.1.0 (2018-11-20)-> j v2.9.7
* ak v2.1.1 (2019-02-15) -> j v2.9.8
* ak v2.2.0 (2019-03-22) -> j v2.9.8
* ak v2.2.1 (2019-06-01) -> j v2.9.8
* ak v2.3.0 (2019-06-24) -> j v2.9.9
* ak v2.3.1 (2019-10-24) -> j v2.10.0
* ak v2.2.2 (2019-12-01) -> j v2.10.0
* ak v2.4.0 (2019-12-13) -> j v2.10.0
* ak v2.4.1 (2020-03-10)-> j v2.10.0
* ak v2.5.0 (2020-04-14) -> j v2.10.2
* ak v2.6.0 (2020-08-03) -> j v2.10.2
* ak v2.5.1 (2020-08-10) -> j v2.10.2

IOW, the following versions are all affected since they use Jackson 2.10.x:

* 2.3.1
* 2.2.2
* 2.4.0
* 2.4.1
* 2.5.0
* 2.5.1
* 2.6.0

and thus the following branches are affected since they now use Jackson 2.10.x:

* 2.2
* 2.3
* 2.4
* 2.5
* 2.6

I've also gone through a number of versions of the https://github.com/FasterXML/jackson-databind code, looking for how MissingNode is used. It's obvious that MissingNode is used a lot more prevalently in 2.10.0. But the documentation for MissingNode is as follows:

{quote}
In most respects this placeholder node will act as {@link NullNode};
for example, for purposes of value conversions, value is considered
to be null and represented as value zero when used for numeric
conversions.
{quote}

So I think it's pretty clear that we should treat JsonNodeType.MISSING as null. This is what this PR tries to do, and I'm now less concerned about unexpected behavioral changes after having read and verified this analysis.

Once this PR is merged to trunk, this fix will need to be backported to the following branches that are all affected because they use Jackson 2.10.x:

* 2.2
* 2.3
* 2.4
* 2.5
* 2.6;;;","02/Oct/20 16:49;rhauch;Merged to trunk, and backported to all of the branches that were changed to use Jackson 2.10 or later: `2.6`, `2.5`, `2.4`, `2.3`, and `2.2`.;;;","12/Oct/20 12:41;shaikzakir.iitm;Thanks [~rhauch] for helping in closing this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeIndex handling may cause data loss in certain back to back failure,KAFKA-10471,13326557,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ramanverma,rshekhar,rshekhar,08/Sep/20 21:03,15/Jun/21 01:39,13/Jul/23 09:17,02/Nov/20 21:09,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,log,,,,0,,,,,"# Active segment for log A going clean shutdown - trim the time index to the latest fill value, set the clean shutdown marker.
 # Broker restarts, loading logs - no recovery due to clean shutdown marker, log A recovers with the previous active segment as current. It also resized the TimeIndex to the max.
 #  Before all the log loads, the broker had a hard shutdown causing a clean shutdown marker left as is.
 #  Broker restarts, log A skips recovery due to the presence of a clean shutdown marker but the TimeIndex file assumes the resized file from the previous instance is all full (it assumes either file is newly created or is full with valid value).
 # The first append to the active segment will result in roll and TimeIndex will be rolled with the timestamp value of the last valid entry (0)
 # Segment's largest timestamp gives 0 (this can cause premature deletion of data due to retention.",,dhruvilshah,iBlackeyes,ijuma,junrao,ramanverma,rshekhar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 21:09:01 UTC 2020,,,,,,,,,,"0|z0ifi8:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"08/Sep/20 21:39;junrao;[~rshekhar]: Thanks for reporting this. This is a very good finding. If the TimeIndex doesn't start in a clean state, it can cause multiple things to go wrong afterward.

One way to fix this issue is to check the presence of the clean shutdown file in LogManager.loadLogs() before loading each individual log. We  then delete the clean shutdown file and pass a clean shutdown flag to Log and use that in Log.recoverLog(). That way, in step 4 above, the TimeIndex will be rebuilt properly.;;;","08/Sep/20 21:46;dhruvilshah;It may be nice to use the time index sanity check to catch issues where an index file is not in a state we expect. We used to perform a sanity check for indices associated with all segments, but with the changes to load segments lazily, we dropped those checks. If we could reintroduce those checks back safely, that may be sufficient to catch and fix such cases. We are taking a similar approach in https://issues.apache.org/jira/browse/KAFKA-10207, so that might help here too.;;;","08/Sep/20 21:59;junrao;[~dhruvilshah] : I agree that the sanity check in the index file is useful to catch bugs like this. For this particular bug, the root cause is that the clean shutdown file doesn't reflect the actual on-disk state correctly. So, it would be useful to fix that directly too.;;;","08/Sep/20 22:00;rshekhar;+1 [~junrao] will detect unclean shutdown more reliably.

[~dhruvilshah] agree with your comment too, just not sure the cost of it during the log load time, so can we do a sanity test in a lazy fashion too?;;;","02/Oct/20 09:08;ramanverma;https://github.com/apache/kafka/pull/9364;;;","02/Nov/20 21:09;junrao;merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zstd decompression with small batches is slow and causes excessive GC,KAFKA-10470,13326529,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yuzawa-san,wolfchimneyrock,wolfchimneyrock,08/Sep/20 17:01,11/Nov/20 02:12,13/Jul/23 09:17,11/Nov/20 02:12,2.5.1,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"Similar to KAFKA-5150 but for zstd instead of LZ4, it appears that a large decompression buffer (128kb) created by zstd-jni per batch is causing a significant performance bottleneck.

The next upcoming version of zstd-jni (1.4.5-7) will have a new constructor for ZstdInputStream that allows the client to pass its own buffer.  A similar fix as [PR #2967|https://github.com/apache/kafka/pull/2967] could be used to have the  ZstdConstructor use a BufferSupplier to re-use the decompression buffer.",,ableegoldman,cbeard,chia7712,rafal_chmielewski,sayuan,wolfchimneyrock,yuzawa-san,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 05:30:48 UTC 2020,,,,,,,,,,"0|z0ifc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 06:26;chia7712;Yep, that is why I submitted the patch to zstd-jni. the pressure of GC produced by kafka Zstd compression is trouble to me also.;;;","25/Sep/20 18:28;yuzawa-san;I also noticed the large amount of allocations and GC activity in my profiling. However, there is an additional issue related to the the number of calls Kafka does to ZstdOutputStream.write(int). Each of these single byte writes gets sent to the JNI for compression. I think an input buffer could improve this, by only crossing over into the JNI code when a critical mass of input has been accumulated. Option 1: We could wrap the ZstdOutputStream with a BufferedOutputStream like how it is done for GZIP currently. Option 2: Alter the library to use buffering. I have this ticket open with the zstd-jni project [https://github.com/luben/zstd-jni/issues/141];;;","20/Oct/20 14:01;wolfchimneyrock;
[zstd-jni version 1.4.5-7|https://github.com/luben/zstd-jni/releases/tag/v1.4.5-7] has been released, which internally implements a BufferPool to re-use the decompression buffer without changing their API.  This fixes the issue of GC pressure but doesn't address [~yuzawa-san]'s concern about JNI boundary crossings.

My initial test with this updated dependency shows memory allocations and GC pressure comparable to gzip. overall throughput with small message batches still seems to lag behind all other codecs.
;;;","20/Oct/20 14:11;yuzawa-san;The maintainer of zstd-jni seems to be back from vacation, so I was going to inquire more about what they think about adding more buffering on that linked github issue above. I'm finding the performance hits are occurring when Kafka writes a few bytes at a time typically from ByteUtils class when using the ZstdOutputStream. I also found that the ZstdInputStream has the same performance hit when reading a few bytes at a time. As a stopgap solution, we could add a BufferedOutputStream and BufferedInputStream like it is done for GZIP:

[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java#L57]

[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java#L69]

but in the zstd enum value [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java#L118]

This would help but at the cost of not being able to reuse the internal buffers within the BufferedOutputStream and and BufferedInputStream.;;;","20/Oct/20 16:27;wolfchimneyrock;I have run a test with the zstd streams wrapped with buffered streams as [~yuzawa-san] mentioned, and together with the upgraded zstd-jni package seems to give zstd a throughput similar to gzip when using small message batches, and there is no additional discernable broker memory overhead.  I think this is the way to go.

I don't think that not being able to re-use the BufferedOutputStream or BufferedInputStream buffers is that important.;;;","20/Oct/20 17:09;yuzawa-san;Ok, I'm still going to attempt to add some buffering within the library per my conversation here https://github.com/luben/zstd-jni/issues/141#issuecomment-712922973;;;","20/Oct/20 18:06;wolfchimneyrock;sounds good;;;","21/Oct/20 02:51;chia7712;[~yuzawa-san] Thanks for your effort on zstd-jni. Do you want to take over this issue to leverage the new BufferPool you introduced ? 

There are two tasks for this issue.

1. update zstd to v1.4.5-8 to use the new interface (BufferPool) introduced by your PR (https://github.com/luben/zstd-jni/pull/146)
2. implement the BufferPool of zstd to reuse the byte buffer (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java)

Also, there is a PR (https://github.com/apache/kafka/pull/9229) trying to use true reusable BufferSupplier to create IO stream (before the PR, the passed BufferSupplier does not recycle the byte buffer).

;;;","26/Oct/20 05:30;chia7712;[~yuzawa-san] I have assigned this issue to you as you have filed a PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect logging hierarchy (KIP-676),KAFKA-10469,13326521,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,tombentley,tombentley,08/Sep/20 16:15,07/Jan/21 05:18,13/Jul/23 09:17,10/Nov/20 14:26,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"{{Log4jController#loggers}} incorrectly uses the root logger's log level for any loggers which lack a configured log level of their own. This is incorrect because loggers without an explicit level inherit their level from their parent logger and this resolved level might be different from the root logger's level. This means that the levels reported from {{Admin.describeConfigs}}, which uses {{Log4jController#loggers}} are incorrect. This can be shown by using the default {{log4j.properties}} and describing a broker's loggers, it reports

{noformat}
kafka.controller=TRACE
kafka.controller.ControllerChannelManager=INFO
kafka.controller.ControllerEventManager$ControllerEventThread=INFO
kafka.controller.KafkaController=INFO
kafka.controller.RequestSendThread=INFO
kafka.controller.TopicDeletionManager=INFO
kafka.controller.ZkPartitionStateMachine=INFO
kafka.controller.ZkReplicaStateMachine=INFO
{noformat}

The default {{log4j.properties}} does indeed set {{kafka.controller}} to {{TRACE}}, but it does not configure the others, so they're actually at {{TRACE}} not {{INFO}} as reported.



",,ecomar,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-08 16:15:21.0,,,,,,,,,,"0|z0ifa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the necessary utilities in Dockerfile should include git,KAFKA-10463,13326285,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,07/Sep/20 10:02,11/Dec/20 03:16,13/Jul/23 09:17,11/Dec/20 03:16,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"the default image of Dockerfile is openjdk:8 and it pre-installed git so it is fine that necessary utilities does not include git. However, the later version of openjdk image does not include git by default and error message ""git: command not found"" ensues.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-07 10:02:32.0,,,,,,,,,,"0|z0idts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaListValidator format checking is incomplete,KAFKA-10460,13325910,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,akumar,robinp-tw,robinp-tw,03/Sep/20 11:54,03/Dec/20 14:37,13/Jul/23 09:17,03/Dec/20 14:37,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"See [https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ConfigHandler.scala#L220] . The logic is supposed to accept only two cases:
 * list of k:v pairs
 * a single '*'

But in practice, since the disjunction's second part only checks that the head is '*', the case where a k:v list is headed by a star is also accepted (and then later broker dies at startup, refusing the value).

This practically happened due to a CruiseControl bug (see [https://github.com/linkedin/cruise-control/issues/1322])

Observed on 2.4, but seems to be present in HEAD's source as well.",,akumar,robinp-tw,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 07:34:51 UTC 2020,,,,,,,,,,"0|z0ibig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 16:47;akumar;Hey [~robinp-tw],

Are you working on this? If not, I will be happy to pick it up.;;;","22/Sep/20 07:34;robinp-tw;Thanks [~akumar], that would be nice.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Probing rebalances are not guaranteed to be triggered by non-leader members,KAFKA-10455,13325570,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,lthomas,ableegoldman,ableegoldman,01/Sep/20 17:45,07/Jan/21 19:51,13/Jul/23 09:17,19/Oct/20 19:49,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,,,,,"Apparently, if a consumer rejoins the group with the same subscription userdata that it previously sent, it will not trigger a rebalance. The one exception here is that the group leader will always trigger a rebalance when it rejoins the group.

This has implications for KIP-441, where we rely on asking an arbitrary thread to enforce the followup probing rebalances. Technically we do ask a thread living on the same instance as the leader, so the odds that the leader will be chosen aren't completely abysmal, but for any multithreaded application they are still at best only 50%.

Of course in general the userdata will have changed within a span of 10 minutes, so the actual likelihood of hitting this is much lower –  it can only happen if the member's task offset sums remained unchanged. Realistically, this probably requires that the member only have fully-restored active tasks (encoded with the constant sentinel -2) and that no tasks be added or removed.

 

One solution would be to make sure the leader is responsible for the probing rebalance. To do this, we would need to somehow expose the memberId of the thread's main consumer to the partition assignor. I'm actually not sure if that's currently possible to figure out or not. If not, we could just assign the probing rebalance to every thread on the leader's instance. This shouldn't result in multiple followup rebalances as the rebalance schedule will be updated/reset on the first followup rebalance.

Another solution would be to make sure the userdata is always different. We could encode an extra bit that flip-flops, but then we'd have to persist the latest value somewhere/somehow. Alternatively we could just encode the next probing rebalance time in the subscription userdata, since that is guaranteed to always be different from the previous rebalance. This might get tricky though, and certainly wastes space in the subscription userdata. Also, this would only solve the problem for KIP-441 probing rebalances, meaning we'd have to individually ensure the userdata has changed for every type of followup rebalance (see related issue below). So the first proposal, requiring the leader trigger the rebalance, would be preferable.

Note that, imho, we should just allow anyone to trigger a rebalance by rejoining the group. But this would presumably require a broker-side change and thus we would still need a workaround for KIP-441 to work with brokers.

 

Related issue:
This also means the Streams workaround for [KAFKA-9821|http://example.com] is not airtight, as we encode the followup rebalance in the member who is supposed to _receive_ a revoked partition, rather than the member who is actually revoking said partition. While the member doing the revoking will be guaranteed to have different userdata, the member receiving the partition may not. Making it the responsibility of the leader to trigger _any_ type of followup rebalance would solve this issue as well.

Note that other types of followup rebalance (version probing, static membership with host info change) are guaranteed to have a change in the subscription userdata, and will not hit this bug",,ableegoldman,eran-levy,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 11 17:54:55 UTC 2020,,,,,,,,,,"0|z0i9ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/20 20:31;vvcephei;Thanks, [~ableegoldman] . This is a bummer.

It seems like when we want to force a rebalance, we just have to make sure the user-data changes in some way. IIUC, each consumer gets a PartitionAssignor instance with the same lifecycle as the consumer itself. Therefore, we can just initialize a single `byte`, which we'll just slap onto the subscription userdata and ignore when deserializing. We can just increment it for each forced rebalance and let it roll over.;;;","01/Sep/20 23:47;ableegoldman;Yeah ok, that sounds reasonable. Initially I was thinking that we should avoid relying on any kind of in-memory counter to enforce a change in the userdata, to avoid losing the counter in the event of a restart and having it get initialized back to the previous value. However, presumably upon rejoining after a restart the member would send in a new subscription, and the broker would save that as the latest subscription userdata, so incrementing the counter upon every subscription should be ok. 

I'll try to look around in the broker code to verify that it actually updates the member's subscription metadata, even if eg static membership is used;;;","03/Sep/20 05:23;guozhang;I'm trying to assess statistically how large this may impact probing rebalances: my understanding is that since in Streams' subscription info we encode `taskOffsetSumsCache`, its value is highly likely to change in each new join-group's subscription, the chance that we will have exactly the same serialized bytes should be small?;;;","03/Sep/20 18:02;ableegoldman;[~guozhang] if the triggering member has only active (and running) tasks, they will always be encoded with a sentinel offset sum of ""-2"" which would not change between rebalances. I do think that if the member has any standbys however, the odds of this happening are small (it could still be the case that the task offset sums remain unchanged, for example if the standbys are completely starved by the active tasks, but this seems like a pretty rare edge case. So realistically it's just the purely-active assignment that we have to worry about);;;","08/Sep/20 18:13;guozhang;Thanks for the info [~ableegoldman]. I think we can have two approaches here which are not necessarily conflicting with each other (i.e. we can probably do both):

1. We a) modify client to always refresh metadata before re-joining the group to reduce the probability of rebalance storm, and then b) modify brokers to just allow any join request to trigger a rebalance. But this depends on brokers and hence people who do not upgrade their brokers would not get this fix.

2. Based on the fact that only consumers from the same instance of the leader thread would trigger rebalances, we can take it as a by-product of multi-threading proposal such that each instance would only have one consumer instead of one-consumer per thread. And with a single consumer per instance, we can avoid this issue as well as simplifying our two-phase assignment algorithm to single-phase as well.;;;","11/Sep/20 17:54;ableegoldman;Yeah, I think you're touching on a related issue – fast detection of source topic deletion – which I agree can/should be solved along the same lines as the probing rebalance bug in this ticket.

Just for some context, to avoid accidental data corruption/loss, we of course want to react as fast as possible on source topic deletion. Currently we detect the absence of source topics during a rebalance and send an error code to all members to shut down. The problem is that there may be a gap of up to the metadata.max.age (default 5min) between the topic deletion and the reaction, ie triggering a rebalance and informing all members to shut down. Since only the leader is guaranteed to trigger a rebalance upon sending a JoinGroup, unless the leader happens to be assigned one of the partitions of the deleted tasks, it will not notice the topic deletion until it refreshes its metadata. If non-leaders are assigned to these deleted partitions and notice the topic deletion, they may not be able to trigger a rebalance even if they rejoin the group.

Both problems could be solved by modifying the userdata to ensure any member's JoinGroup results in a rebalance. We could just add a single byte to the SubscriptionInfo and bump it when rejoining. This actually seems like a better all-around solution, since members of Streams should not be haphazardly sending JoinGroups for no reason – if they do, it must be because they want a rebalance. This way we don't have to worry about changing any broker side code and finding a workaround for older brokers. 

We could also take the approach of making sure the leader is responsible for triggering the rebalance, but this doesn't solve the source topic deletion problem. It also wouldn't help us in any new feature we wanted to add that required arbitrary members to trigger a rebalance. So I think we should just go with bumping a byte in the SubscriptionInfo;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams Stuck in infinite REBALANCING loop when stream <> table join partitions don't match,KAFKA-10454,13325490,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lkokhreidze,lkokhreidze,lkokhreidze,01/Sep/20 09:35,03/Nov/20 14:14,13/Jul/23 09:17,20/Oct/20 22:48,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,2.8.0,,,,,,streams,,,,,0,,,,,"Here's integration test: [https://github.com/apache/kafka/pull/9237]

 

From the first glance, issue is that when one joins stream to table, and table source topic doesn't have same number of partitions as stream topic, `StateChangelogReader` tries to recover state from changelog (which in this case is the same as source topic) for table from partitions that don't exist. Logs are spammed with: 

 
{code:java}
[2020-09-01 12:33:07,508] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,508] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,508] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,510] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,510] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,510] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,510] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,513] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,513] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,513] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,513] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,515] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,515] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,515] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,515] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,517] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,518] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,518] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,518] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,520] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,520] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,520] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,520] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,522] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,522] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,522] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,522] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,524] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,524] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,525] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,525] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,526] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,527] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,527] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,527] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-3 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,529] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-0 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,529] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-1 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716) [2020-09-01 12:33:07,529] INFO stream-thread [app-StreamTableJoinInfiniteLoopIntegrationTestloop-86ae06c3-5758-429f-9d29-94ed516db126-StreamThread-1] End offset for changelog topic-b-StreamTableJoinInfiniteLoopIntegrationTestloop-2 cannot be found; will retry in the next time. (org.apache.kafka.streams.processor.internals.StoreChangelogReader:716)
{code}
And Kafka Streams never moves to RUNNING state.",,ableegoldman,bbejeck,lkokhreidze,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 22:48:13 UTC 2020,,,,,,,,,,"0|z0i8xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/20 09:41;lkokhreidze;CC [~mjsax] [~vvcephei] [~guozhang] 

I may try to fix this as soon as I can - but if by any chance somebody wants to pick this up before me, feel free to do so.;;;","01/Sep/20 09:45;lkokhreidze;I guess, in this scenario, Kafka Streams should do co-partitioning check and enforce num of partitions inherited from table source topic?;;;","01/Sep/20 09:58;lkokhreidze;Workaround we have used so far is to force repartitioning with num of partitions that equal to table source topic. Based on example from the integration test posted above, it would look like this:

 
{code:java}
stream
    .selectKey((key, value) -> key)
    .repartition(Repartitioned.numberOfPartitions(2))
    .join(table, (value1, value2) -> value2)
    .to(outputTopic);

{code};;;","01/Sep/20 15:00;vvcephei;Huh, thanks for the report and repro, [~lkokhreidze] !

I think that if we didn't do selectKey in there, then you _would_ get an exception, since we would check that the stream and table have the same number of partitions for the join. I'm guessing that the selectKey is inserting a repartition node, which is (correctly) configured to be co-partitioned with the table, but the source-changelog optimization is kicking in and ignoring that the repartition node and source topic have different numbers of partitions.

If that sounds right to you, then the solution should be to add a partition-count check before applying the source-changelog optimization. Your workaround looks like it works also to disable the source-changelog optimization because the repartition() operator always forces a repartition node.

Thanks!

-John;;;","09/Sep/20 20:03;lkokhreidze;Hey [~vvcephei] thanks for the feedback.

Finally got time today to look into this issue. I've managed to find the cause and chose slightly different approach then what you suggested. 
Would love you to get your feedback on PR [https://github.com/apache/kafka/pull/9237] ;;;","20/Oct/20 22:48;bbejeck;Resolved via https://github.com/apache/kafka/pull/9237;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Connect's Values class loses precision for integers, larger than 64 bits",KAFKA-10439,13324674,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,odiachenko,odiachenko,odiachenko,26/Aug/20 16:19,06/Oct/20 01:20,13/Jul/23 09:17,06/Oct/20 01:20,,,,,,,,,,,,,,,,,,,,,,,2.0.2,2.1.2,2.2.3,2.3.2,2.4.2,2.5.2,2.6.1,2.7.0,KafkaConnect,,,,,0,,,,,"The `org.apache.kafka.connect.data.Values#parse` method parses integers, which are larger than `Long.MAX_VALUE` as `double` with `

Schema.FLOAT64_SCHEMA`.

 

That means it loses precision for these larger integers.

For example:
{code:java}
SchemaAndValue schemaAndValue = Values.parseString(""9223372036854775808"");
{code}
returns:
{code:java}
SchemaAndValue{schema=Schema{FLOAT64}, value=9.223372036854776E18}
{code}",,kkonstantine,odiachenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 01:20:20 UTC 2020,,,,,,,,,,"0|z0i3w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 01:20;kkonstantine;Merged and back ported to the branches listed under ""Fix version"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderEpochCache is incorrectly recovered on segment recovery for epoch 0,KAFKA-10432,13324511,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lucasbradstreet,lucasbradstreet,lucasbradstreet,25/Aug/20 17:01,20/Dec/22 00:25,13/Jul/23 09:17,08/Sep/20 19:50,2.3.0,2.4.0,2.5.0,2.6.0,,,,,,,,,,,,,,,,,,,2.6.1,,,,,,,,,,,,,0,,,,,"I added some functionality to the system tests to compare epoch cache lineages ([https://github.com/apache/kafka/pull/9213]), and I found a bug in leader epoch cache recovery.

The test hard kills a broker and the cache hasn't been flushed yet, and then it starts up and goes through log recovery. After recovery there is divergence in the epoch caches for epoch 0:
{noformat}
AssertionError: leader epochs for output-topic-1 didn't match
 [{0: 9393L, 2: 9441L, 4: 42656L},
 {0: 0L, 2: 9441L, 4: 42656L}, 
 {0: 0L, 2: 9441L, 4: 42656L}]                                                                                                                                                                
{noformat}
The cache is supposed to include the offset for epoch 0 but in recovery it skips it [https://github.com/apache/kafka/blob/487b3682ebe0eefde3445b37ee72956451a9d15e/core/src/main/scala/kafka/log/LogSegment.scala#L364] due to [https://github.com/apache/kafka/commit/d152989f26f51b9004b881397db818ad6eaf0392]. Then it stamps the epoch with a later offset when fetching from the leader.

I'm not sure why the recovery code includes the condition `batch.partitionLeaderEpoch > 0`. I discussed this with Jason Gustafson and he believes it may have been intended to avoid assigning negative epochs but is not sure why it was added. None of the tests fail with this check removed.
{noformat}
          leaderEpochCache.foreach { cache =>
            if (batch.partitionLeaderEpoch > 0 && cache.latestEpoch.forall(batch.partitionLeaderEpoch > _))
              cache.assign(batch.partitionLeaderEpoch, batch.baseOffset)
          }
{noformat}",,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 17:02:31 UTC 2020,,,,,,,,,,"0|z0i2w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/20 17:02;lucasbradstreet;After further discussion with Jason Gustafson, this this should not result in any incorrect truncations as the end offset search will still find the right offset given that only the first epoch start offset can potentially be incorrect. Overall this should not be a blocker, though it should be fixed because there could be subtle situations where it matters.
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in KafkaConfigBackingStore,KAFKA-10426,13324211,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,xakassi,xakassi,xakassi,24/Aug/20 08:45,22/Oct/20 05:57,13/Jul/23 09:17,22/Oct/20 05:55,2.4.1,2.6.0,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.2,2.6.1,2.7.0,,,,,KafkaConnect,,,,,0,pull-request-available,,,,"Hi, guys!

We faced the following deadlock:

 
{code:java}
KafkaBasedLog Work Thread - _streaming_service_config
priority:5 - threadId:0x00007f18ec22c000 - nativeId:0x950 - nativeId (decimal):2384 - state:BLOCKED
stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at com.company.streaming.platform.kafka.DistributedHerder$ConfigUpdateListener.onSessionKeyUpdate(DistributedHerder.java:1586)
- waiting to lock <0x00000000e6136808> (a com.company.streaming.platform.kafka.DistributedHerder)
at org.apache.kafka.connect.storage.KafkaConfigBackingStore$ConsumeCallback.onCompletion(KafkaConfigBackingStore.java:707)
- locked <0x00000000d8c3be40> (a java.lang.Object)
at org.apache.kafka.connect.storage.KafkaConfigBackingStore$ConsumeCallback.onCompletion(KafkaConfigBackingStore.java:481)
at org.apache.kafka.connect.util.KafkaBasedLog.poll(KafkaBasedLog.java:264)
at org.apache.kafka.connect.util.KafkaBasedLog.access$500(KafkaBasedLog.java:71)
at org.apache.kafka.connect.util.KafkaBasedLog$WorkThread.run(KafkaBasedLog.java:337)



CustomDistributedHerder-connect-1
priority:5 - threadId:0x00007f1a01e30800 - nativeId:0x93a - nativeId (decimal):2362 - state:BLOCKED
stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.kafka.connect.storage.KafkaConfigBackingStore.snapshot(KafkaConfigBackingStore.java:285)
- waiting to lock <0x00000000d8c3be40> (a java.lang.Object)
at com.company.streaming.platform.kafka.DistributedHerder.updateConfigsWithIncrementalCooperative(DistributedHerder.java:514)
- locked <0x00000000e6136808> (a com.company.streaming.platform.kafka.DistributedHerder)
at com.company.streaming.platform.kafka.DistributedHerder.tick(DistributedHerder.java:402)
at com.company.streaming.platform.kafka.DistributedHerder.run(DistributedHerder.java:286)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){code}

DistributedHerder went to updateConfigsWithIncrementalCooperative() synchronized method and called configBackingStore.snapshot() which take a lock on internal object in KafkaConfigBackingStore class.

 

Meanwhile KafkaConfigBackingStore in ConsumeCallback inside synchronized block on internal object got SESSION_KEY record and called updateListener.onSessionKeyUpdate() which take a lock on DistributedHerder.

 

As I can see the problem is here:

[https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java#L737]

 

As I understand this call should be performed outside synchronized block:
{code:java}
if (started)
   updateListener.onSessionKeyUpdate(KafkaConfigBackingStore.this.sessionKey);{code}
 

I'm going to make a PR.

 ",,xakassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 06:21:53 UTC 2020,,,,,,,,,,"0|z0i11k:",9223372036854775807,,ChrisEgerton,,,,,,,,,,,,,,,,,,"25/Aug/20 06:21;xakassi;Hi, [~ChrisEgerton], [~kkonstantine]  !

Could you, please, take a look at my PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
suppress() with cogroup() throws ClassCastException,KAFKA-10417,13323376,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,lct45,wardhapk,wardhapk,18/Aug/20 15:05,16/Dec/20 14:00,13/Jul/23 09:17,16/Dec/20 14:00,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.7.1,2.8.0,,,,,,,streams,,,,,0,kafka-streams,,,,"Streams operation - `cogroup()`, `aggregate()` followed by `suppress()` throws `ClassCastException`

Works fine without the `suppress()`

Code block tested -
{code:java}
val stream1 = requestStreams.merge(successStreams).merge(errorStreams)
                .groupByKey(Grouped.with(Serdes.String(), serdesConfig.notificationSerde()))

        val streams2 = confirmationStreams
                .groupByKey(Grouped.with(Serdes.String(), serdesConfig.confirmationsSerde()))

        val cogrouped = stream1.cogroup(notificationAggregator).cogroup(streams2, confirmationsAggregator)
                .windowedBy(TimeWindows.of(Duration.ofMinutes(notificationStreamsConfig.joinWindowMinutes.toLong())).grace(Duration.ofMinutes(notificationStreamsConfig.graceDurationMinutes.toLong())))
                .aggregate({ null }, Materialized.`as`<String, NotificationMetric, WindowStore<Bytes, ByteArray>>(""time-windowed-aggregated-stream-store"")
                        .withValueSerde(serdesConfig.notificationMetricSerde()))
                 .suppress(Suppressed.untilWindowCloses(unbounded()))
                .toStream()

{code}
Exception thrown is:
{code:java}
Caused by: java.lang.ClassCastException: class org.apache.kafka.streams.kstream.internals.PassThrough cannot be cast to class org.apache.kafka.streams.kstream.internals.KTableProcessorSupplier (org.apache.kafka.streams.kstream.internals.PassThrough and org.apache.kafka.streams.kstream.internals.KTableProcessorSupplier are in unnamed module of loader 'app')
{code}
[https://stackoverflow.com/questions/63459685/kgroupedstream-with-cogroup-aggregate-suppress]",,ableegoldman,bbejeck,duvholt,lct45,mjsax,RensGroothuijsen,vvcephei,wardhapk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Thu Dec 03 08:53:27 UTC 2020,,,,,,,,,,"0|z0hvw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/20 20:35;RensGroothuijsen;A cogrouped stream creates a KTable with processor supplier type PassThrough, which implements ProcessorSupplier rather than  KTableProcessorSupplier. This then causes a problem [here|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L842].;;;","09/Oct/20 17:15;bbejeck;I'm marking this as critical vs. blocker as I don't believe this to be a regression, but an issue we missed in the 2.6 release.;;;","09/Oct/20 18:49;ableegoldman;[~RensGroothuijsen] would you be interested in submitting a PR for this?;;;","09/Oct/20 19:55;RensGroothuijsen;[~ableegoldman] I would, but I'm not familiar enough with the code to determine what the best solution would be.;;;","11/Oct/20 10:57;duvholt;I ended up adding  an instanceof check before casting to KTableProcessorSupplier. I've been using this in production for some weeks now and haven't noticed any issues.

[https://github.com/amedia/kafka/commit/7d2a027f41c246ecb521b85076525f5ed9f3834e]

 

If someone more experienced with the internals of Kafka Streams thinks this is a proper fix I can submit a PR.;;;","14/Oct/20 02:40;vvcephei;Hi [~duvholt] ,

Thanks for sharing that. It looks like it's just ignoring the request if the parent processor isn't the right type?

It might be working for you, but if you have any operations downstream that actually need the old values, then you should beware that you're now silently getting incorrect results. On the other hand, if you're _just_ doing it to work around the bug that manifests when you use suppress, then you're probably ok. It's risky, though, so you might want to take a closer look and/or prioritize putting in a proper fix.

The KTableValueGetter/enableOldValues thing is _super_ confusing, and as a matter of fact, it's something I'm planning to clean up now that KIP-478 is finally over the line. So don't feel bad if it's as clear as mud when you try to figure out the right way to fix it, it's that way for everyone.

What's happening in the cogroup case is that there are a set of aggregator processors whose results all get ""merged"" together, and the KTable is the result of that merge. The aggregation processors themselves are capable of `enableSendOldValues`, but the implementation uses the PassThrough for that merge node, which is not aware of ""send old values"". What seems to be needed is for the cogroup merge processor to get a new kind of PassThrough that is capable of transmitting ""enable send old values"" to its parents. I'm not sure whether or not the method could be added to PassThrough harmlessly, or whether we need some kind of KTableSourcePassThrough supplier.

Does this sound like the kind of thing you want to take a crack at? Although it may not technically be a blocker, it would be ideal to get a fix for this in 2.7 .

Thanks,

-John;;;","14/Oct/20 04:55;mjsax;Just want to point out, that we recently fixed KAFKA-10077 and KAFKA-10494 that might be related?;;;","14/Oct/20 08:25;duvholt;Hi [~vvcephei],

I suspect that I'm not experienced enough with Kafka Streams to solve this issue, but I can give it a try.

Is there any documentation about `enableSendOldValues` which I could read to get a better understanding of the issue?;;;","20/Oct/20 21:22;bbejeck;Since there's no PR for this ticket yet and the code freeze is tomorrow 10/21 as part of the 2.7.0 release I'm going to change the fix version to 2.8.0 and 2.7.1.

\cc [~vvcephei] 

 

 

 ;;;","12/Nov/20 11:47;mimaison;Since there's still no PR, removing 2.6.1 from ""Fix Version/s"";;;","02/Dec/20 22:43;lct45;[~duvholt] are you still planning on working on this? If not I'm happy to pick it up;;;","02/Dec/20 23:29;vvcephei;Hey [~duvholt] , I'm really sorry I missed your comment. Sadly, there's no documentation, but the basic point is that some aggregations need to be able to ""subtract"" the prior value before ""adding"" the new value. For this case, it requests upstream operators to send the old value along with any updates.;;;","03/Dec/20 08:53;duvholt;[~lct45] unfortunately I'm not able to work on this now so feel free to pick it up. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupMetadataManager ignores current_state_timestamp field for GROUP_METADATA_VALUE_SCHEMA_V3,KAFKA-10401,13322678,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,showuon,the.paczek,the.paczek,14/Aug/20 07:15,22/Sep/20 03:23,13/Jul/23 09:17,21/Sep/20 21:06,2.1.1,2.2.2,2.4.1,2.5.1,2.6.0,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.2,2.6.1,2.7.0,,,,offset manager,,,,,0,,,,,"While reading group metadata information from ByteBuffer GroupMetadataManager reads current_state_timestamp only for group schema version 2. For all other versions this value is set to ""None"". 

Piece of code responsible for the bug:

[https://github.com/apache/kafka/blob/2.6.0/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L1412]

Restarting kafka forces GroupMetadataManager manager to reload group metadata from file basically causing [KIP-211|[https://cwiki.apache.org/confluence/display/KAFKA/KIP-211%3A+Revise+Expiration+Semantics+of+Consumer+Group+Offsets]] to be only applicable for schema version 2. 

 

 ",,the.paczek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-14 07:15:53.0,,,,,,,,,,"0|z0hrl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overall memory of container keep on growing due to kafka stream / rocksdb and OOM killed once limit reached,KAFKA-10396,13322468,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rohanpd,vmathapati,vmathapati,13/Aug/20 06:56,26/Mar/21 19:22,13/Jul/23 09:17,26/Mar/21 19:22,2.3.1,2.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"We are observing that overall memory of our container keep on growing and never came down.
After analysis find out that rocksdbjni.so is keep on allocating 64M chunks of memory off-heap and never releases back. This causes OOM kill after memory reaches configured limit.

We use Kafka stream and globalktable for our many kafka topics.

Below is our environment
 * Kubernetes cluster
 * openjdk 11.0.7 2020-04-14 LTS
 * OpenJDK Runtime Environment Zulu11.39+16-SA (build 11.0.7+10-LTS)
 * OpenJDK 64-Bit Server VM Zulu11.39+16-SA (build 11.0.7+10-LTS, mixed mode)
 * Springboot 2.3
 * spring-kafka-2.5.0
 * kafka-streams-2.5.0
 * kafka-streams-avro-serde-5.4.0
 * rocksdbjni-5.18.3

Observed same result with kafka 2.3 version.

Below is the snippet of our analysis
from pmap output we took addresses from these 64M allocations (RSS)

Address Kbytes RSS Dirty Mode Mapping
00007f3ce8000000 65536 65532 65532 rw--- [ anon ]
00007f3cf4000000 65536 65536 65536 rw--- [ anon ]
00007f3d64000000 65536 65536 65536 rw--- [ anon ]

We tried to match with memory allocation logs enabled with the help of Azul systems team.

@ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0x261)[0x7f3e1c65d741] - 0x7f3ce8ff7ca0
 @ /tmp/librocksdbjni6564497922441568920.so:
_ZN7rocksdb15BlockBasedTable3GetERKNS_11ReadOptionsERKNS_5SliceEPNS_10GetContextEPKNS_14SliceTransformEb+0x894)[0x7f3e1c898fd4] - 0x7f3ce8ff9780
 @ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0xfa)[0x7f3e1c65d5da] - 0x7f3ce8ff9750
 @ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0x261)[0x7f3e1c65d741] - 0x7f3ce8ff97c0
 @ /tmp/librocksdbjni6564497922441568920.so:_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0xfa)[0x7f3e1c65d5da] - 0x7f3ce8ffccf0
 @ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0x261)[0x7f3e1c65d741] - 0x7f3ce8ffcd10
 @ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0xfa)[0x7f3e1c65d5da] - 0x7f3ce8ffccf0
 @ /tmp/librocksdbjni6564497922441568920.so:
_Z18rocksdb_get_helperP7JNIEnv_PN7rocksdb2DBERKNS1_11ReadOptionsEPNS1_18ColumnFamilyHandleEP11_jbyteArrayii+0x261)[0x7f3e1c65d741] - 0x7f3ce8ffcd10


We also identified that content on this 64M is just 0s and no any data present in it.

I tried to tune the rocksDB configuratino as mentioned but it did not helped. [https://docs.confluent.io/current/streams/developer-guide/config-streams.html#streams-developer-guide-rocksdb-config]

 

Please let me know if you need any more details",,ableegoldman,cadonna,desai.p.rohan,mjsax,rohanpd,vmathapati,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/20 18:09;vmathapati;CustomRocksDBConfig.java;https://issues.apache.org/jira/secure/attachment/13009889/CustomRocksDBConfig.java","16/Aug/20 15:51;vmathapati;MyStreamProcessor.java;https://issues.apache.org/jira/secure/attachment/13009947/MyStreamProcessor.java","13/Aug/20 10:42;vmathapati;kafkaStreamConfig.java;https://issues.apache.org/jira/secure/attachment/13009767/kafkaStreamConfig.java",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 05:41:18 UTC 2021,,,,,,,,,,"0|z0hqag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/20 10:06;cadonna;[~vmathapati], thank you for the report. Could you please also post your RocksDB config  setter? Do you use time windows or session windows in your topology? Do you have minimal example to reproduce the issue?;;;","13/Aug/20 10:48;vmathapati;Attaching sample example. By default we did not configured any rocksDB config. We did not configured anything in topology. 

I tried with the sample rocksDB config mentioned at below page but issue still observed.
[https://kafka.apache.org/23/documentation/streams/developer-guide/config-streams#rocksdb-config-setter];;;","13/Aug/20 21:30;desai.p.rohan;[~vmathapati] we've hit a similar issue in ksqlDB, which runs streams underneath. We suspect it is due to fragmentation that manifests when running with the glibc allocator, and have found that running with jemalloc ([https://github.com/jemalloc/jemalloc]) avoids the issue. To see if this also helps you, you can try instaling jemalloc and then running your application with the following environment variable:

{{LD_PRELOAD=<path to jemalloc so>}}

If you could provide more details about how you build your image (or maybe share your dockerfile) I could help you get jemalloc installed.;;;","14/Aug/20 06:09;vmathapati;[~desai.p.rohan] Thanks for responding. For debugging same issue i preloaded few of the so files. I can try out same steps.

I am not able to find out jemalloc.so file on the above link. Will it be possible for you to point me to the location of so file.;;;","14/Aug/20 06:33;desai.p.rohan;[~vmathapati] you'd have to install it, and then look for it in the lib directories. The exact steps depend on the system. For example, on debian/ubuntu you can run:

{{apt install libjemalloc-dev}}

Then, run

{{sudo find /usr/lib/ -name libjemalloc.so}}

to get the so path.

 ;;;","14/Aug/20 07:30;vmathapati;i tried with [https://centos.pkgs.org/8/epel-x86_64/jemalloc-5.2.1-2.el8.x86_64.rpm.html] and then used rpm command as i do not have apt installer on my kubernetes cluster. 

I can see ""/usr/lib64/libjemalloc.so.2"". So, i copied only this file into my pod and renamed it to libjemalloc.so and set the environment as follow.

- name: LD_PRELOAD
   value: /cmservice/data/libjemalloc.so 

After the test run i still observed the same issue where memory keep on growing.

 ;;;","14/Aug/20 07:42;vmathapati;Is there anything which i can check to see if really jemalloc coming into picture.;;;","14/Aug/20 07:47;desai.p.rohan;[~vmathapati] it should show up on your pmap. For example:

 

 
{code:java}
sudo pmap -X 13799
13799:   ./a.out
         Address Perm   Offset Device   Inode     Size     Rss     Pss Referenced Anonymous ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked ProtectionKey Mapping
...
    7fa5c36da000 r--p 00000000 103:02  274900       28      28      28         28         0              0              0               0    0       0      0             0 libjemalloc.so.2
    7fa5c36e1000 r-xp 00007000 103:02  274900      608     396     396        396         0              0              0               0    0       0      0             0 libjemalloc.so.2
    7fa5c3779000 r--p 0009f000 103:02  274900       64      64      64         64         0              0              0               0    0       0      0             0 libjemalloc.so.2
    7fa5c3789000 ---p 000af000 103:02  274900        4       0       0          0         0              0              0               0    0       0      0             0 libjemalloc.so.2
    7fa5c378a000 r--p 000af000 103:02  274900       20      20      20         20        16              0              0               0    0       0      0             0 libjemalloc.so.2
    7fa5c378f000 rw-p 000b4000 103:02  274900        4       4       4          4         4              0              0               0    0       0      0             0 libjemalloc.so.2

...{code}
 ;;;","14/Aug/20 07:52;desai.p.rohan;It may also be a different problem. Something else to try would be to configure the write buffer manager from your rocksdb config setter (I don't think the example in the streams doc does this). You could take a look at the ksqlDB config-setter as an example:

[https://github.com/confluentinc/ksql/blob/master/ksqldb-rocksdb-config-setter/src/main/java/io/confluent/ksql/rocksdb/KsqlBoundedMemoryRocksDBConfigSetter.java];;;","14/Aug/20 07:57;vmathapati;I can see below output for pmap -X

7f806e181000 r-xp 00000000 08:90 13 540 204 110 204 0 0 0 libjemalloc.so
 7f806e208000 ---p 00087000 08:90 13 2044 0 0 0 0 0 0 libjemalloc.so
 7f806e407000 r--p 00086000 08:90 13 24 24 20 24 16 0 0 libjemalloc.so
 7f806e40d000 rw-p 0008c000 08:90 13 4 4 4 4 4 0 0 libjemalloc.so;;;","14/Aug/20 08:36;desai.p.rohan;In addition to configuring the write buffer manager, make sure you close the iterator you open in 

{{getDefaultDataFromStore}};;;","14/Aug/20 18:13;vmathapati;[~desai.p.rohan] Thanks a lot . Looks like If i used WriteBufferManager config memory then memory does not grow after some limit. 

Attaching [^CustomRocksDBConfig.java] for reference . 

I can see some documentation and example related to it at [https://kafka.apache.org/23/documentation/streams/developer-guide/memory-mgmt.html] 

confluent documentation i think needs to be updated. 

Still need to test more to confirm but initial results looks promising.

 

I am not able to come up with optimized number. Generally what numbers we should keep ?;;;","14/Aug/20 18:27;vmathapati;In my microservice i am using globalktable for 5 diff topic. So the configuration of rocksDB which i am setting is per ktable/kstream right? ;;;","15/Aug/20 17:45;rohanpd;> I am not able to come up with optimized number. Generally what numbers we should keep ?
 
I'm not sure what you mean by an optimized number. Can you elaborate?
 
> So the configuration of rocksDB which i am setting is per ktable/kstream right? 
 
Looking at your config-setter, it looks like you're passing the same cache and write-buffer-manager to all callers, so the bound should apply across all streams/tables.
 
Also, make sure you fix the leak in `getDefaultDataFromStore`. You need to close `storeIterator`, otherwise the iterator in rocksdb will not be destroyed and if it has blocks pinned then it's likely your configured bounds will be violated.;;;","16/Aug/20 16:03;vmathapati;I mean to say if my topic going to have 10 Million record then what will be the value of Cache and writeBufferManager?

and if topic going to have just 5k records then what will be  the value of Cache and writeBufferManager?

 

As my different topics going to have different number of records so do i need to create different classes as i need to pass just class name

config.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class);

 

The results are with fixing the iterator leak. Updated the file.

Also i did not require to use the libjemalloc.;;;","20/Aug/20 01:02;rohanpd;> I mean to say if my topic going to have 10 Million record then what will be the value of Cache and writeBufferManager? and if topic going to have just 5k records then what will be  the value of Cache and writeBufferManager?

 

You can try to size your cache/wbm according to the available resources. So if you have XGB of memory on your system, choose some value significantly less than of that and allocate that to cache/wbm to bound memory usage from rocksdb. Remember, even if you choose a cache size that's smaller than it could be, the os will still cache data for you (it's just a bit more expensive to access it). So err on the side of choosing something smaller. FWIW we use ~20% of total memory for the cache (so 6GB out of 30). Then that, plus the size of your java heap should give you the total memory usage from Java and RocksDB.;;;","25/Mar/21 23:07;ableegoldman;Hey [~vmathapati] can we resolve this ticket or are you still experiencing issues after fixing the iterator memory leak? ;;;","26/Mar/21 05:41;vmathapati;Yes. By adding RocksDB config memory did not grow much. So we can close this JIRA.

 

Thanks for all support.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopologyTestDriver does not work with dynamic topic routing,KAFKA-10395,13322421,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,12/Aug/20 23:57,26/Aug/20 21:39,13/Jul/23 09:17,18/Aug/20 16:55,,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,test-framework,,,,"The TopologyTestDriver#read(topic) methods all call #getRecordsQueue which checks 

 
{code:java}
final Queue<ProducerRecord<byte[], byte[]>> outputRecords = outputRecordsByTopic.get(topicName);
if (outputRecords == null) {
    if (!processorTopology.sinkTopics().contains(topicName)) {
        throw new IllegalArgumentException(""Unknown topic: "" + topicName); 
    } 
}
{code}
The outputRecordsByTopic map keeps track of all topics that are actually produced to, but obviously doesn't capture any topics that haven't yet received output. The `processorTopology#sinkTopics` is supposed to account for that by checking to make sure the topic is actually registered in the topology, and throw an exception if not in case the user supplied the wrong topic name to read from. 

Unfortunately the TopicNameExtractor allows for dynamic routing of records to any topic, so the topology isn't aware of all the possible output topics. If trying to read from one of these topics that happens to not have received any output yet, the test will throw the above misleading IllegalArgumentException.

We could just relax this check, but warning users who may actually have accidentally passed in the wrong topic to read from seems quite useful. A better solution would be to require registering all possible output topics to the TTD up front. This would obviously require a KIP, but it would be a very small one and shouldn't be too much trouble

 ",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 16:37:41 UTC 2020,,,,,,,,,,"0|z0hq00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/20 00:13;ableegoldman;I think we could just modify the TestOutputTopic constructor to automatically register the output topic with the TTD, which wouldn't need a KIP. This wouldn't help users who just read from the TTD directly instead of using the new TestOutputTopic class, but maybe that's sufficient. We can just tell people to always use TestOutputTopic if their topology has dynamic routing?

 

edit: Actually it looks like the only public APIs that run into this issue are from TestOutputTopic, so we can avoid a KIP. That said, registering the topics for which a TestOutputTopic instance is created is not all that different from relaxing/removing the check altogether. So maybe we should just do that – we can at least log a warning still;;;","13/Aug/20 00:49;ableegoldman;The only really holistic solution I can think of would be to add some API to register the possible output topics on the TopicNameExtractor itself. But that seems like overkill here and honestly just replacing the exception with a warning log sounds good enough;;;","13/Aug/20 16:37;vvcephei;Thanks, [~ableegoldman] , I agree with your analysis.

I think TestOutputTopic is supposed to be the ""only"" way to read data from TopologyTestDriver, so it seems that having it auto-register its topic argument is not practically different than just skipping that check.

Because TopicNameExtractor is only a one-way function from record to topic name, there's no way to check if a topic name ""matches"" a TopicNameExtractor. It seems like if we still really want to have some kind of check, we could add a `TopicNameExtractor#matches(String topicName)` method. But that interface may be unsatisfiable for some implementations.

I did notice that the javadoc on TopicNameExtractor says, ""The topic name must already exist, since the Kafka Streams library will not try to automatically create the topic with the extracted name."" So, maybe it makes more sense to make the TestOutputTopic register the output topics, and then throw an exception when the application produces to a topic that hasn't been ""registered"".

Even that might break some tests, since right now people can register the output topic after they produce data, so it would need a KIP.

Realistically, I don't think it's too bad just to remove that check. We already guarantee that processing is synchronous, so if you use the wrong output topic name, it should be pretty obvious that something is wrong when you get no output. I guess we could split the difference by still enforcing that check when there are only StaticTopicNameExtractors in the topology.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams should overwrite checkpoint excluding corrupted partitions,KAFKA-10391,13322182,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,12/Aug/20 04:23,01/Feb/21 23:08,13/Jul/23 09:17,13/Aug/20 04:23,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"While working on https://issues.apache.org/jira/browse/KAFKA-9450 I discovered another bug in Streams: when some partitions are corrupted due to offsets out of range, we treat it as task corrupted and would close them as dirty and then revive. However we forget to overwrite the checkpoint file excluding those out-of-range partitions to let them be re-bootstrapped from the new log-start offset, and hence when the task is revived, it would still load the old offset and start from there and then get the out-of-range exception again. This may cause {{StreamsUpgradeTest.test_app_upgrade}} to be flaky.

We do not see this often because in the past we always delete the checkpoint file after loading it and we usually only see the out-of-range exception at the beginning of the restoration but not during restoration.",,ableegoldman,cadonna,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-12 04:23:56.0,,,,,,,,,,"0|z0hojs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-server-stop lookup is not specific enough and may kill other processes,KAFKA-10390,13322146,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,showuon,lucasbradstreet,lucasbradstreet,11/Aug/20 22:49,01/Oct/20 01:25,13/Jul/23 09:17,01/Oct/20 01:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,,,,,0,,,,,"kafka-server-stop.sh picks out kafka processes by:


 
{noformat}
PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}'){noformat}
 

This is not specific enough and may match unintended processes, e.g. one that even includes dependencies including *.kafka.kafka.*

**A better match would be:
{noformat}
PIDS=$(ps ax | grep ' kafka\.Kafka ' | grep java | grep -v grep | awk '{print $1}')
{noformat}",,lucasbradstreet,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 03:11:24 UTC 2020,,,,,,,,,,"0|z0hobs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/20 03:11;showuon;Remove the ignore case option is good to me since our entry point is definitely *{{kafka.Kafka}}.* PR submitted. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting errors in tagged struct conversion,KAFKA-10388,13322113,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,11/Aug/20 20:23,12/Aug/20 15:30,13/Jul/23 09:17,12/Aug/20 15:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"The message generator is missing some conversion logic between the generated struct types and instances of `Struct`. This causes casting errors when trying to use the `fromStruct` or `toStruct` methods. For example, in `SimpleExampleMessageData`, the tagged struct `myTaggedStruct` results in the following code in `fromStruct`:

{code}
            if (_taggedFields.containsKey(8)) {
                this.myTaggedStruct = (MyTaggedStruct) _taggedFields.remove(8);
            } else {
                this.myTaggedStruct = new MyTaggedStruct();
            }
{code}",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-11 20:23:05.0,,,,,,,,,,"0|z0ho4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot include SMT configs with source connector that uses topic.creation.* properties,KAFKA-10387,13322099,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,kkonstantine,wicknicks,wicknicks,11/Aug/20 18:39,17/Aug/20 06:26,13/Jul/23 09:17,17/Aug/20 06:26,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,KafkaConnect,,,,,0,,,,,"Let's say we try to create a connector with the following config:

{code:java}
{
  ""connector.class"": ""io.debezium.connector.postgresql.PostgresConnector"",
  ""tasks.max"": ""1"",
  ""database.hostname"": ""localhost"",
  ""database.port"": 32843,
  ""database.user"": ""test"",
  ""database.password"": ""test"",
  ""database.dbname"": ""test"",
  ""database.server.name"": ""tcpsql"",
  ""table.whitelist"": ""public.numerics"",
  ""transforms"": ""abc"",
  ""transforms.abc.type"": ""io.debezium.transforms.ExtractNewRecordState"",
  ""topic.creation.default.partitions"": ""1"",
  ""topic.creation.default.replication.factor"": ""1""
}
{code}


this fails with the following error in the Connector worker:

{code:java}
[2020-08-11 02:47:05,908] ERROR Failed to start task deb-0 (org.apache.kafka.connect.runtime.Worker:560)
org.apache.kafka.connect.errors.ConnectException: org.apache.kafka.common.config.ConfigException: Unknown configuration 'transforms.abc.type'
	at org.apache.kafka.connect.runtime.ConnectorConfig.transformations(ConnectorConfig.java:296)
	at org.apache.kafka.connect.runtime.Worker.buildWorkerTask(Worker.java:605)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:555)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:1251)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$1700(DistributedHerder.java:127)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder$10.call(DistributedHerder.java:1266)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder$10.call(DistributedHerder.java:1262)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: org.apache.kafka.common.config.ConfigException: Unknown configuration 'transforms.abc.type'
	at org.apache.kafka.common.config.AbstractConfig.get(AbstractConfig.java:159)
	at org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig.get(SourceConnectorConfig.java:57)
	at org.apache.kafka.connect.runtime.SourceConnectorConfig.get(SourceConnectorConfig.java:141)
	at org.apache.kafka.common.config.AbstractConfig.getClass(AbstractConfig.java:216)
	at org.apache.kafka.connect.runtime.ConnectorConfig.transformations(ConnectorConfig.java:281)
	... 10 more
{code}

connector creation works fine, if we remove the topic.creation properties above. 

Not entirely sure but it looks like the piece of code that might need a fix is here (as it does not add {{transforms.*}} configs into the returned ConfigDef instances: https://github.com/apache/kafka/blob/2.6.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java#L94",,kkonstantine,wicknicks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 06:25:47 UTC 2020,,,,,,,,,,"0|z0ho1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/20 06:25;kkonstantine;Addition of configs for custom topic creation with KIP-158 created a regression when transformation configs are also included in the configuration of a source connector.

To experience the issue, just enabling topic creation at the worker is not sufficient. A user needs to supply a source connector configuration that contains both transformations and custom topic creation properties.

The issue is that the enrichment of configs in {{SourceConnectorConfig}} happens on top of an {{AbstractConfig}} rather than a {{ConnectorConfig}}. Inheriting from the latter allows enrichment to be composable for both topic creation and transformations.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix record serialization with flexible versions,KAFKA-10386,13322073,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,11/Aug/20 16:04,13/Aug/20 16:53,13/Jul/23 09:17,13/Aug/20 16:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"The generated serde code for the ""records"" type uses a mix of compact and non-compact length representations which leads to serialization errors. We should update the generator logic to use the compact representation consistently.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-11 16:04:18.0,,,,,,,,,,"0|z0hnvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate converters from generated messages,KAFKA-10384,13321867,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,10/Aug/20 19:36,24/Nov/20 20:20,13/Jul/23 09:17,31/Aug/20 19:08,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"Separate the JSON converter classes from the message classes, so that the clients module can be used without Jackson on the CLASSPATH.",,cmccabe,grussell,psmolinski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10378,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 15:53:04 UTC 2020,,,,,,,,,,"0|z0hmm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 15:53;grussell;Also fixed in 2.6.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition reassignments can result in crashed ReplicaFetcherThreads.,KAFKA-10371,13321424,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dajac,steverod,steverod,07/Aug/20 02:10,07/Aug/20 22:30,13/Jul/23 09:17,07/Aug/20 22:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,,,,,0,,,,,"A Kafka system doing partition reassignments got stuck with the reassignment partially done and the system with a non-zero number of URPs and increasing max lag.

Looking in the logs, we see: 
{noformat}
[ERROR] 2020-07-31 21:22:23,984 [ReplicaFetcherThread-0-3] kafka.server.ReplicaFetcherThread - [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error due to
org.apache.kafka.common.errors.NotLeaderOrFollowerException: Error while fetching partition state for foo
[INFO] 2020-07-31 21:22:23,986 [ReplicaFetcherThread-0-3] kafka.server.ReplicaFetcherThread - [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Stopped
{noformat}

Investigating further and with some helpful changes to the exception (which was not generating a stack trace because it was a client-side exception), we see on a test run:

{noformat}
[2020-08-06 19:58:21,592] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error due to (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.NotLeaderOrFollowerException: Error while fetching partition state for topic-test-topic-85
        at org.apache.kafka.common.protocol.Errors.exception(Errors.java:415)
        at kafka.server.ReplicaManager.getPartitionOrException(ReplicaManager.scala:645)
        at kafka.server.ReplicaManager.localLogOrException(ReplicaManager.scala:672)
        at kafka.server.ReplicaFetcherThread.logStartOffset(ReplicaFetcherThread.scala:133)
        at kafka.server.ReplicaFetcherThread.$anonfun$buildFetch$1(ReplicaFetcherThread.scala:316)
        at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:553)
        at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:551)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:920)
        at kafka.server.ReplicaFetcherThread.buildFetch(ReplicaFetcherThread.scala:309)
{noformat}

It appears that the fetcher is attempting to fetch for a partition that has been getting reassigned away. From further investigation, it seems that in KAFKA-10002 the StopReplica code was changed from:
1. Remove partition from fetcher
2. Remove partition from partition map
to the other way around, but now the fetcher may race and attempt to build a fetch for a partition that's no longer mapped.  In particular, since the logOrException code is being called from logStartOffset which isn't protected against NotLeaderOrFollowerException, just against KafkaStorageException, the exception isn't caught and throws all the way out, killing the replica fetcher thread.
We need to switch this back.


",,chia7712,steverod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-07 02:10:46.0,,,,,,,,,,"0|z0hjw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeWindowedDeserializer doesn't allow users to set a custom window size,KAFKA-10366,13321196,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lthomas,lthomas,lthomas,06/Aug/20 00:05,03/Feb/21 01:11,13/Jul/23 09:17,02/Feb/21 13:59,,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,streams,,,,"Related to [KAFKA-4468|https://issues.apache.org/jira/browse/KAFKA-4468], in timeWindowedDeserializer Long.MAX_VALUE is used as _windowSize_ for any deserializer that uses the default constructor. While streams apps can pass in a window size in serdes or while creating a timeWindowedDeserializer, the deserializer that is actually used in processing the messages is created by the Kafka consumer, without passing in the set windowSize. The deserializer the consumer creates uses the configs, but as there is no config for windowSize, the window size is always default.

See _KStreamAggregationIntegrationTest #ShouldReduceWindowed()_ as an example of this issue. Despite passing in the windowSize to both the serdes and the timeWindowedDeserializer, the window size is set to Long.MAX_VALUE. ",,ableegoldman,lthomas,mjsax,SoerenHenning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 21:58:36 UTC 2020,,,,,,,,,,"0|z0hii0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/20 08:44;SoerenHenning;I guess, this is also related to KAFKA-9649, right?;;;","06/Aug/20 19:31;ableegoldman;Yeah, there are two related problems. The first one is KAFKA-9649, where you can get in trouble by using a TimeWindowedSerde constructor that appears legit but sets the windowSize to a random default. I agree with your conclusion in that ticket, we should deprecate the TimeWindowedSerde(innerClass) constructor. If you're forced to pass in the windowSize, then things work, because Streams uses the actual Serde object that you pass into your topology. So for example a TimeWindowedSerde passed in to a Materialized or Consumed would properly set the window end time upon deserialization.

 

Unfortunately, you can only set the deserializer _class_ for the Consumer, which then instantiates its own Deserializer through reflection. You're supposed to do all configuration for it in the Deserializer#configure class. Unfortunately, TimeWindowedDeserializer#deserialize only sets the innerClass.

The second problem, which this ticket targets, isn't really an issue for Streams users. Streams uses a plain Consumer<byte[], byte[]> and does all the deserialization itself, with the passed-in serde objects. It's just a problem for anyone who wants to consume TimeWindowed records directly. Even then, you can always use the workaround of consuming plain bytes and deserializing them yourself, so it's really more of an annoyance. It would be nice to have this fixed for tests in particular.

In general though any Serdes that we support out-of-the-box should probably work throughout Kafka, so we should consider introducing a config to set the windowSize similar to what we do for the inner class;;;","28/Aug/20 05:33;mjsax;Following [~ableegoldman] last comment on the discuss thread of the KIP, I was double checking `KStreamAggregationIntegrationTest#shouldReduceWindowed()` and it seem the issue is in the test setup, or in particular the use helper methods. The method `IntegrationTestUitls#createConsumer` does only take a `Properties` parameter and we should extend it to also take key/value deserializer objects (ie, add an overload for the method – we also need more overloads for other methods for this case). In `KStreamAggregationIntegrationTest#receiveMessages` we actually have a `Deserializer` object on hand, but we just convert it to a config for the consumer instead of forwarding the reference... This should resolve the issue.;;;","01/Sep/20 03:33;ableegoldman;Yeah thanks for tracking that down [~mjsax]. I think we can repurpose/reinterpret this ticket to track fixing the Streams tests/test utils. If we have to add a lot of overloads throughout the call hierarchy between the test and the actual Consumer creation, maybe we can save some work by just adding a ConsumerParameters class (better names welcome) and replacing the ConsumerConfig/Properties parameter in all the methods instead. Then we can just call the appropriate Consumer constructor based on whether the deserializers have been set in the passed in. ConsumerParameters ;;;","02/Sep/20 21:58;mjsax;SGTM. And no worries about naming. It's internal classes only so we can always rename.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When resuming Streams active task with EOS, the checkpoint file should be deleted",KAFKA-10362,13321007,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,DOJI,guozhang,guozhang,05/Aug/20 04:18,03/Feb/21 01:21,13/Jul/23 09:17,07/Oct/20 18:20,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.2,2.7.0,,,,,,,streams,,,,,0,newbie++,,,,"Today when we suspend a task we commit and along with the commit we always write checkpoint file even if we are eosEnabled (since the state is already SUSPENDED). But the suspended task may later be resumed and in that case the checkpoint file should be deleted since it should only be written when it is cleanly closed.

With our latest rebalance protocol in KIP-429, resume would not be called since all suspended tasks would be closed, but with the old eager protocol it may still be called — I think that may be the reason we did not get it often.",,ableegoldman,DOJI,guozhang,high.lee,ipasynkov,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 03 14:19:24 UTC 2020,,,,,,,,,,"0|z0hhcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/20 14:38;ipasynkov;Hello, can I pick this task?;;;","20/Aug/20 23:57;guozhang;Hi [~ipasynkov], sure! If you have read through the related code of Kafka Streams around TaskManager and StreamTask, then feel free to start preparing a PR and ping me whenever it is ready for reviews.;;;","24/Aug/20 08:52;high.lee;[~ipasynkov] 
Are you working on it? If not, can I do PR?;;;","24/Aug/20 09:29;ipasynkov;[~high.lee] Hello, yes I'm working on this task);;;","25/Aug/20 07:22;ipasynkov;[~guozhang] Hello. I've read related TaskManager and StreamTask. Please correct me if I'm wrong, It seems that I have to make a call to OffsetCheckpoint's delete() method in StreamTask's resume() method [when task's state is SUSPENDED];;;","02/Sep/20 18:37;DOJI;Hi [~guozhang], 

I worked on the bug and have raised a [PR|https://github.com/apache/kafka/pull/9247]

could you please review the PR.

and can I get this ticket on my name?;;;","02/Sep/20 21:10;mjsax;[~DOJI], even if the ticket was not assigned yet, it seems that [~ipasynkov] was actually working on it... Not sure how to proceed now. [~ipasynkov] do you already have a PR? I leave it up to both of you to figure it out you takes this ticket.;;;","03/Sep/20 04:23;DOJI;Hi [~ipasynkov], 

Can I continue work on this ticket ?

I am newbie to kafka community, so I thought this would be a place to start and have raised the PR.

Please let me know your thoughts.;;;","03/Sep/20 06:04;ipasynkov;Hello [~DOJI]. You can continue with this ticket since you've already made some progress and have a PR. I'll switch to another task then;;;","03/Sep/20 06:58;DOJI;Thank you [~ipasynkov];;;","03/Sep/20 14:19;mjsax;Thanks [~DOJI] and [~ipasynkov]! We should make sure that tickets are assigned properly to avoid such an overlap in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while reading checkpoint file /tmp/kafka-logs/cleaner-offset-checkpoint (kafka.server.LogDirFailureChannel),KAFKA-10352,13320795,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,dorbae,dorbae,04/Aug/20 06:35,24/Feb/23 20:01,13/Jul/23 09:17,24/Feb/23 20:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log cleaner,,,,,0,,,,,"One of my Kafka brokers(total 3, and version 2.5.0) was shut down suddenly. And then, other brokers also was shut down because of similar causes.

 

Main cause of this problem is '*Error while reading checkpoint file /tmp/kafka-logs/cleaner-offset-checkpoint (kafka.server.LogDirFailureChannel)*
*java.nio.file.NoSuchFileException: /tmp/kafka-logs/cleaner-offset-checkpoint*'

 

I haven't known why this error occurs and how to solve it. Please, give me some answers or comments about it. Thank you.

And I attached the content of log files such as kafkaServer.out, log-cleaner.log

 

kafkaServer.out
{code:java}
[2020-07-30 19:49:05,992] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-07-30 19:49:05,992] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-07-30 19:56:48,080] ERROR Error while reading checkpoint file /tmp/kafka-logs/cleaner-offset-checkpoint (kafka.server.LogDirFailureChannel)java.nio.file.NoSuchFileException: /tmp/kafka-logs/cleaner-offset-checkpoint at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384) at java.nio.file.Files.newInputStream(Files.java:152) at java.nio.file.Files.newBufferedReader(Files.java:2784) at java.nio.file.Files.newBufferedReader(Files.java:2816) at kafka.server.checkpoints.CheckpointFile.liftedTree2$1(CheckpointFile.scala:87) at kafka.server.checkpoints.CheckpointFile.read(CheckpointFile.scala:86) at kafka.server.checkpoints.OffsetCheckpointFile.read(OffsetCheckpointFile.scala:61) at kafka.log.LogCleanerManager.$anonfun$allCleanerCheckpoints$2(LogCleanerManager.scala:134) at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:583) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:597) at scala.collection.mutable.ListBuffer.addAll(ListBuffer.scala:118) at scala.collection.mutable.ListBuffer$.from(ListBuffer.scala:38) at scala.collection.immutable.List$.from(List.scala:617) at scala.collection.immutable.List$.from(List.scala:611) at scala.collection.IterableFactory$Delegate.from(Factory.scala:288) at scala.collection.immutable.Iterable$.from(Iterable.scala:35) at scala.collection.immutable.Iterable$.from(Iterable.scala:32) at scala.collection.IterableFactory$Delegate.from(Factory.scala:288) at scala.collection.IterableOps.flatMap(Iterable.scala:674) at scala.collection.IterableOps.flatMap$(Iterable.scala:674) at scala.collection.AbstractIterable.flatMap(Iterable.scala:921) at kafka.log.LogCleanerManager.$anonfun$allCleanerCheckpoints$1(LogCleanerManager.scala:132) at kafka.log.LogCleanerManager.allCleanerCheckpoints(LogCleanerManager.scala:140) at kafka.log.LogCleanerManager.$anonfun$grabFilthiestCompactedLog$1(LogCleanerManager.scala:171) at kafka.log.LogCleanerManager.grabFilthiestCompactedLog(LogCleanerManager.scala:168) at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:327) at kafka.log.LogCleaner$CleanerThread.tryCleanFilthiestLog(LogCleaner.scala:314) at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:303) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)[2020-07-30 19:56:48,083] WARN [ReplicaManager broker=3] Stopping serving replicas in dir /tmp/kafka-logs (kafka.server.ReplicaManager)[2020-07-30 19:56:48,086] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions HashSet(__consumer_offsets-8, sbchang.test.partition-0, __consumer_offsets-47, sbchang.test.partition-2, sbchang.test.header-2, configtest-0, __ispossible-0, __consumer_offsets-32, __consumer_offsets-35, temp-iot-0, __consumer_offsets-41, __consumer_offsets-23, test-security-sasl-plain-001-0, __consumer_offsets-38, __consumer_offsets-17, test-security-ssl-001-0, sbchang.test.header-1, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-14, resource-v1-CloudIoTCore-Rule-0, __consumer_offsets-20, __consumer_offsets-44, app001-transform-my001-0, sbchang.test.header-0, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, sbchang.test.partition-1) (kafka.server.ReplicaFetcherManager)[2020-07-30 19:56:48,086] INFO [ReplicaAlterLogDirsManager on broker 3] Removed fetcher for partitions HashSet(__consumer_offsets-8, sbchang.test.partition-0, __consumer_offsets-47, sbchang.test.partition-2, sbchang.test.header-2, configtest-0, __ispossible-0, __consumer_offsets-32, __consumer_offsets-35, temp-iot-0, __consumer_offsets-41, __consumer_offsets-23, test-security-sasl-plain-001-0, __consumer_offsets-38, __consumer_offsets-17, test-security-ssl-001-0, sbchang.test.header-1, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-14, resource-v1-CloudIoTCore-Rule-0, __consumer_offsets-20, __consumer_offsets-44, app001-transform-my001-0, sbchang.test.header-0, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, sbchang.test.partition-1) (kafka.server.ReplicaAlterLogDirsManager)[2020-07-30 19:56:48,096] WARN [ReplicaManager broker=3] Broker 3 stopped fetcher for partitions __consumer_offsets-8,sbchang.test.partition-0,__consumer_offsets-47,sbchang.test.partition-2,sbchang.test.header-2,configtest-0,__ispossible-0,__consumer_offsets-32,__consumer_offsets-35,temp-iot-0,__consumer_offsets-41,__consumer_offsets-23,test-security-sasl-plain-001-0,__consumer_offsets-38,__consumer_offsets-17,test-security-ssl-001-0,sbchang.test.header-1,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-14,resource-v1-CloudIoTCore-Rule-0,__consumer_offsets-20,__consumer_offsets-44,app001-transform-my001-0,sbchang.test.header-0,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,sbchang.test.partition-1 and stopped moving logs for partitions  because they are in the failed log directory /tmp/kafka-logs. (kafka.server.ReplicaManager)[2020-07-30 19:56:48,096] WARN Stopping serving logs in dir /tmp/kafka-logs (kafka.log.LogManager)[2020-07-30 19:56:48,098] ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed (kafka.log.LogManager)
{code}
 

*log-cleaner.log*
{code:java}
[2020-07-30 19:56:48,083] ERROR Failed to access checkpoint file cleaner-offset-checkpoint in dir /tmp/kafka-logs (kafka.log.LogCleaner)[2020-07-30 19:56:48,083] ERROR Failed to access checkpoint file cleaner-offset-checkpoint in dir /tmp/kafka-logs (kafka.log.LogCleaner)org.apache.kafka.common.errors.KafkaStorageException: Error while reading checkpoint file /tmp/kafka-logs/cleaner-offset-checkpointCaused by: java.nio.file.NoSuchFileException: /tmp/kafka-logs/cleaner-offset-checkpoint at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) at java.nio.file.Files.newByteChannel(Files.java:361) at java.nio.file.Files.newByteChannel(Files.java:407) at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384) at java.nio.file.Files.newInputStream(Files.java:152) at java.nio.file.Files.newBufferedReader(Files.java:2784) at java.nio.file.Files.newBufferedReader(Files.java:2816) at kafka.server.checkpoints.CheckpointFile.liftedTree2$1(CheckpointFile.scala:87) at kafka.server.checkpoints.CheckpointFile.read(CheckpointFile.scala:86) at kafka.server.checkpoints.OffsetCheckpointFile.read(OffsetCheckpointFile.scala:61) at kafka.log.LogCleanerManager.$anonfun$allCleanerCheckpoints$2(LogCleanerManager.scala:134) at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:583) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:597) at scala.collection.mutable.ListBuffer.addAll(ListBuffer.scala:118) at scala.collection.mutable.ListBuffer$.from(ListBuffer.scala:38) at scala.collection.immutable.List$.from(List.scala:617) at scala.collection.immutable.List$.from(List.scala:611) at scala.collection.IterableFactory$Delegate.from(Factory.scala:288) at scala.collection.immutable.Iterable$.from(Iterable.scala:35) at scala.collection.immutable.Iterable$.from(Iterable.scala:32) at scala.collection.IterableFactory$Delegate.from(Factory.scala:288) at scala.collection.IterableOps.flatMap(Iterable.scala:674) at scala.collection.IterableOps.flatMap$(Iterable.scala:674) at scala.collection.AbstractIterable.flatMap(Iterable.scala:921) at kafka.log.LogCleanerManager.$anonfun$allCleanerCheckpoints$1(LogCleanerManager.scala:132) at kafka.log.LogCleanerManager.allCleanerCheckpoints(LogCleanerManager.scala:140) at kafka.log.LogCleanerManager.$anonfun$grabFilthiestCompactedLog$1(LogCleanerManager.scala:171) at kafka.log.LogCleanerManager.grabFilthiestCompactedLog(LogCleanerManager.scala:168) at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:327) at kafka.log.LogCleaner$CleanerThread.tryCleanFilthiestLog(LogCleaner.scala:314) at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:303) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{code}",,dorbae,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 08 07:02:54 UTC 2020,,,,,,,,,,"0|z0hg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/20 13:53;dongjin;Hi [~dorbae],

It seems like you are using `/tmp/kafka-logs` as a storage directory. right? Then, your OS may delete the directory for garbage collecting the `/tmp` directory - I think it should the cause of the exception.

I strongly recommend not to use the `/tmp` as a storage directory and assign a dedicated device for storage.;;;","08/Aug/20 07:02;dorbae;Thank you for your answer, [~dongjin]!

According to your advice, I have replaced the path of data(log) directory into other places where is not the '/tmp'. And plus, I have changed the path of zookeeper's data directory. It has been working well. Thank you :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source connectors should report error when trying to produce records to non-existent topics instead of hanging forever,KAFKA-10340,13320728,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,wicknicks,wicknicks,03/Aug/20 18:38,16/Mar/22 01:38,13/Jul/23 09:17,18/Mar/21 16:15,2.5.1,2.6.1,2.7.0,2.8.0,,,,,,,,,,,,,,,,,,,2.6.2,2.7.1,2.8.1,3.0.0,,,,,KafkaConnect,,,,,0,,,,,"Currently, a source connector will blindly attempt to write a record to a Kafka topic. When the topic does not exist, its creation is controlled by the {{auto.create.topics.enable}} config on the brokers. When auto.create is disabled, the producer.send() call on the Connect worker will hang indefinitely (due to the ""infinite retries"" configuration for said producer). In setups where this config is usually disabled, the source connector simply appears to hang and not produce any output.

It is desirable to either log an info or an error message (or inform the user somehow) that the connector is simply stuck waiting for the destination topic to be created. When the worker has permissions to inspect the broker settings, it can use the {{listTopics}} and {{describeConfigs}} API in AdminClient to check if the topic exists, the broker can {{auto.create.topics.enable}} topics, and if these cases do not exist, either throw an error.

With the recently merged [KIP-158|https://cwiki.apache.org/confluence/display/KAFKA/KIP-158%3A+Kafka+Connect+should+allow+source+connectors+to+set+topic-specific+settings+for+new+topics], this becomes even more specific a corner case: when topic creation settings are enabled, the worker should handle the corner case where topic creation is disabled, {{auto.create.topics.enable}} is set to false and topic does not exist.",,ableegoldman,ChrisEgerton,mimaison,rhauch,showuon,wicknicks,xakassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12990,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 05 19:17:03 UTC 2021,,,,,,,,,,"0|z0hfns:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"10/Aug/20 06:28;showuon;After investigation, if the {{auto.create.topics.enable}} is disabled, the producer.send will get only {{TimeoutException}} and no other valuable clue for users. So, I will improve the logging in this area.

Proposed to add 2 logs:

1. When the topic creation is disabled or topic is existed:

log.debug({color:#008000}""The topic creation setting is disabled or the topic name {} is already created. "" {color}+
 {color:#008000}""If the topic doesn't exist, we'll rely on the auto.create.topics.enable setting in broker side "" {color}+
 {color:#008000}""to see if the topic can be auto created or not""{color}, topic);

2. Before the producer send the record:

log.trace({color:#008000}""{} is going to send record to {}""{color}, WorkerSourceTask.{color:#000080}this{color}, topic);

 

So, for the request to let user know it's stuck waiting for the destination topic to be created is now basically cannot know from client side because the producer.send will block on waitOnMetadata method, which will keep trying until timeout. There are many possible reasons for this timeout. It's hard to tell.

 

And the dynamically {{describeConfigs}} to get the broker setting is also not easy because the broker name (we need broker name to describeConfig) is not kept in the config, and also there's no other places in Kafka to check the broker setting before doing something. I'd prefer to keep it as is because this behavior(auto create topic or not while producer.send) applied for all kafka, not only for connectors.

 

Thanks.;;;","01/Feb/21 17:16;ChrisEgerton;The logging changes are certainly an improvement, but don't address the underlying issue: source tasks and their producers hang and remain active (i.e., are not closed) indefinitely, even after the task is scheduled for shutdown. If several task instances are scheduled for shutdown and then recreated (which may happen as a result of rebalance or connector reconfiguration), any of those instances in this bad state will keep sending metadata requests to the broker from their producers, and any resources allocated by the {{SourceTask}} instances will not be freed.

This creates a resource leak and the impact is substantial enough that I believe the ticket should be reopened until it has been addressed with a functional fix.;;;","10/Feb/21 15:00;xakassi;Hi, guys!

According to [KAFKA-5295](https://issues.apache.org/jira/browse/KAFKA-5295) I supposed that now topics are always created by AdminClient if  ""topic.creation.enable = true"".

But now I see that topics are created by AdminClient, only when ""topic.creation.enable = true"" and ""topic.creation"" parameters are used.
But I wonder why?
Why do not create topics always by AdminClient if  ""topic.creation.enable = true""?
 
It will partially solve the current problem (described in this ticket) and only the case with ""topic.creation.enable = false"" is the problem.
 
 
And also current approach can be a problem for us - we wanted to set ""auto.create.topics.enable = false"" on Kafka cluster by default. But we cannot do that, because our connectors can be created with 
""topic.creation"" parameters or without them, we cannot guarantee that. And in case connectors have not this property, we get errors.

 

Any comments and your opinion is appreciated! ;;;","11/Feb/21 06:44;xakassi;And also I cannot understand, why we even have the property ""topic.creation.enable"" and do not create all topics by AdminClient? Isn't it a better approach to create topics always explicitly using AdminClient than to rely on auto creation?

And also I cannot see how can it has a bad impact on users. Why someone may want to set ""topic.creation.enable = false""?

According to KIP-158 we have an option to set replication.factor and partitions to -1 to use the broker's default value. So why we cannot always create topics by AdminClient and use these broker's default values in case connector does not have any ""topic.creation"" properties?

 

As far as I understand this will completely solve the problem described in this ticket. And also people will be able to set ""auto.create.topics.enable = false"" on Kafka cluster without any impact.

 

Please, correct me if I am wrong.;;;","01/Mar/21 16:16;rhauch;[~xakassi] wrote:

{quote}
And also I cannot understand, why we even have the property ""topic.creation.enable"" and do not create all topics by AdminClient? Isn't it a better approach to create topics always explicitly using AdminClient than to rely on auto creation?
{quote}

When we introduced KIP-158 to allow Connect to source connectors to specify the rules for creating topics, it was important that we maintained backward compatibility. Many people upgrade Kafka and Connect simply to get fixes, and do not want new features they don't know about nor wish to use enabled by default. This is an essential aspect of all KIPs and AK features, even when by default most users *probably* would want to use the feature.  

{quote}
According to KIP-158 we have an option to set replication.factor and partitions to -1 to use the broker's default value. So why we cannot always create topics by AdminClient and use these broker's default values in case connector does not have any ""topic.creation"" properties?

As far as I understand this will completely solve the problem described in this ticket.
{quote}
It actually would not solve the problem. If Connect were to have the behavior you suggest, then we'd still run into the issue when the broker has autocreation of topics disabled *and* the particular source connector did not specify the topic creation rules.

The bottom line is that many things would be made easier *for some people* if we could easily change defaults, but upgrading would become painfully difficult for those that don't wish to use those new features or behaviors. By maintaining backward compatibility, we dramatically reduce the overhead and risk of upgrades to get fixes, which is an important characteristic that the AK community cherishes.

We will have the option in 3.0 to *change* defaults and behaviors -- it is a major release that may not be entirely backward compatible. Obviously, we'll want to not break things unnecessarily, but changing the default `topic.creation.enable = true` will be one such change we are very much looking forward to making.;;;","01/Mar/21 21:34;rhauch;Merged to `trunk` and backported to:
* `2.7` for inclusion in the upcoming 2.7.1
* `2.6` for inclusion in the upcoming 2.6.2

The `2.8` branch is currently blocked for the 2.8.0 release, so I've created https://github.com/apache/kafka/pull/10238 for the 2.8 backport -- either if this is approved as a 2.8.0 blocker or after 2.8.0 has been released.

Leaving this issue open until 2.8 is addressed.;;;","03/Mar/21 04:54;ableegoldman;Temporarily removed 2.6.2 from the Fix Version so I can proceed with the 2.6.2 release, since it's already been cherrypicked to 2.6 This should be added back once the ticket is resolved;;;","10/Mar/21 18:15;mimaison;[~vvcephei] Are we picking this change in 2.8? 

I wonder if this we should actually resolve this JIRA as the fix has been made to trunk, 2.6 and 2.7. If John decides to pull it in 2.8, then we can just add 2.8.0 to Fix Versions. That would enable us to run the 2.6.2 and 2.7.1 releases without having to tweak Fix Versions;;;","18/Mar/21 16:14;rhauch;{quote}

I wonder if this we should actually resolve this JIRA as the fix has been made to trunk, 2.6 and 2.7. If John decides to pull it in 2.8, then we can just add 2.8.0 to Fix Versions. That would enable us to run the 2.6.2 and 2.7.1 releases without having to tweak Fix Versions.

{quote}

That sounds fine with me. I'll will add the 2.6.2 fix, mark this as resolved. We can then add the 2.8 version (either `2.8.0` or `2.8.1`) if/when [https://github.com/apache/kafka/pull/10238] is merged to 2.8.;;;","05/May/21 19:17;rhauch;I cherry-picked the original PR (https://github.com/apache/kafka/pull/10016) to the `2.8` branch (now that it's not frozen) and updated the fixed versions.

This completes all of the planned work for this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wait for pending async commits in commitSync() even if no offsets are specified,KAFKA-10337,13320521,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,erikvanoosten,thomaslee,thomaslee,02/Aug/20 02:21,08/Jun/23 18:23,13/Jul/23 09:17,07/Jun/23 14:56,,,,,,,,,,,,,,,,,,,,,,,3.6.0,,,,,,,,clients,,,,,0,,,,,"The JavaDoc for commitSync() states the following:
{quote}Note that asynchronous offset commits sent previously with the
{@link #commitAsync(OffsetCommitCallback)}
 (or similar) are guaranteed to have their callbacks invoked prior to completion of this method.
{quote}
But should we happen to call the method with an empty offset map
(i.e. commitSync(Collections.emptyMap())) the callbacks for any incomplete async commits will not be invoked because of an early return in ConsumerCoordinator.commitOffsetsSync() when the input map is empty.

If users are doing manual offset commits and relying on commitSync as a barrier for in-flight async commits prior to a rebalance, this could be an important (though somewhat implementation-dependent) detail.",,erikvanoosten,thomaslee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 08 18:23:26 UTC 2023,,,,,,,,,,"0|z0heds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/20 02:40;thomaslee;Opened [https://github.com/apache/kafka/pull/9111] to address this particular edge case.;;;","06/May/23 06:41;erikvanoosten;Opened [~thomaslee] 's PR again: https://github.com/apache/kafka/pull/13678;;;","06/May/23 06:43;erikvanoosten;[~thomaslee] when we use commitAsync from the rebalance listener (potentially with empty offsets), no polling takes place anymore. Shall I amend the PR so that it does polling from commitAsync as well? WDYT?;;;","08/Jun/23 18:23;erikvanoosten;Thanks for your PR [~thomaslee]. It has been merged now with little changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker2 fails to detect topic if remote topic is created first,KAFKA-10332,13320358,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mimaison,mimaison,mimaison,31/Jul/20 09:05,19/Oct/20 17:28,13/Jul/23 09:17,19/Oct/20 17:28,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,mirrormaker,,,,,0,,,,,"Setup:
- 2 clusters: source and target
- Mirroring data from source to target
- create a topic called source.mytopic on the target cluster
- create a topic called mytopic on the source cluster
At this point, MM2 does not start mirroring the topic.

This also happens if you delete and recreate a topic that is being mirrored.

The issue is in [refreshTopicPartitions()|https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java#L211-L232] which basically does a diff between the 2 clusters.
When creating the topic on the source cluster last, it makes the partition list of both clusters match, hence not triggering a reconfiguration",,ecomar,mimaison,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 19 17:28:18 UTC 2020,,,,,,,,,,"0|z0hddk:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"19/Oct/20 17:28;rhauch;Merged to the `trunk` branch (for future 2.8 release), and cherry-picked to the `2.7` for inclusion in the upcoming 2.7.0, the `2.6` branch for inclusion in the next 2.6.1 if/when it's released, and the `2.5` branch for the next 2.5.2 if/when it's released.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Both serializer and deserializer should be able to see the generated client id,KAFKA-10326,13320103,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,30/Jul/20 04:08,01/Oct/20 03:23,13/Jul/23 09:17,01/Oct/20 03:23,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"Producer and consumer generate client id when users don't define it. the generated client id is passed to all configurable components (for example, metrics reporter) except for serializer/deseriaizer.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-30 04:08:07.0,,,,,,,,,,"0|z0hbtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shouldDieOnInvalidOffsetExceptionDuringStartup would block forever on JDK11,KAFKA-10321,13319900,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bchen225242,bchen225242,29/Jul/20 06:27,03/Aug/20 21:12,13/Jul/23 09:17,03/Aug/20 21:12,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"Have spotted two definite cases where the test shouldDieOnInvalidOffsetExceptionDuringStartup

fails to stop during the whole test suite:
 [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7604/consoleFull]

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7602/console]",,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 06:54:19 UTC 2020,,,,,,,,,,"0|z0hakg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 06:30;bchen225242;Looks like we have an infinite blocking logic in `GlobalStreamThread`
{code:java}
@Override
public synchronized void start() {
    super.start();
    while (!stillRunning()) {
        Utils.sleep(1);
        if (startupException != null) {
            throw startupException;
        }
    }
}
{code};;;","29/Jul/20 06:32;bchen225242;cc [~mjsax] [~vvcephei] who added this test in the PR: [https://github.com/apache/kafka/pull/9075];;;","29/Jul/20 06:54;bchen225242;Actually I think I found the bug, check out the PR here: [https://github.com/apache/kafka/pull/9095];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStorageException on reassignment when offline log directories exist,KAFKA-10314,13319566,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,noa,noa,noa,27/Jul/20 15:45,04/Sep/20 15:34,13/Jul/23 09:17,04/Sep/20 15:34,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,,,,,0,,,,,"If a reassignment of a partition is triggered to a broker with an offline directory, the new broker will fail to follow, instead raising a KafkaStorageException which causes the reassignment to stall indefinitely. The error message we see is the following:

{{[2020-07-23 13:11:08,727] ERROR [Broker id=1] Skipped the become-follower state change with correlation id 14 from controller 1 epoch 1 for partition t2-0 (last update controller epoch 1) with leader 2 since the replica for the partition is offline due to disk error org.apache.kafka.common.errors.KafkaStorageException: Can not create log for t2-0 because log directories /tmp/kafka/d1 are offline (state.change.logger)}}

It seems to me that unless the partition in question already existed on the offline log partition, a better behaviour would simply be to assign the partition to one of the available log directories.

The conditional in [LogManager.scala:769|https://github.com/apache/kafka/blob/11f75691b87fcecc8b29bfd25c7067e054e408ea/core/src/main/scala/kafka/log/LogManager.scala#L769] was introduced to prevent the issue in [KAFKA-4763|https://issues.apache.org/jira/browse/KAFKA-4763] where partitions in offline logdirs would be re-created in an online directory as soon as a LeaderAndISR message gets processed. However, the semantics of isNew seems different in LogManager (the replica is new on this broker) compared to when isNew is set in [KafkaController.scala|https://github.com/apache/kafka/blob/11f75691b87fcecc8b29bfd25c7067e054e408ea/core/src/main/scala/kafka/controller/KafkaController.scala#L879] (where it seems to refer to whether the topic partition in itself is new, all followers gets {{isNew=false}})",,noa,soarez,ymxz,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Tue Aug 04 12:49:13 UTC 2020,,,,,,,,,,"0|z0h8i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/20 12:49;noa;I just opened https://github.com/apache/kafka/pull/9122 that addresses this issue by making sure that the controller, when initiating a reassignment, will only set isNew=false in the LeaderAndISR for replicas already in the replica set;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataCache.getPartitionMetadata may return incorrect error code when partition's leader is not present at the MetadataCache,KAFKA-10312,13319440,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ramanverma,ramanverma,ramanverma,27/Jul/20 04:56,24/Aug/20 18:19,13/Jul/23 09:17,24/Aug/20 18:19,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,,,,,0,,,,,"`MetadataCache.getPartitionMetadata` returns an error code if the partition's leader is not present at the MetadataCache, or if the Listener endpoint is not present for the leader. Newer versions for metadata returns LEADER_NOT_AVAILABLE and LISTENER_NOT_FOUND for the above two cases respectively.

 There is a small bug in this logic. Current code checks the MetadataCache's host broker Id instead of the partition's leader id (in the list of alive brokers) when determining what error code to return. This may result in this call returning LISTENER_NOT_FOUND error code even when partition's leader is not present at MetadataCache and it should have returned LEADER_NOT_AVAILABLE.",,ramanverma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 02 20:17:32 UTC 2020,,,,,,,,,,"0|z0h7q8:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"27/Jul/20 04:57;ramanverma;I have a patch available. Need permission to be able to assign the ticket to myself.;;;","02/Aug/20 20:17;ramanverma;[https://github.com/apache/kafka/pull/9112];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test KafkaAdminClientTest#testMetadataRetries,KAFKA-10311,13319433,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,bchen225242,bchen225242,27/Jul/20 03:23,10/Sep/20 20:37,13/Jul/23 09:17,10/Sep/20 20:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3545/testReport/junit/org.apache.kafka.clients.admin/KafkaAdminClientTest/testMetadataRetries/]

 
h3. Error Message

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1595694629113, tries=1, nextAllowedTryMs=1595694629217) timed out at 1595694629117 after 1 attempt(s)
h3. Stacktrace

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1595694629113, tries=1, nextAllowedTryMs=1595694629217) timed out at 1595694629117 after 1 attempt(s) at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) at org.apache.kafka.clients.admin.KafkaAdminClientTest.testMetadataRetries(KafkaAdminClientTest.java:995) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1595694629113, tries=1, nextAllowedTryMs=1595694629217) timed out at 1595694629117 after 1 attempt(s) Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.",,bchen225242,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 11:54:25 UTC 2020,,,,,,,,,,"0|z0h7oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/20 11:54;chia7712;[https://github.com/apache/kafka/pull/9091] has covered this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlobalThread might loop forever,KAFKA-10306,13319200,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,mjsax,mjsax,24/Jul/20 17:02,26/Jul/20 19:41,13/Jul/23 09:17,26/Jul/20 19:41,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"We did some code refactoring in the `GlobalStateMangerImpl` that can lead to an infinite loop if the global consumer throws an `InvalidOffsetException`. This is a regression bug and thus I marked this ticket as blocker fro 2.6 release.

The issue seem to occur only during the initial bootstrapping of the global store, but not during regular processing.",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-24 17:02:39.0,,,,,,,,,,"0|z0h68w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition#remoteReplicasMap can be empty in certain race conditions,KAFKA-10301,13318977,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,enether,enether,enether,23/Jul/20 15:04,27/Jul/20 18:04,13/Jul/23 09:17,27/Jul/20 07:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"In Partition#updateAssignmentAndIsr, we would previously update the `partition#remoteReplicasMap` by adding the new replicas to the map and then removing the old ones ([source]([https://github.com/apache/kafka/blob/7f9187fe399f3f6b041ca302bede2b3e780491e7/core/src/main/scala/kafka/cluster/Partition.scala#L657)]

During a recent refactoring, we changed it to first clear the map and then add all the replicas to it ([source]([https://github.com/apache/kafka/blob/2.6/core/src/main/scala/kafka/cluster/Partition.scala#L663]))

While this is done in a write lock (`inWriteLock(leaderIsrUpdateLock)`), not all callers that access the map structure use a lock. Some examples:
 - Partition#updateFollowerFetchState
 - DelayedDeleteRecords#tryComplete
 - Partition#getReplicaOrException - called in `checkEnoughReplicasReachOffset` without a lock, which itself is called by DelayedProduce. I think this can fail a  `ReplicaManager#appendRecords` call.

While we want to polish the code to ensure these sort of race conditions become harder (or impossible) to introduce, it sounds safest to revert to the previous behavior given the timelines regarding the 2.6 release. Jira https://issues.apache.org/jira/browse/KAFKA-10302 tracks further modifications to the code.",,enether,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 18:04:46 UTC 2020,,,,,,,,,,"0|z0h4vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 15:15;enether;cc [~rhauch] - this would be good to get in 2.6;;;","27/Jul/20 07:09;enether;Merged in master ([fa6e5b892227db24568e2cec0a0a0969bc1ec4b5|https://github.com/apache/kafka/commit/fa6e5b892227db24568e2cec0a0a0969bc1ec4b5]) and 2.6 ([fb0a79ddc7c2144cd58b9fc27e4d36ab8f063a78|https://github.com/apache/kafka/commit/fb0a79ddc7c2144cd58b9fc27e4d36ab8f063a78]);;;","27/Jul/20 18:04;rhauch;Thanks, [~enether]. I agree it's good to get this into 2.6, so thanks for backporting.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix flaky core/group_mode_transactions_test.py,KAFKA-10300,13318824,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,23/Jul/20 03:44,23/Jul/20 23:05,13/Jul/23 09:17,23/Jul/20 23:05,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,system tests,,,,0,,,,,"{quote}

test_id:    kafkatest.tests.core.group_mode_transactions_test.GroupModeTransactionsTest.test_transactions.failure_mode=hard_bounce.bounce_target=brokers

status:     FAIL

run time:   9 minutes 47.698 seconds

 

 

    copier-0 - Failed to copy all messages in 240s.

Traceback (most recent call last):

  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 134, in run

    data = self.run_test()

  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 192, in run_test

    return self.test_context.function(self.test)

  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 429, in wrapper

    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)

  File ""/opt/kafka-dev/tests/kafkatest/tests/core/group_mode_transactions_test.py"", line 271, in test_transactions

    num_messages_to_copy=self.num_seed_messages)

  File ""/opt/kafka-dev/tests/kafkatest/tests/core/group_mode_transactions_test.py"", line 230, in copy_messages_transactionally

    (copier.transactional_id, copier_timeout_sec))

  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until

    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)

TimeoutError: copier-0 - Failed to copy all messages in 240s.

{quote}

 

this issue is same to KAFKA-10274 so we can apply the same approach",,chia7712,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 23:05:42 UTC 2020,,,,,,,,,,"0|z0h3xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 23:05;junrao;merged to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix flaky streams/streams_eos_test.py,KAFKA-10293,13317564,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,chia7712,chia7712,18/Jul/20 16:47,09/Mar/21 09:21,13/Jul/23 09:17,25/Aug/20 10:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,system tests,,,,0,,,,,"{quote}
Module: kafkatest.tests.streams.streams_eos_test
Class:  StreamsEosTest
Method: test_failure_and_recovery_complex
Arguments:
{
  ""processing_guarantee"": ""exactly_once""
}
{quote}",,ableegoldman,cadonna,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 10:51:23 UTC 2020,,,,,,,,,,"0|z0gw5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/20 10:51;cadonna;[~chia7712] I will close this ticket because it seems this test got fixed. I checked recent runs of system tests and this test has never failed.

Feel free to reopen, if you think this test is still an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix flaky streams/streams_standby_replica_test.py,KAFKA-10287,13317541,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cadonna,chia7712,chia7712,18/Jul/20 11:21,09/Mar/21 09:21,13/Jul/23 09:17,28/Jul/20 16:03,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,system tests,,,,0,,,,,"{quote}
Module: kafkatest.tests.streams.streams_standby_replica_test
Class:  StreamsStandbyTask
Method: test_standby_tasks_rebalance
{quote}

It pass occasionally on my local.  ",,ableegoldman,cadonna,chia7712,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 16:03:46 UTC 2020,,,,,,,,,,"0|z0gw0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 19:45;cadonna;[~chia7712] Sorry, I was not aware that you assigned this ticket to yourself. Could you please review the PR?;;;","23/Jul/20 22:38;vvcephei;From [~cadonna] 's PR:

> In PR [#8962|https://github.com/apache/kafka/pull/8962] we introduced a sentinel UNKNOWN_OFFSET to mark unknown offsets in checkpoint files. The sentinel was set to -2 which is the same value used for the sentinel LATEST_OFFSET that is used in subscriptions to signal that state stores have been used by an active task. Unfortunately, we missed to skip UNKNOWN_OFFSET when we compute the sum of the changelog offsets.

> If a task had only one state store and it did not restore anything before the next rebalance, the stream thread wrote -2 (i.e., UNKNOWN_OFFSET) into the subscription as sum of the changelog offsets. During assignment, the leader interpreted the -2 as if the stream run the task as active although it might have run it as standby. This misinterpretation of the sentinel value resulted in unexpected task assigments.

This seems like a blocker to me, [~rhauch] , what do you think? The condition is that the streams assignor would unexpectedly move work to a node that's not ready, resulting in downtime while that node restores from the changelog.;;;","23/Jul/20 22:47;chia7712;{quote}
Sorry, I was not aware that you assigned this ticket to yourself

{quote}



Don’t worry. All I want to do is to fix all system tests. Thanks for resolving this issue!;;;","28/Jul/20 16:03;vvcephei;Created https://issues.apache.org/jira/browse/KAFKA-10319 to track the trunk migration so we can close this ticket for 2.6;;;","28/Jul/20 16:03;vvcephei;Still needs trunk PR. See https://issues.apache.org/jira/browse/KAFKA-10319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group membership update due to static member rejoin should be persisted,KAFKA-10284,13317302,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,feyman,bchen225242,bchen225242,17/Jul/20 02:17,23/Oct/20 18:23,13/Jul/23 09:17,22/Oct/20 15:16,2.3.0,2.4.0,2.5.0,2.6.0,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,consumer,,,,,0,help-wanted,,,,"For known static members rejoin, we would update its corresponding member.id without triggering a new rebalance. This serves the purpose for avoiding unnecessary rebalance for static membership, as well as fencing purpose if some still uses the old member.id. 

The bug is that we don't actually persist the membership update, so if no upcoming rebalance gets triggered, this new member.id information will get lost during group coordinator immigration, thus bringing up the zombie member identity.

The bug find credit goes to [~hachikuji] ",,ableegoldman,akshaysh,bchen225242,colinkuo,feyman,guozhang,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/20 07:29;feyman;How to reproduce the issue in KAFKA-10284.md;https://issues.apache.org/jira/secure/attachment/13010329/How+to+reproduce+the+issue+in+KAFKA-10284.md",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 14:02:35 UTC 2020,,,,,,,,,,"0|z0gujk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/20 02:20;bchen225242;[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L1042];;;","17/Jul/20 19:11;guozhang;[~bchen225242] Thanks for the find. Just trying to clarify that ""bring up the zombie member ID"" would break correctness, since if the zombie member still exists, it may be able to commit offsets which would rewind the position. Is that right?;;;","17/Jul/20 21:10;bchen225242;If we have a combined scenario like below:
 # A static member X joins the group and updates member.id to M1, then gets stuck
 # Another static member Y with the same instance.id joins and updates member.id to M2, while starts working and commit offsets
 # The group coordinator migrates, and the member.id for the same static member rewinds to M1
 # The static member X goes back online, and validated. It would try to fetch from Y's committed offset

In this flow, I don't think we are violating the offset committing policy here. 

The only downside I could think of is that there is only one member Y who will get fenced by itself after the immigration as stated in the description. [~guozhang];;;","17/Jul/20 21:38;guozhang;Fair enough. So it seems not a correctness breaking issue at the moment.;;;","23/Jul/20 06:24;akshaysh;Hi [~bchen225242]/[~guozhang],

I've raised the bug related to the same issue 6 days ago. Please look at it once.

https://issues.apache.org/jira/browse/KAFKA-10285

 

Analysis,

`when i've not restarted the broker and restarted consumer, I could see below logs` 
 ```
 [2020-07-16 13:56:17,189] INFO [GroupCoordinator 1001]: Preparing to rebalance group 0 in state PreparingRebalance with old generation 0 (__consumer_offsets-48) (reason: Adding new member 1-1594907777144 with group instanceid Some(1)) (kafka.coordinator.group.GroupCoordinator)
 [2020-07-16 13:56:17,236] INFO [GroupCoordinator 1001]: Stabilized group 0 generation 1 (__consumer_offsets-48) (kafka.coordinator.group.GroupCoordinator)
 [2020-07-16 13:56:17,282] INFO [GroupCoordinator 1001]: Assignment received from leader for group 0 for generation 1 (kafka.coordinator.group.GroupCoordinator)

********
 [2020-07-16 13:59:33,613] INFO [GroupCoordinator 1001]: Static member Some(1) with unknown member id rejoins, assigning new member id 1-1594907777335, while old member 1-1594907777144 will be removed. (kafka.coordinator.group.GroupCoordinator)
 [2020-07-16 13:59:33,635] INFO [GroupCoordinator 1001]: Static member joins during Stable stage will not trigger rebalance. (kafka.coordinator.group.GroupCoordinator)
 ```
 when restarted the broker, I could see broker is expecting some other member.id(old member.id of consumer)
 ```
 2020-07-16 14:04:04,953] ERROR given member.id 1-1594907777335 is identified as a known static member 1,but not matching the expected member.id 1-1594907777144 (kafka.coordinator.group.GroupMetadata)
 ```

 

 ;;;","23/Jul/20 17:19;ableegoldman;You know, I think we actually hit this too, but weren't able to recognize the problem at the time. A few weeks ago one of our StreamThreads/Consumers seemed to ""take off"" from the group at some point, as evidenced by the steadily increasing last-rebalance-seconds-ago metric (whereas the other members had rebalanced multiple times since then). Right before this occurred we saw that same error message in the logs:

 
{code:java}
ERROR given member.id X is identified as a known static member 1,but not matching the expected member.id Y (kafka.coordinator.group.GroupMetadata)
{code}
Unfortunately we killed the client trying to remotely debug it so we couldn't get any more useful information. Would you say that this was mysterious encounter was likely due to the bug reported here? [~guozhang] [~bchen225242]

 ;;;","23/Jul/20 19:01;bchen225242;[~akshaysh] I didn't see any trace that the group coordinator gets migrated in the pasted ticket, so it might be a separate issue.

[~ableegoldman] Well, the symptom matches, but I don't know for sure if this is the same cause :);;;","24/Jul/20 01:01;guozhang;What [~ableegoldman] described seems aligned to this, BUT I thought that fenced instance Y should still try to rejoin the group, but in [~ableegoldman] that thread did not try to rejoin at all?;;;","24/Jul/20 05:07;akshaysh;At initial, Group Coordinator will assign member id to the new joined consumer i.e X with generation id 1, when the consumer take off at the same point and try to rejoin there be no re-balance(i.e generation id will be 1), but group coordinator will assign new member id to the consumer i.e Y. Now, when broker restart due to some reason, groupmetadatamanager will reload the metadata and expects that member.id to be present, he is now expecting X member.id. and that's why consumer is fenced.

1) Is this a expected behaviour?

2) when consumer member id changes, groupmetadatamanager not updates new member.id information?

3) consumer is fenced with new instance, even single consumer is running. Why?

[~bchen225242] [~guozhang]

 

 ;;;","24/Jul/20 16:57;ableegoldman;Nope, it just spun in a loop where it would poll and then call Consumer#position to try and initialize some metadata. Poll just returned empty I guess and Consumer#position continued to throw TimeoutException. 

The TimeoutException part seems pretty weird, so maybe it was an unrelated bug (or some other issue like a hung socket being the primary/only guess we had). I just thought of it because it definitely happened immediately after a ""id does not match expected member.id"" error.

I actually still have some of the broker and client side logs from around this incident, if that might help. But again, I'm not really sure if it could be related to this or not;;;","27/Jul/20 18:12;hachikuji;[~bchen225242] Hmm.. Not totally sure I buy this:

> A static member X joins the group and updates member.id to M1, then gets stuck
> Another static member Y with the same instance.id joins and updates member.id to M2, while starts working and commit offsets
> The group coordinator migrates, and the member.id for the same static member rewinds to M1
> The static member X goes back online, and validated. It would try to fetch from Y's committed offset

Why would member X fetch Y's committed offset? If it doesn't know it had been fenced temporarily, it might just commit its latest offsets. This does seem like a correctness problem to me.;;;","28/Jul/20 20:45;bchen225242;That's a good observation, I forgot that for static members they don't actually do another Fetch offset immediately after they rejoin the group. You should be right about this, and I haven't thought about the scenario where EOS and static membership are both turned on.;;;","11/Aug/20 16:13;bchen225242;Resign from this ticket for now, others feel free to pick up.;;;","12/Aug/20 00:39;ableegoldman;[~bchen225242] [~hachikuji] Is this a correctness problem for all applications or only EOS?;;;","12/Aug/20 22:44;bchen225242;I would say for all. Let me check if [~feyman] is interested in picking this up.;;;","23/Aug/20 07:29;feyman;Checked with [~bchen225242] offline, and I just reproduced the issue with the procedure as in the attachment, happy to know if there are more failure scenarios related.[^How to reproduce the issue in KAFKA-10284.md];;;","09/Sep/20 14:02;feyman;Just FYI, created a PR: https://github.com/apache/kafka/pull/9270;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log metrics are removed if a log is deleted and re-created quickly enough,KAFKA-10282,13317289,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bob-barrett,bob-barrett,bob-barrett,17/Jul/20 00:30,31/Jul/20 06:49,13/Jul/23 09:17,31/Jul/20 06:49,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,log,,,,,0,,,,,"When we delete a local log, we mark it for asynchronous deletion by renaming it with a `.delete` extension, and then wait `LogConfig.FileDeleteDelayMs` milliseconds before actually deleting the files on disk. We don't remove the Log metrics from the metrics registry until the actual deletion takes place. If we recreate a log of the same topic partition (for example, if we reassign the partition away from the broker and quickly reassign it back), the metrics are registered when the new log is created, but then unregistered when the async deletion of the original log takes place. This leaves us with a partition that is not reporting any Log metrics (size, offsets, number of segments, etc).

To fix this, the LogManager should check when creating new logs to see if a log for the same topic partition is marked for deletion, and if so, signal to that log not to unregister its metrics when it is deleted.",,bob-barrett,dhruvilshah,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 18 02:17:35 UTC 2020,,,,,,,,,,"0|z0gugo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/20 02:17;ijuma;Have we considered removing the metrics when the log is initially deleted instead of waiting for the async delete?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction system test uses inconsistent timeouts,KAFKA-10274,13316784,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,14/Jul/20 23:16,22/Jul/20 19:11,13/Jul/23 09:17,22/Jul/20 19:11,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We've seen some failures in the transaction system test with errors like the following:
{code}
copier-1 : Message copier didn't make enough progress in 30s. Current progress: 0
{code}
Looking at the consumer logs, we see the following messages repeating over and over:
{code}
[2020-07-14 06:50:21,466] DEBUG [Consumer clientId=consumer-transactions-test-consumer-group-1, groupId=transactions-test-consumer-group] Fetching committed offsets for partitions: [input-topic-1] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2020-07-14 06:50:21,468] DEBUG [Consumer clientId=consumer-transactions-test-consumer-group-1, groupId=transactions-test-consumer-group] Failed to fetch offset for partition input-topic-1: There are unstable offsets that need to be cleared. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
{code}
I think the problem is that the test implicitly depends on the transaction timeout which has been configured to 40s even though it expects progress after 30s.",,chia7712,hachikuji,junrao,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10235,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 19:11:33 UTC 2020,,,,,,,,,,"0|z0grd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 23:30;hachikuji;Looks like this issue was already fixed by KAFKA-10235 and I had an older branch. Closing as a duplicate;;;","14/Jul/20 23:39;hachikuji;Apologies for the noise, but I'm going to reopen this. The recent fix addressed the issue with the transaction timeout exceeding the wait time, but it also opened the door again to the issue reported in KAFKA-9802. ;;;","20/Jul/20 20:34;vvcephei;Hi [~hachikuji] ,

I'm trying to get a green system test build for the 2.5.1 release, and this test seems to be failing quite a bit in the last few days.

I see that you already fixed the test back in May in https://issues.apache.org/jira/browse/KAFKA-9802 for 2.5.1, and that you theorized that https://issues.apache.org/jira/browse/KAFKA-10235 may have re-introduced the test failure.

It doesn't look like KAFKA-10235 was backported to 2.5. Maybe it should have been, but then again, your last comment makes me think that we still need your current fix on top of it.

What do you think we should do here? Backport KAFKA-10235 and then the PR for this ticket once it's merged?

Thanks,

-John

 

PS, the results I looked at:

[http://confluent-kafka-2-5-system-test-results.s3-us-west-2.amazonaws.com/2020-07-18--001.1595065230--confluentinc--2.5]–21e17cd14/report.html

[http://confluent-kafka-2-5-system-test-results.s3-us-west-2.amazonaws.com/2020-07-19--001.1595151548--confluentinc--2.5]–21e17cd14/report.html

[http://confluent-kafka-2-5-system-test-results.s3-us-west-2.amazonaws.com/2020-07-20--001.1595238538--confluentinc--2.5]–21e17cd14/report.html

from [https://jenkins.confluent.io/job/system-test-kafka/job/2.5/];;;","20/Jul/20 21:38;vvcephei;FWIW, I cherry-picked both of these fixes on top of 2.5:

[https://github.com/apache/kafka/compare/2.5...vvcephei:2.5-fix-transactional-test?expand=1]

And the test passes for me locally.

I'm running the whole transactions_test x5 to be sure: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4056/console;;;","22/Jul/20 19:11;junrao;Merged this to trunk and 2.6 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-server-stop.sh fails on IBM i,KAFKA-10272,13316770,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ThePrez,ThePrez,ThePrez,14/Jul/20 20:48,02/Sep/20 15:46,13/Jul/23 09:17,02/Sep/20 15:46,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,,,,,0,easyfix,,,,"On the IBM i platform, the `kafka-server-stop.sh` script always fails with an error message ""No kafka server to stop""

 

The underlying cause is because the script relies on the output of `ps ax` to determine the pid. More specifically:
{code:java}
PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')
{code}
On IBM i, the ps utility is unconventional and truncates the output with these arguments. For instance, here is part of the ps output
{code:java}
 584329      - A     0:00 /QOpenSys/QIBM/ProdData/SC1/OpenSSH/sbin/sshd -R
 584331      - A     0:00 /QOpenSys/QIBM/ProdData/SC1/OpenSSH/libexec/sftp-serv
 584332      - A     0:00 /QOpenSys/QIBM/ProdData/SC1/OpenSSH/sbin/sshd -R
 584334  pts/5 A     0:00 -bash
 584365  pts/7 A     0:08 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGC
 585353  pts/8 A     0:12 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPaus
 585690  pts/9 A     0:00 ps ax
{code}
 

Therefore, the resultant grep always comes up empty. When invoked with `ps -af`, it gives the whole command (when piped) but sticks in the UID by default 
{code:java}
ps -af
     UID    PID   PPID   C    STIME    TTY  TIME CMD
jgorzins 585353 583321   0 12:41:07  pts/8  0:41 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurr
jgorzins 585817 585794   0 14:44:25  pts/4  0:00 ps -af

{code}
So.... the following PID check works for IBM i:

 
{code:java}
PIDS=$(ps -af | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $2}')
{code}
so, a fix would be (I have verified this):
{code:java}
if [[ ""OS400"" == $(uname -s) ]]; then
  PIDS=$(ps -af | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $2}')
else
  PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')
fi
{code}
This all also applies to `zookeeper-server-stop.sh`",IBM i 7.2,ThePrez,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-14 20:48:45.0,,,,,,,,,,"0|z0gra0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression while fetching a key from a single partition,KAFKA-10271,13316717,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dima5rr,dima5rr,dima5rr,14/Jul/20 14:43,09/Oct/20 14:16,13/Jul/23 09:17,08/Oct/20 17:43,2.5.0,2.5.1,2.6.0,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,streams,,,,,0,KAFKA-10030,KAFKA-9445,KIP-562,,"This is follow-up bug for KAFKA-10030 

StreamThreadStateStoreProvider excessive loop over calling internalTopologyBuilder.topicGroups(), which is synchronized, thus causing significant performance degradation to the caller, especially when store has many partitions.
 ",,ableegoldman,dima5rr,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/20 08:34;dima5rr;9020.png;https://issues.apache.org/jira/secure/attachment/13007684/9020.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 16:05:25 UTC 2020,,,,,,,,,,"0|z0gqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/20 08:35;dima5rr;After profiling under load it looks like problem in excessive loops over calling internalTopologyBuilder.topicGroups()
which is synchronized.

!9020.png!;;;","17/Jul/20 16:05;vvcephei;Bumped to 2.5.2, as the 2.5.1 release is already in progress.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"dynamic config like ""--delete-config log.retention.ms"" doesn't work",KAFKA-10268,13316424,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,huxi_2b,zhifeng.peng,zhifeng.peng,13/Jul/20 11:01,29/Jul/20 07:22,13/Jul/23 09:17,24/Jul/20 01:37,2.1.1,,,,,,,,,,,,,,,,,,,,,,2.6.0,2.7.0,,,,,,,log,log cleaner,,,,0,,,,,"After I set ""log.retention.ms=301000"" to clean the data,i use the cmd

""bin/kafka-configs.sh --bootstrap-server 10.129.104.15:9092 --entity-type brokers --entity-default --alter --delete-config log.retention.ms"" to reset to default.

Static broker configuration like log.retention.hours is 168h and no topic level configuration like retention.ms.

it did not take effect actually although server.log print the broker configuration like that.

log.retention.check.interval.ms = 300000
 log.retention.hours = 168
 log.retention.minutes = null
 {color:#ff0000}log.retention.ms = null{color}
 log.roll.hours = 168
 log.roll.jitter.hours = 0
 log.roll.jitter.ms = null
 log.roll.ms = null
 log.segment.bytes = 1073741824
 log.segment.delete.delay.ms = 60000

 

Then we can see that retention time is still 301000ms from the server.log and segments have been deleted.

[2020-07-13 14:30:00,958] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Found deletable segments with base offsets [5005329,6040360] due to retention time 301000ms breach (kafka.log.Log)
 [2020-07-13 14:30:00,959] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Scheduling log segment [baseOffset 5005329, size 1073741222] for deletion. (kafka.log.Log)
 [2020-07-13 14:30:00,959] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Scheduling log segment [baseOffset 6040360, size 1073728116] for deletion. (kafka.log.Log)
 [2020-07-13 14:30:00,959] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Incrementing log start offset to 7075648 (kafka.log.Log)
 [2020-07-13 14:30:00,960] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Found deletable segments with base offsets [5005330,6040410] {color:#FF0000}due to retention time 301000ms{color} breach (kafka.log.Log)
 [2020-07-13 14:30:00,960] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Scheduling log segment [baseOffset 5005330, size 1073732368] for deletion. (kafka.log.Log)
 [2020-07-13 14:30:00,961] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Scheduling log segment [baseOffset 6040410, size 1073735366] for deletion. (kafka.log.Log)
 [2020-07-13 14:30:00,961] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Incrementing log start offset to 7075685 (kafka.log.Log)
 [2020-07-13 14:31:00,959] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Deleting segment 5005329 (kafka.log.Log)
 [2020-07-13 14:31:00,959] INFO [Log partition=test_retention-2, dir=/data/kafka_logs-test] Deleting segment 6040360 (kafka.log.Log)
 [2020-07-13 14:31:00,961] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Deleting segment 5005330 (kafka.log.Log)
 [2020-07-13 14:31:00,961] INFO [Log partition=test_retention-0, dir=/data/kafka_logs-test] Deleting segment 6040410 (kafka.log.Log)
 [2020-07-13 14:31:01,144] INFO Deleted log /data/kafka_logs-test/test_retention-2/00000000000006040360.log.deleted. (kafka.log.LogSegment)
 [2020-07-13 14:31:01,144] INFO Deleted offset index /data/kafka_logs-test/test_retention-2/00000000000006040360.index.deleted. (kafka.log.LogSegment)
 [2020-07-13 14:31:01,144] INFO Deleted time index /data/kafka_logs-test/test_retention-2/00000000000006040360.timeindex.deleted. (kafka.log.LogSegment)

 

Here are a few steps to reproduce it.

1、set log.retention.ms=301000:

bin/kafka-configs.sh --bootstrap-server 10.129.104.15:9092 --entity-type brokers --entity-default --alter --add-config log.retention.ms=301000

2、produce messages to the topic:

bin/kafka-producer-perf-test.sh --topic test_retention --num-records 10000000 --throughput -1 --producer-props bootstrap.servers=10.129.104.15:9092 --record-size 1024

3、reset log.retention.ms to the default:

bin/kafka-configs.sh --bootstrap-server 10.129.104.15:9092 --entity-type brokers --entity-default --alter --delete-config log.retention.ms

 

I have attched server.log. You can see the log from row 238 to row 731. ",,rhauch,zhangzs,zhifeng.peng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/20 11:00;zhifeng.peng;server.log.2020-07-13-14;https://issues.apache.org/jira/secure/attachment/13007542/server.log.2020-07-13-14",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 15:56:49 UTC 2020,,,,,,,,,,"0|z0gp5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/20 15:56;rhauch;This was also cherry-picked to {{2.6}}, but that branch has been frozen while we try to release AK 2.6.0. However, given that this is low-risk, I'll leave it on {{2.6}}, and updated the ""Fix Versions"" field above to include `2.6.0`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test TransactionsTest.testBumpTransactionalEpoch,KAFKA-10264,13316024,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,ableegoldman,ableegoldman,10/Jul/20 00:57,08/Dec/20 17:13,13/Jul/23 09:17,08/Dec/20 17:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,,,,,0,flaky-test,unit-test,,,"h3. Stacktrace

java.lang.AssertionError: Unexpected exception cause org.apache.kafka.common.KafkaException: The client hasn't received acknowledgment for some previously sent messages and can no longer retry them. It is safe to abort the transaction and continue. at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.assertTrue(Assert.java:42) at org.apache.kafka.test.TestUtils.assertFutureThrows(TestUtils.java:557) at kafka.api.TransactionsTest.testBumpTransactionalEpoch(TransactionsTest.scala:637)",,ableegoldman,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 17:39:54 UTC 2020,,,,,,,,,,"0|z0gmo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 07:57;chia7712;[~ableegoldman] Could I take over this issue?;;;","15/Sep/20 17:39;ableegoldman;Go for it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateDirectory is not thread-safe,KAFKA-10262,13316003,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,ableegoldman,ableegoldman,09/Jul/20 23:05,11/Jul/20 18:25,13/Jul/23 09:17,11/Jul/20 18:25,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"As explicitly stated in the StateDirectory javadocs,  ""This class is not thread-safe.""

Despite this, a single StateDirectory is shared among all the StreamThreads of a client. Some of the more ""dangerous"" methods are indeed synchronized, but others are not. For example, the innocent-sounding #directoryForTask is not thread-safe and is called in a number of places. We call it during task creation, and we call it during task closure (through StateDirectory#lock). It's not uncommon for one thread to be closing a task while another is creating it after a rebalance.

In fact, we saw exactly that happen in our test application. This ultimately lead to the following exception

 
{code:java}
org.apache.kafka.streams.errors.ProcessorStateException: task directory [/mnt/run/streams/state/stream-soak-test/1_0] doesn't exist and couldn't be created at org.apache.kafka.streams.processor.internals.StateDirectory.directoryForTask(StateDirectory.java:112) at org.apache.kafka.streams.processor.internals.ProcessorStateManager.<init>(ProcessorStateManager.java:187) at org.apache.kafka.streams.processor.internals.StandbyTaskCreator.createTasks(StandbyTaskCreator.java:85) at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:337)
{code}
 

The exception arises from this line in StateDirectory#directoryForTask:
{code:java}
if (hasPersistentStores && !taskDir.exists() && !taskDir.mkdir()) 
{code}
Presumably, if the taskDir did not exist when the two threads began this method, then they would both attempt to create the directory. One of them will get there first, leaving the other to return unsuccessfully from mkdir and ultimately throw the above ProcessorStateException.

I've only confirmed that this affects 2.6 so far, but the unsafe methods are present in earlier versions. It's possible we made the problem worse somehow during ""The Refactor"" so that it's easier to hit this race condition.",,ableegoldman,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 11 18:25:37 UTC 2020,,,,,,,,,,"0|z0gmjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/20 22:16;ableegoldman;I think we can just do something like 
{code:java}
if (hasPersistentStores && !taskDir.exists())
    try directory-creation-lock
        if (!taskDir.exists() && !taskDir.mkdir())
            throw PSE{code}
It's not exactly on the hot code path, but we might call it pretty frequently after a rebalance/during task initialization IIUC. So we might want a slightly finer grained locking than just throwing a `synchronized` around everything.

We could even lock at the individual task level, but that might be overkill;;;","11/Jul/20 18:25;mjsax;{quote}I've only confirmed that this affects 2.6 so far, but the unsafe methods are present in earlier versions. It's possible we made the problem worse somehow during ""The Refactor"" so that it's easier to hit this race condition.
{quote}
I looked into this, and we call {{directoryForTask}} only in one place: in the constructor of {{ProcessorStateManager}} that is created when a task is created. -- Thus, there is no race condition because a thread should always first revoke a task before the new thread creates it.

The difference to 2.6 is that there we call {{directoryForTask}} on some more places introducing the race condition.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system test kafkatest.tests.core.security_rolling_upgrade_test fails,KAFKA-10257,13315940,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,junrao,junrao,09/Jul/20 16:16,15/Jul/20 18:39,13/Jul/23 09:17,15/Jul/20 18:39,2.7.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"The test failure was reported in http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-07-08--001.1594266883--chia7712--KAFKA-10235--a76224fff/report.html

Saw the following error in the log.

{code:java}
[2020-07-09 00:56:37,575] ERROR [KafkaServer id=1] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
java.lang.IllegalArgumentException: Could not find a 'KafkaServer' or 'internal.KafkaServer' entry in the JAAS configuration. System property 'java.security.auth.login.config' is not set
        at org.apache.kafka.common.security.JaasContext.defaultContext(JaasContext.java:133)
        at org.apache.kafka.common.security.JaasContext.load(JaasContext.java:98)
        at org.apache.kafka.common.security.JaasContext.loadServerContext(JaasContext.java:70)
        at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:131)
        at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:97)
        at kafka.network.Processor.<init>(SocketServer.scala:780)
        at kafka.network.SocketServer.newProcessor(SocketServer.scala:406)
        at kafka.network.SocketServer.$anonfun$addDataPlaneProcessors$1(SocketServer.scala:285)
        at kafka.network.SocketServer.addDataPlaneProcessors(SocketServer.scala:284)
        at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1(SocketServer.scala:251)
        at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1$adapted(SocketServer.scala:248)
        at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:553)
        at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:551)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:920)
        at kafka.network.SocketServer.createDataPlaneAcceptorsAndProcessors(SocketServer.scala:248)
        at kafka.network.SocketServer.startup(SocketServer.scala:122)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:297)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)
        at kafka.Kafka$.main(Kafka.scala:82)
        at kafka.Kafka.main(Kafka.scala)
{code}
",,chia7712,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 18:39:20 UTC 2020,,,,,,,,,,"0|z0gm5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 14:56;chia7712;[~junrao] Could I take over this issue?;;;","14/Jul/20 16:32;junrao;[~chia7712] : Yes. Thanks for helping out.;;;","15/Jul/20 18:39;junrao;Merged to 2.6 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"100% cpu usage by kafkaConsumer poll , when broker can't be connect ",KAFKA-10254,13315870,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,xiaotong.wang,xiaotong.wang,09/Jul/20 11:29,17/Jul/20 16:47,13/Jul/23 09:17,17/Jul/20 16:27,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,,,,,,,,clients,,,,,0,,,,,"steps

1、start kafka broker 

2、start kafka consumer and subscribe some topic with some kafkaConsumer instance and  call  kafkaConsumer.*poll(Duration.ofMillis(pollTimeout))*   and set auto.commit.enabled=false

3、iptables to disable kafka broker  ip  in client vm or shutdown kafka brokers

4、cpu go to 100%

 

*why?*

 

 

left Vserison :2.3.1

right Version:2.5.0

 

for 2.3.1 kafkaConsumer when kafka  brokers go  down,updateAssignmentMetadataIfNeeded will block x ms and return empty records ,

!image-2020-07-09-19-24-20-604.png|width=926,height=164!

 

for 2.5.0

private Map<TopicPartition, List<ConsumerRecord<K, V>>> pollForFetches(Timer timer) {
 *long pollTimeout = coordinator == null ? timer.remainingMs() :*
 *Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());*

i check the source of kafka client ,poll timeout will be change to 0 ms ,when heartbeat timeout ，so  it will call poll without any block ,this will cause cpu go to 100%

 

 

 ",,ableegoldman,arvin.zheng,vvcephei,xiaotong.wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10134,,,,,,,,,,,,,,,"09/Jul/20 11:24;xiaotong.wang;image-2020-07-09-19-24-20-604.png;https://issues.apache.org/jira/secure/attachment/13007368/image-2020-07-09-19-24-20-604.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 16:27:52 UTC 2020,,,,,,,,,,"0|z0glq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 16:41;arvin.zheng;[~xiaotong.wang], see KAFKA-10134;;;","17/Jul/20 16:08;vvcephei;Hey [~xiaotong.wang] , thanks for the report! It looks like a duplicate of KAFKA-10134; is that right?;;;","17/Jul/20 16:27;vvcephei;Changing resolution to ""fixed"", since the linked duplicate is fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In-memory stores are skipped when checkpointing but not skipped when reading the checkpoint,KAFKA-10249,13315723,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,08/Jul/20 20:51,10/Jul/20 15:20,13/Jul/23 09:17,10/Jul/20 02:03,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"As the title suggests, offsets for in-memory stores (including the suppression buffer) are not written to the checkpoint file. However, when reading from the checkpoint file during task initialization, we do not check StateStore#persistent. We attempt to look up the offsets for in-memory stores in the checkpoint file, and obviously do not find them.

With eos we have to conclude that the existing state is dirty and thus throw a TaskCorruptedException. So pretty much any task with in-memory state will always hit this exception when reinitializing from the checkpoint, forcing it to clear the entire state directory and build up all of its state again from scratch (both persistent and in-memory).

This is especially unfortunate for KIP-441, as we will hit this any time a task is moved from one thread to another.",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 02:04:12 UTC 2020,,,,,,,,,,"0|z0gku0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/20 02:04;vvcephei;cc [~rhauch] , I've just pushed the fix to 2.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams may attempt to process after closing a task,KAFKA-10247,13315692,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,08/Jul/20 17:37,11/Jul/20 19:31,13/Jul/23 09:17,11/Jul/20 18:47,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Observed in a system test. A corrupted task was detected, and Stream properly closed it as dirty:
{code:java}
[2020-07-08 17:08:09,345] WARN stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Encountered org.apache.kafka.clients.consumer.OffsetOutOfRangeException fetching records from restore consumer for partitions [SmokeTest-cntStoreName-changelog-1], it is likely that the consumer's position has fallen out of the topic partition offset range because the topic was truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing it later. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Fetch position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[ducker03:9092 (id: 1 rack: null)], epoch=0}} is out of range for partition SmokeTest-cntStoreName-changelog-1
   at org.apache.kafka.clients.consumer.internals.Fetcher.handleOffsetOutOfRange(Fetcher.java:1344)
   at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1296)
   at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:611)
   at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1280)
   at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1238)
   at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1206)
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:433)
   at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:664)
   at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:548)
   at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:507)
[2020-07-08 17:08:09,345] WARN stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Detected the states of tasks {2_1=[SmokeTest-cntStoreName-changelog-1]} are corrupted. Will close the task as dirty and re-create and bootstrap from scratch. (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.TaskCorruptedException: Tasks with changelogs {2_1=[SmokeTest-cntStoreName-changelog-1]} are corrupted and hence needs to be re-initialized
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:446)
   at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:664)
   at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:548)
   at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:507)
Caused by: org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Fetch position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[ducker03:9092 (id: 1 rack: null)], epoch=0}} is out of range for partition SmokeTest-cntStoreName-changelog-1
   at org.apache.kafka.clients.consumer.internals.Fetcher.handleOffsetOutOfRange(Fetcher.java:1344)
   at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1296)
   at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:611)
   at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1280)
   at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1238)
   at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1206)
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:433)
   ... 3 more
[2020-07-08 17:08:09,346] INFO stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] task [2_1] Suspended running (org.apache.kafka.streams.processor.internals.StreamTask)
[2020-07-08 17:08:09,346] DEBUG stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] task [2_1] Closing its state manager and all the registered state stores: {sum-STATE-STORE-0000000050=StateStoreMetadata (sum-STATE-STORE-0000000050 : SmokeTest-sum-STATE-STORE-0000000050-changelog-1 @ null, cntStoreName=StateStoreMetadata (cntStoreName : SmokeTest-cntStoreName-changelog-1 @ 0} (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
[2020-07-08 17:08:09,346] INFO [Consumer clientId=SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2-restore-consumer, groupId=null] Subscribed to partition(s): SmokeTest-minStoreName-changelog-1, SmokeTest-minStoreName-changelog-2, SmokeTest-sum-STATE-STORE-0000000050-changelog-0, SmokeTest-minStoreName-changelog-3, SmokeTest-sum-STATE-STORE-0000000050-changelog-2, SmokeTest-maxStoreName-changelog-1, SmokeTest-cntStoreName-changelog-0, SmokeTest-maxStoreName-changelog-2, SmokeTest-cntStoreName-changelog-2, SmokeTest-maxStoreName-changelog-3, SmokeTest-cntByCnt-changelog-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2020-07-08 17:08:09,348] DEBUG stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Released state dir lock for task 2_1 (org.apache.kafka.streams.processor.internals.StateDirectory)
[2020-07-08 17:08:09,348] INFO stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] task [2_1] Closing record collector dirty (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
[2020-07-08 17:08:09,348] INFO stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] task [2_1] Closed dirty (org.apache.kafka.streams.processor.internals.StreamTask){code}
However, there were already records buffered for it, so later on in the same processing loop, Streams tried to process that task, resulting in an IllegalStateException:
{code:java}
[2020-07-08 17:08:09,352] ERROR stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Failed to process stream task 2_1 due to the following error: (org.apache.kafka.streams.processor.internals.TaskManager)
org.apache.kafka.streams.errors.InvalidStateStoreException: Store cntStoreName is currently closed.
   at org.apache.kafka.streams.state.internals.WrappedStateStore.validateStoreOpen(WrappedStateStore.java:78)
   at org.apache.kafka.streams.state.internals.CachingKeyValueStore.get(CachingKeyValueStore.java:202)
   at org.apache.kafka.streams.state.internals.CachingKeyValueStore.get(CachingKeyValueStore.java:40)
   at org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore.lambda$getWithBinary$0(MeteredTimestampedKeyValueStore.java:63)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:851)
   at org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore.getWithBinary(MeteredTimestampedKeyValueStore.java:62)
   at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:129)
   at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:836)
   at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:236)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:216)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:168)
   at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:96)
   at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:679)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:836)
   at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:679)
   at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1003)
   at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:685)
   at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:548)
   at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:507)
[2020-07-08 17:08:09,352] ERROR stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.InvalidStateStoreException: Store cntStoreName is currently closed.
   at org.apache.kafka.streams.state.internals.WrappedStateStore.validateStoreOpen(WrappedStateStore.java:78)
   at org.apache.kafka.streams.state.internals.CachingKeyValueStore.get(CachingKeyValueStore.java:202)
   at org.apache.kafka.streams.state.internals.CachingKeyValueStore.get(CachingKeyValueStore.java:40)
   at org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore.lambda$getWithBinary$0(MeteredTimestampedKeyValueStore.java:63)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:851)
   at org.apache.kafka.streams.state.internals.MeteredTimestampedKeyValueStore.getWithBinary(MeteredTimestampedKeyValueStore.java:62)
   at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:129)
   at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:836)
   at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:236)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:216)
   at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:168)
   at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:96)
   at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:679)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:836)
   at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:679)
   at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1003)
   at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:685)
   at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:548)
   at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:507)
[2020-07-08 17:08:09,352] INFO stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-07-08 17:08:09,352] INFO stream-thread [SmokeTest-66676ca8-d517-4e4b-bb5f-44203e24e569-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread){code}
Which caused the entire thread to shut down.

 

Instead, we should not attempt to process tasks that are not running.",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-08 17:37:41.0,,,,,,,,,,"0|z0gkn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractProcessorContext topic() throws NullPointerException when modifying a state store within the DSL from a punctuator,KAFKA-10246,13315524,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,pjp1981,pjp1981,08/Jul/20 02:24,30/Jul/21 20:48,13/Jul/23 09:17,30/Jul/21 20:48,2.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"NullPointerException seen when a KTable statestore is being modified by a punctuated method which is added to a topology via the DSL processor/ktable valueTransfomer methods.

It seems valid for AbstractProcessorContext.topic() to return null; however the check below returns a NullPointerException before a null can be returned.
{quote}if (topic.equals(NONEXIST_TOPIC)) {
{quote}
Made a local fix to reverse the ordering of the check (i.e. avoid the null) and this appears to fix the issue and sends the change to the state stores changelog topic.
{quote}if (NONEXIST_TOPIC.equals(topic)) {
{quote}
Stacktrace below

{{2020-07-02 07:29:46,829 [ABC_aggregator-551a90c1-d7c3-4357-a608-3ea79951f4e8-StreamThread-5] ERROR [o.a.k.s.p.i.StreamThread]: stream-thread [ABC_aggregator-5}}
 {{51a90c1-d7c3-4357-a608-3ea79951f4e8-StreamThread-5] Encountered the following error during processing:}}
 {{java.lang.NullPointerException: null}}
 \{{ at org.apache.kafka.streams.processor.internals.AbstractProcessorContext.topic(AbstractProcessorContext.java:115)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.putInternal(CachingKeyValueStore.java:141)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.put(CachingKeyValueStore.java:123)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.put(CachingKeyValueStore.java:36)}}
 \{{ at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$put$3(MeteredKeyValueStore.java:144)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.put(MeteredKeyValueStore.java:144)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl$KeyValueStoreReadWriteDecorator.put(ProcessorContextImpl.java:487)}}
 \{{ at org.apache.kafka.streams.kstream.internals.KTableKTableJoinMerger$KTableKTableJoinMergeProcessor.process(KTableKTableJoinMerger.java:118)}}
 \{{ at org.apache.kafka.streams.kstream.internals.KTableKTableJoinMerger$KTableKTableJoinMergeProcessor.process(KTableKTableJoinMerger.java:97)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180)}}
 \{{ at org.apache.kafka.streams.kstream.internals.KTableKTableOuterJoin$KTableKTableOuterJoinProcessor.process(KTableKTableOuterJoin.java:118)}}
 \{{ at org.apache.kafka.streams.kstream.internals.KTableKTableOuterJoin$KTableKTableOuterJoinProcessor.process(KTableKTableOuterJoin.java:65)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180)}}
 \{{ at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:45)}}
 \{{ at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:28)}}
 \{{ at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$setFlushListener$1(MeteredKeyValueStore.java:119)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.putAndMaybeForward(CachingKeyValueStore.java:92)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.lambda$initInternal$0(CachingKeyValueStore.java:72)}}
 \{{ at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:151)}}
 \{{ at org.apache.kafka.streams.state.internals.NamedCache.evict(NamedCache.java:244)}}
 \{{ at org.apache.kafka.streams.state.internals.ThreadCache.maybeEvict(ThreadCache.java:240)}}
 \{{ at org.apache.kafka.streams.state.internals.ThreadCache.put(ThreadCache.java:150)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.putInternal(CachingKeyValueStore.java:131)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.put(CachingKeyValueStore.java:123)}}
 \{{ at org.apache.kafka.streams.state.internals.CachingKeyValueStore.put(CachingKeyValueStore.java:36)}}
 \{{ at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$put$3(MeteredKeyValueStore.java:144)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.put(MeteredKeyValueStore.java:144)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorContextImpl$KeyValueStoreReadWriteDecorator.put(ProcessorContextImpl.java:487)}}
 \{{ at com.pjp1981.streambuilder.StreamsBuilderHelper$1.lambda$init$0(StreamsBuilderHelper.java:55) // punctuated lambda - user code}}
 \{{ at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) //iterates over the state store and cleans up old items}}
 \{{ at com.pjp1981.streambuilder.StreamsBuilderHelper$1.lambda$init$1(StreamsBuilderHelper.java:47)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$punctuate$3(ProcessorNode.java:161)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.processor.internals.ProcessorNode.punctuate(ProcessorNode.java:161)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamTask.lambda$punctuate$4(StreamTask.java:445)}}
 \{{ at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamTask.punctuate(StreamTask.java:445)}}
 \{{ at org.apache.kafka.streams.processor.internals.PunctuationQueue.mayPunctuate(PunctuationQueue.java:54)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamTask.maybePunctuateSystemTime(StreamTask.java:868)}}
 \{{ at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.punctuate(AssignedStreamsTasks.java:502)}}
 \{{ at org.apache.kafka.streams.processor.internals.TaskManager.punctuate(TaskManager.java:557)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamThread.maybePunctuate(StreamThread.java:951)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:823)}}
 \{{ at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)}}
 {

{ at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670)}

}","linux, windows, java 11",ableegoldman,cadonna,chia7712,mjsax,pjp1981,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 17:47:38 UTC 2020,,,,,,,,,,"0|z0gjmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 17:47;vvcephei;Hi [~pjp1981] , thanks for the report and sorry for the trouble.

It seems like you have a good proposal for a fix. Would you like to submit a PR to fix it?

Thanks,

-John;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException while processing connection setup timeouts,KAFKA-10243,13315358,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,rsivaram,rsivaram,07/Jul/20 09:15,20/Jul/20 21:28,13/Jul/23 09:17,07/Jul/20 15:49,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,network,,,,,0,,,,,"From [~guozhang] in [https://github.com/apache/kafka/pull/8683:]

{quote}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
	at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
	at org.apache.kafka.clients.NetworkClient.handleTimedOutConnections(NetworkClient.java:822)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:574)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:419)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359)
{quote}

While processing connection set up timeouts, we are iterating through the connecting nodes to process timeouts and we disconnect within the loop, removing the entry from the set in the loop that it iterating over the set:

{code}
    for (String nodeId : connectingNodes) {
            if (connectionStates.isConnectionSetupTimeout(nodeId, now)) {
                this.selector.close(nodeId);
                log.debug(
                    ""Disconnecting from node {} due to socket connection setup timeout. "" +
                    ""The timeout value is {} ms."",
                    nodeId,
                    connectionStates.connectionSetupTimeoutMs(nodeId));
                processDisconnection(responses, nodeId, now, ChannelState.LOCAL_CLOSE);
            }
    }
{code}",,dajac,rsivaram,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 16:11:40 UTC 2020,,,,,,,,,,"0|z0gim0:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,"07/Jul/20 14:45;vvcephei;Hey all, it seems like this is fixing a regression in 2.5.1/2.6.0 . Should it be a blocker and tagged for those releases?;;;","07/Jul/20 14:52;dajac;[~vvcephei] This only affects trunk AFAIK.;;;","07/Jul/20 16:11;vvcephei;Thanks, [~dajac] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink tasks should not throw WakeupException on shutdown,KAFKA-10240,13315233,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,06/Jul/20 20:27,14/Jul/20 16:28,13/Jul/23 09:17,14/Jul/20 04:13,2.0.0,2.0.1,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,2.6.0,,,,,,,,,,2.7.0,,,,,,,,KafkaConnect,,,,,0,,,,,"* When a task is scheduled for shutdown, the framework [wakes up the consumer|https://github.com/apache/kafka/blob/8a24da376b801c6eb6522ad4861b83f5beb5826c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L159] for that task.

 * As is noted in the [Javadocs for that method|https://github.com/apache/kafka/blob/8a24da376b801c6eb6522ad4861b83f5beb5826c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L2348], “If no thread is blocking in a method which can throw {{org.apache.kafka.common.errors.WakeupException}}, the next call to such a method will raise it instead.”

 * It just so happens that, if the framework isn’t in the middle of a call to the consumer and then the task gets stopped, the next call the framework will make on the consumer may be to commit offsets, which will immediately throw a {{WakeupException}}.

 * Currently, the framework handles this by [immediately retrying the offset commit|https://github.com/apache/kafka/blob/8a24da376b801c6eb6522ad4861b83f5beb5826c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L337-L339] until it either throws a different exception or succeeds, and then throwing the original {{WakeupException}}. If this synchronous commit of offsets occurs during task shutdown (as opposed to in response to a consumer rebalance), it's unnecessary to throw the {{WakeupException}} back to the caller, and can cause alarming {{ERROR}}-level messages to get logged by the worker.",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 16:11:40 UTC 2020,,,,,,,,,,"0|z0ghu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 16:11;ChrisEgerton;For some background: this was actually discussed years ago on [https://github.com/apache/kafka/pull/1511], but never implemented.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The groupInstanceId field in DescribeGroup response should be ignorable,KAFKA-10239,13315226,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,bchen225242,hachikuji,hachikuji,06/Jul/20 19:53,07/Jul/20 03:35,13/Jul/23 09:17,07/Jul/20 03:35,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,,,,,,0,,,,,"We noticed the following error in the logs in the handling of a DescribeGroup request:
```
org.apache.kafka.common.errors.UnsupportedVersionException: Attempted to write a non-default groupInstanceId at version 3
```
The problem is that the field is not marked as ignorable. So if the user is relying on static membership and uses an older AdminClient, they will see this error.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-06 19:53:41.0,,,,,,,,,,"0|z0ghso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky transactions_test.py,KAFKA-10235,13315036,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,05/Jul/20 15:43,14/Jul/20 23:35,13/Jul/23 09:17,09/Jul/20 16:35,,,,,,,,,,,,,,,,,,,,,,,2.6.0,2.7.0,,,,,,,,,,,,0,,,,,"{code}
=hard_bounce.bounce_target=clients.check_order=False.use_group_metadata=False: FAIL: copier-1 : Message copier didn't make enough progress in 30s. Current progress: 0
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 134, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 192, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 429, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 254, in test_transactions
    num_messages_to_copy=self.num_seed_messages, use_group_metadata=use_group_metadata)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 195, in copy_messages_transactionally
    self.bounce_copiers(copiers, clean_shutdown)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/transactions_test.py"", line 120, in bounce_copiers
    % (copier.transactional_id, str(copier.progress_percent())))
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: copier-1 : Message copier didn't make enough progress in 30s. Current progress: 0

{code}",,chia7712,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10274,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 16:35:23 UTC 2020,,,,,,,,,,"0|z0ggmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 16:35;junrao;merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The license term about jersey is not correct,KAFKA-10224,13314548,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,RensGroothuijsen,jaredli2020,jaredli2020,01/Jul/20 19:42,29/Jul/20 07:53,13/Jul/23 09:17,28/Jul/20 18:10,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.2,2.6.0,2.7.0,,,,core,,,,,0,pull-request-available,,,,"Kafka 2.3.0 and later bundle jersey 2.28. Since 2.28, jersey changed the license type from CDDL/GPLv2+CPE to EPLv2. But in Kafka 2.5.0's LICENSE file [https://github.com/apache/kafka/blob/2.5/LICENSE], it still said

""This distribution has a binary dependency on jersey, which is available under the CDDL"".

This should be corrected ASAP.",,jaredli2020,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 17:23:14 UTC 2020,,,,,,,,,,"0|z0gdr4:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"28/Jul/20 17:23;rhauch;Kafka upgraded to Jersey 2.28 starting in 2.3.0 via [https://github.com/apache/kafka/pull/6665]. 

Also, just to clarify, per [https://www.apache.org/legal/resolved.html#weak-copyleft-licenses] it is still valid for Apache Kafka to include Jersey in binary form:

{quote}
Software under the following licenses may be included in binary form within an Apache product if the inclusion is appropriately labeled (see above):

 Common Development and Distribution Licenses: CDDL 1.0 and CDDL 1.1
 Common Public License: CPL 1.0
 Eclipse Public License: EPL 1.0
 IBM Public License: IPL 1.0
 Mozilla Public Licenses: MPL 1.0, MPL 1.1, and MPL 2.0
 Sun Public License: SPL 1.0
 Open Software License 3.0
 Erlang Public License
 UnRAR License (only for unarchiving)
 SIL Open Font License
 Ubuntu Font License Version 1.0
 IPA Font License Agreement v1.0
 Ruby License (including the older version when GPLv2 was a listed alternative Ruby 1.9.2 license)
 Eclipse Public License 2.0: EPL 2.0
{quote};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaNotAvailableException must be retriable to handle reassignments,KAFKA-10223,13314430,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rsivaram,rsivaram,rsivaram,01/Jul/20 08:48,23/Jul/21 11:34,13/Jul/23 09:17,19/Jul/20 10:28,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,clients,,,,,0,,,,,ReplicaNotAvailableException should be a retriable `InvalidMetadataException` since consumers may throw this during reassignments.,,dongjoon,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 23:05:26 UTC 2020,,,,,,,,,,"0|z0gd0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/20 18:31;dongjoon;Hi, [~rsivaram]. Is this only 2.6.0 issue?;;;","10/Aug/20 10:17;rsivaram;[~dongjoon] This is not a 2.6.0 issue, ReplicaNotAvailable has always been non-retriable. This turned out to be a bigger issue in non-Java consumers since fetch-from-follower was introduced in  2.4.0. Java consumers always handled this case correctly and were not affected.;;;","10/Aug/20 23:05;dongjoon;Thank you for the confirmation, [~rsivaram].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect methods show up in 0.10 Kafka Streams docs,KAFKA-10222,13314424,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,huxi_2b,huxi_2b,huxi_2b,01/Jul/20 08:23,07/Jul/20 18:33,13/Jul/23 09:17,07/Jul/20 18:33,0.10.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,documentation,,,,,0,,,,,"In 0.10 Kafka Streams doc([http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html]), two wrong methods show up, as show below:

_builder.from(""my-input-topic"").mapValue(value -> value.length().toString()).to(""my-output-topic"");_

 

There is no method named `from` or `mapValues`. They should be `stream` and `mapValues` respectively.

 

 ",,githubbot,huxi_2b,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 17:47:34 UTC 2020,,,,,,,,,,"0|z0gczk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/20 09:08;githubbot;huxihx opened a new pull request #272:
URL: https://github.com/apache/kafka-site/pull/272


   https://issues.apache.org/jira/browse/KAFKA-10222
   
   Non-existent methods show up in the doc:
   _builder.from(""my-input-topic"").mapValue(value -> value.length().toString()).to(""my-output-topic"");_
   
   There is no method named `from` or `mapValues`. They should be `stream` and `mapValues` respectively.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","01/Jul/20 09:08;githubbot;huxihx commented on pull request #272:
URL: https://github.com/apache/kafka-site/pull/272#issuecomment-652295642


   @ijuma Please review this patch. Thanks.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Jul/20 01:01;githubbot;mjsax commented on pull request #272:
URL: https://github.com/apache/kafka-site/pull/272#issuecomment-653828716


   Thanks for the PR @huxihx -- note, that this pages are generated from the JavaDocs. Thus, if we fix it, we should fix it in the source code?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Jul/20 02:19;githubbot;huxihx commented on pull request #272:
URL: https://github.com/apache/kafka-site/pull/272#issuecomment-653833022


   @mjsax Thanks for reminding. Will close this PR and open a new PR to track this issue.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Jul/20 02:19;githubbot;huxihx closed pull request #272:
URL: https://github.com/apache/kafka-site/pull/272


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Jul/20 17:46;githubbot;huxihx opened a new pull request #272:
URL: https://github.com/apache/kafka-site/pull/272


   https://issues.apache.org/jira/browse/KAFKA-10222
   
   Non-existent methods show up in the doc:
   _builder.from(""my-input-topic"").mapValue(value -> value.length().toString()).to(""my-output-topic"");_
   
   There is no method named `from` or `mapValues`. They should be `stream` and `mapValues` respectively.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Jul/20 17:47;githubbot;mjsax commented on pull request #272:
URL: https://github.com/apache/kafka-site/pull/272#issuecomment-655021969


   We can still merge this PR, because otherwise the web-page would not be updated.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Jul/20 17:47;githubbot;mjsax merged pull request #272:
URL: https://github.com/apache/kafka-site/pull/272


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport fix for KAFKA-9603 to 2.5 ,KAFKA-10221,13314277,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cadonna,cadonna,cadonna,30/Jun/20 10:50,09/Mar/21 09:15,13/Jul/23 09:17,08/Jul/20 02:53,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,,,,,,,,streams,,,,,0,,,,,"The fix for [KAFKA-9603|https://issues.apache.org/jira/browse/KAFKA-9603] shall be backported to 2.5. ",,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-30 10:50:32.0,,,,,,,,,,"0|z0gc2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when describing resources,KAFKA-10220,13314272,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,showuon,ecomar,ecomar,30/Jun/20 10:26,09/Jul/20 09:28,13/Jul/23 09:17,09/Jul/20 08:59,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,core,,,,,0,,,,,"In current trunk code 
 Describing a topic from the CLI can fail with an NPE in the broker

on the line 

{{          resource.configurationKeys.asScala.forall(_.contains(configName))}}

 

(configurationKeys is null)

{{[2020-06-30 11:10:39,464] ERROR [Admin Manager on Broker 0]: Error processing describe configs request for resource DescribeConfigsResource(resourceType=2, resourceName='topic1', configurationKeys=null) (kafka.server.AdminManager)}}{{java.lang.NullPointerException}}{{at kafka.server.AdminManager.$anonfun$describeConfigs$3(AdminManager.scala:395)}}{{at kafka.server.AdminManager.$anonfun$describeConfigs$3$adapted(AdminManager.scala:393)}}{{at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:248)}}{{at scala.collection.Iterator.foreach(Iterator.scala:929)}}{{at scala.collection.Iterator.foreach$(Iterator.scala:929)}}{{at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)}}{{at scala.collection.IterableLike.foreach(IterableLike.scala:71)}}{{at scala.collection.IterableLike.foreach$(IterableLike.scala:70)}}{{at scala.collection.AbstractIterable.foreach(Iterable.scala:54)}}{{at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:247)}}{{at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:245)}}{{at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:104)}}{{at scala.collection.TraversableLike.filter(TraversableLike.scala:259)}}{{at scala.collection.TraversableLike.filter$(TraversableLike.scala:259)}}{{at scala.collection.AbstractTraversable.filter(Traversable.scala:104)}}{{at kafka.server.AdminManager.createResponseConfig$1(AdminManager.scala:393)}}{{at kafka.server.AdminManager.$anonfun$describeConfigs$1(AdminManager.scala:412)}}{{at scala.collection.immutable.List.map(List.scala:283)}}{{at kafka.server.AdminManager.describeConfigs(AdminManager.scala:386)}}{{at kafka.server.KafkaApis.handleDescribeConfigsRequest(KafkaApis.scala:2595)}}{{at kafka.server.KafkaApis.handle(KafkaApis.scala:165)}}{{at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)}}{{at java.lang.Thread.run(Thread.java:748)}}",,ecomar,huxi_2b,ijuma,mimaison,omkreddy,showuon,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 08:59:33 UTC 2020,,,,,,,,,,"0|z0gc1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/20 11:19;huxi_2b;[~ecomar]  Thanks for reporting. I could not reproduce this issue with trunk. Are you using the latest code for clients?;;;","30/Jun/20 11:32;ecomar;[~huxi_2b] I initially hit this when using the 2.5.0 cli (from the binary distribution) against trunk (running in eclipse)

I was later able to reproduce using the cli built from trunk (./gradlew assemble -PscalaVersion=2.12);;;","30/Jun/20 11:49;ecomar;ok, a bit more steps to reproduce.... looks like the issue was linked to having created and described a topic with 2.5

{{#on current trunk}}
{{$ git lg}}
{{55b5b248c - (HEAD -> trunk, origin/trunk, origin/HEAD)}}

{{#build}}
{{$ ./gradlew clean}}
{{$ ./gradlew assemble -PscalaVersion=2.12}}

{{#run zookepeer and one broker}}
{{$ export SCALA_VERSION=2.12}}
{{$  bin/zookeeper-server-start.sh config/zookeeper.properties}}
{{$ bin/kafka-server-start.sh config/server.properties}}

{{#create topic with CLI}}
{{$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topic1 --partitions 1 --replication-factor 1}}
{{# works fine}}
{{# describe topic with CLI}}
{{$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe}}
{{Topic: topic1 PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824Topic: topic1 PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 Topic: topic1 Partition: 0 Leader: 0 Replicas: 0 Isr: 0}}
{{#works fine}}

{{in another terminal, use the kafka_2.13-2.5.0 binary distribution:}}
{{$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topic25 --partitions 1 --replication-factor 1}}
{{Created topic topic25.}}
{{#now describe it (still with 2.5) ... boom}}
{{$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe}}
{{Error while executing topic command : org.apache.kafka.common.errors.UnknownServerException: The server experienced an unexpected error when processing the request.Error while executing topic command : org.apache.kafka.common.errors.UnknownServerException: The server experienced an unexpected error when processing the request.[2020-06-30 12:39:48,581] ERROR java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownServerException: The server experienced an unexpected error when processing the request. at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)}}

{{#in the broker terminal, the exception is }}

{{[2020-06-30 12:40:27,543] ERROR [Admin Manager on Broker 0]: Error processing describe configs request for resource DescribeConfigsResource(resourceType=2, resourceName='topic1', configurationKeys=null) (kafka.server.AdminManager)[2020-06-30 12:40:27,543] ERROR [Admin Manager on Broker 0]: Error processing describe configs request for resource DescribeConfigsResource(resourceType=2, resourceName='topic1', configurationKeys=null) (kafka.server.AdminManager)java.lang.NullPointerException at kafka.server.AdminManager.$anonfun$describeConfigs$3(AdminManager.scala:359) at kafka.server.AdminManager.$anonfun$describeConfigs$3$adapted(AdminManager.scala:357) at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:291) at scala.collection.Iterator.foreach(Iterator.scala:943) at scala.collection.Iterator.foreach$(Iterator.scala:943) at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:290) at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:284) at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108) at scala.collection.TraversableLike.filter(TraversableLike.scala:382) at scala.collection.TraversableLike.filter$(TraversableLike.scala:382) at scala.collection.AbstractTraversable.filter(Traversable.scala:108) at kafka.server.AdminManager.$anonfun$describeConfigs$1(AdminManager.scala:357) at kafka.server.AdminManager.describeConfigs(AdminManager.scala:350) at kafka.server.KafkaApis.handleDescribeConfigsRequest(KafkaApis.scala:2594) at kafka.server.KafkaApis.handle(KafkaApis.scala:165) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70) at java.base/java.lang.Thread.run(Thread.java:834)}};;;","30/Jun/20 11:50;ecomar;{{also note that if the describe comes from the current trunk, it succeeds:}}

{{(trunk)$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe}}
 {{Topic: topic1 PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 }}

{{Topic: topic1 Partition: 0 Leader: 0 Replicas: 0 Isr: 0}}

{{Topic: topic25 PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 }}

{{Topic: topic25 Partition: 0 Leader: 0 Replicas: 0 Isr: 0}};;;","30/Jun/20 13:36;ijuma;Does this affect 2.6 as well or just trunk?;;;","30/Jun/20 14:44;huxi_2b;[~ijuma] This should affect 2.6 as well since `configurationKeys` starts to be initialized in 2.7, due to the refinement introduced by [KAFKA-9432|https://issues.apache.org/jira/browse/KAFKA-9432]. Anyway, since `configurationKeys` is a nullable, an empty check should be added when processing the resources in AdminManager.;;;","30/Jun/20 15:08;ijuma;From the link you referenced, it seems like this only affects trunk, not 2.6.;;;","30/Jun/20 15:30;huxi_2b;Well, from the broker perspective, you are right. Only trunk is affected. What I mean is we'll also hit NPE when using 2.6 clients talking to trunk broker.;;;","01/Jul/20 07:12;omkreddy;looks like this got introduced in [https://github.com/apache/kafka/pull/8312]

cc [~tombentley];;;","01/Jul/20 07:44;tombentley;[~omkreddy] I'm happy to fix it, but Luke is already assigned. In any case, sorry folks ☹. I took another look at my PR and since {{configurationKeys}} is the only nullable field I don't think it's a systematic mistake.


One other thought: We could easily have the message generator add {{@Nullable}} annotations to nullable fields to enable static checking for this kind of error.;;;","01/Jul/20 08:22;showuon;[~tombentley], I'll submit a PR for your review later. Thanks.;;;","09/Jul/20 08:59;omkreddy;Issue resolved by pull request 8966
[https://github.com/apache/kafka/pull/8966];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DistributedHerder's canReadConfigs field is never reset to true,KAFKA-10218,13314164,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,29/Jun/20 22:18,29/Sep/20 00:08,13/Jul/23 09:17,29/Sep/20 00:08,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.2.0,2.2.1,2.2.2,2.2.3,2.3.0,2.3.1,2.3.2,2.4.0,2.4.1,2.4.2,2.5.0,2.5.1,2.5.2,2.6.0,2.6.1,2.7.0,2.4.2,2.5.2,2.6.1,2.7.0,,,,,KafkaConnect,,,,,0,,,,,"If the {{DistributedHerder}} encounters issues reading to the end of the config topic, it [takes note of this fact|https://github.com/apache/kafka/blob/7db52a46b00eed652e791dd4eae809d590626a1f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1109] by setting a field {{canReadConfigs}} to {{false}} and then acts accordingly at the [start of its tick loop|https://github.com/apache/kafka/blob/7db52a46b00eed652e791dd4eae809d590626a1f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L319] by trying again to read to the end of the config topic. However, if a subsequent attempt to read to the end of the config topic succeeds, the {{canReadConfigs}} field is never set back to {{true}} again, so no matter what, the herder will always attempt to read to the end of the config topic at the beginning of each tick.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 00:08:30 UTC 2020,,,,,,,,,,"0|z0gbds:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"29/Jun/20 23:25;ChrisEgerton;This goes back beyond 1.0.0. I've added a handful of versions to {{Affects Version/s}}, if anyone wants to add more feel free.;;;","06/Jul/20 15:42;rhauch;Thanks, [~ChrisEgerton]. You didn't really describe the impact of this bug, but IIUC the herder still behaves correctly with this bug but does extra work with every herder tick. Specifically, the config log's `producer.flush()` will have work to do only if there were recent new, removed, or changed connector configs or state change requests for that herder. And although the config log's consumer does have to fetch end offsets with every tick, only when the producer will have flushed records might the consumer have to block while it consumes records. (I say ""might"" because it's possible, though less likely, the consumer had just finished consuming those records and does not have to block.)

IOW, most of the time the only impact of this bug is that the Connect worker's herder unnecessarily fetches the config topic's end offsets on every tick. (Normally, the herder's config log already reads to the end any time anything of interest happens anyway.) But in some cases a lossy network or Kafka cluster transient outage coupled with the short default timeout (3 seconds) could cause the worker to leave the group, when it might not normally need to. 

Is my analysis correct?

(This isn't to suggest we not fix this; I'm just trying to understand the impact of the bug and to properly prioritize the fix.);;;","08/Jul/20 19:40;ChrisEgerton;[~rhauch] The primary impact here is that there are erroneous and confusing log messages that start to get emitted as soon as the worker enters a bad state and there is no fix except to restart the worker. The fix and the test are fairly simple and I think warrant review before the upcoming 2.7 release so that we can include them in upcoming bug fix releases before then.;;;","29/Sep/20 00:08;rhauch;Merged into the `trunk` branch for inclusion in the upcoming 2.7.0, and cherry-picked to the `2.6.x`, `2.5.x`, and `2.4.x` branches for inclusion in the next 2.6.1, 2.5.2, and 2.4.2 patch releases if/when they happen.

Even though the bug was present in much earlier versions, the project's policy is to backport only critical fixes earlier than the most recent 2-3 releases. This issue doesn't appear to fit that threshold.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockProcessorContext doesn't work with SessionStores,KAFKA-10215,13314082,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,29/Jun/20 15:25,13/Oct/20 16:51,13/Jul/23 09:17,13/Oct/20 16:51,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"The recommended pattern for testing custom Processor implementations is to use the test-utils MockProcessorContext. If a Processor implementation needs a store, the store also has to be initialized with the same context. However, the existing (in-memory and persistent) Session store implementations perform internal casts that result in class cast exceptions if you attempt to initialize them with the MockProcessorContext.

A workaround is to instead embed the processor in an application and use the TopologyTestDriver instead.

The fix is the same as for KAFKA-10200",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10200,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 16:51:09 UTC 2020,,,,,,,,,,"0|z0gavk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/20 03:04;vvcephei;Note, this will be fixed for the new PAPI MockProcessorContext, at least, by [https://github.com/apache/kafka/pull/9396] .;;;","13/Oct/20 16:51;vvcephei;Fixed in the new processor API.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix flaky zookeeper_tls_test.py,KAFKA-10214,13314067,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,29/Jun/20 14:14,01/Jul/20 11:39,13/Jul/23 09:17,01/Jul/20 11:39,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"After https://github.com/apache/kafka/commit/3661f981fff2653aaf1d5ee0b6dde3410b5498db, security_config is cached. Hence, the later changes to security flag can't impact the security_config used by later tests.",,chia7712,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 11:39:56 UTC 2020,,,,,,,,,,"0|z0gas8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/20 11:39;omkreddy;Issue resolved by pull request 8949
[https://github.com/apache/kafka/pull/8949];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describing a topic with the TopicCommand fails if unauthorised to use ListPartitionReassignments API,KAFKA-10212,13314055,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dajac,dajac,dajac,29/Jun/20 13:10,30/Jun/20 01:21,13/Jul/23 09:17,30/Jun/20 01:21,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,,,,,,1,,,,,"Since https://issues.apache.org/jira/browse/KAFKA-8834, describing topics with the TopicCommand requires privileges to use ListPartitionReassignments or fails to describe the topics with the following error:

{quote}
Error while executing topic command : Cluster authorization failed. 
{quote}

This is a quite hard restriction has most of the secure clusters do not authorize non admin members to access ListPartitionReassignments.",,dajac,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-29 13:10:34.0,,,,,,,,,,"0|z0gapk:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix connect_rest_test.py after the introduction of new connector configs,KAFKA-10209,13313923,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,29/Jun/20 05:47,04/Jul/20 00:47,13/Jul/23 09:17,03/Jul/20 17:41,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"There are two new configs introduced by [371f14c3c12d2e341ac96bd52393b43a10acfa84|https://github.com/apache/kafka/commit/371f14c3c12d2e341ac96bd52393b43a10acfa84] and [1c4eb1a5757df611735cfac9b709e0d80d0da4b3|https://github.com/apache/kafka/commit/1c4eb1a5757df611735cfac9b709e0d80d0da4b3] so we have to update the expected configs in connect_rest_test.py",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-29 05:47:57.0,,,,,,,,,,"0|z0g9w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockProcessorContext doesn't work with WindowStores,KAFKA-10200,13313486,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,25/Jun/20 17:17,10/Oct/20 03:01,13/Jul/23 09:17,10/Oct/20 03:01,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,streams-test-utils,,,,0,,,,,"The recommended pattern for testing custom Processor implementations is to use the test-utils MockProcessorContext. If a Processor implementation needs a store, the store also has to be initialized with the same context. However, the existing (in-memory and persistent) Windowed store implementations perform internal casts that result in class cast exceptions if you attempt to initialize them with the MockProcessorContext.

A workaround is to instead embed the processor in an application and use the TopologyTestDriver instead.",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8630,,,,,,,,,,KAFKA-10215,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 02:33:03 UTC 2020,,,,,,,,,,"0|z0g77c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/20 17:18;vvcephei;Submitted a fix: https://github.com/apache/kafka/pull/8927;;;","29/Jun/20 15:27;vvcephei;Created and linked https://issues.apache.org/jira/browse/KAFKA-10215 for SessionStores;;;","09/Oct/20 20:20;ableegoldman;[~vvcephei] can we resolve this ticket or was there some followup work/improvement that isn't yet merged?;;;","10/Oct/20 02:33;vvcephei;Thanks, Sophie, yes, this is done. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dirty tasks may be recycled instead of closed,KAFKA-10198,13313343,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,24/Jun/20 22:38,25/Jun/20 03:53,13/Jul/23 09:17,25/Jun/20 03:41,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"We recently added a guard to `Task#closeClean` to make sure we don't accidentally clean-close a dirty task, but we forgot to also add this check to `Task#closeAndRecycleState`. This meant an otherwise dirty task could be closed clean and recycled into a new task when it should have just been closed.

This manifest as an NPE in our test application. Specifically, task 1_0 was active on StreamThread-2 but reassigned as a standby. During handleRevocation we hit a TaskMigratedException while flushing the tasks and bailed on trying to flush and commit the remainder. This left task 1_0 with dirty keys in the suppression buffer and the `commitNeeded` flag still set to true.

During handleAssignment, we should have closed all the tasks with pending state as dirty (ie any task with commitNeeded = true). Since we don't know about the TaskMigratedException we hit during handleRevocation, we rely on the guard in Task#closeClean` to throw an exception and force the task to be closed dirty.

Unfortunately, we left this guard out of `closeAndRecycleState`, which meant task 1_0 was able to slip through without being closed dirty. Once reinitialized as a standby task, we eventually tried to commit it. The suppression buffer of course tried to flush its remaining dirty keys from its previous life as an active task. But since it's now a standby task, it should not be sending anything to the changelog and has a null RecordCollector. We tried to access it, and hit the NPE.

 

The fix is simple, we just need to add the guard in closeClean to closeAndRecycleState as well",,ableegoldman,mjsax,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 03:41:58 UTC 2020,,,,,,,,,,"0|z0g6bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 23:07;rhauch;Thanks, [~ableegoldman]. I agree this should be fixed in 2.6, so merge whenever the PR is ready and cherry-pick to the `2.6` branch. (I'm the 2.6 release manager.);;;","25/Jun/20 03:41;ableegoldman;[~rhauch] the fix has been merged and picked to the 2.6 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add preemption for controller events that have callbacks,KAFKA-10193,13312916,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,22/Jun/20 22:18,07/Aug/20 17:19,13/Jul/23 09:17,07/Aug/20 17:19,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,controller,metrics,,,,0,controller,,,,"There are classes that extends `ControllerEvent` and have callbacks but do not provide preemption:
 1. [ApiPartitionReassignment|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala#L2073-L2074]

2. [ListPartitionReassignments|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala#L2098-L2099]

Currently, the KafkaApi requests timeout for these events when the state of the controller changes. Instead we should preempt these events so that the requests are responded with a `NOT_CONTROLLER` error as we do for `ControlledShutdown` and `ReplicaLeaderElection` events. We can provide preemption [here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala#L1862-L1870].  ",,jeffkbkim,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 17:19:42 UTC 2020,,,,,,,,,,"0|z0g3ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/20 02:45;jeffkbkim;PR: [https://github.com/apache/kafka/pull/9050];;;","07/Aug/20 17:19;omkreddy;Issue resolved by pull request 9050
[https://github.com/apache/kafka/pull/9050];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test BlockingConnectorTest#testBlockInConnectorStop,KAFKA-10192,13312912,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,bchen225242,bchen225242,22/Jun/20 21:29,09/Mar/21 18:35,13/Jul/23 09:17,09/Mar/21 18:35,,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.1,2.8.0,3.0.0,,,,KafkaConnect,,,,,0,,,,,"h3. [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1218/]
h3. Error Message

org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not execute PUT request. Error response: \{""error_code"":500,""message"":""Request timed out""}
h3. Stacktrace

org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not execute PUT request. Error response: \{""error_code"":500,""message"":""Request timed out""} at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.putConnectorConfig(EmbeddedConnectCluster.java:346) at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.configureConnector(EmbeddedConnectCluster.java:300) at org.apache.kafka.connect.integration.BlockingConnectorTest.createConnectorWithBlock(BlockingConnectorTest.java:185) at org.apache.kafka.connect.integration.BlockingConnectorTest.testBlockInConnectorStop(BlockingConnectorTest.java:140) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:830)",,bchen225242,ChrisEgerton,mjsax,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 09 15:06:42 UTC 2021,,,,,,,,,,"0|z0g3o0:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"23/Jun/20 15:36;bchen225242;Failed again: [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1225/testReport/junit/org.apache.kafka.connect.integration/BlockingConnectorTest/testBlockInConnectorStop/];;;","23/Jun/20 16:36;ChrisEgerton;[~bchen225242] I have a hypothesis for why these tests are failing but I cannot reproduce locally and the Jenkins logs I've been able to find are incomplete (based on lines like ""{{...[truncated 2495514 chars]...""}}). Is there a way to get complete logs for this test from Jenkins?;;;","23/Jun/20 17:22;ChrisEgerton;I think what might be happening is that the worker is taking too long to initialize and the first request [to create a connector|https://github.com/apache/kafka/blob/14137def712a43b5bbbf02b067f2b1d4a12926b6/connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java#L140] is timing out. This isn't a problem in other integration tests because the REST request timeout for Connect is 90 seconds, but that timeout is [artificially reduced|https://github.com/apache/kafka/blob/14137def712a43b5bbbf02b067f2b1d4a12926b6/connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java#L66-L67] for these tests since some requests are supposed to time out and minute-and-a-half-long integration tests would be painful to deal with.

This is impossible to verify without either a reproduction or complete logs from the test run, though, and I can't reproduce locally.;;;","25/Jun/20 17:25;ChrisEgerton;Attempted a fix with [https://github.com/apache/kafka/pull/8928]. If I can get some assistance from anyone with access to Jenkins to try to verify the effectiveness of that fix, that'd be great.;;;","12/Feb/21 05:44;showuon;{code:java}
Stacktraceorg.opentest4j.AssertionFailedError: Condition not met within timeout 30000. Worker did not complete startup in time ==> expected: <true> but was: <false>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40)
	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:193)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$3(TestUtils.java:303)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:351)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:319)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:300)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:290)
	at org.apache.kafka.connect.integration.BlockingConnectorTest.setup(BlockingConnectorTest.java:133){code};;;","12/Feb/21 05:45;showuon;Error happened in tests in BlockingConnectorTests.

ex: [https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/475/testReport/junit/org.apache.kafka.connect.integration/BlockingConnectorTest/testBlockInConnectorStop/]

 

[https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/475/testReport/junit/org.apache.kafka.connect.integration/BlockingConnectorTest/testBlockInConnectorConfig/]

\;;;","19/Feb/21 00:01;mjsax;Failed in setup: [https://github.com/apache/kafka/pull/10134/checks?check_run_id=1915517987] 
{quote} {{org.opentest4j.AssertionFailedError: Condition not met within timeout 30000. Worker did not complete startup in time ==> expected: <true> but was: <false>
	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40)
	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:193)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$3(TestUtils.java:303)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:351)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:319)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:300)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:290)
	at org.apache.kafka.connect.integration.BlockingConnectorTest.setup(BlockingConnectorTest.java:133)}}
{quote};;;","09/Mar/21 15:06;ChrisEgerton;[~mjsax] FYI, there's a PR up for this from [~showuon] if you have time for a quick one-liner: [https://github.com/apache/kafka/pull/10118]

 

I think the change is described and justified in the PR well-enough that you might be able to review, even if you're not very familiar with Connect.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reset metric EventQueueTimeMs ,KAFKA-10189,13312562,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jeffkbkim,jeffkbkim,jeffkbkim,19/Jun/20 22:38,21/Jul/20 00:56,13/Jul/23 09:17,21/Jul/20 00:56,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,controller,core,metrics,,,0,,,,,"The metric [EventQueueTimeMs|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerEventManager.scala#L81] does not reset and therefore misrepresents the controller event queue time in these two scenarios:
 1. upon losing leader election - `EventQueueTimeMs` portrays the last event queue time of the previous controller and not the current controller
 2. no controller events are added to the queue - `EventQueueTimeMs` portrays the most recent event queue time, not the current queue time (which is 0)

For both cases, we should reset the controller event queue time to 0.

Implementation:

Instead of using `LinkedBlockingQueue.take()` [here|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/ControllerEventManager.scala#L118], we can use `LinkedBlockingQueue.poll(long timeout, TimeUnit unit)` and reset `EventQueueTimeMs` if the queue is empty.",,jeffkbkim,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 00:56:27 UTC 2020,,,,,,,,,,"0|z0g1j4:",9223372036854775807,,jsancio,,,,,,,,,,,,,,,,,,"27/Jun/20 00:49;jeffkbkim;[https://github.com/apache/kafka/pull/8935];;;","21/Jul/20 00:56;junrao;merged to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink task preCommit method gets called after task is stopped,KAFKA-10188,13312536,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,19/Jun/20 18:33,06/Oct/20 19:02,13/Jul/23 09:17,06/Oct/20 19:02,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,KafkaConnect,,,,,0,,,,,"When the [final cleanup for a sink task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java#L191] is initiated, the framework [first calls stop() on the task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L167], and then [closes the consumer for the task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L171]. Closing the consumer has the side effect of triggering [the onPartitionsRevoked method|https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html#onPartitionsRevoked-java.util.Collection-] of its {{ConsumerRebalanceListener}}, which in turn [causes the framework to call WorkerSinkTask::closePartitions|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L694], which in turn [calls WorkerSinkTask::commitOffsets|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L618], and finally, in turn [calls SinkTask:preCommit|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L386].

Calling {{SinkTask:preCommit}} after {{SinkTask::stop}} is likely to cause errors with tasks as they should be performing resource cleanup during {{stop}}, and the [current documentation on the SinkTask lifecycle|https://kafka.apache.org/25/javadoc/org/apache/kafka/connect/sink/SinkTask.html] makes no mention of anything happening after tasks are stopped.

 

The framework already [ensures that offsets are committed|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L196] for tasks after the last call to {{SinkTask:put}} has been made, so the offset commit after {{SinkTask::stop}} has already been invoked can and should be removed with no compromise of existing delivery guarantees provided by the framework.",,ChrisEgerton,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 19:02:29 UTC 2020,,,,,,,,,,"0|z0g1dc:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"06/Oct/20 19:02;rhauch;Merged to the `trunk` branch (for inclusion in the upcoming 2.7.0), and cherry-picked to the `2.6` branch (cleanly) and `2.5` branch (with merge conflicts).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TLSv1.3 system tests should not run under Java 8,KAFKA-10180,13312004,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nizhikov,rndgstn,rndgstn,17/Jun/20 16:50,28/Jun/20 15:50,13/Jul/23 09:17,27/Jun/20 16:29,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,system tests,,,,,0,,,,,"18 system tests relates to TLSv1.3 are running and failing under Java 8.  These system tests should not run except when Java 11 or later is in use.

http://testing.confluent.io/confluent-kafka-system-test-results/?prefix=2020-06-16--001.1592310680--confluentinc--master--d07ee594d/

(e.g. http://testing.confluent.io/confluent-kafka-system-test-results/?prefix=2020-06-16--001.1592310680--confluentinc--master--d07ee594d/Benchmark/test_end_to_end_latency/interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy/)
",,chia7712,ijuma,junrao,nizhikov,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 27 16:29:08 UTC 2020,,,,,,,,,,"0|z0fy2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/20 23:23;ijuma;[~nizhikov] Can you please look into this? It looks related to the changes to enable TLSv1.3 in system tests.;;;","25/Jun/20 07:32;nizhikov;{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.8
session_id:       2020-06-23--020
run time:         87 minutes 37.040 seconds
tests run:        62
passed:           62
failed:           0
ignored:          0
====================================================================================================
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.num_producers=3.acks=1
status:     PASS
run time:   1 minute 46.188 seconds
{""records_per_sec"": 371040.75211400003, ""mb_per_sec"": 35.39}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   3 minutes 2.029 seconds
{""records_per_sec"": 0.0, ""mb_per_sec"": 0.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 19.078 seconds
{""records_per_sec"": 2283105.0228, ""mb_per_sec"": 217.7339}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   3 minutes 9.450 seconds
{""records_per_sec"": 160402.9322, ""mb_per_sec"": 15.2972}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 20.393 seconds
{""records_per_sec"": 2320185.6148, ""mb_per_sec"": 221.2701}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   2 minutes 41.115 seconds
{""records_per_sec"": 0.0, ""mb_per_sec"": 0.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   1 minute 16.667 seconds
{""records_per_sec"": 2328288.7078, ""mb_per_sec"": 222.0429}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=none
status:     PASS
run time:   1 minute 1.228 seconds
{""latency_99th_ms"": 9.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 21.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_PLAINTEXT.compression_type=snappy
status:     PASS
run time:   1 minute 1.096 seconds
{""latency_99th_ms"": 9.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 27.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=none
status:     PASS
run time:   1 minute 9.127 seconds
{""latency_99th_ms"": 9.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 28.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=SASL_SSL.compression_type=snappy
status:     PASS
run time:   1 minute 11.056 seconds
{""latency_99th_ms"": 9.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 31.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   2 minutes 23.215 seconds
{""consumer"": {""records_per_sec"": 264718.3397, ""mb_per_sec"": 25.2455}, ""producer"": {""records_per_sec"": 267122.555829, ""mb_per_sec"": 25.47}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 15.770 seconds
{""consumer"": {""records_per_sec"": 648592.5542, ""mb_per_sec"": 61.8546}, ""producer"": {""records_per_sec"": 630437.523641, ""mb_per_sec"": 60.12}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   3 minutes 11.870 seconds
{""consumer"": {""records_per_sec"": 197180.3214, ""mb_per_sec"": 18.8046}, ""producer"": {""records_per_sec"": 231229.911901, ""mb_per_sec"": 22.05}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 16.796 seconds
{""consumer"": {""records_per_sec"": 683060.1093, ""mb_per_sec"": 65.1417}, ""producer"": {""records_per_sec"": 650068.257167, ""mb_per_sec"": 62.0}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   2 minutes 27.676 seconds
{""consumer"": {""records_per_sec"": 317359.5684, ""mb_per_sec"": 30.2658}, ""producer"": {""records_per_sec"": 310337.336685, ""mb_per_sec"": 29.6}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   1 minute 14.612 seconds
{""consumer"": {""records_per_sec"": 621967.9065, ""mb_per_sec"": 59.3155}, ""producer"": {""records_per_sec"": 618238.021638, ""mb_per_sec"": 58.96}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 14.825 seconds
{""latency_99th_ms"": 6.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 19.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 13.338 seconds
{""latency_99th_ms"": 7.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 16.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 11.572 seconds
{""latency_99th_ms"": 6.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 16.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 12.718 seconds
{""latency_99th_ms"": 7.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 14.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   1 minute 5.121 seconds
{""latency_99th_ms"": 6.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 18.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_end_to_end_latency.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   1 minute 6.154 seconds
{""latency_99th_ms"": 6.0, ""latency_50th_ms"": 1.0, ""latency_999th_ms"": 17.0}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   2 minutes 45.360 seconds
{""0"": {""records_per_sec"": 241534.2254, ""mb_per_sec"": 23.03}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 14.977 seconds
{""0"": {""records_per_sec"": 837310.558486, ""mb_per_sec"": 79.85}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   2 minutes 44.524 seconds
{""0"": {""records_per_sec"": 289922.300823, ""mb_per_sec"": 27.65}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 13.278 seconds
{""0"": {""records_per_sec"": 771426.367353, ""mb_per_sec"": 73.57}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   2 minutes 16.288 seconds
{""0"": {""records_per_sec"": 399968.00256, ""mb_per_sec"": 38.14}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_long_term_producer_throughput.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   1 minute 7.659 seconds
{""0"": {""records_per_sec"": 762718.328121, ""mb_per_sec"": 72.74}}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 29.595 seconds
{""records_per_sec"": 1010067.128236, ""mb_per_sec"": 9.63}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 18.901 seconds
{""records_per_sec"": 984506.124844, ""mb_per_sec"": 9.39}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 15.225 seconds
{""records_per_sec"": 195225.745455, ""mb_per_sec"": 18.62}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   58.411 seconds
{""records_per_sec"": 472098.839254, ""mb_per_sec"": 45.02}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 14.473 seconds
{""records_per_sec"": 23085.139319, ""mb_per_sec"": 22.02}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 1.329 seconds
{""records_per_sec"": 38194.934548, ""mb_per_sec"": 36.43}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 12.420 seconds
{""records_per_sec"": 2477.113326, ""mb_per_sec"": 23.62}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 13.590 seconds
{""records_per_sec"": 2360.773967, ""mb_per_sec"": 22.51}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 12.806 seconds
{""records_per_sec"": 455.069515, ""mb_per_sec"": 43.4}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.2.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 10.757 seconds
{""records_per_sec"": 448.229793, ""mb_per_sec"": 42.75}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 26.116 seconds
{""records_per_sec"": 857730.828221, ""mb_per_sec"": 8.18}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 12.146 seconds
{""records_per_sec"": 958424.164524, ""mb_per_sec"": 9.14}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 16.114 seconds
{""records_per_sec"": 187795.858402, ""mb_per_sec"": 17.91}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 4.061 seconds
{""records_per_sec"": 374073.857302, ""mb_per_sec"": 35.67}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 15.083 seconds
{""records_per_sec"": 21895.106036, ""mb_per_sec"": 20.88}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=1000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 0.812 seconds
{""records_per_sec"": 33058.374384, ""mb_per_sec"": 31.53}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 13.124 seconds
{""records_per_sec"": 2416.021602, ""mb_per_sec"": 23.04}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=10000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 15.004 seconds
{""records_per_sec"": 2333.275382, ""mb_per_sec"": 22.25}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=none
status:     PASS
run time:   1 minute 15.194 seconds
{""records_per_sec"": 450.486741, ""mb_per_sec"": 42.96}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.tls_version=TLSv1.3.message_size=100000.topic=topic-replication-factor-three.security_protocol=SSL.acks=1.compression_type=snappy
status:     PASS
run time:   1 minute 6.961 seconds
{""records_per_sec"": 447.03531, ""mb_per_sec"": 42.63}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-one.acks=1
status:     PASS
run time:   58.297 seconds
{""records_per_sec"": 406966.949666, ""mb_per_sec"": 38.81}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=-1
status:     PASS
run time:   1 minute 8.630 seconds
{""records_per_sec"": 141415.762301, ""mb_per_sec"": 13.49}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.acks=1
status:     PASS
run time:   1 minute 6.691 seconds
{""records_per_sec"": 244343.164027, ""mb_per_sec"": 23.3}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10
status:     PASS
run time:   1 minute 20.735 seconds
{""records_per_sec"": 1028094.369973, ""mb_per_sec"": 9.8}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100
status:     PASS
run time:   1 minute 4.227 seconds
{""records_per_sec"": 263326.858937, ""mb_per_sec"": 25.11}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=1000
status:     PASS
run time:   1 minute 9.448 seconds
{""records_per_sec"": 30229.054054, ""mb_per_sec"": 28.83}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=10000
status:     PASS
run time:   1 minute 9.055 seconds
{""records_per_sec"": 3347.717635, ""mb_per_sec"": 31.93}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=none.acks=1.message_size=100000
status:     PASS
run time:   1 minute 5.133 seconds
{""records_per_sec"": 982.430454, ""mb_per_sec"": 93.69}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10
status:     PASS
run time:   1 minute 8.243 seconds
{""records_per_sec"": 957398.673229, ""mb_per_sec"": 9.13}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100
status:     PASS
run time:   53.956 seconds
{""records_per_sec"": 528416.141732, ""mb_per_sec"": 50.39}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=1000
status:     PASS
run time:   53.178 seconds
{""records_per_sec"": 59440.655447, ""mb_per_sec"": 56.69}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=10000
status:     PASS
run time:   1 minute 8.507 seconds
{""records_per_sec"": 3325.322101, ""mb_per_sec"": 31.71}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.topic=topic-replication-factor-three.security_protocol=PLAINTEXT.compression_type=snappy.acks=1.message_size=100000
status:     PASS
run time:   1 minute 3.395 seconds
{""records_per_sec"": 907.369844, ""mb_per_sec"": 86.53}
----------------------------------------------------------------------------------------------------
{noformat};;;","27/Jun/20 16:29;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Store Passes Wrong Changelog Topic to Serde for Optimized Source Tables,KAFKA-10179,13311987,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,cadonna,cadonna,17/Jun/20 15:36,09/Mar/21 09:14,13/Jul/23 09:17,13/Jul/20 15:54,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,streams,,,,,0,,,,,"{{MeteredKeyValueStore}} passes the name of the changelog topic of the state store to the state store serdes. Currently, it always passes {{<application ID>-<store name>-changelog}} as the changelog topic name. However, for optimized source tables the changelog topic is the source topic. 
Most serdes do not use the topic name passed to them. However, if the serdes actually use the topic name for (de)serialization, e.g., when Kafka Streams is used with Confluent's Schema Registry, a {{org.apache.kafka.common.errors.SerializationException}} is thrown.",,ableegoldman,agavra,bulbfreeman,cadonna,desai.p.rohan,guozhang,mjsax,rayokota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10252,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 17:37:16 UTC 2020,,,,,,,,,,"0|z0fxz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/20 19:05;desai.p.rohan;I'm not sure it's correct to use the same ""topic"" name for materializing optimized source tables, as it's logically different data. In the normal flow (not recovery), we're taking the topic data, validating/transforming it by deserializing it (which might apply some transforms like projecting just fields of interest), and then serializing it, and then writing it into the store. So the ""topic"" we pass to the serializer should be different since it represents different data from the source topic.

This has consequences in practice when used with a schema registry using the confluent serializers. If we use the same topic, `serialize` might register a different schema with the source subject, which we probably don't want.

I think the technically correct thing to do (though this is of course more expensive) would be (when the source table is optimized) to deserialize and serialize each record when restoring.

Another issue that I think exists (need to try to reproduce) that deserializing/serializing would solve is skipped validation. The source topic deserializer functions as a sort of validator for records from the source topic. When the streams app is configured to skip on deserialization errors, bad source records are just skipped. However if we restore by just writing those records to the state store, we now hit the deserialization error when reading the state store, which is a query-killing error.

 ;;;","19/Jun/20 00:42;mjsax;{quote}I'm not sure it's correct to use the same ""topic"" name for materializing optimized source tables, as it's logically different data. In the normal flow (not recovery), we're taking the topic data, validating/transforming it by deserializing it (which might apply some transforms like projecting just fields of interest), and then serializing it, and then writing it into the store. So the ""topic"" we pass to the serializer should be different since it represents different data from the source topic.

For this case, the soure-topic-changelog optimization does no apply, and the store would always have its own changelog topic. And thus, the input-topic schema registered in the SR should not be ""touched"", and the write to the changelog topic should register a new scheme using the changelog topic name. Thus, no naming issue in SR should happen.
{quote}
The source-topic-changelog optimization really only applies, if the data in the input topic is exactly the same as in the changelog topic and thus, we avoid creating the changelog topic. To ensure this, we don't allow any processing to happen in between. The data would be deserialized and re-serialized using the same Serde (this is inefficiency we pay, as we also need to send the de-serialized data downstream for further processing).
{quote}Another issue that I think exists (need to try to reproduce) that deserializing/serializing would solve is skipped validation. The source topic deserializer functions as a sort of validator for records from the source topic. When the streams app is configured to skip on deserialization errors, bad source records are just skipped. However if we restore by just writing those records to the state store, we now hit the deserialization error when reading the state store, which is a query-killing error.
{quote}
This is a known issue and tracked via: https://issues.apache.org/jira/browse/KAFKA-8037;;;","19/Jun/20 03:18;desai.p.rohan;Deserialization may itself be a transformation. For example, suppose I have source data with 10 fields, but only care about 3 of them for my stream processing app. It seems that it would be reasonable to provide a deserializer that just extracts those 3 fields. I suppose you could express this as a projection after creating the table, but that does preclude optimizations that use selective deserialization. And it may be much more expensive to do the materialization (since you're potentially materializing lots of data unnecessarily). I think there should be some way to achieve each of the following: 

 
 * optimized and the data in the store is exactly the same as the topic data . In this case (what's implemented today) the data can be restored by writing the source records into the store
 * optimized and the deserializer transforms the data somehow. In this case the data can be restored by deserializing/serializing each row from the source topic before writing it into the store. I don't think this is possible today.
 * not optimized (w/ which you could have a transforming deserializer and faster recovery, at the cost of extra data in kafka). I don't think this is possible today without turning all optimizations off.

 

> This is a known issue and tracked via: https://issues.apache.org/jira/browse/KAFKA-8037
  ack - thanks!;;;","19/Jun/20 03:30;desai.p.rohan;Also, it's not really clear from the documentation that `serialize(deserialize())` is assumed to be the identity function  for `ktable(..)`.;;;","19/Jun/20 04:37;mjsax;What you say is fair I guess. Given the current code, if you want to do any of those, you need to disable the optimization.

However, for the actual bug this ticket is about, the problem seems to be, that if the optimization is turned on, at some point in the code we pass the changelog topic name into the serde instead of the source topic name. And thus the schema cannot be found and the serde crashes. Thus, this ticket should focus on this bug.

Not sure if KAFKA-8037 covers all cases you describe. Maybe you want to follow up on this ticket (so we can extent its scope) or create a new ticket that describes the shortcomings of the current implementation.;;;","19/Jun/20 08:14;cadonna;[~desai.p.rohan] While I find the idea of optimizing the materialization in the deserializer intriguing, I think the performance penalty that we would pay by deserializing and serializing each record during restoration is not worthwhile. Additionally -- if optimization is turned on -- we would need to read the original data from the source topic instead of the projected data from the changelog topic during each restoration which would again hit performance. Of course, we would need experiments to better understand the implications. 

An alternative idea would be to allow to plugin a byte-based transformation that does not need to deserialize and serialize each record. However, that would not solve the issue of having to read the unprojected data during each restoration.
 
If you are concerned with the amount of data to materialize a solution could be to optimize on topology-level by introducing a {{map()}} that makes the projection followed by a {{toTable()}} to materialize the data. That data read from the input topic would be the unprojected data but the one materialized is the projected one and also during restoration we would just read the projected data. An additional advantage of this method is that you can leave the source table optimization turned on, because it would not apply to this case.

In summary, the source table optimization was not introduced for the case you describe. IMO, it is not even an optimization in that case. ;;;","23/Jun/20 19:21;ableegoldman;[~desai.p.rohan] I'm not sure I understand why it's a problem for the deserializer to modify the value slightly, by dropping fields to take your example. We would end up restoring the full bytes into the store, sure, but the plain bytes are never actually used right? We would always go through the deserializer when reading the value from the store and using it in an operation. So the ""extra"" fields would still get dropped.

Maybe if your values are bloated with a lot of useful information that you didn't want to store, this could blow up the disk usage. But I think there's a difference between a simple operation on data to extract only the relevant bits – eg dropping a field you don't care about – and fundamentally transforming the data to get it into a different form. The former seems reasonable to do during a deserialization, but the latter should be its own operation in the topology.

Of course, this just applies to modifying the values. If your deserializer modifies the key in any way, this would be a problem since lookups by key would fail after a restoration copies over the plain bytes. But I would argue that it's illegal to modify the key during de/serialization at all, not because of the restoration issue but because it can cause incorrect partitioning.

Anyways, I'm probably overlooking something obvious, but I'm struggling to see exactly where and how this breaks. That said I do agree we should clarify that `serialize(deserialize())` must be the identity for keys;;;","27/Jun/20 00:48;agavra;[~ableegoldman] I confirmed locally that nothing ""breaks"" if we use a deserializer that projects a subset of the fields in the record, as you suspected, but consider the following points:
 # Some of the most popular serdes are asymmetric (e.g. avro builds in the concept of reader/writer schema into their APIs)
 # It may be impossible to determine, for a given serde, whether it is symmetric
 # State after recovery should be identical to before recovery for predictable operations (especially in cloud environments)
 # Some of the most popular serdes have side effects (e.g. Confluent schema registry serdes will create subjects on your behalf)

In practice, the first three points in conjunction with what [~mjsax] said (the source-topic-changelog optimization really only applies, if the data in the input topic is exactly the same as in the changelog topic and thus, we avoid creating the changelog topic), means that we can't safely turn on the source-topic-changelog optimization unless the user indicates either (a) they are using a symmetrical serde or (b) they are willing to waive 3 in order to speed up recovery ([~cadonna] if we consider 3 a matter of correctness, we can't sacrifice correctness for performance without the user's consent).

Even if the user indicates (a) or (b) above, I still don't think we can implement the fix described here because of the fourth point. It may be possible that the user is using a symmetric serde but their schema is not identical to the one that wrote to the kafka topic (e.g. ksql, for example, generates a new schema where all the fields are the same but the schema has a different name, I can also easily imagine a schema with _more_ fields that would write the same value as it read from an event with fewer fields).

I'm not sure I understand this comment: ""The data would be deserialized and re-serialized using the same Serde (this is inefficiency we pay, as we also need to send the de-serialized data downstream for further processing)."" Why can't we just always pass-through the data into the state store if the optimization is enabled?

 ;;;","20/Jul/20 17:37;guozhang;I agree with Almog and Rohan’s arguments here. What I’m thinking is how we could define a principle for users to indicate that:

1) the bytes in the source topic are exactly the same as bytes in the state store (i.e. the serdes are symmetric).
2) there’s no side-effects that serde incurs; only 1) and 2) together means it is safe to skip serde during restoration.
3) and also, there’s no corrupted or ill-formatted data from source topics that should be skipped when loading into state stores. This is https://issues.apache.org/jira/browse/KAFKA-8037

During restoration time, compared with during normal processing time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferUnderflowException during Kafka Streams Upgrade,KAFKA-10173,13311732,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,karsten.schnitter,karsten.schnitter,16/Jun/20 13:58,12/Jan/22 19:03,13/Jul/23 09:17,06/Jul/20 19:09,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,streams,,,,,0,suppress,,,,"I migrated a Kafka Streams application from version 2.3.1 to 2.5.0. I followed the steps described in the upgrade guide and set the property {{migrate.from=2.3}}. On my dev system with just one running instance I got the following exception:

{noformat}
stream-thread [0-StreamThread-2] Encountered the following error during processing:
java.nio.BufferUnderflowException: null
	at java.base/java.nio.HeapByteBuffer.get(Unknown Source)
	at java.base/java.nio.ByteBuffer.get(Unknown Source)
	at org.apache.kafka.streams.state.internals.BufferValue.extractValue(BufferValue.java:94)
	at org.apache.kafka.streams.state.internals.BufferValue.deserialize(BufferValue.java:83)
	at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.restoreBatch(InMemoryTimeOrderedKeyValueBuffer.java:368)
	at org.apache.kafka.streams.processor.internals.CompositeRestoreListener.restoreBatch(CompositeRestoreListener.java:89)
	at org.apache.kafka.streams.processor.internals.StateRestorer.restore(StateRestorer.java:92)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.processNext(StoreChangelogReader.java:350)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:94)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:401)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:779)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670)
{noformat}

I figured out, that this problem only occurs for stores, where I use the suppress feature. If I rename the changelog topics during the migration, the problem will not occur. ",,ableegoldman,bchen225242,cadonna,karsten.schnitter,mjsax,pmeister,vvcephei,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10336,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 19:10:28 UTC 2020,,,,,,,,,,"0|z0fwew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 17:25;vvcephei;Thanks for the report [~karsten.schnitter] . I'm sorry for the trouble.

I've marked it as a blocker for the ongoing 2.5.1 release. I'll investigate the root cause ASAP.;;;","16/Jun/20 20:10;mjsax;Would be great to know if this might be a blocker for 2.6, too. \cc [~rhauch] (just as a potential heads-up).;;;","17/Jun/20 16:19;rhauch;[~mjsax], [~vvcephei]: I guess whether we should include this in 2.6 depends on the timing and the risk. We'll start cutting RCs pretty soon.;;;","19/Jun/20 22:23;vvcephei;Hi [~karsten.schnitter] ,

I've had a chance now to look into this. It's actually quite strange, although I'm probably missing something.

That stacktrace means that Streams is actually having trouble deserializing data written with changelog schema version 2, which IIRC was introduced in version 2.4.0. If you only ran the application with version 2.3.1, stopped it, then started it with version 2.5.0, it shouldn't be possible to hit this branch, as it means reading data that 2.3.1 couldn't possibly have written.

The only thing I can imagine is if you had previously run Streams with a version _less than_ 2.3.0 AND if some of your data happened to contain a header with the name ""v"", then it could lead to this kind of corruption. But, in that case, you should have gotten an exception while running with version 2.3.1 already. So, this doesn't sound like it fits.

The only thing that seems to fit your report is that the serialization/deserialization logic is actually wrong. But it's quite heavily tested, and has been exercised in production for quite a while also. There is always the possibility of a rare edge case, though. I've just given the logic another pass, and I'm not seeing what could be the problem. Is there any chance you could find the changelog record that caused this exception and just send me the bytes for it?

Short of that, I'm not sure how to proceed.

-John

P.s., while reviewing the code, I did find one flaw, which I've submitted [https://github.com/apache/kafka/pull/8905] for. I doubt that was your problem, but we might as well fix it anyway.;;;","19/Jun/20 22:29;vvcephei;[~rhauch] and [~mjsax] , you can consider my last comment for yourselves, but it doesn't seem like this would need to block 2.6.0. The code in question doesn't seem to have changed since 2.4.0.;;;","22/Jun/20 07:01;karsten.schnitter;Hi [~vvcephei],

The application has been running in different version for quite some time. Since it consumes the Kafka dependencies via Spring, I am very confident, that it never ran in any 2.4.x version. It started with earlier versions though. I try to get the offending message, but this is not an easy endeavour so far. I am not sure what topic to look at: Do I need to investigate the suppress topics or the original store changelog? Or can this stem from even other topics? How can I identify the message within the topic? And lastly once identified, how can I get the bytes? Soo far I am using the Kafka-console-consumer.sh for these kinds of investigations. But as far as I know this is not well suited to extract the bytes, right?;;;","22/Jun/20 12:20;karsten.schnitter;Hi [~vvcephei],

after trying for the whole weekend, I finally found a solution to provide the messages. My approach is to change {{org.apache.kafka.streams.state.internals.BufferValue}} in a way to log during deserialisation:

{code:java}
public final class BufferValue {
        // omitted

	private static final Logger LOG = LoggerFactory.getLogger(BufferValue.class);

	static BufferValue deserialize(final ByteBuffer buffer) {
		final ProcessorRecordContext context = ProcessorRecordContext.deserialize(buffer);

		LOG.debug(""Deserialize with context <{}>"", context);

		try {
			final byte[] priorValue = extractValue(buffer);

			final byte[] oldValue;
			final int oldValueLength = buffer.getInt();
			if (oldValueLength == NULL_VALUE_SENTINEL) {
				oldValue = null;
			} else if (oldValueLength == OLD_PREV_DUPLICATE_VALUE_SENTINEL) {
				oldValue = priorValue;
			} else {
				oldValue = new byte[oldValueLength];
				buffer.get(oldValue);
			}

			final byte[] newValue = extractValue(buffer);

			return new BufferValue(priorValue, oldValue, newValue, context);
		} catch (BufferUnderflowException underflow) {
			LOG.error(""Error deserializing buffer <{}>"", bytesToHex(buffer.array()));
			throw underflow;
		}
	}

	private static final char[] HEX_ARRAY = ""0123456789ABCDEF"".toCharArray();

	private static String bytesToHex(byte[] bytes) {
		char[] hexChars = new char[bytes.length * 2];
		for (int j = 0; j < bytes.length; j++) {
			int v = bytes[j] & 0xFF;
			hexChars[j * 2] = HEX_ARRAY[v >>> 4];
			hexChars[j * 2 + 1] = HEX_ARRAY[v & 0x0F];
		}
		return new String(hexChars);
	}

        // omitted
}
{code}

The result of this is somewhat confusing to me:

{noformat}
DEBUG.   Deserialize with context <ProcessorRecordContext{topic='logs-ingress', partition=57, offset=0, timestamp=1592648746703, headers=RecordHeaders(headers = [], isReadOnly = false)}>
ERROR.   Error deserializing buffer <00000172D14346CF00000000000000000000000C6C6F67732D696E6772657373000000390000000000000059FFFFFFFF00000051080110AC051A2436346139616437662D313838352D346234342D613163302D633134346565633162636665222436346139616437662D313838352D346234342D613163302D633134346565633162636665FFFFFFFF00000172D14346CF>
{noformat}

# The DEBUG log of the context tells me, that a message from my main topic was read. I would have expected a message from a store changelog. I guess the context is misleading here?
# The ERROR log has a rather short message, that fits much better to a store changelog. As a background: This is most likely a protobuf message containing two uuids encoded as strings together with some integer or long numbers.

The system I used was freshly created on Saturday, June 20th. It is running a Kafka Cluster on version 2.4.1 and the Kafka Streams application in version 2.3.1. The error was recorded with the same Kafka Streams application ugraded to v2.5.0 with the modification from above. Since there are two stream threads, I can provide another buffer that fails:

{noformat}
ERROR.   Error deserializing buffer <00000172D1434655000000000000006F0000000C6C6F67732D696E6772657373000000180000000000000059FFFFFFFF00000051080110AC051A2436346139616437662D313838352D346234342D613163302D633134346565633162636665222436346139616437662D313838352D346234342D613163302D633134346565633162636665FFFFFFFF00000172D1434655>
{noformat}

Best Regards,
Karsten;;;","22/Jun/20 22:36;vvcephei;Wow, thanks for that great information, [~karsten.schnitter] !

I was able to reproduce your exception with this code:
{code:java}
public final class BufferValue {
...

public static void main(String[] args) {
    final String str=""00000172D14346CF00000000000000000000000C6C6F67732D696E6772657373000000390000000000000059FFFFFFFF00000051080110AC051A2436346139616437662D313838352D346234342D613163302D633134346565633162636665222436346139616437662D313838352D346234342D613163302D633134346565633162636665FFFFFFFF00000172D14346CF"";
    final byte[] arr = hexStringToByteArray(str);

    final BufferValue deserialize = deserialize(ByteBuffer.wrap(arr));
    System.out.println(deserialize);
}

public static byte[] hexStringToByteArray(String s) {
    int len = s.length();
    byte[] data = new byte[len / 2];
    for (int i = 0; i < len; i += 2) {
        data[i / 2] = (byte) ((Character.digit(s.charAt(i), 16) << 4)
            + Character.digit(s.charAt(i+1), 16));
    }
    return data;
}

...
}
{code}
I also see the context deserialized as:
{code:java}
ProcessorRecordContext{topic='logs-ingress', partition=57, offset=0, timestamp=1592648746703, headers=RecordHeaders(headers = [], isReadOnly = false)}{code}
Note, this is not the record context of the suppression changelog, it's the context of the input record that got suppressed.

Incidentally, the context I get for that second record is:
{code:java}
ProcessorRecordContext{topic='logs-ingress', partition=24, offset=111, timestamp=1592648746581, headers=RecordHeaders(headers = [], isReadOnly = false)}{code}
One hypothesis I was hoping to verify was that we are in fact correctly attempting to deserialize a ""version 2"" record, not another record we're erroneously interpreting as ""version 2"". It looks like this is the case, otherwise it seems like we'd be unlikely to get a valid-looking ProcessorRecordContext. Still, maybe you can confirm that your topic ""logs-ingress"" really has a partition 57, and that the record at offset 0 has timestamp 1592648746703? And also, I guess, that it doesn't have any headers?

At least, that looks like a plausible timestamp:
{code:java}
jshell> new Date(1592648746703L)
$1 ==> Sat Jun 20 05:25:46 CDT 2020

{code}
Just looking at what is getting unpacked from this buffer (which ultimately results in that exception):
 # the context above
 # a ""prior value"" for this key (a byte array of length 89) of (hex): ""FFFFFFFF00000051080110AC051A2436346139616437662D313838352D346234342D613163302D633134346565633162636665222436346139616437662D313838352D346234342D613163302D633134346565633162636665""
 ** Surprisingly, the ""prior value"" for the other record you gave me is exactly the same! It comes from a different partition of logs-ingress, though, so I don't think it could be the same record.
 # a null ""old value""
 # When we come to extract the ""new value"" part, we read the length of the new value as 370, but at this point, we're at position 141 in the buffer, which is only 145 bytes long, so we get the underflow because we try to read 370 bytes, but there are only 4 bytes remaining.

Unfortunately, I still don't have any good answers at this point.

I think it's suspicious that we are reading a record that is apparently from offset zero of its topic partition, yet it has a ""prior value"".

[~karsten.schnitter], I'm sorry to put this back on you, but can you try to:
 # Verify that the context we got above looks legitimate? If not, it'll help narrow down where my serializer went wrong.
 # If you're able, perhaps you can take a look at the input records from the partition and offset that we see encoded here to see if it has the same key as the record we're trying to restore from the suppression changelog?
 # In addition to the information you've captured so far, can you also try to dump out the whole record we're restoring in `restoreBatch`? That one will let me look at the metadata for the changelog entry itself, not just the record we suppressed.
 # I'm really sorry to do this to you, but perhaps you can add some more debugging to the ByteBuffer#serialize path as well, so that we can see what actually went in when we wrote these records?

I'm still a little mystified about how we even wind up in this position... We're deserializing a version 2 changelog record, which must have been written by the _upgraded_ application, since version 2 was introduced in 2.4.0. But if all you're doing to repro this is run the app with 2.3.1 and then restarting with version 2.5.0, it shouldn't even be possible to get a version 2 record in the restore path.

I'll keep trying to figure it out, but any further assistance you can provide is greatly appreciated. Thanks (and sorry).

-John;;;","23/Jun/20 15:59;karsten.schnitter;Hi [~vvcephei],

the original message is lost due to retention. I will implement the additional logging and start from a clean slate once again. Today, the system was unavailable to me, but I hope to get to more testing tomorrow or the day after. If you have any further suggestions, were I can add logging statements to extract information, please let me know. So far I can verify, that the context looks legitimate. There are actually 60 partitions, so 57 is fine. Can the equal priors be explained by very similar data in both partitions? It is not unlikely, that both partitions may have been aggregated to very similar data.

Best Regards,
Karsten;;;","23/Jun/20 16:15;vvcephei;Hi [~karsten.schnitter] ,

Thanks for your efforts. I'm also adding some additional system tests specifically for suppression in combination with an upgrade from 2.3.1 to 2.5.0.

Thanks for the confirmation about the context. I think logging both the serialization and deserialization path should provide a lot of clarity for now. The stacktrace above implies the record was written by the 2.5.0 application, so it will be interesting to see if it really prints out your serialization log messages.

I'll also add some ""trace"" level log messages to suppression in AK to help in future debugging efforts.

If you're able, I guess this should be sufficient for now:
 * Log the who record during restore
 * Log the data in InMemoryTimeOrderedKeyValueBuffer#logValue, both the Key and BufferValue before serialization and the byte array that we return from BufferValue#serialize

In response to your last question, certainly! You'd know best what data this application is producing. If the value for two different records could be exactly the same, then the identical priorValue may be expected.

What doesn't seem expected to me is that there would be a priorValue at all for the record that was at offset zero of the input. It makes me wonder if the application state is corrupted somehow, but I can't wrap my head around _how_.

I'll let you know how my testing efforts progress today.

-John;;;","24/Jun/20 15:13;vvcephei;Hi [~karsten.schnitter] ,

I have ""good"" news! I managed to reproduce this exception in a system test:
{code:java}
java.nio.BufferUnderflowException
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:151)
	at java.nio.ByteBuffer.get(ByteBuffer.java:715)
	at org.apache.kafka.streams.state.internals.BufferValue.extractValue(BufferValue.java:94)
	at org.apache.kafka.streams.state.internals.BufferValue.deserialize(BufferValue.java:83)
	at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.restoreBatch(InMemoryTimeOrderedKeyValueBuffer.java:368)
	at org.apache.kafka.streams.processor.internals.CompositeRestoreListener.restoreBatch(CompositeRestoreListener.java:89)
	at org.apache.kafka.streams.processor.internals.StateRestorer.restore(StateRestorer.java:92)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.processNext(StoreChangelogReader.java:350)
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:94)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:401)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:779)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670){code}
I know it's a pain for you to debug on your end. Please feel free to continue if you like to, but I should be able to make progress by iterating on the test for now. I'll let you know what I learn.

Thanks so much for your help,

-John;;;","24/Jun/20 15:46;karsten.schnitter;Hi [~vvcephei],

That is great news. Actually, I had some problems with my setup and could not do further testing today.

Best Regards,
Karsten;;;","24/Jun/20 23:21;vvcephei;Ok, I'm horrified and embarrassed to report that I know what the problem is, as well as the solution. I'll update my PR tomorrow, and I'll re-escalate this ticket to a blocker for 2.5.1 and 2.6.0.

In a nutshell, although we have a version number on these changelog records so that we can deserialize old formats, I subtly changed the serialization format in 2.4.0 without updating the version number, so although the serialization format is incompatible between 2.3.1 and 2.5.0, they both claim to be at ""version 2"" (I was mistaken about this before).

This mistake happens to also render the serialization compatibility test I had written to be ineffectual.

And I've also just discovered that our system test that covers application upgrades had suffered an oversight that made it skip these versions.

Needless to say, in addition to fixing this bug, I'm fixing the serialization test to avoid using the same code paths as the application, and I'm also revamping the upgrade system test to be sure we'll have a much more robust test going forward.

Although this bug was introduced in 2.4.0, I'd still classify it as a blocker, since it's so severe (you simply can't upgrade the application until we fix it).

Thanks for the excellently detailed report, and, again, my sincere apologies,

-John;;;","25/Jun/20 01:17;bchen225242;cc [~rhauch];;;","25/Jun/20 09:06;karsten.schnitter;Hi [~vvcephei],

I am really glad to hear, that you found the problem and the solution. No need to feel embarrassed though. I really appreciate all the hard work, that goes into this project. Keep it up.

One more background information: I consume the Kafka Streams dependency transitively through Spring Boot. Spring skipped the 2.4.x releases and went directly from 2.3.1 to 2.5.0. So I would expect, that more people will have this kind of migration.

Best Regards,
Karsten
;;;","25/Jun/20 17:07;vvcephei;Thanks for that information, [~karsten.schnitter] . It really reinforces the importance of blocking the 2.5.1 bugfix release in particular.;;;","27/Jun/20 02:52;vvcephei;Ok, I've just merged the fix ([https://github.com/apache/kafka/pull/8905] ), and I'm backporting it now.

To control the scope of the change, I've separated the upgrade system tests out for a second PR, which I'll submit next week. I did run them locally, and they pass after #8905, so I have confidence the bug is really fixed.;;;","27/Jun/20 03:41;vvcephei;Ok, the fix is now backported all the way back to 2.4. I'll leave this ticket open until I get the system test PR in.;;;","27/Jun/20 18:30;vvcephei;Here's the system test PR: https://github.com/apache/kafka/pull/8938;;;","06/Jul/20 19:10;vvcephei;Hey all, I'm resolving this ticket to unblock the 2.6.0 release. I'm still working on backporting the system test fixes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaException: Failing batch since transaction was aborted,KAFKA-10169,13311603,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,16/Jun/20 02:24,24/Jun/20 17:28,13/Jul/23 09:17,24/Jun/20 17:28,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"We've seen the following exception in our eos-beta test application recently:
{code:java}
[2020-06-13T00:09:14-07:00] (streams-soak-2-6-all-fixes-eos-beta_soak_i-0ae30dd12c4fb7018_streamslog) org.apache.kafka.streams.errors.StreamsException: Error encountered sending record to topic stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000025-changelog for task 1_2 due to: [2020-06-13T00:09:14-07:00] (streams-soak-2-6-all-fixes-eos-beta_soak_i-0ae30dd12c4fb7018_streamslog) org.apache.kafka.common.KafkaException: Failing batch since transaction was aborted [2020-06-13T00:09:14-07:00] (streams-soak-2-6-all-fixes-eos-beta_soak_i-0ae30dd12c4fb7018_streamslog) Exception handler choose to FAIL the processing, no more records would be sent. at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError(RecordCollectorImpl.java:213) at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.lambda$send$0(RecordCollectorImpl.java:185) at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1347) at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:231) at org.apache.kafka.clients.producer.internals.ProducerBatch.abort(ProducerBatch.java:159) at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortUndrainedBatches(RecordAccumulator.java:781) at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:425) at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:313) at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240) at java.lang.Thread.run(Thread.java:748) [2020-06-13T00:09:14-07:00] (streams-soak-2-6-all-fixes-eos-beta_soak_i-0ae30dd12c4fb7018_streamslog) Caused by: org.apache.kafka.common.KafkaException: Failing batch since transaction was aborted at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:423) ... 3 more
{code}
Somewhat unclear if this is an issue with eos-beta specifically, or just eos in general. But several threads have died over the course of a few days in the eos-beta application, while none so far have died on the eos-alpha application.

It's also unclear (at least to me) whether this is definitely an issue in Streams or possibly a bug in the producer (or even the broker, although that seems unlikely)",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10186,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 03:03:22 UTC 2020,,,,,,,,,,"0|z0fvmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 03:03;ableegoldman;I haven't seen anything damning (or even particularly interesting) in the logs so far. The only consistent pattern was that it tended to occur after a #handleAssignment where the doomed task was both a previous and current active task (ie owned before and after the rebalance). But it might just be a coincidence, and/or completely unrelated. I'll keep a look out for new thread deaths;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams EOS-Beta should not try to get end-offsets as read-committed,KAFKA-10167,13311589,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,guozhang,guozhang,16/Jun/20 00:20,18/Jun/20 23:13,13/Jul/23 09:17,18/Jun/20 23:13,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"This is a bug discovered with the new EOS protocol (KIP-447), here's the context:

In Streams when we are assigned with the new active tasks, we would first try to restore the state from the changelog topic all the way to the log end offset, and then we can transit from the `restoring` to the `running` state to start processing the task.

Before KIP-447, the end-offset call is only triggered after we've passed the synchronization barrier at the txn-coordinator which would guarantee that the txn-marker has been sent and received (otherwise we would error with CONCURRENT_TRANSACTIONS and let the producer retry), and when the txn-marker is received, it also means that the marker has been fully replicated, which in turn guarantees that the data written before that marker has been fully replicated. As a result, when we send the list-offset with `read-committed` flag we are guaranteed that the returned offset == LSO == high-watermark.

After KIP-447 however, we do not fence on the txn-coordinator but on group-coordinator upon offset-fetch, and the group-coordinator would return the fetching offset right after it has received the replicated the txn-marker sent to it. However, since the txn-marker are sent to different brokers in parallel, and even within the same broker markers of different partitions are appended / replicated independently as well, so when the fetch-offset request returns it is NOT guaranteed that the LSO on other data partitions would have been advanced as well. And hence in that case the `endOffset` call may returned a smaller offset, causing data loss.",,ableegoldman,cadonna,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10148,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 00:42:42 UTC 2020,,,,,,,,,,"0|z0fvjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 00:40;guozhang;The proposed solution is that even under EOS, do not try to use consumer.endOffset that would set `read-committed` flag, but to just use list-offset with `read-uncommitted` to get the end-offset.

The rationale is that, since we know that this changelog-topic is a single-writer, single-reader, and we control all the writer / reader of it, we can safely assume that the on-going txn is only from our previous writer. 

If the task migration is due to a graceful rebalance (i.e. the task is indeed being revoked from the other host), then the old host would always commit in which it would block on `producer.flush` to make sure all data are written (although by default we do not override replication factor on changelog topics and producer's ack.mode, so if user change the one without the other they may bump into other issues where data are not replicated completely and hence high-watermark returned from list-offset can be smaller). And therefore the end-offset returned would return the actual log-end-offset with or without the txn-marker, either of which is fine.

If the task migration is due to an unexpected task migration (i.e. the task was not proactively revoked, the old host may not know it is out of the group or has been crashed), then although not all records sent from the old host are guaranteed to be on the broker and be covered with end-offset, it is fine since these records will be aborted eventually anyways.;;;","16/Jun/20 00:42;guozhang;I've also thought about whether we need to block on mainConsumer#committed before getting the end-offset, and now I think that is not necessary either since the end-offset only requires that data is flushed --- again, remember we have this single-writer single-reader scenario and when we are in the initialize-changelog-reader phase, we know that the other old producer would not be able to write any more unabortable data to that partition.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive TaskCorruptedException seen in testing,KAFKA-10166,13311541,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,15/Jun/20 18:36,09/Mar/21 09:20,13/Jul/23 09:17,07/Jul/20 00:19,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"As the title indicates, long-running test applications with injected network ""outages"" seem to hit TaskCorruptedException more than expected.

Seen occasionally on the ALOS application (~20 times in two days in one case, for example), and very frequently with EOS (many times per day)",,ableegoldman,cadonna,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 00:19:21 UTC 2020,,,,,,,,,,"0|z0fv94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 20:28;rhauch;[~ableegoldman], [~cadonna]: Do we want to continue treating this as a blocker for the 2.6.0 release? If so, what's the timeframe for fixing this?

If this should not block the release, should we downgrade the priority and/or change the fix versions to 2.6.1 and/or 2.7.0?;;;","25/Jun/20 00:41;ableegoldman;I think this should be a blocker, yes. It's a regression in 2.6 and causes Streams to unnecessarily rebuild state from the changelog which can mean a very long stall.

One root cause just occurred to me while looking at some related code, which I'll open a PR for right away. I'm not sure it's the _only_ root cause but I'll begin testing right away to see if it fixes the majority of the problem or not.

[~cadonna] do you want to split up this ticket? There are two kinds of TaskCorruptedException, both of which we see more than expected. It probably makes sense to look at these individually and in parallel. Can you look into the TaskCorruptedException thrown in StoreChangelogReader#restore? I'll investigate my theory for the exceptions thrown in ProcessorStateManager;;;","25/Jun/20 09:25;cadonna;[~ableegoldman] I agree that splitting is a good idea.;;;","26/Jun/20 11:37;cadonna;[~ableegoldman] [~rhauch]

Good news regarding the {{TaskCorruptedException}} thrown in {{StoreChangelogReader#restore()}}. This seems to occur due to deleted segments on the brokers. When a segment is deleted the start offset is increased to the new start offset of the new first segment. After that Streams tries to restore from the deleted segment and the {{TaskCorruptedException}} is thrown.

In the broker logs I see
{code:java}
[2020-06-25T12:20:51+02:00]  ... INFO [Log partition=stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-1, dir=/mnt/kafka/data] Incremented log start offset to 64642732 due to segment deletion (kafka.log.Log)
{code}
and in the Streams logs
{code:java}
[2020-06-25T13:12:47+02:00] ... org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Fetch position FetchPosition{offset=64292139, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[...:9092 (id: 1002 rack: null)], epoch=0}} is out of range for partition stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-1
{code}
Start offset after segment deletion is 64642732. 
 Start offset Streams wants to read from is 64292139.

The {{OffsetOutOfRangeException}} then triggers the {{TaskCorruptedException}}.

The segments are deleted because of the configured retention size. So everything works as intended.

Hence, this blocker only depends on the {{TaskCorruptedException}} thrown by the {{ProcessorStateManager}}.;;;","26/Jun/20 17:14;ableegoldman;Thanks for the detailed analysis Bruno. In that case we are just waiting on the one PR, which should be mergeable sometime today;;;","30/Jun/20 22:06;ableegoldman;Found two edge cases we missed earlier so I'm reopening this blocker; the fixes are minor so I'll have a PR ready shortly cc [~rhauch];;;","06/Jul/20 17:32;rhauch;[~ableegoldman] do you have an updated ETA?;;;","06/Jul/20 17:43;ableegoldman;[~rhauch] I think we can get it merged sometime today;;;","07/Jul/20 00:19;guozhang;I've merged in [~ableegoldman]'s PR, and [~cadonna] has also confirmed the other corruption exception are actually legitimate, so I'm going to close this ticket now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Percentiles metric leaking memory,KAFKA-10165,13311534,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,ableegoldman,ableegoldman,15/Jun/20 17:38,17/Jun/20 19:27,13/Jul/23 09:17,17/Jun/20 14:37,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,metrics,streams,,,,0,,,,,"We've hit several OOM in our soak cluster lately. We were finally able to get a heap dump right after the OOM, and found over 3.5 GB of memory being retained by the percentiles (or specifically by the 1MB float[] used by the percentiles). 

The leak does seem specific to the Percentiles class, as we see ~3000 instances of the Percentiles object vs only ~500 instances of the Max object, which is also used in the same sensor as the Percentiles

We did recently lower the size from 1MB to 100kB, but it's clear there is a leak of some kind and a ""smaller leak"" is not an acceptable solution. If the cause fo the leak is not immediately obvious we should just revert the percentiles in 2.6 and work on stabilizing them for 2.7",,ableegoldman,cadonna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10177,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-15 17:38:40.0,,,,,,,,,,"0|z0fv7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka MM2 consumer configuration,KAFKA-10160,13311400,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sbellapu,pipoth,pipoth,15/Jun/20 07:01,10/Mar/22 08:37,13/Jul/23 09:17,19/Mar/21 12:17,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,,2.4.2,2.8.0,,,,,,,,,,,,1,configuration,kafka,mirror-maker,,"[https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMakerConfig.java#L51,] according this producer/consumer level properties should be configured as e.g. somesource->sometarget.consumer.client.id, i try to set somesource->sometarget.consumer.auto.offset.reset=latest, but without success, consumer always tries to fetch earliest, not sure if bug or my misconfiguration, but then at least some update to docu would be useful",,harsham,mimaison,pipoth,ryannedolan,sbellapu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 10 08:37:17 UTC 2022,,,,,,,,,,"0|z0fue0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 12:28;ryannedolan;Agree this is a bug. This property is hard-coded but doesn't need to be. MM2 should probably default to earliest, but shouldn't override the property if it's manually configured, IMO.;;;","17/Jun/20 03:35;sbellapu;Yes, It is hard-coded [https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java]

 [~ryannedolan] i hope this don't required KIP as can be wrapped into {{mm2.properties}} (consumer.auto.offset.reset=latest) ? i want to work on this issue can you please assign to me.;;;","24/Jun/20 00:29;sbellapu;Created a pull request [https://github.com/apache/kafka/pull/8921];;;","12/Mar/21 11:03;mimaison;Reopening this issue as it seems it was only fixed in the 2.4 branch. 
[~sbellapu] can you open a PR against trunk? ;;;","10/Mar/22 02:01;harsham;Is there a reason we are holding this from going into trunk? [~ryannedolan] , [~sbellapu] ;;;","10/Mar/22 08:19;mimaison;[~harsham] This fix is in trunk, see https://github.com/apache/kafka/commit/cf202cb6acf38c64a3e8b9e541673a12ee55eaaa;;;","10/Mar/22 08:37;harsham;Thanks for pointing that out, [~mimaison]. My bad, I was looking at an older commit ([https://github.com/apache/kafka/pull/8921/commits/75b1c77542fe8269b89fb4f209520bb59e70ca3e]) and expecting that to be in trunk. I just realized that the code was changed based on some comments. Thanks again!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky kafka.admin.TopicCommandWithAdminClientTest#testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress,KAFKA-10158,13311214,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,bbyrne,chia7712,chia7712,12/Jun/20 23:25,17/Aug/20 18:31,13/Jul/23 09:17,17/Aug/20 18:31,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,unit tests,,,,,0,,,,,"Altering the assignments is a async request so it is possible that the reassignment is still in progress when we start to verify the ""under-replicated-partitions"". In order to make it stable, it needs a wait for the reassignment completion before verifying the topic command with ""under-replicated-partitions"".",,bchen225242,chia7712,lucasbradstreet,rhauch,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 18:28:28 UTC 2020,,,,,,,,,,"0|z0ft8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 20:03;rhauch;Since this is not a blocker issue, as part of the 2.6.0 release process I'm changing the fix version to `2.7.0`. If this is incorrect, please respond and discuss on the ""[DISCUSS] Apache Kafka 2.6.0 release"" discussion mailing list thread.;;;","29/Jun/20 15:01;lucasbradstreet;I think the problem here is subtly different to the one diagnosed. The test is attempting to show that an in progress reassignment does not show up as under replication partitions. The problem is that the reassignment may be completing as the `--under-replicated-partitions` check is taking place, and unfortunately there is some inconsistency in how the command line tool performs this check. It first performs a topic describe and then looks up whether a reassignment is in progress. As these checks are performed at different times, it can end up seeing the topic describe while the reassignment is in progress, and then not see the reassignment is in progress immediately after.

This inconsistency would not be a problem for this test if there wasn't a second problem. We set a replication throttle
{noformat}
TestUtils.setReplicationThrottleForPartitions(adminClient, brokerIds, Set(tp), throttleBytes = 1) {noformat}
however the throttle does not become effective due to the way the check works in the replica fetcher:
{noformat}
!fetchState.isReplicaInSync && quota.isThrottled(topicPartition) && quota.isQuotaExceeded
{noformat}
The first thread causes the follower to think it's in sync, and the follow up fetch causes fetchState.isReplicaInSync to return true, which means it does not throttle itself.;;;","29/Jun/20 15:40;chia7712;[~lucasbradstreet] not sure whether I have caught you point. It seems to me the issue you described is that the throttle does not work expectedly and so the following check gets failed, right?

{code}
    // let's wait until the LAIR is propagated
    TestUtils.waitUntilTrue(() => {
      val reassignments = adminClient.listPartitionReassignments(Collections.singleton(tp)).reassignments().get()
      !reassignments.get(tp).addingReplicas().isEmpty
    }, ""Reassignment didn't add the second node"")

    // describe the topic and test if it's under-replicated
    val simpleDescribeOutput = TestUtils.grabConsoleOutput(
      topicService.describeTopic(new TopicCommandOptions(Array(""--topic"", testTopicName))))
    val simpleDescribeOutputRows = simpleDescribeOutput.split(""\n"")
    assertTrue(simpleDescribeOutputRows(0).startsWith(s""Topic: $testTopicName""))
    assertEquals(2, simpleDescribeOutputRows.size)
{code};;;","29/Jun/20 15:50;lucasbradstreet;For me that check passes and the following check fails:
{code:java}
val underReplicatedOutput = TestUtils.grabConsoleOutput( topicService.describeTopic(new TopicCommandOptions(Array(""--under-replicated-partitions"")))) 
assertEquals(s""--under-replicated-partitions shouldn't return anything: '$underReplicatedOutput'"", """", underReplicatedOutput) {code}
 

When this check fails the reassignment is usually finishing because the replication throttle didn't quite work the way the test expects it to. The throttle doesn't work because only one fetch request is required to get the follower into sync.

That said, I think depending on the timing I could see the lines you point to failing too.;;;","29/Jun/20 15:59;chia7712;Any suggestion about fixing the throttle?

For another, does it make sense to wait the reassignment to complete before checking the under replicated partitions? ;;;","29/Jun/20 16:25;lucasbradstreet;[~chia7712] I'm quite sure what the right way to fix it is. I think if we produced messages in multiple batches, and set 

""replica.fetch.max.bytes"" low enough it would ensure that the follower throttled itself prior to joining the ISR.

I think checking for the reassignment to complete before checking for under replicated partitions defeats the purpose of the test. I think the test was designed to show that in progress reassignments would not show up as URPs. I think the test could be improved by checking that a reassignment is still in progress at the end of the test, after the --under-replicated-partitions check is made.;;;","29/Jun/20 16:59;bchen225242;failed again: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7155/]

 

org.junit.ComparisonFailure: --under-replicated-partitions shouldn't return anything: ' Topic: testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress-onVcRPnNWU Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1 ' expected:<[]> but was:<[ Topic: testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress-onVcRPnNWU Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1 ]> at org.junit.Assert.assertEquals(Assert.java:117) at kafka.admin.TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress(TopicCommandWithAdminClientTest.scala:702) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor25.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:834);;;","29/Jun/20 23:20;bchen225242;Failed again: 

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3175/]
h3. Stacktrace

org.junit.ComparisonFailure: --under-replicated-partitions shouldn't return anything: ' Topic: testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress-ryTXst4I8P Partition: 0 Leader: 2 Replicas: 0,2 Isr: 2 ' expected:<[]> but was:<[ Topic: testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress-ryTXst4I8P Partition: 0 Leader: 2 Replicas: 0,2 Isr: 2 ]> at org.junit.Assert.assertEquals(Assert.java:117) at kafka.admin.TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress(TopicCommandWithAdminClientTest.scala:702) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.lang.Thread.run(Thread.java:748);;;","14/Aug/20 18:28;rsivaram;[~bbyrne] We can close this now, right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multiple tests failed due to ""Failed to process feature ZK node change event""",KAFKA-10157,13311048,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kprakasam,apovzner,apovzner,12/Jun/20 04:14,12/Jun/20 21:20,13/Jul/23 09:17,12/Jun/20 21:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Multiple tests failed due to ""Failed to process feature ZK node change event"". Looks like a result of merge of this PR: [https://github.com/apache/kafka/pull/8680]

Note that running tests without `--info` gives output like this one: 
{quote}Process 'Gradle Test Executor 36' finished with non-zero exit value 1
{quote}
kafka.network.DynamicConnectionQuotaTest failed:
{quote}
kafka.network.DynamicConnectionQuotaTest > testDynamicConnectionQuota STANDARD_OUT
 [2020-06-11 20:52:42,596] ERROR [feature-zk-node-event-process-thread]: Failed to process feature ZK node change event. The broker will eventually exit. (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:76)
 java.lang.InterruptedException
 at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
 at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
 at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
 at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.$anonfun$doWork$1(FinalizedFeatureChangeListener.scala:147){quote}
 

kafka.api.CustomQuotaCallbackTest failed:

{quote}    [2020-06-11 21:07:36,745] ERROR [feature-zk-node-event-process-thread]: Failed to process feature ZK node change event. The broker will eventually exit. (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:76)

    java.lang.InterruptedException

        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)

        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)

        at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)

        at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.$anonfun$doWork$1(FinalizedFeatureChangeListener.scala:147)

        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)

        at scala.util.control.Exception$Catch.apply(Exception.scala:227)

        at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.doWork(FinalizedFeatureChangeListener.scala:147)

        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)

at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
 at scala.util.control.Exception$Catch.apply(Exception.scala:227)
 at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.doWork(FinalizedFeatureChangeListener.scala:147)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{quote}
 

kafka.server.DynamicBrokerReconfigurationTest failed:

{quote}    [2020-06-11 21:13:01,207] ERROR [feature-zk-node-event-process-thread]: Failed to process feature ZK node change event. The broker will eventually exit. (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:76)

    java.lang.InterruptedException

        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)

        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)

        at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)

        at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.$anonfun$doWork$1(FinalizedFeatureChangeListener.scala:147)

        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)

        at scala.util.control.Exception$Catch.apply(Exception.scala:227)

        at kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread.doWork(FinalizedFeatureChangeListener.scala:147)

        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
{quote}
 

 ",,apovzner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-12 04:14:44.0,,,,,,,,,,"0|z0fs7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test ReassignPartitionsUnitTest.testModifyBrokerThrottles,KAFKA-10155,13311029,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,ableegoldman,ableegoldman,12/Jun/20 01:56,17/Jun/20 21:54,13/Jul/23 09:17,17/Jun/20 21:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,admin,,,,,0,,,,,"Seems to fail more often than not. I've seen this fail on almost every single build of the latest PRs

 
h3. Stacktrace

java.lang.NullPointerException at kafka.admin.ReassignPartitionsUnitTest.verifyBrokerThrottleResults(ReassignPartitionsUnitTest.scala:577) at kafka.admin.ReassignPartitionsUnitTest.testModifyBrokerThrottles(ReassignPartitionsUnitTest.scala:540) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.lang.Thread.run(Thread.java:830)
h3. Standard Output

Current partition replica assignment \{""version"":1,""partitions"":[{""topic"":""bar"",""partition"":0,""replicas"":[2,3,0],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":0,""replicas"":[0,1,2],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":1,""replicas"":[1,2,3],""log_dirs"":[""any"",""any"",""any""]}]} Proposed partition reassignment configuration \{""version"":1,""partitions"":[{""topic"":""bar"",""partition"":0,""replicas"":[0,3,1],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":0,""replicas"":[3,0,1],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":1,""replicas"":[0,1,3],""log_dirs"":[""any"",""any"",""any""]}]} Current partition replica assignment \{""version"":1,""partitions"":[{""topic"":""foo"",""partition"":0,""replicas"":[0,1,2],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":1,""replicas"":[1,2,3],""log_dirs"":[""any"",""any"",""any""]}]} Proposed partition reassignment configuration \{""version"":1,""partitions"":[{""topic"":""foo"",""partition"":0,""replicas"":[3,0,1],""log_dirs"":[""any"",""any"",""any""]},\{""topic"":""foo"",""partition"":1,""replicas"":[0,1,2],""log_dirs"":[""any"",""any"",""any""]}]}",,ableegoldman,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 21:53:40 UTC 2020,,,,,,,,,,"0|z0fs3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/20 02:56;chia7712;This issue will be resolved by KAFKA-10147;;;","12/Jun/20 03:41;ableegoldman;Oh cool, thanks for the heads up. I'll assign this ticket to you then;;;","17/Jun/20 21:01;mjsax;We still see test failures: [https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1172/testReport/kafka.admin/ReassignPartitionsUnitTest/testModifyBrokerThrottles/]

Are we sure it's fixed properly?;;;","17/Jun/20 21:28;ableegoldman;The fix was only merged 6 hours ago. Did it fail on a build that was kicked off less than 6 hours ago?;;;","17/Jun/20 21:53;mjsax;Oh. I was just looking at the comment that is 5 days old. Thanks for clarifying!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attempt to write checkpoint without first committing during recycle,KAFKA-10152,13310984,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,11/Jun/20 19:24,13/Jun/20 02:06,13/Jul/23 09:17,13/Jun/20 02:06,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Seen causing failure of `SmokeTestDirverIntegrationTest#shouldWorkWithRebalance locally:
{code:java}
Caused by: java.lang.IllegalStateException: A checkpoint should only be written if no commit is needed. at org.apache.kafka.streams.processor.internals.StreamTask.writeCheckpointIfNeed(StreamTask.java:534) at org.apache.kafka.streams.processor.internals.StreamTask.closeAndRecycleState(StreamTask.java:482) at org.apache.kafka.streams.processor.internals.StandbyTaskCreator.createStandbyTaskFromActive(StandbyTaskCreator.java:115) at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:288)
{code}
See comment here: https://github.com/apache/kafka/pull/8833/files#r438953766",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10150,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 13 02:06:01 UTC 2020,,,,,,,,,,"0|z0frtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/20 01:02;mjsax;[~ableegoldman] Should we just close this as duplicate for KAFKA-10150 ?;;;","13/Jun/20 02:02;ableegoldman;Yeah, technically different but since we're fixing both with the same task/TaskManager cleanup let's just track with the one ticket;;;","13/Jun/20 02:06;ableegoldman;We should be committing offsets but not checkpointing recycled tasks. Fixed as part of the task management cleanup for KAFKA-10150 in [https://github.com/apache/kafka/pull/8856];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreUpgradeIntegrationTest.shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi,KAFKA-10151,13310979,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ableegoldman,ableegoldman,11/Jun/20 19:04,17/Jun/20 17:42,13/Jul/23 09:17,17/Jun/20 17:42,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,flaky-test,integration-test,,,"I've started seeing this fail in the past week or so. Checked out the logs and there's nothing obviously wrong (ie no ERROR or exception) so it might just be flaky? 

 

java.lang.AssertionError: Condition not met within timeout 60000. Could not get expected result in time. at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:401) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:398) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:388) at org.apache.kafka.streams.integration.StoreUpgradeIntegrationTest.verifyCountWithTimestamp(StoreUpgradeIntegrationTest.java:367) at org.apache.kafka.streams.integration.StoreUpgradeIntegrationTest.shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi(StoreUpgradeIntegrationTest.java:183) at org.apache.kafka.streams.integration.StoreUpgradeIntegrationTest.shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi(StoreUpgradeIntegrationTest.java:109)",,ableegoldman,chia7712,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/20 19:04;ableegoldman;StoreUpgradeIntegrationTest.shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi.stdout.rtf;https://issues.apache.org/jira/secure/attachment/13005516/StoreUpgradeIntegrationTest.shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi.stdout.rtf",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 17:41:03 UTC 2020,,,,,,,,,,"0|z0frsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 16:57;ableegoldman;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6986/testReport/junit/org.apache.kafka.streams.integration/StoreUpgradeIntegrationTest/shouldMigratePersistentWindowStoreToTimestampedWindowStoreUsingPapi/];;;","16/Jun/20 18:54;chia7712;[~ableegoldman] Could I take over this issue?;;;","16/Jun/20 22:10;ableegoldman;Go for it. For some context, it seems to have started failing sometime in the last two weeks or so. But I'm not sure whether we actually merged something that broke this recently, or maybe Jenkins is just being extra-flaky these days.;;;","17/Jun/20 06:19;chia7712;[~ableegoldman] I feel you have resolved this issue by https://github.com/apache/kafka/commit/2239004907b29e00811fee9ded5a790172701a03

the root cause is the in-flight records to changelog are not completed when closing streamTasks.
{code:java}
    @Override
    public Map<TopicPartition, OffsetAndMetadata> prepareCommit() {
        switch (state()) {
            case RUNNING:
            case RESTORING:
            case SUSPENDED:
                maybeScheduleCheckpoint(); //checkpoint is not up-to-date if there are in-flight requests
                stateMgr.flush();
                recordCollector.flush();
{code}
see [https://github.com/apache/kafka/blob/03ed08d0d17a10ca4f96c8cc0a8694834ae01e6d/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L345]

The commit (https://github.com/apache/kafka/commit/2239004907b29e00811fee9ded5a790172701a03) update the checkpoint after calling recordCollector.flush so the checkpoint is up-to-date.

Also, I loop StoreUpgradeIntegrationTest.shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi 30 times with [https://github.com/apache/kafka/commit/2239004907b29e00811fee9ded5a790172701a03]. All pass.
;;;","17/Jun/20 17:41;ableegoldman;Huh, I didn't even notice that bug. It's just a happy accident that we fixed it in that PR.

 

Nice investigation! I think we can resolve this ticket then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException when revoking task in CREATED state,KAFKA-10150,13310966,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,11/Jun/20 18:12,16/Jun/20 23:49,13/Jul/23 09:17,16/Jun/20 23:49,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Seen killing threads in soak. During handleAssignment we call #suspend and #prepareCommit on all revoked tasks. Only RUNNING tasks are transitioned to SUSPENDED in #suspend, but #prepareCommit will assert that the state is either RUNNING or SUSPENDED.

So tasks that get revoked while in CREATED will hit an IllegalStateException during prepareCommit:
{code:java}
[2020-06-11T00:39:57-07:00] (streams-soak-KAFKA-10144-corrupted-standby-fix-eos-beta-broker-trunk_soak_i-09ef677ab88c040c8_streamslog) [2020-06-11 07:39:56,852] ERROR [stream-soak-test-90c4a6b9-e730-4211-9edb-146b74c54c8f-StreamThread-2] stream-thread [stream-soak-test-90c4a6b9-e730-4211-9edb-146b74c54c8f-StreamThread-2] Failed to close task 1_2 cleanly. Attempting to close remaining tasks before re-throwing: (org.apache.kafka.streams.processor.internals.TaskManager)[2020-06-11T00:39:57-07:00] (streams-soak-KAFKA-10144-corrupted-standby-fix-eos-beta-broker-trunk_soak_i-09ef677ab88c040c8_streamslog) [2020-06-11 07:39:56,852] ERROR [stream-soak-test-90c4a6b9-e730-4211-9edb-146b74c54c8f-StreamThread-2] stream-thread [stream-soak-test-90c4a6b9-e730-4211-9edb-146b74c54c8f-StreamThread-2] Failed to close task 1_2 cleanly. Attempting to close remaining tasks before re-throwing: (org.apache.kafka.streams.processor.internals.TaskManager)[2020-06-11T00:39:57-07:00] (streams-soak-KAFKA-10144-corrupted-standby-fix-eos-beta-broker-trunk_soak_i-09ef677ab88c040c8_streamslog) java.lang.IllegalStateException: Illegal state CREATED while preparing standby task 1_2 for committing  at org.apache.kafka.streams.processor.internals.StandbyTask.prepareCommit(StandbyTask.java:142) at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:228) at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1350) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:279) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:421) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:440) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:506) at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1263) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1229) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1204) at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:763) at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:623) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:550) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:509)
{code}",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10152,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-11 18:12:50.0,,,,,,,,,,"0|z0frpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams Restores too few Records with eos-beta Enabled ,KAFKA-10148,13310878,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cadonna,cadonna,cadonna,11/Jun/20 09:23,09/Mar/21 09:20,13/Jul/23 09:17,19/Jun/20 15:46,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"System test {{StreamsEosTest.test_failure_and_recovery}} for eos-beta exposes a bug that results in wrong results in the output topic. The cause seems to be a too low end offset during restoration of a state store.

Example:

The system test computes a minimum aggregate over records in an input topic and writes the results to an output topic. The input topic partition {{data-1}} contains the following records among others:

{code}
...
offset: 125 CreateTime: 1591690264681 keysize: 5 valuesize: 4 sequence: 125 headerKeys: [] key: 14920 payload: 9215
...
offset: 1611 CreateTime: 1591690297424 keysize: 5 valuesize: 4 sequence: 1611 headerKeys: [] key: 14920 payload: 1595
...
offset: 2104 CreateTime: 1591690308542 keysize: 5 valuesize: 4 sequence: 2104 headerKeys: [] key: 14920 payload: 9274
...
{code}

The output topic partition {{min-1}} contains:

{code}
...
offset: 125 CreateTime: 1591690264681 keysize: 5 valuesize: 4 sequence: 125 headerKeys: [] key: 14920 payload: 9215
...
offset: 1828 CreateTime: 1591690297424 keysize: 5 valuesize: 4 sequence: 1213 headerKeys: [] key: 14920 payload: 1595
...
offset: 2324 CreateTime: 1591690308542 keysize: 5 valuesize: 4 sequence: 10 headerKeys: [] key: 14920 payload: 9215
...
{code} 

The last record is obviously wrong because 1595 is less than 9215.

To test the resilience to an unexpected failure of a Streams client, the system tests aborts a Streams client, i.e., the client is closed in a dirty manner. This dirty close causes the Streams client to restore its local state store that maintains the minimum aggregate from the beginning of the changelog topic partitions {{EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-1}}. The partition {{EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-1}} contains:

{code}
...
offset: 125 CreateTime: 1591690264681 keysize: 5 valuesize: 4 sequence: 125 headerKeys: [] key: 14920 payload: 9215
...
offset: 1828 CreateTime: 1591690297424 keysize: 5 valuesize: 4 sequence: 1213 headerKeys: [] key: 14920 payload: 1595
...
offset: 2324 CreateTime: 1591690308542 keysize: 5 valuesize: 4 sequence: 10 headerKeys: [] key: 14920 payload: 9215
...
{code}

Also here the last record is wrong. 

During the restoration, the Streams client uses its Kafka consumer to issue a list offsets request to get the end offset of the changelog topic partition. The response to the list offsets request contains end offset 1518 for {{EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-1}} as can be seen here:

{code}
[2020-06-09 08:11:49,250] DEBUG [Consumer clientId=EosTest-12216046-2d5d-48cd-864c-21b0aa570fae-StreamThread-1-restore-consumer, groupId=null] Received LIST_OFFSETS response from node 2 for request with header RequestHeader(apiKey=LIST_OFFSETS, apiVersion=5, clientId=EosTest-12216046-2d5d-48cd-864c-21b0aa570fae-StreamThread-1-restore-consumer, correlationId=3): (type=ListOffsetResponse, throttleTimeMs=0, responseData={EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-4=PartitionData(errorCode: 0, timestamp: -1, offset: 1478, leaderEpoch: Optional[0]), EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-1=PartitionData(errorCode: 0, timestamp: -1, offset: 1518, leaderEpoch: Optional[0])}) (org.apache.kafka.clients.NetworkClient)
{code}

Offset 1518 is before record in {{EosTest-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-1}}

{code}
offset: 1828 CreateTime: 1591690297424 keysize: 5 valuesize: 4 sequence: 1213 headerKeys: [] key: 14920 payload: 1595
{code}

Hence, this record is not restored into the local state store. However, after the restoration the input topic partition {{data-1}} is read starting with offset 2094. That means that record

{code}
offset: 1611 CreateTime: 1591690297424 keysize: 5 valuesize: 4 sequence: 1611 headerKeys: [] key: 14920 payload: 1595
{code} 

is not read there either because it has a lower offset. Instead the following record with with key 14920 and value 9274 is read, but since 9274 is not less than 9215, value 9215 is written a second time to the output topic.

I ran the system tests 10x with eos_alpha and 10x with eos_beta and only eos_beta failed a couple of times.",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10167,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-11 09:23:14.0,,,,,,,,,,"0|z0fr60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockAdminClient#describeConfigs(Collection<ConfigResource>) is unable to handle broker resource,KAFKA-10147,13310838,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,chia7712,chia7712,chia7712,11/Jun/20 07:54,17/Jun/20 21:54,13/Jul/23 09:17,17/Jun/20 15:38,,,,,,,,,,,,,,,,,,,,,,,2.6.0,2.7.0,,,,,,,,,,,,0,,,,,MockAdminClient#describeConfigs(Collection<ConfigResource>) has new implementation introduced by https://github.com/apache/kafka/commit/48b56e533b3ff22ae0e2cf7fcc649e7df19f2b06. It does not handle broker resource so ReassignPartitionsUnitTest#testModifyBrokerThrottles throws NPE,,chia7712,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 15:38:27 UTC 2020,,,,,,,,,,"0|z0fqx4:",9223372036854775807,,bchen225242,,,,,,,,,,,,,,,,,,"12/Jun/20 01:57;chia7712;give blocked as it breaks test (ReassignPartitionsUnitTest#testModifyBrokerThrottles);;;","17/Jun/20 15:38;rhauch;Merged to `trunk` and backported to `2.6`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted standby tasks are not always cleaned up,KAFKA-10144,13310797,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,11/Jun/20 02:21,13/Jun/20 01:07,13/Jul/23 09:17,13/Jun/20 01:07,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Thread death on the 2.6-eos-beta soak was due to re-registration of a standby task changelog that was already registered. The root cause was that the task had been marked corrupted, but `commit` threw a TaskMigratedException before we could get to calling TaskManager#handleCorruption and properly clean up the task.

For corrupted active tasks this is not a problem, since #handleLostAll will then finish the cleanup. But we intentionally don't clear standbys tasks on TaskMigratedException, leaving the task corrupted and partially registered",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-11 02:21:08.0,,,,,,,,,,"0|z0fqo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High CPU issue during rebalance in Kafka consumer after upgrading to 2.5,KAFKA-10134,13310505,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,seanguo,seanguo,10/Jun/20 00:07,15/Oct/20 16:36,13/Jul/23 09:17,11/Sep/20 01:05,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,clients,,,,,3,,,,,"We want to utilize the new rebalance protocol to mitigate the stop-the-world effect during the rebalance as our tasks are long running task.

But after the upgrade when we try to kill an instance to let rebalance happen when there is some load(some are long running tasks >30S) there, the CPU will go sky-high. It reads ~700% in our metrics so there should be several threads are in a tight loop. We have several consumer threads consuming from different partitions during the rebalance. This is reproducible in both the new CooperativeStickyAssignor and old eager rebalance rebalance protocol. The difference is that with old eager rebalance rebalance protocol used the high CPU usage will dropped after the rebalance done. But when using cooperative one, it seems the consumers threads are stuck on something and couldn't finish the rebalance so the high CPU usage won't drop until we stopped our load. Also a small load without long running task also won't cause continuous high CPU usage as the rebalance can finish in that case.

 

""executor.kafka-consumer-executor-4"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable  [0x00007fe119aab000]""executor.kafka-consumer-executor-4"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable  [0x00007fe119aab000]   java.lang.Thread.State: RUNNABLE at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:467) at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) at

 

By debugging into the code we found it looks like the clients are  in a loop on finding the coordinator.

I also tried the old rebalance protocol for the new version the issue still exists but the CPU will be back to normal when the rebalance is done.

Also tried the same on the 2.4.1 which seems don't have this issue. So it seems related something changed between 2.4.1 and 2.5.0.

 ",,ableegoldman,aklochkov,amuraru,davispw,dibbhatt,guozhang,ijuma,lkokhreidze,lusong,neowu0,rhauch,rng,seanguo,xiaotao183,zhowei,,,,,,,,,,,,,,,,,,,,KAFKA-10254,,,,,,,,,,,,,,,,,,,"20/Aug/20 08:49;zhowei;consumer3.log.2020-08-20.log;https://issues.apache.org/jira/secure/attachment/13010141/consumer3.log.2020-08-20.log","22/Jul/20 03:03;zhowei;consumer5.log.2020-07-22.log;https://issues.apache.org/jira/secure/attachment/13008127/consumer5.log.2020-07-22.log",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 27 22:04:04 UTC 2020,,,,,,,,,,"0|z0fovk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/20 17:25;ijuma;[~ableegoldman] [~guozhang] Any idea?;;;","15/Jun/20 17:50;guozhang;This is a bit weird to me -- discover of coordinator logic did not change from 2.4 -> 2.5 AFAIK.

[~seanguo] could you list the configs of consumer when you used cooperative rebalance, v.s. eager rebalance?;;;","16/Jun/20 04:24;seanguo;[~guozhang] 
Cooperative:
{noformat}
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [xxx,xxx,xxx,xxx,xxx,xxx]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = xxx-consumer-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 1800000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer
{noformat}

Eager:
{noformat}
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [xxx,xxx,xxx,xxx,xxx,xxx]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = xxx-consumer-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 1800000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer
{noformat}

With 2.5.1 we can reproduce the high CPU issue with both eager and cooperative rebalancing protocol but not in 2.4.1. The difference between eager and cooperative with 2.5.1 is that for eager rebalance the CPU can go back to normal after the rebalance is done but for cooperative it seems it stuck on rebalancing and never ends.;;;","24/Jun/20 13:27;ijuma;Any additional thoughts [~guozhang]? [~seanguo] would you be able to share a profile of the consumer while the high CPU usage is going on?;;;","24/Jun/20 20:25;rhauch;[~guozhang], [~ijuma], [~seanguo]: what's the status of this? Do we want to continue treating this as a blocker for the 2.6.0 release? If so, what's the timeframe for fixing this?

If this should not block the release, should we downgrade the priority and/or change the fix versions to 2.6.1 and/or 2.7.0?;;;","26/Jun/20 14:42;neowu0;we have experienced same issue,

one easy way to reproduce is: 1. start both kafka server and java consumer thread 2. stop kafka

then you should observe consumer thread keeps busy cpu loop and cause very high cpu

the reason is due to java consumer 2.5.0 code change
{code:java}
// code placeholder
org.apache.kafka.clients.consumer.KafkaConsumer  line: 1235
if (includeMetadataInTimeout) {
    // try to update assignment metadata BUT do not need to block on the timer,
    // since even if we are 1) in the middle of a rebalance or 2) have partitions
    // with unknown starting positions we may still want to return some data
    // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms
    // to never block on completing the rebalance procedure if there's any
    updateAssignmentMetadataIfNeeded(time.timer(0L));
}
{code}
if it fails to fetch metadata, this line never blocks, and it run thru and keep cpu busy loop, due to 
{code:java}
// code placeholder
while (timer.notExpired());
{code}
please help on this issue, this is blocker for us to upgrade to java client 2.5.0

it's particular bad if we deploy both kafka/java consumer in same VM/(k8s node)
 if something wrong make kafka hiccup, all java consumers cause cpu high, and make kafka even slower to recover (like restart by k8s), and eventually make entire node/VM not be able to going forward. 

(java consumers keep busy loop to wait kafka ready, kafka has too little cpu to move on)

 ;;;","26/Jun/20 15:17;neowu0; 
{code:java}
do {
    client.maybeTriggerWakeup();

    if (includeMetadataInTimeout) {
        // try to update assignment metadata BUT do not need to block on the timer,
        // since even if we are 1) in the middle of a rebalance or 2) have partitions
        // with unknown starting positions we may still want to return some data
        // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms
        // to never block on completing the rebalance procedure if there's any
        updateAssignmentMetadataIfNeeded(time.timer(0L));
    } else {
        while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {
            log.warn(""Still waiting for metadata"");
        }
    }

    final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);
    if (!records.isEmpty()) {
        // before returning the fetched records, we can send off the next round of fetches
        // and avoid block waiting for their responses to enable pipelining while the user
        // is handling the fetched records.
        //
        // NOTE: since the consumed position has already been updated, we must not allow
        // wakeups or any other errors to be triggered prior to returning the fetched records.
        if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {
            client.transmitSends();
        }

        return this.interceptors.onConsume(new ConsumerRecords<>(records));
    }
} while (timer.notExpired());
{code}
from the current design, i guess one possible fix is to add exponentially retry if metadata is not available and nothing returned by pollForFetches(timer), until timer expired.


then let outside application code to call consumer.poll(timeout) again

 ;;;","26/Jun/20 22:25;guozhang;[~seanguo] I think [~neowu0]'s brought up issue is a valid one to tackle, but I'm not sure if it is the same root cause you've seen: basically the consumer maybe tied in the metadata refresh loop if it cannot find the coordinator, but that is only the case if the coordinator broker is indeed not available during that time.

In your observation though, there should be no broker unavailability since you're just bouncing the consumer instance and the coordinator should be quick to discover.;;;","27/Jun/20 00:42;guozhang;I've provided a PR for trunk, but it should apply cleanly to 2.5 as well (I will cherry-pick this when merging). [~neowu0] [~seanguo] please let me know if you could apply the patch and verify if it helps resolving the issue.;;;","27/Jun/20 01:27;neowu0;Hi, [~guozhang]

Thanks for quick update! 

I reviewed and tested your patch, and still found some issue
{code:java}
if (subscriptions.fetchablePartitions(tp -> true).isEmpty()) {
    updateAssignmentMetadataIfNeeded(timer);
} else {
    final Timer updateMetadataTimer = time.timer(0L);
    updateAssignmentMetadataIfNeeded(updateMetadataTimer);
    timer.update(updateMetadataTimer.currentTimeMs());
}
{code}
there are 2 scenarios
1) start java consumer with kafka stopped, the code work as expected, it flows to first branch, and wait with timer, 
and will be able to connect to kafka when kafka is up (with low cpu usage due to timer)

however in 2nd secnarios

 

2) start both java consumer and kafka, and let java consumer successfully subscribe some topics, then force stop kafka
then subscriptions.fetchablePartitions(tp -> true) will return non empty result, then it will go into second branch without blocking
and it will trigger busy cpu wait

Thanks,;;;","27/Jun/20 01:41;neowu0;maybe besides fetchablePartitions, it also should check records = pollForFetches(timer)?;;;","27/Jun/20 01:48;neowu0;something like this fix my issues, but i am not sure whether this is right thing to do to fit bigger picture
{code:java}
// poll for new data until the timeout expires
Map<TopicPartition, List<ConsumerRecord<K, V>>> records = null;
do {
    client.maybeTriggerWakeup();
    if (includeMetadataInTimeout) {
        // try to update assignment metadata BUT do not need to block on the timer if we still have
        // some assigned partitions, since even if we are 1) in the middle of a rebalance
        // or 2) have partitions with unknown starting positions we may still want to return some data
        // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms
        // to never block on completing the rebalance procedure if there's any
        if (subscriptions.fetchablePartitions(tp -> true).isEmpty() || records == null || records.isEmpty()) {
            updateAssignmentMetadataIfNeeded(timer);
        } else {
            final Timer updateMetadataTimer = time.timer(0L);
            updateAssignmentMetadataIfNeeded(updateMetadataTimer);
            timer.update(updateMetadataTimer.currentTimeMs());
        }
    } else {
        while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {
            log.warn(""Still waiting for metadata"");
        }
    }

    records = pollForFetches(timer);
    if (!records.isEmpty()) {
        // before returning the fetched records, we can send off the next round of fetches
        // and avoid block waiting for their responses to enable pipelining while the user
        // is handling the fetched records.
        //
        // NOTE: since the consumed position has already been updated, we must not allow
        // wakeups or any other errors to be triggered prior to returning the fetched records.
        if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {
            client.transmitSends();
        }

        return this.interceptors.onConsume(new ConsumerRecords<>(records));
    }
} while (timer.notExpired());
{code};;;","27/Jun/20 04:21;seanguo;We can try the patch next week to see whether this also addresses the issue we met as we had also seen a lots of operations on finding the coordinator when this issue happens.;;;","29/Jun/20 22:50;guozhang;[~neowu0] What's still puzzling me is that, even in the second branch, since we always keep calling `timer.update` then we should still eventually exit the while loop with `timer.expired`. So why would we observe that it blocks inside the while-loop forever is not clear to me.;;;","29/Jun/20 23:00;guozhang;Here's my reasoning: if there are still some owned partitions that are fetchable even during a rebalance, then within the while-loop even if we exit the updateAssignmentMetadataIfNeeded we can still try to fetch from then and then break out of the while loop. However, if we do not have fetchable partitions then we may be blocked in the while-loop for a long time until the poll timeout expires. During this period of time we would then see high CPU usage indeed. I think [~neowu0]'s idea is better: we should also use long poll if there are fetchable partitions but not data fetched.

What's puzzling me however, is that even in that case we should still be able to exit the busy while-loop eventually since the `timer.update` should still be called in pollForFetches within that while loop and hence advance the timer, why with cooperative rebalance we would block inside the while loop forever is not clear to me.;;;","30/Jun/20 01:31;neowu0;Hi, [~guozhang]

for ""What's still puzzling me is that, even in the second branch, since we always keep calling `timer.update` then we should still eventually exit the while loop with `timer.expired`. So why would we observe that it blocks inside the while-loop forever is not clear to me.""

yes, it will exit while loop eventually, but usually the app code is like following (at least in my case)
{code:java}
while (!shutdown) {
    try {
        ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofSeconds(30));
        if (records.isEmpty()) continue;
        processRecords(records);
    } catch (Throwable e) {
        if (shutdown) break;
        logger.error(""failed to pull message, retry in 10 seconds"", e);
        Threads.sleepRoughly(Duration.ofSeconds(10));
    }
}
{code}
so during 30s, if kafak is down in the middle, since fetchablePartitions return non-empty, the consumer.poll keeps busy loop,

and as soon as it exists, the application usually will try to poll immediately.

in application level, sure i can put delay if poll return empty, but still it will trigger high cpu time to time, and timeout passed into consumer.poll can't be high, say if i use 5 secs,

in the second case, the thread acts like, high cpu 5sec, -> sleep 5s -> 100%cpu 5s, (which is still not considered as healthy behavior)

 ;;;","30/Jun/20 02:21;guozhang;Hey [~neowu0] thanks a lot for your comments. Much appreciated!

Just to clarify, what I was puzzling is the original observation that [~seanguo] made, that is with eager rebalance we would eventually escape the while-loop / high CPU, whereas with the new cooperative rebalance protocol we are stuck in this loop. Do you observe the same pattern? If yes what's your take on it?;;;","30/Jun/20 02:49;neowu0;What I observed is not exactly same as Sean, I only tested client 2.5.0 on test env, where all kafka/app pod are in one k8s node.

and during k8s deployment, new pod joins and old pod fades out (this is similar as Sean's case),

during the deployment, since the new pod (consumer) are not fully joined the kafka consumer group yet, it causes high cpu, and made kafka even slower to assign group, and eventually it goes to negative loop, make both new pod and kafka stuck.

i suppose Sean's case is more or less similar, and the current fix should be able to resolve it. ;;;","30/Jun/20 17:35;guozhang;[~neowu0] Thanks for your comments. I will incorporate them in my PR and ping you and [~seanguo] to test out before merging.;;;","01/Jul/20 05:37;guozhang;I've updated the PR.;;;","02/Jul/20 04:20;seanguo;[~guozhang] Looks like this also resolves our issue. After applied this patch the high CPU issue is not reproduced in our local environment. Thank you for the quick fix. [~neowu0] Thank you for pointing out the cause of this issue.
One thing to note is the latest code in the PR has a small issue ""random.nextInt(2 << retries)"" when retries is larger than 30 , this method will throw exception, we have modified to use timer.remainingMs() instead in that situation.;;;","05/Jul/20 03:24;guozhang;[~seanguo] thanks for confirming! As the root cause is known now I've thought about an alternative solution to fix it: I think ideally we would split {{updateAssignmentMetadataIfNeeded}} into three different logic: 1) discover coordinator if necessary, 2) join-group if necessary, 3) refresh metadata and fetch position if necessary. Then we can just make 2) to be best-effort if there are still some fetchable partitions.


But that’s a rather big change to make as a last minute blocker fix for 2.6, so I made a smaller change to make updateAssignmentMetadataIfNeeded has an optional boolean flag to indicate if 2) above should wait until either expired or complete, otherwise do not wait on the join-group future and just try once with the timer which would return if there’s anything written on the socket. I’ve updated the PR for this, if people agree this would be a reasonable fix for 2.6 I can add the test coverage and merge it. LMK.;;;","06/Jul/20 17:33;rhauch;[~guozhang] do you have an updated ETA to complete this issue?;;;","08/Jul/20 16:54;guozhang;I've merged the PR and would like [~seanguo] [~neowu0] to verify if it has fixed their observed issue.;;;","08/Jul/20 18:49;neowu0;Hi, [~guozhang]

Your latest change fixed all my issues, Thanks!;;;","08/Jul/20 19:04;guozhang;Thanks for the confirmation! I'm resolving this ticket then.;;;","08/Jul/20 19:05;guozhang;cc 2.5.1 release manager [~vvcephei] I'm merging it to 2.5 branch too.;;;","10/Jul/20 07:50;seanguo;[~guozhang] Looks like the latest change bring back the high CPU issue based on our local tests.;;;","10/Jul/20 18:18;guozhang;Hmm, interesting. What setup are you using with the local tests? I tried to setup a single consumer and a single broker, and then shutdown the broker; under this scenario the issue does not show up anymore.;;;","12/Jul/20 05:13;ijuma;[~guozhang] Should we reopen this issue given [~seanguo]'s comment?;;;","12/Jul/20 06:28;zhowei;[~guozhang], I have three brokers and 10 consumers. When I restart one of consumers, some of other consumers will be with high CPU issue. 


{code:java}
// from KafkaConsumer.java (a fine fix)
// private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout);
                if (includeMetadataInTimeout) {
                    // try to update assignment metadata BUT do not need to block on the timer if we still have
                    // some assigned partitions, since even if we are 1) in the middle of a rebalance
                    // or 2) have partitions with unknown starting positions we may still want to return some data
                    // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms
                    // to never block on completing the rebalance procedure if there's any
                    if (subscriptions.fetchablePartitions(tp -> true).isEmpty()) {
                        updateAssignmentMetadataIfNeeded(timer);
                    } else {
                        final Timer updateMetadataTimer = time.timer(0L);
                        updateAssignmentMetadataIfNeeded(updateMetadataTimer);
                        timer.update(updateMetadataTimer.currentTimeMs());
                    }
                } else {
                    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {
                        log.warn(""Still waiting for metadata"");
                    }
                }
{code}


{code:java}
// from KafkaConsumer.java (last commit)
// private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout);

               if (includeMetadataInTimeout) {
                    // try to update assignment metadata BUT do not need to block on the timer for join group
                    updateAssignmentMetadataIfNeeded(timer, false);
                } else {
                    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {
                        log.warn(""Still waiting for metadata"");
                    }
                }
{code}

Per the above two commits I have one question about `updateAssignmentMetadataIfNeeded(timer, false);` why the second parameter is false? Per my understanding when it's false, actually it's same as before `updateAssignmentMetadataIfNeeded(time.timer(0L));` 
I've tested w/ true, it looks fine for us and I'm thinking when it's w/ true, the behavior is similar to [the commit|https://github.com/apache/kafka/pull/8934/commits/333a967ec22ea22babf32b18349b76b6552a2fac].;;;","12/Jul/20 19:10;guozhang;[~zhowei] Just to clarify are you working with [~seanguo] on the same issue?

The rationale for the final fix is that we only need `timer.timer(0L)` for join-group, but not for others, for example even if the flag is set to false we would still use the original timer trying to discover the coordinator etc, because our setting was based on the observation that when the coordinator is not available, we are spending busy loops looking for it.

From your description, your case actually is not the same as my setup or [~neowu0]'s, i.e. the coordinator is fine, but all the partitions are revoked and hence you have none to fetch from while looping for the join-group request to complete. I've prepared a new PR that adds the fetchable logic back in a more efficient way, LMK if it works for you: https://github.com/apache/kafka/pull/9011;;;","14/Jul/20 00:20;zhowei;[~guozhang], yes, [~seanguo] and I are working on the same issue.
I've verified PR:https://github.com/apache/kafka/pull/9011. looks like the CPU issue has gone away, but the new consumer spends much more long time joining group than before, it's about 2mins.;;;","17/Jul/20 19:01;guozhang;[~zhowei] [~seanguo] I tried to reproduce your high CPU with {{three brokers and 10 consumers, and restart one of consumers}} but I failed to do that locally.

I've prepared a patch just improving on some log4j entries as we suspect your issue maybe related to heartbeats: https://github.com/apache/kafka/pull/9038 Could you try it out while reproducing the issue, and share your logs (you'd have to enable it to at least ERROR level) here so we can further understand the root cause?;;;","22/Jul/20 03:08;zhowei;[~guozhang] refer to the attached file (consumer5.log.2020-07-22.log), the last re-join happened at "" 10:52:41.247 [pool-1-thread-1] INFO  o.a.k.c.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer5, groupId=consumerGroupId] (Re-)joining group"" because there is one consumer trying to join. interesting, this time looks better than before, but still spend more than one min.;;;","22/Jul/20 22:44;aklochkov;Can somebody please clarify if this defect affects consumers if we don't use the new cooperative protocol? We're considering whether to upgrade to 2.4.1 or 2.5.0 and it looks like this particular defect makes staying on 2.4.1 a better idea. Thanks!;;;","23/Jul/20 16:45;guozhang;[~zhowei] Did your run include both the log4j improvement and the other PR depending on fetchable partitions to do long polling?;;;","23/Jul/20 16:49;guozhang;BTW I found that the main latency during rebalance is on discovering the coordinator while we keep getting ""Join group failed with org.apache.kafka.common.errors.DisconnectException"" and it kept retrying for about a minute. But I think you did not shutdown the broker in your experiment, is there anything else happening that cause that broker node to be not reachable?;;;","24/Jul/20 09:00;zhowei;[~guozhang] I'm using logback and PR: https://github.com/apache/kafka/pull/9011 included as well.  yeah, just to restart consumer w/o broker change, both brokers &consumers were running on my local laptop, not found any other issue.;;;","25/Jul/20 23:27;ijuma;[~zhowei] So you're saying you see the issue when restarting the consumers, not when restarting the brokers?;;;","26/Jul/20 08:38;zhowei;[~ijuma] correct, just while restarting consumer, not broker restart.;;;","28/Jul/20 00:18;guozhang;What I did not see from my local run is the following:

{code}
Join group failed with org.apache.kafka.common.errors.DisconnectException
{code}

Which indicates that the socket connecting to the brokers cannot be established, and the consumer has to retry discovering (the same) broker, and then re-connect to it, and somehow after one minute or two the issue goes away itself. If you did not restart the broker during that time, then I can only think of transient network issues..

Actually, could you try only patching the logback PR but not the 9011 PR (i.e. let it to falls into busy loop) and upload the logs so I can also check what's causing the busy loops as well?;;;","04/Aug/20 07:13;zhowei;The long time joining group is related to *max.poll.records*, looks like rejoin is trigged in *poll()*, it means only when processing complete and *poll()* be called. And *DisconnectException* should not be a real socket error, before leader consumer issues a rejoin request, looks like rejoin request from other consumers will be failed w/ *DisconnectException*.;;;","13/Aug/20 13:53;zhowei;Kafka clients v2.6.0 is already released, but high cpu issue doesn't go away for our scenario and find there is a significant difference between v2.6.0 and PR #9011:

[Kafka branch2.6|https://github.com/apache/kafka/blob/fe0279026bd3d854a7291f5bf99503469e900038/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1230]
{code:java}
if (includeMetadataInTimeout) {
    // try to update assignment metadata BUT do not need to block on the timer for join group
    updateAssignmentMetadataIfNeeded(timer, false);
} else {
    ...
}
{code}

[PR #9011|https://github.com/apache/kafka/pull/9011]
{code:java}
if (includeMetadataInTimeout) {
    // try to update assignment metadata;
    // do not need to block on the timer for join group if we have any fetchable partitions
    updateAssignmentMetadataIfNeeded(timer, !subscriptions.hasAnyFetchablePartitions());
} 
{code}
;;;","13/Aug/20 14:41;ijuma;What's in PR #9011 was meant to help debug the remaining issue, we know it was not merged. I reopened the Jira to avoid confusion. [~guozhang], thoughts on the remaining issue?;;;","19/Aug/20 19:26;guozhang;I think I'd need more information to further investigate this issue. [~zhowei] could you apply https://github.com/apache/kafka/pull/9038 only (this is for improved log4j entries) on top of 2.6 to reproduce this issue and then upload the enhanced log file?;;;","20/Aug/20 01:52;zhowei;[~guozhang] sure, I'll porting PR #9038 on 2.6.0 and share log with u. thx.
please refer to the attached file: consumer3.log.2020-08-20.log;;;","25/Aug/20 00:42;guozhang;[~zhowei] Thanks for the new log files, it has been very helpful for me to nail down the root causes and I will refine an existing WIP PR https://github.com/apache/kafka/pull/8834 as a final fix for this. Please stay tuned.;;;","26/Aug/20 01:36;guozhang;[~zhowei] could you try out https://github.com/apache/kafka/pull/8834 and lmk if it works fixing the issue now.;;;","26/Aug/20 09:38;zhowei;[~guozhang] cool, I'll try it later, thanks;;;","31/Aug/20 06:39;zhowei;[~guozhang] I've tested against PR #8834, it works fine for our scenario. appreciate.;;;","01/Sep/20 00:17;zhowei;[~guozhang] one more question about PR #8834, whether or not *GroupCoordinator* changes is mandatory. I mean Kafka server changes should be more expensive that clients.;;;","01/Sep/20 03:35;guozhang;Hello [~zhowei] that broker-side change is not mandatory, I just included that part to make the whole PR complete, but it is not a necessary change for your situation.;;;","01/Sep/20 23:57;zhowei;[~guozhang] thanks, I've tested clients changes in PR #8834 against legacy Kafka server, it works fine as well. thanks for your response. ;;;","04/Sep/20 16:03;davispw;I'd like to see the title of this bug clarified: It is worse than just ""High CPU usage"".  I have a couple of Kafka Streams apps with a high number of tasks/threads and this issue is causing infinite rebalance loops where the entire *cluster stops processing and cannot successfully rebalance*.  This causes hard downtime.  I've had to roll back to 2.4.1.

Edit: clarification: tested with 2.6.0.  Have not tested 2.5.

I'm currently working on building the patch, will test.

 

Late to the party since you've already got the fix in progress, but in case it helps, I'd like to share what I'm seeing:

The rebalance failures seems to be associated the TimeoutExceptions, DisconnectionExceptions and other side effects as noted in earlier comments.  When many StreamThreads are all spinning, then each time a rebalance is attempted, when there are a large number of threads it is likely that _some_ thread will fail, and the rebalance never succeeds.  The downward spiral begins as ConsumerThreads become ""fenced"" and it triggers a full (not incremental) rebalance, and eventually all data flow gets blocked.  I've tried different combinations of session.timeout.ms, rebalance.timeout.ms, max.poll.time.ms, default.api.timeout.ms (as recommended in the text of the timeout exceptions) to no avail.

Of my applications, the ones that are affected include
 * one stateless app with num.stream.threads=24.  With more than 1 instance (2-4x=48-96 threads), it will often never rebalance correctly, or only after multiple attempts (30+ minutes).  
 * one stateful app with 36 partitions of large-ish (500MB-1GB each) state stores which can take a while to restore.  This app successfully starts if I shut down all instances, delete state stores, set initial rebalance delay, and start all up simultaneously – but if any instance restarts or I attempt to scale up later, then rebalance will never succeed.  Additionally, when state stores are reassigned, there are ""LockExceptions"" (DEBUG level logs) in a tight loop, and the state stores fail to be closed cleanly, which forces the restore process to begin all over again.  The only way I can successfully do a rolling restart is if I use static membership and increase the session timeout.  If there is only a single instance of the app, then it works with no problems (but this is not a solution as I need multiple instances for scale).

Other side effects: the tight loop logs several DEBUG logs, which filled up log storage and caused pod evictions, which caused state stores to become invalid and restore (workaround: disable this logging).

Additionally, have seen the following exceptions sporadically, not sure if these are separate bugs:

{{2020-08-31T00:40:47.786Z ERROR Uncaught stream processing error! KafkaStreamsConfiguration java.lang.IllegalStateException: There are insufficient bytes available to read assignment from the sync-group response (actual byte size 0) , this is not expected; it is possible that the leader's assign function is buggy and did not return any assignment for this member, or *because static member is configured and the protocol is buggy* hence did not get the assignment for this member}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:367)}}
 {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:440)}}
 {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359)}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:513)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1268)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1230)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:766)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:624)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:551)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:510)}}

{{2020-09-03T15:53:17.524Z ERROR Uncaught stream processing error! KafkaStreamsConfiguration java.lang.IllegalStateException: Active task 3_0 should have been suspended}}
 {{    at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:281)}}
 {{    ... 13 common frames omitted}}
 {{Wrapped by: java.lang.RuntimeException: Unexpected failure to close 1 task(s) [[3_0]]. First unexpected exception (for task 3_0) follows.}}
 {{    at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:349)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1428)}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:279)}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:421)}}
 {{    ... 10 common frames omitted}}
 {{Wrapped by: org.apache.kafka.common.KafkaException: User rebalance callback throws an error}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:436)}}
 {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:440)}}
 {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359)}}
 {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:513)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1268)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1230)}}
 {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:766)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:628)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:551)}}
 {{    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:510)}};;;","08/Sep/20 22:31;ableegoldman;Hey [~davispw],

It looks like you might have run into a few distinct issues: the rebalancing problems, the ""insufficient bytes available"" IllegalStateException, and the ""Active task 3_0 should have been suspended"" IllegalStateException.

The rebalancing seems to point to this issue, as the full fix did not make it into 2.6.0 in time. It would be great if you could test out the patch and see if that helps (building from [pull/8834|https://github.com/apache/kafka/pull/8834] specifically, which is not yet merged). The patch I linked also includes a fix for KAFKA-10122, another cause of unnecessary rebalances.

For the two IllegalStateException issues, could you open separate tickets? They seem unrelated to this, and to each other, but definitely merit a closer look. Any logs you have from the time of the exceptions would help a lot. 

Thanks!;;;","11/Sep/20 01:04;guozhang;The PR has been merged to trunk and 2.6;;;","24/Sep/20 23:09;zhowei;[~guozhang] looks the fix version is 2.7.0 &2.6.1, do u know when to release them? thanks.;;;","27/Sep/20 22:04;guozhang;I don't think there's a concrete plan for 2.6.1 yet, for 2.7.0 it is planned for Nov.;;;"
Cannot compress messages in destination cluster with MM2,KAFKA-10133,13310503,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,yangguo1220,steveatbat,steveatbat,09/Jun/20 23:56,04/Sep/20 19:05,13/Jul/23 09:17,04/Sep/20 17:26,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,mirrormaker,,,,,0,,,,,"When configuring mirrormaker2 using kafka connect, it is not possible to configure things such that messages are compressed in the destination cluster. Dump Log shows that batching is occuring, but no compression. If this is possible, then this is a documentation bug, because I can find no documentation on how to do this.

baseOffset: 4208 lastOffset: 4492 count: 285 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 2 isTransactional: false isControl: false position: 239371 CreateTime: 1591745894859 size: 16362 magic: 2 compresscodec: NONE crc: 1811507259 isvalid: true

 

 ",kafka 2.5.0 deployed via the strimzi operator 0.18,steveatbat,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 21:35:12 UTC 2020,,,,,,,,,,"0|z0fov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 20:57;steveatbat;Got this working finally. compression.type has to be set on the cluster config (kafka connect worker config). Not on the kafkaconnect cluster as a producer override (which seems like it should work from the documentation but doesn't). Overall this was extremely confusing and improved documentation would save a ton of hassle.;;;","24/Aug/20 06:17;yangguo1220;[~steveatbat] My experiment seems to show different conclusion.

In order to preserve the compression type from upstream producer, in MM2 config file, I override the producer config, such as,

{code:java}
<target_cluster_alias>.producer.compression.type = gzip
{code}

Then run this command on the target cluster to verify:

{code:java}
/opt/kafka/bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --files /kafka/kafka-logs-kafka2-0/primary.test-0/00000000000000000000.log
{code}

Output:

{code:java}
Starting offset: 0
offset: 0 position: 0 CreateTime: 1598249096409 isvalid: true keysize: 4 valuesize: 6 magic: 2 compresscodec: GZIP producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: []
offset: 1 position: 0 CreateTime: 1598249106432 isvalid: true keysize: 4 valuesize: 6 magic: 2 compresscodec: GZIP producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: []
offset: 2 position: 0 CreateTime: 1598249158144 isvalid: true keysize: 4 valuesize: 6 magic: 2 compresscodec: GZIP producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: []
{code}

The output shows that the messages replicated to the target cluster are compressed with GZIP.

So I am wondering what exact config you use at kafka connect worker level to make compression work

;;;","24/Aug/20 13:52;steveatbat;That is how I was confirming compression as well. The example you gave is the same as what I said above. It sets the property on the kafka connect worker.  Not in the sourceconnector/checkpointconnector etc. ;;;","24/Aug/20 16:03;yangguo1220;Great. Then I will probably make a pr to clarify some use cases like this. Thanks;;;","24/Aug/20 16:51;steveatbat;That would be incredibly helpful. Just an example of where this setting goes would save a lot of trouble!;;;","24/Aug/20 16:58;yangguo1220;There is no bug in the code, but need some efforts on doc to clarify on where some frequently used configs should be set.;;;","24/Aug/20 21:35;yangguo1220;https://github.com/apache/kafka/pull/9215;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression resetting offsets in consumer when fetching from old broker,KAFKA-10123,13310292,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mumrah,hachikuji,hachikuji,09/Jun/20 05:33,18/Jun/20 05:33,13/Jul/23 09:17,18/Jun/20 05:33,,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,,,,,,0,,,,,"We saw this error in system tests:
{code}
java.lang.NullPointerException
        at org.apache.kafka.clients.consumer.internals.Fetcher.prepareFetchRequests(Fetcher.java:1111)
        at org.apache.kafka.clients.consumer.internals.Fetcher.sendFetches(Fetcher.java:246)
        at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1296)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216)
        at kafka.tools.ConsoleConsumer$ConsumerWrapper.receive(ConsoleConsumer.scala:437)
        at kafka.tools.ConsoleConsumer$.process(ConsoleConsumer.scala:103)
        at kafka.tools.ConsoleConsumer$.run(ConsoleConsumer.scala:77)
        at kafka.tools.ConsoleConsumer$.main(ConsoleConsumer.scala:54)
        at kafka.tools.ConsoleConsumer.main(ConsoleConsumer.scala)
{code}

The logs showed that the consumer was in the middle of an offset reset when this happened. We changed the validation logic in KAFKA-9724 to include the following check with the intent of skipping validation for old brokers:
{code}
            NodeApiVersions nodeApiVersions = apiVersions.get(leaderAndEpoch.leader.get().idString());
            if (nodeApiVersions == null || hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {
                return assignedState(tp).maybeValidatePosition(leaderAndEpoch);
            } else {
                // If the broker does not support a newer version of OffsetsForLeaderEpoch, we skip validation
                completeValidation(tp);
                return false;
            }
{code}

The problem seems to be the shortcut call to `completeValidation`, which executes the following logic:
{code}
            if (hasPosition()) {
                transitionState(FetchStates.FETCHING, () -> this.nextRetryTimeMs = null);
            }
{code}

We should be protected by the call to `hasPosition` here, but in the case of the `AWAIT_RESET` state, we are incorrectly returning true. This causes us to enter the `FETCHING` state without a position, which ultimately leads to the NPE.",,ableegoldman,chia7712,hachikuji,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10119,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 20:51:35 UTC 2020,,,,,,,,,,"0|z0fnk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 20:51;vvcephei;I've just also added this as a blocker for 2.5.1, because the commit for KAFKA-9724 is also included in 2.5.1, which alone is enough reason to include the fix in 2.5.1.

We also think it might have been the root cause of the Streams EOS soak cluster losing all its threads over the course of a week.

I'm including this information in case it helps...

Here's the stack trace:
{code:java}
[2020-06-03 02:19:01,503] ERROR [stream-soak-test-d8d40c42-1175-4611-994a-ff3fde11cef2-StreamThread-1] stream-thread [stream-soak-test-d8d40c42-1175-4611-994a-ff3fde11cef2-StreamThread-1] Error caught during partition revocation, will abort the current process and re-throw at the end of rebalance:  (org.apache.kafka.streams.processor.internals.StreamThread)

org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-d8d40c42-1175-4611-994a-ff3fde11cef2-StreamThread-1] failed to suspend stream tasks
        at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:253)
        at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:116)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:297)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:393)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:439)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:490)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216)
        at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:853)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:745)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670)
Caused by: org.apache.kafka.streams.errors.StreamsException: org.apache.kafka.clients.consumer.NoOffsetForPartitionException: Undefined offset with no reset policy for partitions: [logs.json.zookeeper-0, logs.operator-0, logs.kubernetes-0, logs.json.kafka-0]
        at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:512)
        at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:632)
        at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:601)
        at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendRunningTasks(AssignedStreamsTasks.java:146)
        at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendOrCloseTasks(AssignedStreamsTasks.java:129)
        at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:246)
        ... 13 more
Caused by: org.apache.kafka.clients.consumer.NoOffsetForPartitionException: Undefined offset with no reset policy for partitions: [logs.json.zookeeper-0, logs.operator-0, logs.kubernetes-0, logs.json.kafka-0]
        at org.apache.kafka.clients.consumer.internals.SubscriptionState.resetMissingPositions(SubscriptionState.java:658)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateFetchPositions(KafkaConsumer.java:2392)
        at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1764)
        at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1723)
        at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:503)
        ... 18 more{code}
Not the same error, but it seems related-ish.

Note:

Streams/Client version and commit: 2.5.x {color:#e01e5a}5842d2f3{color}

Broker version and commit: kafka_2.12-2.5.x bad93b775

(the previous soak test with the clients on commit {color:#e01e5a}b59f880f{color} did not have this error);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DescribeLogDirsResult exposes internal classes,KAFKA-10120,13310174,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,08/Jun/20 15:51,03/Aug/20 21:13,13/Jul/23 09:17,30/Jul/20 13:10,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"DescribeLogDirsResult (returned by AdminClient#describeLogDirs(Collection)) exposes a number of internal types:
 * {{DescribeLogDirsResponse.LogDirInfo}}
 * {{DescribeLogDirsResponse.ReplicaInfo}}
 * {{Errors}}

{{}}",,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-08 15:51:05.0,,,,,,,,,,"0|z0fmu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogTruncationException sets fetch offsets incorrectly,KAFKA-10113,13309886,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,06/Jun/20 19:04,19/Jun/20 01:13,13/Jul/23 09:17,19/Jun/20 01:13,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"LogTruncationException, which extends OffsetOutOfRangeException, takes the divergent offsets in the constructor. These are the first offsets known to diverge from what the consumer read. These are then passed to the OffsetOutOfRangeException incorrectly as the out of range fetch offsets. ",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-06 19:04:29.0,,,,,,,,,,"0|z0fl20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkTaskContext.errantRecordReporter() added in KIP-610 should be a default method,KAFKA-10111,13309766,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,rhauch,rhauch,05/Jun/20 16:30,05/Jun/20 20:56,13/Jul/23 09:17,05/Jun/20 20:54,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"[KIP-610|https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors] added a new `errantRecordReporter()` method to `SinkTaskContext`, but the KIP didn't make this method a default method. While the AK project can add this method to all of its implementations (actual and test), other projects such as connector projects might have their own mock implementations just to help test the connector implementation. That means when those projects upgrade, they'd get compilation problems for their own implementations of `SinkTaskContext`.

Making this method default will save such problems with downstream projects, and is actually easy since the method is already defined to return null if no reporter is configured.",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9971,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 20:54:44 UTC 2020,,,,,,,,,,"0|z0fkbc:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"05/Jun/20 20:54;rhauch;Merged to `trunk` and backported to the `2.6` branch where the `SinkTaskContext.errantRecordReporter()` method was first introduced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectDistributed fails with NPE when Kafka cluster has no ID,KAFKA-10110,13309753,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,rhauch,rhauch,rhauch,05/Jun/20 15:01,05/Jun/20 20:54,13/Jul/23 09:17,05/Jun/20 20:54,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"When a Connect worker starts, recent changes from [KIP-606|https://cwiki.apache.org/confluence/display/KAFKA/KIP-606%3A+Add+Metadata+Context+to+MetricsReporter] / KAFKA-9960 attempt to put the Kafka cluster ID into the new KafkaMetricsContext. But the Kafka cluster ID can be null, resulting in an NPE shown in the following log snippet:
{noformat}
[2020-06-04 15:01:02,900] INFO Kafka cluster ID: null (org.apache.kafka.connect.util.ConnectUtils)
...
[2020-06-04 15:01:03,271] ERROR Stopping due to error (org.apache.kafka.connect.cli.ConnectDistributed)
java.lang.NullPointerException
    at org.apache.kafka.common.metrics.KafkaMetricsContext.lambda$new$0(KafkaMetricsContext.java:48)
    at java.util.HashMap.forEach(HashMap.java:1289)
    at org.apache.kafka.common.metrics.KafkaMetricsContext.<init>(KafkaMetricsContext.java:48)
    at org.apache.kafka.connect.runtime.ConnectMetrics.<init>(ConnectMetrics.java:100)
    at org.apache.kafka.connect.runtime.Worker.<init>(Worker.java:135)
    at org.apache.kafka.connect.runtime.Worker.<init>(Worker.java:121)
    at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:111)
    at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:78)
{noformat}",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 20:54:41 UTC 2020,,,,,,,,,,"0|z0fk8o:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"05/Jun/20 20:54;rhauch;Merged to `trunk` and backported to the `2.6` branch where the `KafkaMetricsContext` class was first introduced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-acls.sh/AclCommand opens multiple AdminClients,KAFKA-10109,13309728,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,05/Jun/20 13:26,09/Jul/20 09:29,13/Jul/23 09:17,09/Jul/20 09:29,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,tools,,,,,0,,,,,"{{AclCommand.AclCommandService}} uses {{withAdminClient(opts: AclCommandOptions)(f: Admin => Unit)}} to abstract the execution of an action using an {{AdminClient}} instance. Unfortunately the use of this method in implemeting {{addAcls()}} and {{removeAcls()}} calls {{listAcls()}}. This causes the creation of a second {{AdminClient}} instance. When the {{--command-config}} option has been used to specify a {{client.id}} for the Admin client, the second instance  fails to register an MBean, resulting in a warning being logged.

{code}
./bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config config/broker_connection.conf.reproducing --add --allow-principal User:alice --operation Describe --topic 'test' --resource-pattern-type prefixed
Adding ACLs for resource `ResourcePattern(resourceType=TOPIC, name=test, patternType=PREFIXED)`: 
 	(principal=User:alice, host=*, operation=DESCRIBE, permissionType=ALLOW) 

[2020-06-03 18:43:12,190] WARN Error registering AppInfo mbean (org.apache.kafka.common.utils.AppInfoParser)
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=administrator_data
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:500)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:444)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:59)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39)
	at kafka.admin.AclCommand$AdminClientService.withAdminClient(AclCommand.scala:105)
	at kafka.admin.AclCommand$AdminClientService.listAcls(AclCommand.scala:146)
	at kafka.admin.AclCommand$AdminClientService.$anonfun$addAcls$1(AclCommand.scala:123)
	at kafka.admin.AclCommand$AdminClientService.$anonfun$addAcls$1$adapted(AclCommand.scala:116)
	at kafka.admin.AclCommand$AdminClientService.withAdminClient(AclCommand.scala:108)
	at kafka.admin.AclCommand$AdminClientService.addAcls(AclCommand.scala:116)
	at kafka.admin.AclCommand$.main(AclCommand.scala:78)
	at kafka.admin.AclCommand.main(AclCommand.scala)
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=test, patternType=PREFIXED)`: 
 	(principal=User:alice, host=*, operation=DESCRIBE, permissionType=ALLOW)
{code}",,omkreddy,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 09:29:24 UTC 2020,,,,,,,,,,"0|z0fk34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 09:29;omkreddy;Issue resolved by pull request 8808
[https://github.com/apache/kafka/pull/8808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source node references not updated after rebuilding topology,KAFKA-10102,13309488,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,04/Jun/20 16:33,09/Jun/20 16:36,13/Jul/23 09:17,08/Jun/20 23:10,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Luckily this bug was caught by RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterReassignment – we saw it fail with an NPE during SourceNode#deserializeKey, implying that the key deserializer was null which in turns implies that the source node was never initialized.

This can happen when a task is updated with new regex matched topic partitions. In order to update the topology with the new input partitions, we actually just create an entirely new topology from scratch including building new source node objects. We then re/initialize this new topology once the task is resumed.

The problem is that the task's RecordQueues save a reference to the corresponding source node, and use this to pass polled records into the topology. But the RecordQueues aren't updated with the newly built source nodes and still point to the original nodes.

If the task had not completed restoration before being updated with new partitions, it would never have initialized the original topology or source nodes, resulting in an NPE when the RecordQueue passes a record to the old, uninitialized source node.

This is the only specific known bug, but I haven't checked the entire code base so it's possible there are other node references saved that might result in bugs. We should try and avoid rebuilding an entirely new topology if at all possible, and see if we can just update the input partitions only where necessary",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-04 16:33:42.0,,,,,,,,,,"0|z0fils:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
recovery point is advanced without flushing the data after recovery,KAFKA-10101,13309482,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,junrao,junrao,04/Jun/20 15:47,27/Feb/21 17:59,13/Jul/23 09:17,26/Feb/21 22:43,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"Currently, in Log.recoverLog(), we set recoveryPoint to logEndOffset after recovering the log segment. However, we don't flush the log segments after recovery. The potential issue is that if the broker has another hard failure, segments may be corrupted on disk but won't be going through recovery on another restart.

This logic was introduced in KAFKA-5829 since 1.0.0.",,chia7712,ijuma,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 15:46:27 UTC 2020,,,,,,,,,,"0|z0fikg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/20 15:49;junrao;[~ijuma] : Could you double check if this is an issue? Thanks.;;;","04/Jun/20 15:59;junrao;If this is an issue, one way to fix that is to simply not advance recoveryPoint after recovery.;;;","05/Jun/20 13:28;ijuma;[~junrao] That's a good point. I agree with you that it's probably simplest to remove the code that changes the recover point. The previous code had:

 
{code:java}
if(hasCleanShutdownFile) {
  this.recoveryPoint = activeSegment.nextOffset()
  return
}{code}
It has since been consolidated into the same path. We can probably keep that. I'll submit a PR.;;;","05/Jun/20 15:46;ijuma;Discussed with Jun a bit offline. This is a bit unlikely in practice as the hard failure has to happen after we write the recovery point (which happens in a scheduled thread) and before we roll/flush the truncated segment. Still, good to fix it. Also, it's easier to reason about if we only update the recovery point after we roll segments. While looking at the code, we noticed a few more edge cases. Will try to fix them at the same time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The stale ssl engine factory is not closed after reconfigure,KAFKA-10089,13309137,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,03/Jun/20 08:46,03/Jun/20 18:10,13/Jul/23 09:17,03/Jun/20 18:10,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"{code}
    @Override
    public void reconfigure(Map<String, ?> newConfigs) throws KafkaException {
        SslEngineFactory newSslEngineFactory = createNewSslEngineFactory(newConfigs);
        if (newSslEngineFactory != this.sslEngineFactory) {
            this.sslEngineFactory = newSslEngineFactory; // we should close the older one
            log.info(""Created new {} SSL engine builder with keystore {} truststore {}"", mode,
                    newSslEngineFactory.keystore(), newSslEngineFactory.truststore());
        }
    }
{code}",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-03 08:46:50.0,,,,,,,,,,"0|z0fgfs:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly throw LogTruncation exception from OffsetForLeaderEpoch future,KAFKA-10087,13309089,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,03/Jun/20 03:26,18/Jun/20 04:04,13/Jul/23 09:17,18/Jun/20 04:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"For OffsetForLeaderEpoch#onSuccess, we could throw either OffsetOutOfRange or LogTruncation exceptions, which are swallowed by the AsyncClient logic:



 
{code:java}
try { 
    future.complete(handleResponse(node, requestData, resp)); 
} catch (RuntimeException e) {
  if (!future.isDone()) { 
      future.raise(e); 
   }
}
{code}

 We should fix the exception case to throw it to the upstream. In the meantime, we should ensure that any discard exception case gets retried eventually for LeaderOffset call.

 ",,bchen225242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-03 03:26:24.0,,,,,,,,,,"0|z0fg54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compute lag correctly for optimized source changelogs,KAFKA-10085,13309019,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ableegoldman,ableegoldman,ableegoldman,02/Jun/20 19:09,11/Jun/20 15:33,13/Jul/23 09:17,11/Jun/20 15:33,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"During KIP-441 we originally decided to leave the special handling of optimized source changelogs as a potential future improvement, since over-estimating lag was acceptable.

 

But as always things have changed during the course of implementing this KIP, and the algorithm we ended up with requires accurate computation of lag. We should branch the lag computation in the assignor and use the correct end offset sum when computing lag",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-02 19:09:49.0,,,,,,,,,,"0|z0ffpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix failed testReassignmentWithRandomSubscriptionsAndChanges,KAFKA-10083,13308942,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,02/Jun/20 12:26,25/Nov/20 02:46,13/Jul/23 09:17,25/Nov/20 02:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/815/]

 
h3. Error Message

java.lang.AssertionError
h3. Stacktrace

java.lang.AssertionError at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor$PartitionMovements.getTheActualPartitionToBeMoved(AbstractStickyAssignor.java:836) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor$PartitionMovements.access$100(AbstractStickyAssignor.java:780) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.reassignPartition(AbstractStickyAssignor.java:699) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.reassignPartition(AbstractStickyAssignor.java:689) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.performReassignments(AbstractStickyAssignor.java:661) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.balance(AbstractStickyAssignor.java:597) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.generalAssign(AbstractStickyAssignor.java:352) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor.assign(AbstractStickyAssignor.java:85) at org.apache.kafka.clients.consumer.CooperativeStickyAssignor.assign(CooperativeStickyAssignor.java:64) at org.apache.kafka.clients.consumer.CooperativeStickyAssignorTest.verifyValidityAndBalance(CooperativeStickyAssignorTest.java:68) at org.apache.kafka.clients.consumer.internals.AbstractStickyAssignorTest.testReassignmentWithRandomSubscriptionsAndChanges(AbstractStickyAssignorTest.java:654)",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-02 12:26:27.0,,,,,,,,,,"0|z0ff8g:",9223372036854775807,,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix failed kafka.api.PlaintextConsumerTest.testMultiConsumerStickyAssignment,KAFKA-10082,13308884,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,02/Jun/20 07:09,03/Jun/20 01:27,13/Jul/23 09:17,03/Jun/20 01:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk14/runs/153/log/?start=0]

[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk8/runs/4596/log/?start=0]

[https://builds.apache.org/blue/rest/organizations/jenkins/pipelines/kafka-trunk-jdk11/runs/1523/log/?start=0]

 

kafka.api.PlaintextConsumerTest > testMultiConsumerStickyAssignment FAILED
 java.lang.AssertionError: Expected only two topic partitions that have switched to other consumers. expected:<9> but was:<14>
 at org.junit.Assert.fail(Assert.java:89)
 at org.junit.Assert.failNotEquals(Assert.java:835)
 at org.junit.Assert.assertEquals(Assert.java:647)
 at kafka.api.PlaintextConsumerTest.testMultiConsumerStickyAssignment(PlaintextConsumerTest.scala:929)",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-02 07:09:23.0,,,,,,,,,,"0|z0fevk:",9223372036854775807,,ableegoldman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execution failed for task ':clients:spotbugsMain' in recent build,KAFKA-10081,13308860,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,02/Jun/20 04:23,02/Jun/20 23:13,13/Jul/23 09:17,02/Jun/20 16:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,": Task failed with an exception.
 -----------
 * What went wrong:
 Execution failed for task ':clients:spotbugsMain'.
 > A failure occurred while executing com.github.spotbugs.snom.internal.SpotBugsRunnerForWorker$SpotBugsExecutor
 > Verification failed: SpotBugs execution thrown exception

 

check here:

[https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk8/detail/kafka-trunk-jdk8/4596/pipeline] 

[https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk11/detail/kafka-trunk-jdk11/1523/pipeline]

[https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk14/detail/kafka-trunk-jdk14/153/pipeline]

 

 ",,ableegoldman,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-02 04:23:02.0,,,,,,,,,,"0|z0feq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException after duplicate CompleteCommit append to transaction log,KAFKA-10080,13308849,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,02/Jun/20 02:47,03/Jun/20 17:59,13/Jul/23 09:17,03/Jun/20 17:59,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We noticed this exception in the logs:
{code}
java.lang.IllegalStateException: TransactionalId foo completing transaction state transition while it does not have a pending state                                                                            
        at kafka.coordinator.transaction.TransactionMetadata.$anonfun$completeTransitionTo$1(TransactionMetadata.scala:357)
        at kafka.coordinator.transaction.TransactionMetadata.completeTransitionTo(TransactionMetadata.scala:353)
        at kafka.coordinator.transaction.TransactionStateManager.$anonfun$appendTransactionToLog$3(TransactionStateManager.scala:595)                                                                                                                       
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.coordinator.transaction.TransactionMetadata.inLock(TransactionMetadata.scala:188)
        at kafka.coordinator.transaction.TransactionStateManager.$anonfun$appendTransactionToLog$15$adapted(TransactionStateManager.scala:587)                                                                                                              
        at kafka.server.DelayedProduce.onComplete(DelayedProduce.scala:126)
        at kafka.server.DelayedOperation.forceComplete(DelayedOperation.scala:70)
        at kafka.server.DelayedProduce.tryComplete(DelayedProduce.scala:107)
        at kafka.server.DelayedOperation.maybeTryComplete(DelayedOperation.scala:121)
        at kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:378)
        at kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:280)
        at kafka.cluster.DelayedOperations.checkAndCompleteAll(Partition.scala:122)
        at kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:1023)
        at kafka.cluster.Partition.updateFollowerFetchState(Partition.scala:740)
{code}

After inspection, we found that there were two CompleteCommit entries in the transaction state log which explains the failed transition. Indeed the logic for writing the CompleteCommit message does seem prone to race conditions.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 17:58:51 UTC 2020,,,,,,,,,,"0|z0feo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 17:58;hachikuji;Note that this seems to have been introduced in KAFKA-9777, which has not been part of any released version. I have merged the patch into the 2.6 branch, so it should not affect 2.6.0 either.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve thread-level stickiness of stateful tasks,KAFKA-10079,13308789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,01/Jun/20 19:46,10/Jun/20 15:13,13/Jul/23 09:17,10/Jun/20 15:13,,,,,,,,,,,,,,,,,,,,,,,2.6.0,2.7.0,,,,,,,streams,,,,,0,,,,,"Now that KAFKA-9501 is fixed, we should make sure that KIP-441 will work with in-memory stores.

Since we don't handoff stores between threads, any in-memory state will be wiped out if assigned to a different thread",,ableegoldman,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-01 19:46:10.0,,,,,,,,,,"0|z0feao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The user-defined ""predicate"" and ""negate"" are not removed from Transformation",KAFKA-10069,13308352,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,29/May/20 15:13,04/Jun/20 21:50,13/Jul/23 09:17,04/Jun/20 21:50,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"There are official configDef for both ""predicate"" and ""negate"" so we should remove user-defined configDef. However, current behavior does incorrect comparison so the duplicate key will destroy the following embed configDef.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-29 15:13:54.0,,,,,,,,,,"0|z0fbm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopologyTestDriver isn't taking record headers into account during de/serialization,KAFKA-10066,13308268,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,sdutry,sdutry,29/May/20 08:18,04/Jun/20 23:30,13/Jul/23 09:17,04/Jun/20 23:30,1.1.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,streams-test-utils,,,,,0,,,,,"When testing a Kafka stream we need the TopologyTestDriver.createOutputTopic to take record headers into account.

Is it possible to use the record headers when deserialising when using the TopologyTestDriver.createOutputTopic?

The only thing that needs to change is: 
{code:java}
final K key = keyDeserializer.deserialize(record.topic(), record.key());
final V value = valueDeserializer.deserialize(record.topic(), record.value());{code}
into: 
{code:java}
final K key = keyDeserializer.deserialize(record.topic(), record.headers(), record.key());
final V value = valueDeserializer.deserialize(record.topic(), record.headers(), record.value());{code}
 ",,ableegoldman,mjsax,sdutry,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 21:32:03 UTC 2020,,,,,,,,,,"0|z0fb3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/20 21:26;vvcephei;Hi [~sdutry] ,

Thanks for the report! It sounds like you have a pretty good idea of the fix; would you like to send a pull request?

Thanks,

-John;;;","29/May/20 23:31;mjsax;Technically, the bug is also in older version, ie, even since `TopologyTestDriver` was introduced.;;;","29/May/20 23:35;mjsax;Just saw your comment now, [~vvcephei] – already picked it up myself.;;;","30/May/20 16:13;vvcephei;Hey, @mjsax, since you’re already proposing a PR, what do you think about including this in 2.5.1?;;;","30/May/20 21:20;mjsax;Including it in 2.5.1 was my intention :) ;;;","30/May/20 21:32;vvcephei;Great! Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperation when querying cleaner metrics after shutdown,KAFKA-10063,13308158,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,hachikuji,hachikuji,28/May/20 21:27,09/Jun/20 16:31,13/Jul/23 09:17,08/Jun/20 23:47,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We have a few log cleaner metrics which iterate the set of cleaners. For example:

{code}
  newGauge(""max-clean-time-secs"", () => cleaners.iterator.map(_.lastStats.elapsedSecs).max.toInt)
{code}

It seems possible currently for LogCleaner metrics to get queried after shutdown of the log cleaner, which clears the `cleaners` collection. This can lead to the following error:
{code}
java.lang.UnsupportedOperationException: empty.max
	at scala.collection.IterableOnceOps.max(IterableOnce.scala:952)
	at scala.collection.IterableOnceOps.max$(IterableOnce.scala:950)
	at scala.collection.AbstractIterator.max(Iterator.scala:1279)
	at kafka.log.LogCleaner.kafka$log$LogCleaner$$$anonfun$new$9(LogCleaner.scala:132)
	at kafka.log.LogCleaner$$anonfun$4.value(LogCleaner.scala:132)
	at kafka.log.LogCleaner$$anonfun$4.value(LogCleaner.scala:132)
{code}",,chia7712,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 31 14:50:05 UTC 2020,,,,,,,,,,"0|z0faew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/20 14:50;chia7712;[~hachikuji] Could I take over this issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test `ReassignPartitionsIntegrationTest .testCancellation`,KAFKA-10061,13308142,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,28/May/20 20:25,29/May/20 19:18,13/Jul/23 09:17,29/May/20 19:18,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We have seen this a few times:
{code}
org.scalatest.exceptions.TestFailedException: Timed out waiting for verifyAssignment result VerifyAssignmentResult(Map(foo-0 -> PartitionReassignmentState(List(0, 1, 3, 2),List(0, 1, 3),false), baz-1 -> PartitionReassignmentState(List(0, 2, 3, 1),List(0, 2, 3),false)),true,Map(),false).  The latest result was VerifyAssignmentResult(Map(foo-0 -> PartitionReassignmentState(ArrayBuffer(0, 1, 3),List(0, 1, 3),true), baz-1 -> PartitionReassignmentState(ArrayBuffer(0, 2, 3),List(0, 2, 3),true)),false,HashMap(),false)
{code}

It looks like the reassignment is completing earlier than the test expects. See the following from the log:

{code}
Successfully started partition reassignments for baz-1,foo-0
==> verifyAssignment(adminClient, jsonString={""version"":1,""partitions"":[{""topic"":""foo"",""partition"":0,""replicas"":[0,1,3],""log_dirs"":[""any"",""any"",""any""]},{""topic"":""baz"",""partition"":1,""replicas"":[0,2,3],""log_dirs"":[""any"",""any"",""any""]}]})
Status of partition reassignment:
Reassignment of partition baz-1 is still in progress.
Reassignment of partition foo-0 is complete.
{code}

A successful run looks like this:
{code}
Successfully started partition reassignments for baz-1,foo-0
==> verifyAssignment(adminClient, jsonString={""version"":1,""partitions"":[{""topic"":""foo"",""partition"":0,""replicas"":[0,1,3],""log_dirs"":[""any"",""any"",""any""]},{""topic"":""baz"",""partition"":1,""replicas"":[0,2,3],""log_dirs"":[""any"",""any"",""any""]}]})
Status of partition reassignment:
Reassignment of partition baz-1 is still in progress.
Reassignment of partition foo-0 is still in progress.
{code}",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-28 20:25:38.0,,,,,,,,,,"0|z0fabc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer metadata may use outdated groupSubscription that doesn't contain newly subscribed topics,KAFKA-10056,13307988,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,28/May/20 10:11,29/May/20 17:52,13/Jul/23 09:17,29/May/20 17:52,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,consumer,,,,,0,,,,,"From [~hai_lin] in KAFKA-9181:

I did notice some issue after this patch, here is what I observe.

Consumer metadata might skip first metadata update, cause grouopSubscription is not reset. In my case, the consumer coordinator thread hijack the update by calling newMetadataRequestAndVersion with outdated groupSubscription before joinPrepare() happen. The groupSubscription will get reset later and it will eventually get update later, and this won't be an issue for initial consumer subscribe(since the groupSubscription is empty anyway), but it might happen the following subscribe when groupSubscription is not empty. This will create a discrepancy between subscription and groupSubscription, if any new metadata request happened in between, metadataTopics will return outdated group information. 

 
h4. The happy path
 * Consumer call subscribe > Update {{needUpdated}}, bump up {{requestVersion}} and update {{subscription}} in {{SubscriptionState}} > {{prepareJoin()}} was call in first {{poll()}} to reset {{groupSubscription}} -> next time when metadata update was call and {{metadataTopics()}} returns {{subscription}} since {{groupSubscription}} is empty -> update call issue to broker to fetch partition information for new topic

h4. In our case
 * Consumer call subscribe > Update {{needUpdated}}, bump up {{requestVersion}} and update {{subscription}}(not {{groupSubscription}}) in {{SubscriptionState}} > Consumer Coordinator heartbeat thread call metadata request and {{SubscriptionState}} gave away the current requestVersion and outdated {{groupSubscription}} > making request for metadata update with outdated subscription -> request comes back to client and since {{requestVersion}} is up to latest, it reset {{needUpdated}} flag -> {{joinPrepare()}} called and reset {{groupSubscription}} > no new metadata update request follow cause {{needUpdated}} was reset -> metadata request will happen when {{metadata.max.age}} reaches.",,hai_lin,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 17:18:50 UTC 2020,,,,,,,,,,"0|z0f9dc:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"28/May/20 11:33;rsivaram;Submitted PR: https://github.com/apache/kafka/pull/8739;;;","28/May/20 19:20;hai_lin;Thanks for the patch [~rsivaram], are we going to back port this to 2.4? I was running 2.4.1 when I ran into this issue.;;;","29/May/20 09:11;rsivaram;[~hai_lin] Yes, we will backport the fix to all the branches where KAFKA-9181 was applied.;;;","29/May/20 17:18;hai_lin;Thanks, much appreciated. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test InternalTopicsIntegrationTest.testCreateInternalTopicsWithFewerReplicasThanBrokers,KAFKA-10052,13307807,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,kkonstantine,ableegoldman,ableegoldman,27/May/20 17:11,28/May/20 07:05,13/Jul/23 09:17,28/May/20 07:05,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,flaky-test,integration-test,,,"h3. Stacktrace

 
{code:java}
java.lang.NullPointerException
  at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertTopicSettings(EmbeddedConnectClusterAssertions.java:207) 
  at org.apache.kafka.connect.integration.InternalTopicsIntegrationTest.assertInternalTopicSettings(InternalTopicsIntegrationTest.java:148) 
  at
org.apache.kafka.connect.integration.InternalTopicsIntegrationTest.testCreateInternalTopicsWithFewerReplicasThanBrokers(InternalTopicsIntegrationTest.java:118){code}
 

 

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6539/testReport/junit/org.apache.kafka.connect.integration/InternalTopicsIntegrationTest/testCreateInternalTopicsWithFewerReplicasThanBrokers/]",,ableegoldman,kkonstantine,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 07:05:36 UTC 2020,,,,,,,,,,"0|z0f8aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 18:55;mjsax;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/677/testReport/junit/org.apache.kafka.connect.integration/InternalTopicsIntegrationTest/testCreateInternalTopicsWithFewerReplicasThanBrokers/];;;","27/May/20 21:13;mjsax;Different test method: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6554/testReport/junit/org.apache.kafka.connect.integration/InternalTopicsIntegrationTest/testCreateInternalTopicsWithDefaultSettings/]
{quote}java.lang.NullPointerException at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertTopicSettings(EmbeddedConnectClusterAssertions.java:207) at org.apache.kafka.connect.integration.InternalTopicsIntegrationTest.assertInternalTopicSettings(InternalTopicsIntegrationTest.java:148) at org.apache.kafka.connect.integration.InternalTopicsIntegrationTest.testCreateInternalTopicsWithDefaultSettings(InternalTopicsIntegrationTest.java:77){quote};;;","28/May/20 07:05;kkonstantine;This must have been improved now, but please reopen if you notice failures happening again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka_log4j_appender.py broken on JDK11,KAFKA-10050,13307671,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nizhikov,nizhikov,nizhikov,27/May/20 09:39,28/May/20 07:30,13/Jul/23 09:17,28/May/20 07:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"kafka_log4j_appender.py brokern on jdk11

{noformat}
[INFO:2020-05-27 02:31:27,662]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Data: None
================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.7
session_id:       2020-05-27--002
run time:         1 minute 41.177 seconds
tests run:        4
passed:           0
failed:           4
ignored:          0
================================================================================
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT
status:     FAIL
run time:   27.509 seconds


    KafkaLog4jAppender-0-140270269628496-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'

Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/tools/log4j_appender_test.py"", line 84, in test_log4j_appender
    self.appender.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 72, in wait
    self._propagate_exceptions()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 98, in _propagate_exceptions
    raise Exception(self.errors)
Exception: KafkaLog4jAppender-0-140270269628496-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'


--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL
status:     FAIL
run time:   28.121 seconds


    KafkaLog4jAppender-0-140270269498000-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'

Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/tools/log4j_appender_test.py"", line 84, in test_log4j_appender
    self.appender.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 72, in wait
    self._propagate_exceptions()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 98, in _propagate_exceptions
    raise Exception(self.errors)
Exception: KafkaLog4jAppender-0-140270269498000-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'


--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT
status:     FAIL
run time:   22.451 seconds


    KafkaLog4jAppender-0-140270597865104-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'

Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/tools/log4j_appender_test.py"", line 84, in test_log4j_appender
    self.appender.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 72, in wait
    self._propagate_exceptions()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 98, in _propagate_exceptions
    raise Exception(self.errors)
Exception: KafkaLog4jAppender-0-140270597865104-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'


--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL
status:     FAIL
run time:   22.986 seconds


    KafkaLog4jAppender-0-140270597865104-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'

Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/tools/log4j_appender_test.py"", line 84, in test_log4j_appender
    self.appender.wait()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 72, in wait
    self._propagate_exceptions()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 98, in _propagate_exceptions
    raise Exception(self.errors)
Exception: KafkaLog4jAppender-0-140270597865104-worker-1: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/background_thread.py"", line 36, in _protected_worker
    self._worker(idx, node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 42, in _worker
    cmd = self.start_cmd(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka_log4j_appender.py"", line 48, in start_cmd
    cmd = fix_opts_for_new_jvm(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm
    if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0:
AttributeError: 'ClusterNode' object has no attribute 'version'


--------------------------------------------------------------------------------
ducker-ak test failed

{noformat}",,nizhikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 11:30:17 UTC 2020,,,,,,,,,,"0|z0f7go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 11:28;nizhikov;Results with the patch:
{noformat}
nizhikov@kafka:~/kafka$ TC_PATHS=""tests/kafkatest/tests/tools/log4j_appender_test.py"" bash tests/docker/run_tests.sh

> Configure project :
Building project 'core' with Scala version 2.13.2
Building project 'streams-scala' with Scala version 2.13.2

Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.
Use '--warning-mode all' to show the individual deprecation warnings.
See https://docs.gradle.org/6.4.1/userguide/command_line_interface.html#sec:command_line_warnings

BUILD SUCCESSFUL in 1s
111 actionable tasks: 2 executed, 109 up-to-date
docker exec ducker01 bash -c ""cd /opt/kafka-dev && ducktape --cluster-file /opt/kafka-dev/tests/docker/build/cluster.json  ./tests/kafkatest/tests/tools/log4j_appender_test.py""
[INFO:2020-05-27 04:25:23,949]: starting test run with session id 2020-05-27--006...
[INFO:2020-05-27 04:25:23,949]: running 4 tests...
[INFO:2020-05-27 04:25:23,949]: Triggering test 1 of 4...
[INFO:2020-05-27 04:25:23,961]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/tools', 'file_name': 'log4j_appender_test.py', 'method_name': 'test_log4j_appender', 'cls_name': 'Log4jAppenderTest', 'injected_args': {'security_protocol': 'SASL_PLAINTEXT'}}
[INFO:2020-05-27 04:25:23,966]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: Setting up...
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  m.add_string(self.Q_C.public_numbers().encode_point())
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:94: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point
  self.curve, Q_S_bytes
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:109: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  hm.add_string(self.Q_C.public_numbers().encode_point())
[INFO:2020-05-27 04:25:27,733]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: Running...
[INFO:2020-05-27 04:25:50,830]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: PASS
[INFO:2020-05-27 04:25:50,831]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: Tearing down...
[INFO:2020-05-27 04:25:58,992]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: Summary: 
[INFO:2020-05-27 04:25:58,993]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT: Data: None
[INFO:2020-05-27 04:25:59,009]: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[INFO:2020-05-27 04:25:59,010]: Triggering test 2 of 4...
[INFO:2020-05-27 04:25:59,020]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/tools', 'file_name': 'log4j_appender_test.py', 'method_name': 'test_log4j_appender', 'cls_name': 'Log4jAppenderTest', 'injected_args': {'security_protocol': 'SASL_SSL'}}
[INFO:2020-05-27 04:25:59,024]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: Setting up...
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  m.add_string(self.Q_C.public_numbers().encode_point())
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:94: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point
  self.curve, Q_S_bytes
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:109: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  hm.add_string(self.Q_C.public_numbers().encode_point())
[INFO:2020-05-27 04:26:02,760]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: Running...
[INFO:2020-05-27 04:26:32,156]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: PASS
[INFO:2020-05-27 04:26:32,156]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: Tearing down...
[INFO:2020-05-27 04:26:39,792]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: Summary: 
[INFO:2020-05-27 04:26:39,792]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL: Data: None
[INFO:2020-05-27 04:26:39,801]: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[INFO:2020-05-27 04:26:39,802]: Triggering test 3 of 4...
[INFO:2020-05-27 04:26:39,812]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/tools', 'file_name': 'log4j_appender_test.py', 'method_name': 'test_log4j_appender', 'cls_name': 'Log4jAppenderTest', 'injected_args': {'security_protocol': 'PLAINTEXT'}}
[INFO:2020-05-27 04:26:39,816]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: Setting up...
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  m.add_string(self.Q_C.public_numbers().encode_point())
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:94: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point
  self.curve, Q_S_bytes
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:109: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  hm.add_string(self.Q_C.public_numbers().encode_point())
[INFO:2020-05-27 04:26:43,644]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: Running...
[INFO:2020-05-27 04:26:56,333]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: PASS
[INFO:2020-05-27 04:26:56,334]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: Tearing down...
[INFO:2020-05-27 04:27:03,583]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: Summary: 
[INFO:2020-05-27 04:27:03,584]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT: Data: None
[INFO:2020-05-27 04:27:03,593]: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[INFO:2020-05-27 04:27:03,593]: Triggering test 4 of 4...
[INFO:2020-05-27 04:27:03,604]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/tools', 'file_name': 'log4j_appender_test.py', 'method_name': 'test_log4j_appender', 'cls_name': 'Log4jAppenderTest', 'injected_args': {'security_protocol': 'SSL'}}
[INFO:2020-05-27 04:27:03,609]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Setting up...
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  m.add_string(self.Q_C.public_numbers().encode_point())
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:94: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point
  self.curve, Q_S_bytes
/usr/local/lib/python2.7/dist-packages/paramiko/kex_ecdh_nist.py:109: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.
  hm.add_string(self.Q_C.public_numbers().encode_point())
[INFO:2020-05-27 04:27:07,645]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Running...
[INFO:2020-05-27 04:27:28,674]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: PASS
[INFO:2020-05-27 04:27:28,674]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Tearing down...
[INFO:2020-05-27 04:27:35,937]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Summary: 
[INFO:2020-05-27 04:27:35,937]: RunnerClient: kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL: Data: None
================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.7
session_id:       2020-05-27--006
run time:         2 minutes 11.997 seconds
tests run:        4
passed:           4
failed:           0
ignored:          0
================================================================================
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_PLAINTEXT
status:     PASS
run time:   35.025 seconds
--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SASL_SSL
status:     PASS
run time:   40.767 seconds
--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=PLAINTEXT
status:     PASS
run time:   23.767 seconds
--------------------------------------------------------------------------------
test_id:    kafkatest.tests.tools.log4j_appender_test.Log4jAppenderTest.test_log4j_appender.security_protocol=SSL
status:     PASS
run time:   32.328 seconds
--------------------------------------------------------------------------------
nizhikov@kafka:~/kafka$ git status
On branch KAFKA-10050
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
	modified:   tests/docker/run_tests.sh

no changes added to commit (use ""git add"" and/or ""git commit -a"")
nizhikov@kafka:~/kafka$ git diff
diff --git a/tests/docker/run_tests.sh b/tests/docker/run_tests.sh
index 063e24d17..6278f9ac6 100755
--- a/tests/docker/run_tests.sh
+++ b/tests/docker/run_tests.sh
@@ -30,6 +30,6 @@ if [ ""$REBUILD"" == ""t"" ]; then
 fi
 
 if ${SCRIPT_DIR}/ducker-ak ssh | grep -q '(none)'; then
-    ${SCRIPT_DIR}/ducker-ak up -n ""${KAFKA_NUM_CONTAINERS}"" || die ""ducker-ak up failed""
+    ${SCRIPT_DIR}/ducker-ak up -j 'openjdk:11' -n ""${KAFKA_NUM_CONTAINERS}"" || die ""ducker-ak up failed""
 fi
 ${SCRIPT_DIR}/ducker-ak test ${TC_PATHS} ${_DUCKTAPE_OPTIONS} || die ""ducker-ak test failed""
nizhikov@kafka:~/kafka$ 
 {noformat};;;","27/May/20 11:30;nizhikov;Hello, [~ijuma]

I found that {{kafka_log4j_appender.py}} broken for JDK11.
This was broken by my patch - https://github.com/apache/kafka/commit/befd80b38d3ccb1aa0c6d99a899129fd5cf27774

I've prepared oneliner fix - https://github.com/apache/kafka/pull/8731

Can you, please, take a look?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTable-KTable Foreign Key join throwing Serialization Exception ,KAFKA-10049,13307633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,abellemare,amicngh,amicngh,27/May/20 07:13,12/Jun/20 16:56,13/Jul/23 09:17,12/Jun/20 16:56,2.5.0,2.6.0,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,streams,,,,,0,,,,," I want to make use of _KTable-KTable_ Foreign Key join feature released in *_2.5.0_* but facing issue while running the code. 
{code:java}
 

 public static void main(String[] args) {

     Properties props = new Properties();
     props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""my-stream-processing-application-2"");
     props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:9092"");
     props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
     props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, new JSONSerdeComp<>().getClass());
     props.put(StreamsConfig.STATE_DIR_CONFIG, ""C:\\temp"");
     props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");

     StreamsBuilder builder = new StreamsBuilder();
     KTable<String, OrderObject> ordersTable = builder.<String, OrderObject>table(TOPIC_Agora);
     KTable<String, StockMarketData> stockTable = builder.<String, StockMarketData>table(TOPIC_Stock_Data);

     KTable<String, EnrichedOrder> enriched = ordersTable.leftJoin(stockTable, OrderObject:: getSymbol, new ValueJoiner<OrderObject, StockMarketData, EnrichedOrder>() {

            @Override
            public EnrichedOrder apply(OrderObject order, StockMarketData stock) {
                EnrichedOrder enOrder = EnrichedOrder.builder()
                        .orderId(order.getOrderId())
                        .execPrice(order.getPrice())
                        .symbol(order.getSymbol())
                        .quanity(order.getQuanity())
                        .side(order.getSide())
                        .filledQty(order.getFilledQty())
                        .leaveQty(order.getLeaveQty())
                        .index(order.getIndex())
                        .vWaprelative(order.getVWaprelative())
                        .stockAsk(stock!=null?stock.getAsk().doubleValue():0.0)
                        .stockBid(stock!=null?stock.getBid().doubleValue():0.0)
                        .stockLast(stock!=null?stock.getLast().doubleValue():0.0)
                        .stockClose(stock!=null?stock.getClose().doubleValue():0.0)
                        .build();
                return enOrder;
            }
        } , Materialized.with(Serdes.String(), new JSONSerdeComp<>()));

     enriched.toStream().foreach(new ForeachAction<String, EnrichedOrder>() \{
         @Override
        public void apply(String arg0, EnrichedOrder arg1) {

             logger.info(String.format(""key = %s, value = %s"", arg0, arg1));
        }
    });

     KafkaStreams streams = new KafkaStreams(builder.build(), props);
     streams.start();

     Runtime.getRuntime().addShutdownHook(new Thread(() -> streams.close()));
}}}



 

    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>2.5.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-streams</artifactId>
        <version>2.5.0</version>
    </dependency>

{code}
*+Exception:+*
{code:java}
18:49:31.525 [my-stream-processing-application-2-37cfd34a-6eb4-411a-a2dc-faa9194ce04e-StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.ProcessorStateManager - stream-thread [my-stream-processing-application-2-37cfd34a-6eb4-411a-a2dc-faa9194ce04e-StreamThread-1] task [0_0] Failed to flush state store orders-STATE-STORE-0000000000: 
    org.apache.kafka.streams.errors.StreamsException: ClassCastException while producing data to a sink topic. A serializer (key: org.apache.kafka.common.serialization.StringSerializer / value: org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde$SubscriptionWrapperSerializer) is not compatible to the actual key or value type (key type: java.lang.String / value type: org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapper). Change the default Serdes in StreamConfig or provide correct Serdes via method parameters (for example if using the DSL, `#to(String topic, Produced<K, V> produced)` with `Produced.keySerde(WindowedSerdes.timeWindowedSerdeFrom(String.class))`).
        at org.apache.kafka.streams.processor.internals.SinkNode.process(SinkNode.java:94) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:133) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.kstream.internals.foreignkeyjoin.ForeignJoinSubscriptionSendProcessorSupplier$UnbindChangeProcessor.process(ForeignJoinSubscriptionSendProcessorSupplier.java:157) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.kstream.internals.foreignkeyjoin.ForeignJoinSubscriptionSendProcessorSupplier$UnbindChangeProcessor.process(ForeignJoinSubscriptionSendProcessorSupplier.java:71) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:142) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:142) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:45) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.kstream.internals.TimestampedCacheFlushListener.apply(TimestampedCacheFlushListener.java:28) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$setFlushListener$1(MeteredKeyValueStore.java:119) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.CachingKeyValueStore.putAndMaybeForward(CachingKeyValueStore.java:92) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.CachingKeyValueStore.lambda$initInternal$0(CachingKeyValueStore.java:72) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:151) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:109) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.ThreadCache.flush(ThreadCache.java:124) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.CachingKeyValueStore.flush(CachingKeyValueStore.java:272) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$flush$7(MeteredKeyValueStore.java:192) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.flush(MeteredKeyValueStore.java:192) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:282) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.AbstractTask.flushState(AbstractTask.java:177) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamTask.flushState(StreamTask.java:554) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:490) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:478) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.AssignedTasks.commit(AssignedTasks.java:226) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:543) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:977) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:823) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697) [kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670) [kafka-streams-2.5.0.jar:?]
    Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to com.messages.JSONSerdeCompatible
        at com.messages.JSONSerdeComp.serialize(JSONSerdeComp.java:1) ~[classes/:?]
        at org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde$SubscriptionWrapperSerializer.serialize(SubscriptionWrapperSerde.java:79) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde$SubscriptionWrapperSerializer.serialize(SubscriptionWrapperSerde.java:51) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.common.serialization.Serializer.serialize(Serializer.java:62) ~[kafka-clients-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send(RecordCollectorImpl.java:176) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send(RecordCollectorImpl.java:111) ~[kafka-streams-2.5.0.jar:?]
        at org.apache.kafka.streams.processor.internals.SinkNode.process(SinkNode.java:89) ~[kafka-streams-2.5.0.jar:?]
        ... 34 more
{code}",,abellemare,ableegoldman,amicngh,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/20 18:02;abellemare;10049-bellemare.patch;https://issues.apache.org/jira/secure/attachment/13004371/10049-bellemare.patch",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 31 14:50:32 UTC 2020,,,,,,,,,,"0|z0f788:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 16:55;vvcephei;I've marked this as a 2.6 blocker. It's not immediately clear to me what the root cause might be.;;;","27/May/20 17:20;mjsax;Assigned the ticket to [~vvcephei] for now. Not sure if [~abellemare] would be interested to pick it up? Think we should try to fix it for 2.6.0 if possible (and 2.5.1). We have like 2 weeks till code freeze.;;;","27/May/20 18:12;abellemare;Is this us? I think this is their serializer complaining about their input types.

 
{code:java}
    Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to com.messages.JSONSerdeCompatible
        at com.messages.JSONSerdeComp.serialize(JSONSerdeComp.java:1) ~[classes/:?]
{code}
Isn't it that the com.messages.JSONSerdeComp.serialize() function is expecting a com.message.JSONSerdeCompatible?

com.messages.* isn't showing up in maven repo for me, so I'm pretty sure this is a private library issue.

To [~amicngh], try using one of the basic StringSerdes instead and see if this still happens. The unit tests suggest it should not, but I just want to exclude the com.messages.JSONSerdeComp library before going any further.;;;","28/May/20 09:46;amicngh;I am using JSON Compatible Serde form [This Link.|https://github.com/apache/kafka/blob/2.3/streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java#L83]

{code}
public static class JSONSerdeComp<T extends JSONSerdeCompatible> implements Serializer<T>, Deserializer<T>, Serde<T> {
        private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();

        @Override
        public void configure(final Map<String, ?> configs, final boolean isKey) {}

        @SuppressWarnings(""unchecked"")
        @Override
        public T deserialize(final String topic, final byte[] data) {
            if (data == null) {
                return null;
            }

            try {
                return (T) OBJECT_MAPPER.readValue(data, JSONSerdeCompatible.class);
            } catch (final IOException e) {
                throw new SerializationException(e);
            }
        }

        @Override
        public byte[] serialize(final String topic, final T data) {
            if (data == null) {
                return null;
            }

            try {
                return OBJECT_MAPPER.writeValueAsBytes(data);
            } catch (final Exception e) {
                throw new SerializationException(""Error serializing JSON message"", e);
            }
        }

        @Override
        public void close() {}

        @Override
        public Serializer<T> serializer() {
            return this;
        }

        @Override
        public Deserializer<T> deserializer() {
            return this;
        }
    }


   /**
     * An interface for registering types that can be de/serialized with {@link JSONSerde}.
     */
    @SuppressWarnings(""DefaultAnnotationParam"") // being explicit for the example
    @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = ""_t"")
    @JsonSubTypes({
                      @JsonSubTypes.Type(value = OrderObject.class, name = ""OrderObject""),
                      @JsonSubTypes.Type(value = StockMarketData.class, name = ""StockMarketData""),
                      @JsonSubTypes.Type(value = EnrichedOrder.class, name = ""EnrichedOrder""),
           
                  })
    public interface JSONSerdeCompatible {

    }
{code}

 

 ;;;","28/May/20 09:52;amicngh;[~abellemare] Don't know why it is expecting String while serializing value.  In my case Key is String while value is +JSONSerdeCompatible+ ;;;","28/May/20 10:27;amicngh;worth mentioning here same application works with same primary key joins. ;;;","28/May/20 17:46;abellemare;[~amicngh] Okay, I was able to reproduce it. I'll look into determining the root cause, but for now the easiest workaround is to use `Consumed.with` for both your input KTables:
{code:java}
KTable<String, OrderObject> ordersTable = builder.<String, OrderObject>table(TOPIC_Agora, Consumed.with(Serdes.String(), new JSONSerde<>());
KTable<String, StockMarketData> stockTable = builder.<String, StockMarketData>table(TOPIC_Stock_Data, Consumed.with(Serdes.String(), new JSONSerde<>());
{code}
This will ensure the proper serializers / deserializers are being used.


I'm not entirely sure what's happening right now, but I had a similar issue previously where KTables without explicitly defined serdes via `Consumed.with` exhibited the same behaviour.;;;","28/May/20 19:17;abellemare;[~vvcephei] Hey John, didn't you fix an issue around this a while back? This seems really familiar....;;;","29/May/20 01:29;abellemare;Found the bug. The SubscriptionWrapperSerde wraps a Key Serde, but it is being overwritten by a valueSerde during init. Here's the line where the key is overwritten:
[https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerde.java#L68]

Here's the init where it calls setIfUnset (notice that it is passing in the valSerializer and valDeserializer, whereas SubscriptionWrapperSerde is expecting to be passed in a keySerde).
[https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java#L72
https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNode.java#L92|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java#L72]

I'll take a look at fixing it, but the workflow is fuzzy to me, so it may take a bit of time for me to understand it.;;;","29/May/20 17:10;vvcephei;Woah, thanks for digging into that, [~abellemare] !

That's 100% the bug, and it is a regression. It seems I'm doomed to be haunted forever by my own code. This bug probably seems familiar because there have been like half a dozen bugs around the serdes for this feature. I don't think anyone could have predicted that the serdes would wind up to be the most complex part of foreign-key joins.

The workflow is very fuzzy, because the ""default serde"" thing makes everything confusing. Hopefully we can fix it sometime soon.

In the mean time, I think the safest thing to do is just let the wrapper serde itself decide whether it should wrap the key or value serde, since it happens to always be fixed by the type of the wrapper serde.

What do you think about: [https://github.com/apache/kafka/pull/8756] ?

Thanks for your help,

-John;;;","29/May/20 17:58;mjsax;Thanks for digging into it [~abellemare] and [~vvcephei]!;;;","29/May/20 18:03;abellemare;Hey [~vvcephei]  - thanks for the PR! I appreciate you taking a look!

I took a look at it and noticed that the SourceNode wasn't updated, so I went ahead and did that one too. I also have included the test I was using to validate it (though it's not really ready to commit - this is dev-level testing). I have added it as a patch to this ticket (incorporating your changes as well).

If you get a chance to integrate the changes first, great. If not, I will take a crack at it hopefully in the next few days to simplify the unit test down. Not sure if you think we should put it in, but I like how it exercises our code path with a serializer/deserializer that is very different than the standard primitive ones we tend to use. [^10049-bellemare.patch]I think it may have some value in ensuring that the default serdes work as expected when using the FKJ.;;;","29/May/20 18:14;vvcephei;Thanks for the test, [~abellemare] ,

Nice catch on the SourceNode.

You're more than welcome to lead the fix for this. I just sent the PR as a way to communicate what I was thinking.

How about I assign this ticket to you, and close my PR. Then, you can start your own PR, and I'll review it?

I do think we should include the test. I didn't know that we already had jackson in the test scope; that makes the test much easier.

Regarding the specific test, what do you think about modifying the org.apache.kafka.streams.kstream.internals.KTableKTableForeignKeyJoinScenarioTest instead of adding a new test?

Looking at the methods in that class, it seems we intended to test for exactly this condition, but missed it because in the test, the key and value types are both String.

Thanks,

-John;;;","29/May/20 18:23;abellemare;Can do. I'll take care of it. Thanks for your help John, I greatly appreciate it!;;;","31/May/20 14:50;abellemare;PR is up.;;;","31/May/20 14:50;abellemare;Thanks [~amicngh] for finding and reporting this bug!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible data gap for a consumer after a failover when using MM2,KAFKA-10048,13307624,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,asdaraujo,asdaraujo,asdaraujo,27/May/20 06:44,02/Oct/20 16:39,13/Jul/23 09:17,02/Oct/20 16:36,2.5.0,2.6.0,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,mirrormaker,,,,,0,,,,,"I've been looking at some MM2 scenarios and identified a situation where consumers can miss consuming some data in the even of a failover.
 
When a consumer subscribes to a topic for the first time and commits offsets, the offsets for every existing partition of that topic will be saved to the cluster's {{__consumer_offset}} topic. Even if a partition is completely empty, the offset {{0}} will still be saved for the consumer's consumer group.

 
When MM2 is replicating the checkpoints to the remote cluster, though, it [ignores anything that has an offset equals to zero|https://github.com/apache/kafka/blob/856e36651203b03bf9a6df2f2d85a356644cbce3/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java#L135], replicating offsets only for partitions that contain data.
 
This can lead to a gap in the data consumed by consumers in the following scenario:
 # Topic is created on the source cluster.
 # MM2 is configured to replicate the topic and consumer groups
 # Producer starts to produce data to the source topic but for some reason some partitions do not get data initially, while others do (skewed keyed messages or bad luck)
 # Consumers start to consume data from that topic and their consumer groups' offsets are replicated to the target cluster, *but only for partitions that contain data*. The consumers are using the default setting auto.offset.reset = latest.
 # A consumer failover to the second cluster is performed (for whatever reason), and the offset translation steps are completed. The consumer are not restarted yet.
 # The producers continue to produce data to the source cluster topic and now produce data to the partitions that were empty before.
 # *After* the producers start producing data, consumers are started on the target cluster and start consuming.
For the partitions that already had data before the failover, everything works fine. The consumer offsets will have been translated correctly and the consumers will start consuming from the correct position.
For the partitions that were empty before the failover, though, any data written by the producers to those partitions *after the failover but before the consumers start* will be completely missed, since the consumers will jump straight to the latest offset when they start due to the lack of a zero offset stored locally on the target cluster.",,asdaraujo,ecomar,ryannedolan,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-27 06:44:21.0,,,,,,,,,,"0|z0f768:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecated PartitionGrouper config is ignored,KAFKA-10046,13307560,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,vvcephei,vvcephei,26/May/20 22:18,09/Jul/21 20:41,13/Jul/23 09:17,09/Jul/21 20:41,2.6.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,0,,,,,"It looks like at some point, we accidentally broke the chain that lets the user-provided PartitionGrouper config actually get to the Consumer-managed StreamsPartitionAssignor. The effect is that any configured value would just be ignored.

Investigation is needed to determine when this regression took place and whether we should fix it or just remove the config.

See the discussion in [https://github.com/apache/kafka/pull/8716/] . The TaskAssignorIntegrationTest in that PR can be used to verify this regression.",,ableegoldman,chia7712,kkonstantine,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7785,KAFKA-8927,KAFKA-4117,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 09 20:41:21 UTC 2021,,,,,,,,,,"0|z0f6s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/21 21:51;ableegoldman;We're removing this config in 3.0 so I guess we can go ahead and close this once https://issues.apache.org/jira/browse/KAFKA-7785 and/or https://issues.apache.org/jira/browse/KAFKA-12527 are merged;;;","09/Jul/21 20:41;kkonstantine;[~ableegoldman] the two issues you mention above have been resolved. Thus, marking the issue here as resolved too and I assume it's completed for AK 3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception while fetching a key from a single partition,KAFKA-10030,13306444,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dima5rr,dima5rr,dima5rr,21/May/20 09:45,02/Jun/20 01:36,13/Jul/23 09:17,02/Jun/20 01:36,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,streams,,,,,0,KAFKA-9445,KIP-562,,,"StreamThreadStateStoreProvider#stores throws exception whenever taskId is not found, which is not correct behaviour in multi-threaded env where state store partitions are distributed among several StreamTasks. 
{code:java}
final Task task = tasks.get(keyTaskId);
if (task == null) {
 throw new InvalidStateStoreException(
 String.format(""The specified partition %d for store %s does not exist."",
 storeQueryParams.partition(),
 storeName));
}{code}
Reproducible with KStream number of threads more then 1 

StoreQueryIntegrationTest#streamsConfiguration

config.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2);

 

Suggested solution is to not throw exception if at least one state store is found",StreamsConfig.NUM_STREAM_THREADS_CONFIG=2,ableegoldman,dima5rr,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 21:51:48 UTC 2020,,,,,,,,,,"0|z0ezwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/20 18:23;mjsax;Thanks for the bug report and PR! – I added you to the list of contributors and assigned the ticket to you. You can now also self-assign tickets.;;;","29/May/20 21:51;vvcephei;I'm getting ready for the 2.5.1 release at the moment. It looks like this PR is close, so I'll leave it targeted for 2.5.1 for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selector.completedReceives should not be modified when channel is closed,KAFKA-10029,13306413,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,21/May/20 08:52,29/May/20 08:44,13/Jul/23 09:17,29/May/20 08:44,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,network,,,,,0,,,,,"Selector.completedReceives are processed using `forEach` by SocketServer and NetworkClient when processing receives from a poll. Since we may close channels while processing receives, changes to the map while closing channels can result in ConcurrentModificationException. We clear the entire map after each poll anyway, so we don't need to remove channel from the map while closing channels.",,ijuma,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 14:33:44 UTC 2020,,,,,,,,,,"0|z0ezpk:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"21/May/20 12:59;ijuma;Good catch. Is this a recent regression?;;;","21/May/20 14:33;rsivaram;[~ijuma] It was a regression in 2.5.0 introduced by KAFKA-7639.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When reading to the end of the config log, check if fetch.max.wait.ms is greater than worker.sync.timeout.ms",KAFKA-10021,13305844,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,skaundinya,skaundinya,19/May/20 08:18,22/Feb/21 16:11,13/Jul/23 09:17,10/Feb/21 01:52,2.3.0,2.4.0,2.5.0,2.6.0,2.7.0,,,,,,,,,,,,,,,,,,2.5.2,2.6.2,2.7.1,2.8.0,,,,,KafkaConnect,,,,,0,,,,,"Currently in the Connect code in DistributedHerder.java, we see the following piece of code

 

{{            if (!canReadConfigs && !readConfigToEnd(workerSyncTimeoutMs))
                return; // Safe to return and tick immediately because readConfigToEnd will do the backoff for us}}

where the workerSyncTimeoutMs passed in is the timeout given to read to the end of the config log. This is a bug as we should check if fetch.wait.max.ms is greater than worker.sync.timeout.ms and if it is, use worker.sync.timeout.ms as the fetch.wait.max.ms. A better fix would be to use the AdminClient to read to the end of the log, but at a minimum we should check the configs.",,ableegoldman,hachikuji,rhauch,skaundinya,wicknicks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12326,KAFKA-12340,KAFKA-12339,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 02:10:21 UTC 2021,,,,,,,,,,"0|z0ew74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/20 16:39;hachikuji;For a little more detail, the problem today with the `readToLogEnd` function in `KafkaBasedLog` is that it can get blocked by `fetch.max.wait.ms`. This is because the connection that is used for finding the end offset is also shared by the consumer fetching from the log. If the topic has low volume, then it is in fact likely that the ListOffset request gets stuck behind a Fetch which is blocking on the broker. This can cause a timeout when syncing configs or even just slowness when reading offsets using `OffsetStorageReader`. The simplest fix would be to use a shared `AdminClient` to fetch the end offset instead of the consumer.;;;","19/May/20 20:06;wicknicks;[~hachikuji] [~skaundinya] do consumers in a JVM share a connection to a broker or do they all create their own connections? if they share a connection, then this problem can occur if a connector/task has its consumer overrides that sets a high `fetch.max.wait.ms`. In this case, the worker should not allow overriding this value in a connector to more than what is allowed by the workerSyncTimeoutMs.;;;","19/May/20 22:09;hachikuji;[~wicknicks] Each consumer instance has a separate connection to the brokers. This issue wouldn't be a problem if we used a separate AdminClient to query the end offsets.;;;","19/May/20 22:12;wicknicks;Thanks, [~hachikuji]!;;;","10/Feb/21 02:10;rhauch;Including in 2.6.2 due to reports of rebalance storms because of the logs getting stuck. cc [~ableegoldman].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Change command line tools from /bin/sh to /bin/bash,KAFKA-10018,13305792,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,adally,adally,adally,19/May/20 01:17,27/May/20 21:32,13/Jul/23 09:17,27/May/20 21:31,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,admin,,,,,0,,,,,"""#!/bin/sh"" is used in kafka-server-stop.sh and zookeeper-server-stop.sh. [[ is a bash-builtin and used.
    Modern Debian and Ubuntu systems, which symlink sh to dash by default. So ""[[: not found"" will occur.
    Change ""#!/bin/sh"" into ""#!/bin/bash"" can avoid this error. Modify and make all scripts using bash.
",,adally,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,2020-05-19 01:17:37.0,,,,,,,,,,"0|z0evvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta,KAFKA-10017,13305760,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,ableegoldman,ableegoldman,18/May/20 21:05,23/Feb/21 03:19,13/Jul/23 09:17,09/Feb/21 22:21,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,streams,,,,,0,flaky-test,unit-test,,,"Creating a new ticket for this since the root cause is different than https://issues.apache.org/jira/browse/KAFKA-9966

With injectError = true:
h3. Stacktrace

java.lang.AssertionError: Did not receive all 20 records from topic multiPartitionOutputTopic within 60000 ms Expected: is a value equal to or greater than <20> but: <15> was less than <20> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:563) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:559) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:530) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427)",,ableegoldman,bbejeck,dima5rr,guozhang,huxi_2b,mimaison,mjsax,rhauch,showuon,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 14:38:54 UTC 2020,,,,,,,,,,"0|z0evo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/20 21:34;ableegoldman;With injectError = false:
h3. Stacktrace

java.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms Expected: is a value equal to or greater than <10> but: <5> was less than <10> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:563) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:559) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:530) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427);;;","26/May/20 14:41;bbejeck;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6487/testReport/junit/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/shouldUpgradeFromEosAlphaToEosBeta_false_/]

 
{noformat}
Error Messagejava.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <10>
     but: <0> was less than <10>Stacktracejava.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <10>
     but: <0> was less than <10>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:563)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:559)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:530)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy5.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:413)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834){noformat};;;","26/May/20 14:43;bbejeck;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/625/testReport/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/shouldUpgradeFromEosAlphaToEosBeta_false_/]
{noformat}
Error Messagejava.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <10>
     but: <0> was less than <10>Stacktracejava.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <10>
     but: <0> was less than <10>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:563)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:559)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:530)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy5.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:413)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:830){noformat};;;","26/May/20 14:45;bbejeck;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/625/testReport/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/shouldUpgradeFromEosAlphaToEosBeta_true_/]

 
{noformat}
Error Messagejava.lang.AssertionError: Did not receive all 20 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <20>
     but: <0> was less than <20>Stacktracejava.lang.AssertionError: Did not receive all 20 records from topic multiPartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <20>
     but: <0> was less than <20>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:563)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:559)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:530)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy5.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:413)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:830){noformat};;;","27/May/20 14:39;vvcephei;I've just marked this as a blocker for 2.6.0. It's been failing with very high frequency on branch builds, and it looks like it may reveal an actual bug in EOS Beta.;;;","24/Jun/20 20:23;rhauch;[~vvcephei], [~ableegoldman]: what's the status of this? Is this more than just a flaky test, and should we keep this as a blocker for the 2.6.0 release? If so, what's the timeframe for fixing this?;;;","25/Jun/20 01:01;ableegoldman;cc [~mjsax];;;","25/Jun/20 02:03;mjsax;The test is still subject to other bugs that we are currently working on. So it's hard to say atm. Feel free to cut an RC right away (I update the ticket as critical for now). However, if this test surfaces another bug, we might kill an RC.;;;","30/Jun/20 17:29;guozhang;[~rhauch] I'm making it back as a blocker for 2.6 since we have seen it failed again after re-enabling it.;;;","06/Jul/20 11:19;huxi_2b;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1419/testReport/junit/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/shouldUpgradeFromEosAlphaToEosBeta_false_/]

 
{code:java}
Error Message
java.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms, currently accumulated data is [] Expected: is a value equal to or greater than <10> but: <0> was less than <10>
Stacktrace
java.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms, currently accumulated data is [] Expected: is a value equal to or greater than <10> but: <0> was less than <10> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:599) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:595) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:568) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961) at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor2.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:119) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:832)
{code};;;","06/Jul/20 18:55;rhauch;These seem to fail pretty regularly in AK builds now. Here are a few more cases:

* https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1423/
* https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3267/
* https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/7278/

Some sample output:
{noformat}
java.lang.AssertionError: Did not receive all 10 records from topic multiPartitionOutputTopic within 60000 ms,  currently accumulated data is []
Expected: is a value equal to or greater than <10>
     but: <0> was less than <10>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:599)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:595)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:568)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:759)
{noformat}

{noformat}
java.lang.AssertionError: Did not receive all 20 records from topic multiPartitionOutputTopic within 60000 ms,  currently accumulated data is [KeyValue(1, 55), KeyValue(1, 66), KeyValue(1, 78), KeyValue(1, 91), KeyValue(1, 105), KeyValue(3, 55), KeyValue(3, 66), KeyValue(3, 78), KeyValue(3, 91), KeyValue(3, 105)]
Expected: is a value equal to or greater than <20>
     but: <10> was less than <20>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:599)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:449)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:595)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:568)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.readResult(EosBetaUpgradeIntegrationTest.java:973)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.verifyCommitted(EosBetaUpgradeIntegrationTest.java:961)
	at org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta(EosBetaUpgradeIntegrationTest.java:427)
{noformat};;;","06/Jul/20 18:56;rhauch;[~guozhang], do you have an ETA on this?;;;","06/Jul/20 21:10;guozhang;Hi [~rhauch] I've just pushed a PR from  [~ableegoldman] to fix this (the jenkins job on that PR did not fail for this test). Sophie will run the test again 100X locally to confirm it is not flaky anymore and after that we can close the ticket.;;;","06/Jul/20 23:35;ableegoldman;The local runs just completed successfully so I think we can close this out. Ran 100 times with
{code:java}
for i in {0..100}; do ./gradlew --no-build-cache cleanTest streams:test --tests=${TEST}; if [ $? -ne 0 ]; then break; fi; done; 
{code};;;","06/Jul/20 23:56;guozhang;Thanks for the confirmation Sophie! I'm closing it now.;;;","07/Jul/20 16:01;dima5rr;Hi [~ableegoldman],

I just found that 

shouldUpgradeFromEosAlphaToEosBeta always fail when create KStream instances with more then 1 threads  (StreamsConfig.NUM_STREAM_THREADS_CONFIG=2)

May it indicate actual problem with EOS Beta, worth to create new ticket?

 ;;;","07/Jul/20 17:14;ableegoldman;Hey [~dima5rr]

This test assumes only one thread per instance in order to hook into Streams and and force a commit or a crash, for example. I would not expect it to pass if you change the number of threads.

It could be extended to work with/test a multithreaded upgrade if you'd be interested in submitting a PR, though :) ;;;","30/Jul/20 20:56;mimaison;A couple of more failures:
- https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1780/consoleFull
- https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1771/consoleFull;;;","01/Oct/20 14:38;bbejeck;Have seen a few more failures

 

[https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-9237/8/testReport/junit/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/Build___JDK_8___shouldUpgradeFromEosAlphaToEosBeta_true__2/]

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-9237/8/testReport/junit/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/Build___JDK_15___shouldUpgradeFromEosAlphaToEosBeta_true_/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Always try to close all channels in Selector#close,KAFKA-10014,13305609,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,18/May/20 07:27,10/Jun/20 19:25,13/Jul/23 09:17,10/Jun/20 19:24,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"{code:java}
    public void close() {
        List<String> connections = new ArrayList<>(channels.keySet());
        try {
            for (String id : connections)
                close(id); // this line
        } finally {
{code}

KafkaChannel has a lot of releasable objects so we ought to try to close all channels.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-18 07:27:23.0,,,,,,,,,,"0|z0euqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lockedTaskDirectories should be cleared when task gets closed dirty in HandleLostAll,KAFKA-10011,13305529,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,17/May/20 18:57,01/Jun/20 18:12,13/Jul/23 09:17,01/Jun/20 18:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Tasks who get closed in handleLostAll don't clear out their position inside lockedTaskDirectories, which causes an illegal state afterwards:
{code:java}
[2020-05-17T06:21:54-07:00] (streams-soak-trunk-eos-beta_soak_i-0b021dbf00474b6aa_streamslog) [2020-05-17 13:21:54,127] ERROR [stream-soak-test-150cf9ae-793b-4aac-bea0-0fb61d228b39-StreamThread-3] stream-thread [stream-soak-test-150cf9ae-793b-4aac-bea0-0fb61d228b39-StreamThread-3] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-05-17T06:21:54-07:00] (streams-soak-trunk-eos-beta_soak_i-0b021dbf00474b6aa_streamslog) org.apache.kafka.streams.errors.ProcessorStateException: task directory [/mnt/run/streams/state/stream-soak-test/3_1] doesn't exist and couldn't be created
        at org.apache.kafka.streams.processor.internals.StateDirectory.directoryForTask(StateDirectory.java:112)
        at org.apache.kafka.streams.processor.internals.StateDirectory.checkpointFileFor(StateDirectory.java:121)
        at org.apache.kafka.streams.processor.internals.TaskManager.getTaskOffsetSums(TaskManager.java:498)
        at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.subscriptionUserData(StreamsPartitionAssignor.java:239)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.metadata(ConsumerCoordinator.java:222)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendJoinGroupRequest(AbstractCoordinator.java:560)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.initiateJoinGroup(AbstractCoordinator.java:495)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:417)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:506)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1265)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1231)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1206)
        at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:770)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:630)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:550)
{code}",,ableegoldman,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-17 18:57:53.0,,,,,,,,,,"0|z0eu8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should make state store registration idempotent,KAFKA-10010,13305525,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,17/May/20 18:05,01/Jun/20 18:12,13/Jul/23 09:17,01/Jun/20 18:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"The current lost all logic doesn't close standby task, which could potentially lead to a tricky condition like below:



1. The standby task was initializing as `CREATED` state, and task corrupted exception was thrown from registerStateStores

2. The task corrupted exception was caught, and do a non-affected task commit

3. The task commit failed due to task migrated exception

4. The handleLostAll didn't close the standby task, leaving it as CREATED state

5. Next rebalance complete, the same task was assigned back as standby task.

6. Illegal Argument exception caught :
{code:java}
[2020-05-16T11:56:18-07:00] (streams-soak-trunk-eos-beta_soak_i-065b27929d3e7014a_streamslog) [2020-05-16 18:56:18,050] ERROR [stream-soak-test-ce5e4abb-24f5-48cd-9608-21c59b6b42a3-StreamThread-1] stream-thread [stream-soak-test-ce5e4abb-24f5-48cd-9608-21c59b6b42a3-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-05-16T11:56:18-07:00] (streams-soak-trunk-eos-beta_soak_i-065b27929d3e7014a_streamslog) java.lang.IllegalArgumentException: stream-thread [stream-soak-test-ce5e4abb-24f5-48cd-9608-21c59b6b42a3-StreamThread-1] standby-task [1_2] Store KSTREAM-AGGREGATE-STATE-STORE-0000000007 has already been registered.
        at org.apache.kafka.streams.processor.internals.ProcessorStateManager.registerStore(ProcessorStateManager.java:269)
        at org.apache.kafka.streams.processor.internals.AbstractProcessorContext.register(AbstractProcessorContext.java:112)
        at org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore.init(AbstractRocksDBSegmentedBytesStore.java:191)
        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
        at org.apache.kafka.streams.state.internals.RocksDBWindowStore.init(RocksDBWindowStore.java:48)
        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
        at org.apache.kafka.streams.state.internals.ChangeLoggingWindowBytesStore.init(ChangeLoggingWindowBytesStore.java:54)
        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
        at org.apache.kafka.streams.state.internals.CachingWindowStore.init(CachingWindowStore.java:74)
        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
        at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$init$0(MeteredWindowStore.java:85)
        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:804)
        at org.apache.kafka.streams.state.internals.MeteredWindowStore.init(MeteredWindowStore.java:85)
        at org.apache.kafka.streams.processor.internals.StateManagerUtil.registerStateStores(StateManagerUtil.java:82)
        at org.apache.kafka.streams.processor.internals.StandbyTask.initializeIfNeeded(StandbyTask.java:89)
        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:358)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:664)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:550)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:509)
{code}",,ableegoldman,bchen225242,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 21:37:01 UTC 2020,,,,,,,,,,"0|z0eu80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/20 18:25;guozhang;We intentionally did not close standby tasks because they are likely to be reassigned back. Here I feel the issue is that when we get an exception in registerStateStores, in order to keep the standby tasks in CREATED state we need to make sure any partially registered stores are de-registered in `closeStateManager`, thoughts?;;;","17/May/20 19:21;bchen225242;The gain for keeping the standby task open is not much when the thread is already under corruption or task migrated. Adding a de-register logic would certainly increase the complexity, which I'm not sure at the moment whether it is the correct move, because we need to handle that logic properly with another set of try-catch.;;;","18/May/20 18:38;ableegoldman;It's possible the active <-> standby task conversion PR would actually fix this on the side, as it skips re-registering any store that's already registered. I'd like to avoid closing standbys during handleLostAll since this will completely clear out any in-memory stores, for example;;;","18/May/20 18:43;ableegoldman;When I first started looking into the store registration and initialization logic for that PR, I remember thinking there was a bug since we would attempt to re-register stores if we hit an exception halfway through registration. I snooped around and it seemed like there wasn't really a way to hit this bug, but I fixed it anyways.

Seems like there actually was a way to hit this bug after all, so nice catch [~bchen225242];;;","18/May/20 21:35;bchen225242;Had offline discussion with the team, so far some action items:


 # Make the state store registration idempotent to unblock the trunk soak
 # Add a logic to avoid aborting the txn when the task is in initialization phase (Get a separate ticket)

 ;;;","18/May/20 21:37;bchen225242;For more context, the [reason|[https://github.com/apache/kafka/pull/8440/files#r407722022]] we have to keep the txn commit before handle task corruption, since otherwise under EOS beta the stream thread could actually abort other healthy tasks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigCommand fails to find default broker configs without ZK,KAFKA-10004,13305210,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,15/May/20 13:59,26/May/20 20:56,13/Jul/23 09:17,19/May/20 01:46,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,,,,,,0,,,,,"When running
{code:java}
bin/kafka-configs.sh --describe --bootstrap-server localhost:9092 --entity-type brokers
 {code}
the output will be:
 Dynamic configs for broker 0 are: ....
 Dynamic configs for broker <default> are:
 *The entity name for brokers must be a valid integer broker id, found: <default>*

 

The default entity cannot successfully get the configs.",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-15 13:59:17.0,,,,,,,,,,"0|z0esa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store's own restore listener should be triggered in store changelog reader,KAFKA-10001,13305079,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,15/May/20 00:49,16/May/20 15:27,13/Jul/23 09:17,16/May/20 15:27,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,streams,,,,"Streams' state store ``register()`` function passed in `restoreCallback` can potentially also be a `RestoreListener`, in which case its corresponding `onRestoreStart / End / batchRestored` should be triggered.

This is a regression in trunk -- 2.5 has this logic right but got regressed.",,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-15 00:49:41.0,,,,,,,,,,"0|z0ergw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade zookeeper to 3.5.8 to address security vulnerabilities,KAFKA-9996,13305010,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,macche,macche,14/May/20 18:31,18/Jul/20 17:41,13/Jul/23 09:17,18/Jul/20 17:41,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,packaging,,,,,0,,,,,"Kafka is now using zookeeper 3.5.7, which is affected by CVE-2020-8840 and CVE-2020-9488. Those 2 are resolved in 3.5.8.",,chia7712,macche,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-14 18:31:05.0,,,,,,,,,,"0|z0er1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch TaskMigrated exception in task corruption code path ,KAFKA-9994,13304995,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,14/May/20 17:13,18/May/20 17:40,13/Jul/23 09:17,18/May/20 17:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"We have seen a case where the TaskMigrated exception gets thrown from taskManager.commit(). This should be prevented by proper catching.

Looking at the stack trace, the TaskMigrated was thrown from preCommit() call inside corrupted task exception commit.
{code:java}
[2020-05-14T05:47:25-07:00] (streams-soak-trunk-eos_soak_i-0b5b559dda7970618_streamslog) [2020-05-14 12:47:25,635] ERROR [stream-soak-test-db8d1d42-5677-4a54-a0a1-89b7b0766493-StreamThread-1] stream-thread [stream-soak-test-db8d1d42-5677-4a54-a0a1-89b7b0766493-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-05-14T05:47:25-07:00] (streams-soak-trunk-eos_soak_i-0b5b559dda7970618_streamslog) org.apache.kafka.streams.errors.TaskMigratedException: Producer got fenced trying to send a record [stream-thread [stream-soak-test-db8d1d42-5677-4a54-a0a1-89b7b0766493-StreamThread-1] task [1_1]]; it means all tasks belonging to this thread should be migrated.
        at org.apache.kafka.streams.processor.internals.StreamsProducer.send(StreamsProducer.java:216)
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.send(RecordCollectorImpl.java:171)
        at org.apache.kafka.streams.state.internals.StoreChangeLogger.logChange(StoreChangeLogger.java:69)
        at org.apache.kafka.streams.state.internals.ChangeLoggingTimestampedWindowBytesStore.log(ChangeLoggingTimestampedWindowBytesStore.java:36)
        at org.apache.kafka.streams.state.internals.ChangeLoggingWindowBytesStore.put(ChangeLoggingWindowBytesStore.java:112)
        at org.apache.kafka.streams.state.internals.ChangeLoggingWindowBytesStore.put(ChangeLoggingWindowBytesStore.java:34)
        at org.apache.kafka.streams.state.internals.CachingWindowStore.putAndMaybeForward(CachingWindowStore.java:111)
        at org.apache.kafka.streams.state.internals.CachingWindowStore.lambda$initInternal$0(CachingWindowStore.java:91)
        at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:151)
        at org.apache.kafka.streams.state.internals.NamedCache.flush(NamedCache.java:109)
        at org.apache.kafka.streams.state.internals.ThreadCache.flush(ThreadCache.java:124)
        at org.apache.kafka.streams.state.internals.CachingWindowStore.flush(CachingWindowStore.java:296)
        at org.apache.kafka.streams.state.internals.WrappedStateStore.flush(WrappedStateStore.java:84)
        at org.apache.kafka.streams.state.internals.MeteredWindowStore.lambda$flush$4(MeteredWindowStore.java:200)
        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:804)
        at org.apache.kafka.streams.state.internals.MeteredWindowStore.flush(MeteredWindowStore.java:200)
        at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:402)
        at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:317)
        at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:753)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:573)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:517)
{code}",,ableegoldman,bchen225242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-14 17:13:05.0,,,,,,,,,,"0|z0eqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedKafkaCluster.deleteTopicAndWait not working with kafka_2.13,KAFKA-9992,13304934,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,akatona,akatona,akatona,14/May/20 13:07,19/May/20 19:09,13/Jul/23 09:17,19/May/20 18:55,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,packaging,streams,,,,0,,,,,"Kafka Streams artifact is depending on kafka_2.12 as of now, it is in the [kafka-streams-2.4.1.pom|https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams/2.4.1/kafka-streams-2.4.1.pom]:
{code}
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka_2.12</artifactId>
  <version>2.4.1</version>
  <scope>test</scope>
</dependency>  
{code}
But it is not hardcoded, whatever scala version was used to compile this component before uploading, that will be present in the pom.

When I'm using these deps:
{code}
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-streams</artifactId>
  <version>2.4.1</version>
  <classifier>test</classifier>
  <scope>test</scope>
</dependency>

<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka_2.13</artifactId>
  <version>2.4.1</version>
  <classifier>test</classifier>
  <scope>test</scope>
</dependency>
{code}

My test fails with the following exception (deleteTopicAndWait is called in my @After method):
{noformat}
java.lang.NoSuchMethodError: scala.collection.JavaConverters.setAsJavaSetConverter(Lscala/collection/Set;)Lscala/collection/convert/Decorators$AsJava;
        at org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster$TopicsDeletedCondition.conditionMet(EmbeddedKafkaCluster.java:316)
        at org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:370)
        at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)
        at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385)
        at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368)
        at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:356)
        at org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster.deleteTopicsAndWait(EmbeddedKafkaCluster.java:266)
        at org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster.deleteTopicAndWait(EmbeddedKafkaCluster.java:221)
{noformat}

I modified kafka build locally to separate artifacts based on scala version just like it is done with kafka core, and I pulled in kafka-streams_2.13 from my local mvn repo and test was working again.

I was only trying with 2.4.1, but I'm assuming other versions are also affected, please add the proper versions and proper components too (in case it's not packaging).",,ableegoldman,akatona,viktorsomogyi,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 18:59:36 UTC 2020,,,,,,,,,,"0|z0eqko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 15:13;vvcephei;Hello Andras,

Just to make sure I understand, is this just a problem with our test artifact? The test artifacts are not public APIs, and I would not recommend depending on them.

If you want to submit a PR for your patch we can merge it, but I don’t think we can call this a bug. 

Note that there have been a few conversations in Jira and on the mailing list about actually creating a public EmbeddedKafkaCluster for testing. So far, no one has picked it up, though. If you’re interested in doing that, it would be appreciated, and I can help with the KIP process. 

My standard advice is to copy/paste the EmbeddedKafkaCluster into your own test module so that you don’t depend on upstream tests. ;;;","15/May/20 11:55;viktorsomogyi;I think the public embedded kafka cluster would be a good idea, I often see this functionality being used in projects for integration testing.;;;","19/May/20 18:55;akatona;Thank  you, [~vvcephei]!;;;","19/May/20 18:57;vvcephei;No problem, [~akatona] ! Thanks again for the contribution. I'm in the process of cherry-picking back to 2.4.;;;","19/May/20 18:59;akatona;Okay :) cool!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosAlphaEnabled,KAFKA-9991,13304805,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ableegoldman,ableegoldman,14/May/20 01:40,10/Jun/20 21:11,13/Jul/23 09:17,10/Jun/20 21:10,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,flaky-test,unit-test,,,"[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6280/testReport/junit/org.apache.kafka.streams.integration/KTableSourceTopicRestartIntegrationTest/shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosAlphaEnabled/]

 
h3. Stacktrace

java.lang.AssertionError: Condition not met within timeout 30000. Table did not read all values at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:381) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:378) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.assertNumberValuesRead(KTableSourceTopicRestartIntegrationTest.java:205) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled(KTableSourceTopicRestartIntegrationTest.java:159) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosAlphaEnabled(KTableSourceTopicRestartIntegrationTest.java:143)",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9182,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 01:31:01 UTC 2020,,,,,,,,,,"0|z0eps0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/20 18:57;mjsax;Different test method: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2519/testReport/junit/org.apache.kafka.streams.integration/KTableSourceTopicRestartIntegrationTest/shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration/]
{quote}java.lang.AssertionError: Condition not met within timeout 30000. Table did not read all values at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:381) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:378) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.assertNumberValuesRead(KTableSourceTopicRestartIntegrationTest.java:205) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration(KTableSourceTopicRestartIntegrationTest.java:186){quote};;;","05/Jun/20 01:29;ableegoldman;[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/898/testReport/junit/org.apache.kafka.streams.integration/KTableSourceTopicRestartIntegrationTest/shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosAlphaEnabled/]
h3. Stacktrace

java.lang.AssertionError: Condition not met within timeout 30000. Table did not read all values at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:381) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:378) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.assertNumberValuesRead(KTableSourceTopicRestartIntegrationTest.java:205) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled(KTableSourceTopicRestartIntegrationTest.java:159) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosAlphaEnabled(KTableSourceTopicRestartIntegrationTest.java:143);;;","05/Jun/20 01:31;ableegoldman;Different test method: [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2734/testReport/junit/org.apache.kafka.streams.integration/KTableSourceTopicRestartIntegrationTest/shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration/]
h3. Stacktrace

java.lang.AssertionError: Condition not met within timeout 30000. Table did not read all values at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:381) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:378) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.assertNumberValuesRead(KTableSourceTopicRestartIntegrationTest.java:205) at org.apache.kafka.streams.integration.KTableSourceTopicRestartIntegrationTest.shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration(KTableSourceTopicRestartIntegrationTest.java:186);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink connector consuming DLQ topic may exhaust broker,KAFKA-9985,13304506,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mmolimar,mmolimar,mmolimar,13/May/20 02:52,11/Jun/20 03:11,13/Jul/23 09:17,11/Jun/20 03:11,2.3.1,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,,,,,,0,,,,,"When a sink connector is configured with a DLQ and its topic is the same (or matches) as the topic in which the connector reads, the broker and/or connector might be exhausted in case the record send to the topic is invalid.

Based on the broker/connect config, the connector might fail throwing a RecordTooLargeException previous to exhaust the broker/connector.",,mmolimar,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 03:11:29 UTC 2020,,,,,,,,,,"0|z0enxs:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"11/Jun/20 03:11;rhauch;Merged to `trunk`, and backported to `2.6` (for upcoming 2.6.0), `2.5` (for upcoming 2.5.1), and `2.4` (for future 2.4.2).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should fail the subscription when pattern is empty,KAFKA-9984,13304496,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,zhaohaidao,bchen225242,bchen225242,13/May/20 01:29,14/May/20 21:19,13/Jul/23 09:17,14/May/20 21:18,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,consumer,,,,,0,,,,,"We have seen a case where the consumer subscribes to an empty string pattern:
```

[Consumer ...  ] Subscribed to pattern:  ''

```

which doesn't make any sense and usually indicate a configuration error. The `consumer.subscribe(pattern)` call should fail with illegal argument for this case.",,ableegoldman,bchen225242,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 03:53:46 UTC 2020,,,,,,,,,,"0|z0envk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/20 18:16;guozhang;Sounds good to me :);;;","14/May/20 03:53;bchen225242;[~guozhang] Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running a dedicated mm2 cluster with more than one nodes,When the configuration is updated the task is not aware and will lose the update operation.",KAFKA-9981,13304388,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,qq619618919,qq619618919,12/May/20 15:15,14/Feb/23 21:39,13/Jul/23 09:17,09/Feb/23 15:50,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,3.5.0,,,,,,,,mirrormaker,,,,,5,,,,,"DistributedHerder.reconfigureConnector induction config update as follows:
{code:java}
if (changed) {
    List<Map<String, String>> rawTaskProps = reverseTransform(connName, configState, taskProps);
    if (isLeader()) {
        configBackingStore.putTaskConfigs(connName, rawTaskProps);
        cb.onCompletion(null, null);
    } else {
        // We cannot forward the request on the same thread because this reconfiguration can happen as a result of connector
        // addition or removal. If we blocked waiting for the response from leader, we may be kicked out of the worker group.
        forwardRequestExecutor.submit(new Runnable() {
            @Override
            public void run() {
                try {
                    String leaderUrl = leaderUrl();
                    if (leaderUrl == null || leaderUrl.trim().isEmpty()) {
                        cb.onCompletion(new ConnectException(""Request to leader to "" +
                                ""reconfigure connector tasks failed "" +
                                ""because the URL of the leader's REST interface is empty!""), null);
                        return;
                    }
                    String reconfigUrl = RestServer.urlJoin(leaderUrl, ""/connectors/"" + connName + ""/tasks"");
                    log.trace(""Forwarding task configurations for connector {} to leader"", connName);
                    RestClient.httpRequest(reconfigUrl, ""POST"", null, rawTaskProps, null, config, sessionKey, requestSignatureAlgorithm);
                    cb.onCompletion(null, null);
                } catch (ConnectException e) {
                    log.error(""Request to leader to reconfigure connector tasks failed"", e);
                    cb.onCompletion(e, null);
                }
            }
        });
    }
}
{code}
KafkaConfigBackingStore task checks for configuration updates,such as topic whitelist update.If KafkaConfigBackingStore task is not running on leader node,an HTTP request will be send to notify the leader of the configuration update.However,dedicated mm2 cluster does not have the HTTP server turned on,so the request will fail to be sent,causing the update operation to be lost.",,apriceq,ChrisEgerton,durban,fvissing,gsavinov,jitabc,Justinwins,qq619618919,ryannedolan,vaibhavjaimini,yangguo1220,,,,,,,,,,,,,,,,,,,KAFKA-10586,,,,,KAFKA-13808,KAFKA-10857,KAFKA-12893,KAFKA-12150,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 21 04:14:30 UTC 2022,,,,,,,,,,"0|z0en7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/20 04:10;apriceq;Not sure if related but I think I'm seeing issues where task configs don't seem to be getting updated consistently when new topics/partitions are found. 

 ;;;","15/May/20 01:00;qq619618919;A new topic/partition  is created, but the data is not synchronized.;;;","15/May/20 22:05;ryannedolan;Configuration updates only come from the REST API, afaik, which doesn't exist when running with connect-mirror-maker.sh. So I'm not sure what would be triggering the logic in the PR. In order to configuration changes to be picked up at all, the leader must be restarted. Generally you can't know which nodes are leaders of which flows, so generally it makes sense to just restart everything with a new config.

This would change if we added back the REST API to connect-mirror-maker.sh (it is purposefully turned off at present). If we had a REST API, _then_ configuration could change and workers would need to notify their leaders. But that is not the case now.;;;","16/May/20 05:19;ChrisEgerton;[~ryannedolan] I think these configuration updates come from the connector requesting task reconfiguration from the framework: [https://github.com/apache/kafka/blob/62fa8fc9a95d738780d1f73d2d758d7329828feb/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java#L232]

 

In distributed mode, this causes the framework to generate new task configs from the connector and then, if they've changed, try to write them to the config topic. However, only the leader is allowed to write directly to the config topic, so if the connector is hosted on a follower node, then the node has to forward those configs to the leader via the REST API: [https://github.com/apache/kafka/blob/62fa8fc9a95d738780d1f73d2d758d7329828feb/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1316-L1340]

 

The endpoint for receiving these task configs was the subject of KIP-507, which sought to close a security loophole that it presented at the time. You can see the code for that internal endpoint here: [https://github.com/apache/kafka/blob/62fa8fc9a95d738780d1f73d2d758d7329828feb/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java#L268-L278]

 

We might consider enabling a bare-bones REST API for MM2 that only supports this internal endpoint? As long as the {{SESSIONED}} protocol introduced in KIP-507 is used by the cluster, this wouldn't present any obvious security risks since requests would have to be signed with a session key that's distributed via the config topic and presumably only readable by workers in the cluster or trusted principals that have access to that topic.;;;","18/May/20 08:45;qq619618919;[~ChrisEgerton] [~ryannedolan] hi. 
In case of dedicated mm2 clusters, If the configBackingStore task is hosted on a follower node，Can the following node write directly into the config topic?;;;","18/May/20 17:38;ChrisEgerton;[~qq619618919] we could but it wouldn't be simple. We'd have to take care to ensure that writes from zombie workers would be ignored, which is done right now by only allowing the leader to write to the config topic.

I think it'd be easier to bring up the task configs endpoint for MM2 than to re-architect the Connect framework, especially given the compatibility and migration concerns that would have to be addressed in order to allow non-leader workers to write to the config topic. But either approach would work.;;;","21/May/20 01:00;qq619618919;[~ChrisEgerton] mm2 has realized data backup. How does kafkaproduce realize automatic failover transparently?
How can the same kafkaproduce object automatically switch between two clusters?;;;","22/May/20 06:24;ChrisEgerton;[~qq619618919] I'm afraid I don't know too much about MirrorMaker 2 itself; I'm more familiar with the Connect framework that it's built on top of.

[~ryannedolan] may know more?;;;","07/Oct/20 10:56;vaibhavjaimini;[~qq619618919] I also faced the same issue with our multiple node cross DC mirror maker. I've tested the changes that you've made and they are working. Can you confirm if you went ahead with these or you found another workaround ?;;;","12/Oct/20 03:22;qq619618919;[~vaibhavjaimini] That's how My prd environment works,I also do dynamic whitelist based on ZooKeeper;;;","04/Aug/21 12:35;durban;This KIP aims to fix the issue by adding the REST API to MM2, and also improving the config provider reference handling in the MM2 configs: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-710%3A+Full+support+for+distributed+mode+in+dedicated+MirrorMaker+2.0+clusters];;;","21/Jun/22 04:14;Justinwins;[~durban]  [~vaibhavjaimini] any workaround available for this issue ?  i met this in production env too, and got stuck for a few days. 

thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug where alterClientQuotas could not set default client quotas,KAFKA-9980,13304302,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbyrne,d8tltanc,d8tltanc,12/May/20 07:44,11/May/22 22:10,13/Jul/23 09:17,22/May/20 00:51,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"quota_tests.py is failing. Specifically for this test:
{quote}
 [INFO:2020-05-11 19:22:47,493]: RunnerClient: Loading test \{'directory': '/opt/kafka-dev/tests/kafkatest/tests/client', 'file_name': 'quota_test.py', 'method_name': 'test_quota', 'cls_name': 'QuotaTest', 'injected_args': {'quota_type': 'client-id', 'override_quota': False}}
{quote}
 

I log into the docker container and do

 
{quote}
 /opt/kafka-dev/bin/kafka-configs.sh --bootstrap-server ducker03:9093 --describe --entity-type clients --command-config /opt/kafka-dev/bin/hi.properties
{quote}
 


 and the command return

 
{quote}Configs for the default client-id are consumer_byte_rate=2000000.0, producer_byte_rate=2500000.0
 Configs for client-id 'overridden_id' are consumer_byte_rate=1.0E9, producer_byte_rate=1.0E9
 Seems like the config is properly but the quota is not effective
  
{quote}

 For investigation, I added a logging at 
{quote}{{AdminZKClient.changeConfigs()}}
{quote}
 

 
{quote}def changeConfigs(entityType: String, entityName: String, configs: Properties): Unit =

{

        warn(s""entityType = $entityType entityName = $entityName configs = $configs"") ...

}
{quote}
And use --bootstrap-server and --zookeeper to --alter the default client quota. I got

 
{quote}
 Alter with --zookeeper:WARN entityType = clients entityName = <default> configs = \{producer_byte_rate=1000000000, consumer_byte_rate=1000000000} (kafka.zk.AdminZkClient)
{quote}
 


 and

 
{quote}
 Alter with --bootstrap-server:WARN entityType = clients entityName = %3Cdefault%3E configs = \{producer_byte_rate=1000000000, consumer_byte_rate=1000000000} (kafka.zk.AdminZkClient)
{quote}
 


 I guess the encoding difference might cause the issue. The encoding happens in

 
{quote}
 Sanitizer.sanitize()
{quote}",,bbyrne,d8tltanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 15:17:38 UTC 2020,,,,,,,,,,"0|z0emog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 07:54;d8tltanc;Patch here: [https://github.com/apache/kafka/pull/8652/files];;;","21/May/20 15:17;bbyrne;Updated patch here: https://github.com/apache/kafka/pull/8658;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted standby task could be committed,KAFKA-9972,13303550,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,08/May/20 05:57,12/May/20 00:58,13/Jul/23 09:17,11/May/20 23:48,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"A corrupted standby task could revive and transit to the CREATED state, which will then trigger by `taskManager.commitAll` in next runOnce, causing an illegal state:

```

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,646] WARN [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] Encountered org.apache.kafka.clients.consumer.OffsetOutOfRangeException fetching records from restore consumer for partitions [stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-1], it is likely that the consumer's position has fallen out of the topic partition offset range because the topic was truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing it later. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,646] WARN [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] Detected the states of tasks \{1_1=[stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-1]} are corrupted. Will close the task as dirty and re-create and bootstrap from scratch. (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) org.apache.kafka.streams.errors.TaskCorruptedException: Tasks with changelogs \{1_1=[stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-1]} are corrupted and hence needs to be re-initialized

        at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:428)

        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:680)

        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:558)

        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:517)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,652] INFO [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] [Consumer clientId=stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,652] INFO [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] standby-task [1_1] Prepared dirty close (org.apache.kafka.streams.processor.internals.StandbyTask)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,679] INFO [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] standby-task [1_1] Closed dirty (org.apache.kafka.streams.processor.internals.StandbyTask)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,751] ERROR [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) java.lang.IllegalStateException: Illegal state CREATED while preparing standby task 1_1 for committing

        at org.apache.kafka.streams.processor.internals.StandbyTask.prepareCommit(StandbyTask.java:134)

        at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:752)

        at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:741)

        at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:863)

        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:725)

        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:558)

        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:517)

[2020-05-07T20:57:23-07:00] (streams-soak-trunk-eos-beta_soak_i-0f819f0a58017b05b_streamslog) [2020-05-08 03:57:22,751] INFO [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] stream-thread [stream-soak-test-5ab3951c-2ca8-40a8-9096-0957e70a21b7-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)

```

Two solutions here: either we deprecate `commitAll` and always enforce state check to selectively commit tasks, or we enforce a state check inside standby task commitNeeded call to reference its state. Added a fix for option one here.",,ableegoldman,bchen225242,cadonna,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 00:58:28 UTC 2020,,,,,,,,,,"0|z0ei1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/20 20:45;ableegoldman;I see that we already committed a fix for this, but I'm a little concerned that we're only covering this one specific use case rather than the general problem of illegally attempting to commit a task in CREATED. AFAIK the corrupted standby case is just the one we happened to catch in soak, not the only possible case. WDYT? cc [~guozhang] [~vvcephei] – see comment on https://github.com/apache/kafka/commit/7907b5a6e921cec3b5fad2a4f84a78a851140755#r39046991;;;","09/May/20 18:12;vvcephei;Hi Sophie,

You hinted at a general solution, but I’m not sure exactly what you would propose. If I have to guess, maybe you mean we should push the filter inside of commitAll, so then all callers would just call commitAll() with the understanding it would skip over any tasks not in a commit-friendly state?;;;","10/May/20 01:05;ableegoldman;Not exactly, I was proposing to remove the filter entirely and just handle each state inside Task#commit. That is, instead of throwing an exception inside Task#commit if the task is anything other than RUNNING or RESTORING, we just don't do anything. Like we do in Task#suspend, for example: the TaskManager shouldn't have to keep track of which task API's it's allowed to call on a task of a given state, it should just call the API and have the Task implementation choose the appropriate action based on its state.

 

We've pushed most of the task state handling into the Task implementation already, but some of it still bleeds into the TaskManager. It would be nice to be consistent and establish reasonable expectations for the TaskManager: to take another example, Task#initializeIfNeeded allows all task states but only takes any action if the state is CREATED. But Task#completeRestoration will throw an exception if the task is not in RESTORING. ;;;","10/May/20 21:06;mjsax;{quote}the TaskManager shouldn't have to keep track of which task API's it's allowed to call on a task of a given state
{quote}
I am not sure about this. For example, is state is not initialized and the TM tries to process data on that task, it indicates a bug and the task should not just do a no-op.

I guess the original refactoring might not have been totally strict and I agree that we should define some contract who we want to handle it. \cc [~guozhang]

 ;;;","11/May/20 15:06;vvcephei;Thanks for the clarification. The reason I didn’t choose to do that in the original fix is that committing is not encapsulated inside the task. The algorithm looks like:
1. prepareCommit
2. Do other stuff for the commit
3. Commit

If all three parts have to reason about the state of the task, which could even change in the middle, we wind up with a significantly more complicated algorithm. Much better if we just assert that we never even try to commit unless the task is in a legal state for committing. ;;;","11/May/20 18:08;ableegoldman;Fair enough. Then would you propose that the contract should be: Task implementation validates and enforces the state for any multi-phase API (eg preCommit + postCommit) but accepts and internally handles any fully encapsulated API (eg initializeIfNeeded). I don't think we need to drop everything to get all the existing APIs in line, but it would be nice to have something informing future changes in the TaskManager-Task interactions.

Anyways, in that case then I'll amend my proposal for this specific topic and instead suggest we go through the same TaskManager code path for all committing. We're still vulnerable to calling Task#commit on a CREATED task in #handleAssignment at least. 

I'd also prefer to do the task filtration in TaskManager rather than in StreamThread. If we can't keep this logic encapsulated inside Task, we should at least aim to keep it encapsulated inside TaskManager. Compare with TaskManager#tryToCompleteRestoration for example, which filters out the non-RESTORING tasks inside the TaskManager (of course, whether we have to do this filtering at all for Task#completeRestoration is a question for the above contract..);;;","11/May/20 19:11;guozhang;Since at least for now the task-manager (and then its included tasks) are processed single-threaded, the task state should not be changed in 1/2/3 steps above, so I think just checking the state of the task is fine. This is also because today we have three steps as John listed above.

If we can actually merge them in one call as part of the cleanup (which I think may be possible as we discussed in the other PR), then I'm more inclined to Sophie's proposal as to let the task itself decide whether it should commit or not and not exposing the state out of it to the task-manager (I know today even without this we already have other places where task-manager checks the state of individual tasks, but personally I think it's better if we can make them all captured inside task itself... just my preference).;;;","12/May/20 00:57;vvcephei;Thanks, all. To be clear, I also prefer Sophie’s suggestion to keep the state check internal in a world where “commit” is just a single call and we can safely encapsulate the state check as “task has not been running, therefore nothing to commit, so we can safely ignore”.;;;","12/May/20 00:58;vvcephei;Oh, and I would also be in favor of at least encapsulating the state check into the TaskManager.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectorClientConfigRequest is loaded in isolation and throws LinkageError,KAFKA-9969,13303474,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,07/May/20 19:50,11/Jun/20 15:12,13/Jul/23 09:17,11/Jun/20 15:12,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"ConnectorClientConfigRequest (added by [KIP-458|https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy]) is a class in connect-api, and should always be loaded by the system classloader. If a plugin packages the connect-api jar, the REST API may fail with the following stacktrace:

{noformat}
java.lang.LinkageError: loader constraint violation: loader (instance of sun/misc/Launcher$AppClassLoader) previously initiated loading for a different type with name ""org/apache/kafka/connect/connector/policy/ConnectorClientConfigRequest"" at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) at java.net.URLClassLoader.access$100(URLClassLoader.java:74) at java.net.URLClassLoader$1.run(URLClassLoader.java:369) at java.net.URLClassLoader$1.run(URLClassLoader.java:363) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:362) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.kafka.connect.runtime.AbstractHerder.validateClientOverrides(AbstractHerder.java:416) at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:342) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$6.call(DistributedHerder.java:745) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$6.call(DistributedHerder.java:742) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:342) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:282) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ... 1 more
{noformat}

It appears that the other class in org.apache.kafka.connect.connector.policy, ConnectorClientConfigOverridePolicy had a similar issue in KAFKA-8415, and received a fix.",,gharris1727,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 15:12:17 UTC 2020,,,,,,,,,,"0|z0ehkw:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"11/Jun/20 15:12;rhauch;[~kkonstantine] merged to `trunk` and backported to:
* `2.6` for inclusion in upcoming 2.6.0
* `2.5` for inclusion in upcoming 2.5.1
* `2.4` for inclusion in a future 2.4.2
* `2.3` for inclusion in a future 2.3.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Authorizer APIs may be invoked more than once for a given request,KAFKA-9956,13302795,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,dhruvilshah,dhruvilshah,05/May/20 06:32,12/May/20 00:18,13/Jul/23 09:17,12/May/20 00:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Authorizer#authorize may be invoked more than once in some cases for a given request. I noticed this in for `DescribeConfigsRequest` but other requests could be affected as well.

The reason for this is the misuse of the scala `partition` API in code like this:
{code:java}
val (authorizedResources, unauthorizedResources) = describeConfigsRequest.resources.asScala.partition { resource =>
  resource.`type` match {
    case ConfigResource.Type.BROKER | ConfigResource.Type.BROKER_LOGGER =>
      authorize(request.context, DESCRIBE_CONFIGS, CLUSTER, CLUSTER_NAME)
    case ConfigResource.Type.TOPIC =>
      authorize(request.context, DESCRIBE_CONFIGS, TOPIC, resource.name)
    case rt => throw new InvalidRequestException(s""Unexpected resource type $rt for resource ${resource.name}"")
  }
}
{code}
As per Scala docs, the `partition` API could traverse the collection twice, depending on the implementation. [https://www.scala-lang.org/api/current/scala/collection/Iterable.html#partition(p:A=%3EBoolean):(C,C)]

It is also not a good practice to include side effects as part of the lambda passed into `partition`. We should clean up such usages.",,dhruvilshah,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 00:18:56 UTC 2020,,,,,,,,,,"0|z0edp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/20 00:18;ijuma;This issue is not in any released version so I kept ""Fix version"" as empty.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions thrown from SinkTask::close shadow other exceptions,KAFKA-9955,13302775,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gharris1727,gharris1727,gharris1727,05/May/20 03:52,16/May/20 18:09,13/Jul/23 09:17,16/May/20 18:09,0.10.0.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"If an exception is thrown from SinkTask::close, the exception will shadow any other previous exception, because SinkTask::close is called from within a finally block.

Steps to reproduce:
 # Throw an exception from SinkTask::start or SinkTask::put 
 # Throw an exception from SinkTask::close

Expected behavior:
 * All exceptions are visible in separate log messages
 * The error from SinkTask::start or SinkTask::put is logged as the exception that caused the task to stop.

Actual behavior:
 * The exception from SinkTask::close is logged as the exception that caused the task to stop.
 * The exceptions from either SinkTask::start or SinkTask::put are swallowed and don't appear in the logs at all.",,gharris1727,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-05 03:52:20.0,,,,,,,,,,"0|z0edko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker2 sharing of ConfigDef can lead to ConcurrentModificationException,KAFKA-9950,13302563,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,04/May/20 04:35,21/May/20 04:22,13/Jul/23 09:17,21/May/20 04:22,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,mirrormaker,,,,,0,,,,,"The [MirrorConnectorConfig::CONNECTOR_CONFIG_DEF|https://github.com/apache/kafka/blob/34824b7bff64ba387a04466d74ac6bbbd10bf37c/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java#L397] object is reused across multiple MirrorMaker2 classes, which is fine the most part since it's a constant. However, the actual {{ConfigDef}} object itself is mutable, and is mutated when the {{MirrorTaskConfig}} class [statically constructs its own ConfigDef|https://github.com/apache/kafka/blob/34824b7bff64ba387a04466d74ac6bbbd10bf37c/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorTaskConfig.java#L62].

This has two unintended effects:
 # Since the two {{ConfigDef}} objects for the {{MirrorConnectorConfig}} and {{MirrorTaskConfig}} classes are actually the same object, the additional properties that the {{MirrorTaskConfig}} class defines for its {{ConfigDef}} are also added to the {{MirrorConnectorConfig}} class's {{ConfigDef}}. The impact of this isn't huge since both additional properties have default values, but this does cause those properties to appear in the {{/connectors/\{name}/config/validate}} endpoint once the {{MirrorTaskConfig}} class is loaded for the first time.
 # It's possible that, if a config for a MirrorMaker2 connector is submitted at approximately the same time that the {{MirrorTaskConfig}} class is loaded, a {{ConcurrentModificationException}} will be thrown by the {{AbstractHerder}} class when it tries to [iterate over all of the keys of the connector's ConfigDef|https://github.com/apache/kafka/blob/34824b7bff64ba387a04466d74ac6bbbd10bf37c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java#L357].",,ChrisEgerton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-04 04:35:19.0,,,,,,,,,,"0|z0ec9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TransactionsBounceTest may leave threads running,KAFKA-9947,13302373,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,01/May/20 23:10,06/May/20 22:23,13/Jul/23 09:17,06/May/20 22:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"I saw this failure recently:
```
14:28:23 kafka.api.TransactionsBounceTest > testWithGroupId FAILED
14:28:23     org.scalatest.exceptions.TestFailedException: Consumed 0 records before timeout instead of the expected 200 records
14:28:23         at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
14:28:23         at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
14:28:23         at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
14:28:23         at org.scalatest.Assertions.fail(Assertions.scala:1091)
14:28:23         at org.scalatest.Assertions.fail$(Assertions.scala:1087)
14:28:23         at org.scalatest.Assertions$.fail(Assertions.scala:1389)
14:28:23         at kafka.utils.TestUtils$.pollUntilAtLeastNumRecords(TestUtils.scala:843)
14:28:23         at kafka.api.TransactionsBounceTest.testWithGroupId(TransactionsBounceTest.scala:110)
```

This was followed by a bunch of test failures such as the following:
```
14:28:38 kafka.api.TransactionsBounceTest > classMethod FAILED
14:28:38     java.lang.AssertionError: Found unexpected threads during @AfterClass, allThreads=HashSet(controller-event-thread, ExpirationReaper-0-topic, ExpirationReaper-0-ElectLeader, ExpirationReaper-0-Heartbeat, metrics-meter-tick-thread-2, main, metrics-meter-tick-thread-1, data-plane-kafka-socket-acceptor-ListenerName(PLAINTEXT)-PLAINTEXT-41287, scala-execution-context-global-246, transaction-log-manager-0, Reference Handler, scala-execution-context-global-24107, /127.0.0.1:35460 to /127.0.0.1:42451 workers Thread 2, /127.0.0.1:35460 to /127.0.0.1:42451 workers Thread 3, kafka-log-cleaner-thread-0, ExpirationReaper-0-Fetch, scala-execution-context-global-12253, ExpirationReaper-0-Rebalance, Common-Cleaner, daemon-broker-bouncer-EventThread, Signal Dispatcher, SensorExpiryThread, daemon-broker-bouncer-SendThread(127.0.0.1:32919), kafka-scheduler-0, kafka-scheduler-3, kafka-scheduler-4, kafka-scheduler-1, kafka-scheduler-2, kafka-scheduler-7, ExpirationReaper-0-DeleteRecords, kafka-scheduler-8, kafka-scheduler-5, kafka-scheduler-6, scala-execution-context-global-4200, kafka-scheduler-9, LogDirFailureHandler, TxnMarkerSenderThread-0, /config/changes-event-process-thread, ExpirationReaper-0-AlterAcls, group-metadata-manager-0, Test worker, Finalizer, scala-execution-context-global-4199, ThrottledChannelReaper-Produce, data-plane-kafka-request-handler-3, data-plane-kafka-request-handler-2, Controller-0-to-broker-0-send-thread, data-plane-kafka-request-handler-1, data-plane-kafka-network-thread-0-ListenerName(PLAINTEXT)-PLAINTEXT-1, data-plane-kafka-request-handler-0, data-plane-kafka-network-thread-0-ListenerName(PLAINTEXT)-PLAINTEXT-0, data-plane-kafka-network-thread-0-ListenerName(PLAINTEXT)-PLAINTEXT-2, scala-execution-context-global-572, scala-execution-context-global-573, data-plane-kafka-request-handler-7, data-plane-kafka-request-handler-6, data-plane-kafka-request-handler-5, scala-execution-context-global-137, data-plane-kafka-request-handler-4, ThrottledChannelReaper-Request, ExpirationReaper-0-Produce, ThrottledChannelReaper-Fetch), unexpected=HashSet(controller-event-thread, daemon-broker-bouncer-EventThread)
```
The test case needs to ensure that `BounceScheduler` gets shutdown properly.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-01 23:10:52.0,,,,,,,,,,"0|z0eb3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KAFKA-9539/StopReplicaRequest deletePartition changes may cause premature topic deletion handling in controller,KAFKA-9946,13302326,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,dajac,lucasbradstreet,lucasbradstreet,01/May/20 16:21,06/May/20 02:10,13/Jul/23 09:17,06/May/20 02:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,,,,,0,,,,,"It seems like [https://github.com/apache/kafka/commit/7c7d55dbd8d42f6378d13ba02d62633366a7ede8] does not handle StopReplicaRequest where deletePartition(s) is set to false correctly when another delete topic request is outstanding at the time of the response being received.

In the failing code it seems like two StopReplicaRequest(s) are sent, one with the delete flag set on partitions, and one without. It seems like the request without the delete flag set on any partitions is prematurely triggering the controller to believe that the topic was deleted successfully.

We previously didn't set a callback if the StopReplicaRequest was not a delete request [https://github.com/apache/kafka/commit/7c7d55dbd8d42f6378d13ba02d62633366a7ede8#diff-987fef43991384a3ebec5fb55e53b577L570|https://github.com/apache/kafka/commit/7c7d55dbd8d42f6378d13ba02d62633366a7ede8#diff-987fef43991384a3ebec5fb55e53b577L570,]. Now we set it unconditionally [https://github.com/apache/kafka/commit/7c7d55dbd8d42f6378d13ba02d62633366a7ede8#diff-987fef43991384a3ebec5fb55e53b577L570,] but the callback does not distinguish between the partition states where a delete was being performed and where it was not. This happens on all IBP versions.",,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-01 16:21:44.0,,,,,,,,,,"0|z0easw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicCommand should support --if-exists and --if-not-exists when --bootstrap-server is used,KAFKA-9945,13302260,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vinoth,cmccabe,cmccabe,01/May/20 05:52,04/Jun/20 21:25,13/Jul/23 09:17,02/Jun/20 23:19,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-01 05:52:48.0,,,,,,,,,,"0|z0eae8:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigCommand fails to set client quotas for default users with --bootstrap-server.,KAFKA-9942,13301967,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbyrne,d8tltanc,d8tltanc,30/Apr/20 03:39,22/May/20 01:21,13/Jul/23 09:17,22/May/20 01:18,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"{quote}$ bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=1000000000,consumer_byte_rate=1000000000 --entity-type clients --entity-default
{quote}
This usage of --entity-default with --bootstrap-server for alternating configs will trigger the exception below. Similar for --describe
{quote}/opt/kafka-dev/bin/kafka-configs.sh --bootstrap-server ducker04:9093 --describe --entity-type clients --entity-default --command-config /opt/kafka-dev/bin/hi.properties
{quote}
 
{quote}java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownServerException: Path must not end with / character

at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)

at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)

at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:104)

at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:272)

at kafka.admin.ConfigCommand$.getAllClientQuotasConfigs(ConfigCommand.scala:501)

at kafka.admin.ConfigCommand$.getClientQuotasConfig(ConfigCommand.scala:487)

at kafka.admin.ConfigCommand$.alterConfig(ConfigCommand.scala:361)

at kafka.admin.ConfigCommand$.processCommand(ConfigCommand.scala:292)

at kafka.admin.ConfigCommand$.main(ConfigCommand.scala:91)

at kafka.admin.ConfigCommand.main(ConfigCommand.scala)

Caused by: org.apache.kafka.common.errors.UnknownServerException: Path must not end with / character
{quote}
However, if the --entity-type is brokers, the alternation works fine. 
{quote}$ No exception, works properly

bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-default --alter --add-config unclean.leader.election.enable=true

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type brokers --entity-default
{quote}
 

Update:

 

For --describe:

Commands work properly:
{quote}bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type brokers --entity-default

bin/kafka-configs.sh --zookeeper localhost:2181 --describe --broker-defaults
{quote}
Commands do not work:
{quote}bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type topics --entity-default

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type users --entity-default

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type clients --entity-default

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --client-defaults

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user-defaults

 
{quote}
 

For --alter:

Commands work properly:
{quote}bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config max.messages.bytes=128000 --entity-type topics --entity-default (an entity name must be specified with --alter of topics)

bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config unclean.leader.election.enable=true --entity-type brokers --entity-default
{quote}
 

Commands do not work:
{quote}bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=1000000000,consumer_byte_rate=1000000000 --entity-type clients --entity-default

bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=44444 --entity-type users --entity-default (No exception thrown but failed to add the config)

bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=1000000000,consumer_byte_rate=1000000000 --client-defaults

bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=44444 --user-defaults (No exception thrown but failed to add the config)

 
{quote}
 ",,d8tltanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-30 03:39:50.0,,,,,,,,,,"0|z0e8l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch request metrics are updated twice for delayed fetches,KAFKA-9939,13301906,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,29/Apr/20 19:53,01/May/20 22:30,13/Jul/23 09:17,01/May/20 22:30,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We track fetch request rate metrics in `BrokerTopicStats`. We have both topic level and total metrics. Currently these values are updated in `ReplicaManager.readFromLocalLog`. Inside `fetchMessages`, we invoke `readFromLocalLog` first to see if the fetch can be immediately satisfied. If not, then we put it in purgatory. After completing purgatory, we make a second call to `readFromLocalLog` which will again update the request rate metrics. Hence we are overcounting the fetch request rate.",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-29 19:53:07.0,,,,,,,,,,"0|z0e87k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent ReplicaFetcherThread from throwing UnknownTopicOrPartitionException upon topic creation and deletion.,KAFKA-9930,13301680,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,agencer,agencer,agencer,29/Apr/20 01:31,07/Oct/20 19:55,13/Jul/23 09:17,07/Oct/20 19:55,0.10.0.0,0.11.0.0,1.0.0,1.1.0,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,2.5.0,,,,,,,,,,,,,,,,,,,,,logging,,,,,0,,,,,"When does UnknownTopicOrPartitionException typically occur?
 * Upon a topic creation, a follower broker of a new partition starts replica fetcher before the prospective leader broker of the new partition receives the leadership information from the controller. Apache Kafka has a an open issue about this (see KAFKA-6221)
 * Upon a topic deletion, a follower broker of a to-be-deleted partition starts replica fetcher after the leader broker of the to-be-deleted partition processes the deletion information from the controller.
 * As expected, clusters with frequent topic creation and deletion report UnknownTopicOrPartitionException with relatively higher frequency.

What is the impact?
 * Exception tracking systems identify the error logs with UnknownTopicOrPartitionException as an exception. This results in a lot of noise for a transient issue that is expected to recover by itself and a natural process in Kafka due to its asynchronous state propagation.

Why not move it to a lower than warn-level log?
 * Despite typically being a transient issue, UnknownTopicOrPartitionException may also indicate real issues if it doesn't fix itself after a short period of time. To ensure detection of such scenarios, we set the log level to warn.",,agencer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-29 01:31:39.0,,,,,,,,,,"0|z0e6tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky GlobalKTableEOSIntegrationTest#shouldKStreamGlobalKTableLeftJoin[exactly_once_beta],KAFKA-9928,13301652,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,guozhang,guozhang,28/Apr/20 20:42,08/May/20 06:02,13/Jul/23 09:17,08/May/20 06:02,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,,,,,"{code}
Stacktrace
java.lang.AssertionError: Condition not met within timeout 30000. waiting for final values
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:381)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:429)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:380)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368)
	at org.apache.kafka.streams.integration.GlobalKTableEOSIntegrationTest.shouldKStreamGlobalKTableLeftJoin(GlobalKTableEOSIntegrationTest.java:178)
{code}

I looked at the below examples:

https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/149/testReport/junit/org.apache.kafka.streams.integration/GlobalKTableEOSIntegrationTest/shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_/

https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6017/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_beta__2/

https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6017/testReport/junit/org.apache.kafka.streams.integration/GlobalKTableEOSIntegrationTest/shouldKStreamGlobalKTableLeftJoin_exactly_once_beta__2/

And also reproduced the flakiness locally after about 180 runs, and the failed one did not have any obvious different traces compared with the successful ones.",,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 22:23:52 UTC 2020,,,,,,,,,,"0|z0e6n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 22:23;guozhang;I found that for the failed run, around the time when the producer of {{produceTopicValues(streamTopic);}} around line 172 is being closed, the following entries are printed (whereas succeeded runs do not have those), cc [~mjsax]:

{code}
[2020-04-28 15:10:58,458] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Fetch offset 9 is out of range for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0, resetting offset (org.apache.kafka.clients.consumer.internals.Fetcher:1261)
[2020-04-28 15:10:58,458] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Resetting offset for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-28 15:10:58,459] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Fetch offset 9 is out of range for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0, resetting offset (org.apache.kafka.clients.consumer.internals.Fetcher:1261)
[2020-04-28 15:10:58,460] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Resetting offset for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-28 15:10:58,461] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Fetch offset 9 is out of range for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0, resetting offset (org.apache.kafka.clients.consumer.internals.Fetcher:1261)
[2020-04-28 15:10:58,461] INFO [Consumer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-consumer, groupId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_] Resetting offset for partition stream-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-28 15:10:58,566] INFO [Producer clientId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-StreamThread-1-producer, transactionalId=globalTable-eos-test-shouldKStreamGlobalKTableLeftJoin_exactly_once_beta_-0a424027-ab72-4de4-9d83-58989a76b029-1] Discovered group coordinator localhost:54279 (id: 0 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager:1525)
[2020-04-28 15:11:00,740] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
{code}

Note that this CLUSTER only have one broker.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-key KTable Joining may result in duplicate schema name in confluence schema registry,KAFKA-9925,13301551,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,nikuis,nikuis,28/Apr/20 12:15,29/Apr/20 22:35,13/Jul/23 09:17,29/Apr/20 22:35,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,streams,,,,,0,,,,,"The second half of issue Andy Bryant reported in KAFKA-9390 looks like still exist.

When testing non-key join method without passing in ""Named"", I noticed that there are schema subjects registered in confluent schema registry without consumer group Id still, 
e.g. 
{noformat}
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000005-topic-pk-key"",
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000005-topic-fk-key"",
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000005-topic-vh-value"",
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000025-topic-pk-key"",
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000025-topic-fk-key"",
""KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-0000000025-topic-vh-value""
{noformat}
Code in KTableImpl which constructed above naming :
https://github.com/apache/kafka/blob/2.4.1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L959

When we have multiple topologies using foreignKey join and registered to same schema registry, we can have a name clash, and fail to register schema. 

In order to clean up these schema subjects, we will need to know the internal naming of a consumer group's topology, which is not straightforward and error prone.

",,ableegoldman,guozhang,mjsax,nikuis,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 22:35:00 UTC 2020,,,,,,,,,,"0|z0e60w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/20 15:23;vvcephei;Ah, right you are. So sorry I overlooked that part of the bug report when I submitted my fix for it.

The issue is that these ""pseudo topics"" are being created the same way that real repartition topics get created in the DSL layer, but for real repartition topics, we add them to the InternalTopologyBuilder, which later on invokes org.apache.kafka.streams.processor.internals.InternalTopologyBuilder#decorateTopic to add the applicationId prefix. Of course, this will never happen for the pseudo-topics, since we don't add them to the InternalTopologyBuilder.

The complication is that we don't know the applicationId until the application is started. Currently, both the DSL builder and the runtime are isolated from this because the DSL builder only has to register the topic with the InternalTopologyBuilder, and then the runtime code only has to deal with pre-configured Serdes, which get the pre-decorated topics injected at startup.

I'll submit a PR shortly to fix it.;;;","28/Apr/20 17:57;vvcephei;Ok, I've opened [https://github.com/apache/kafka/pull/8574] . If you have the time, a review would help speed things along. Thanks for the report!;;;","28/Apr/20 18:42;guozhang;[~vvcephei] Thanks for getting a look into this issue.

I'm wondering if now is a good time to deprecate the `StreamsBuilder#build()` function to let users use `build(final Properties props)` instead as a tiny KIP. There's risk of course that the props passed in `build` is not the same as the one passed into the `KafkaStreams` constructor. I think we can remember the reference of the Props when building the topology, and then at construction if we found they are not the same (by reference), we can log a warning such that ""found the topology is built with some StreamsConfig already, which is not the same as the config passed in the constructor"".;;;","28/Apr/20 19:01;mjsax;{quote}I'm wondering if now is a good time to deprecate the `StreamsBuilder#build()` function to let users use `build(final Properties props)` instead as a tiny KIP.
{quote}
Just FYI: this is already proposed in KIP-591.

However, IMHO, we should fix it for older versions, too?;;;","28/Apr/20 19:10;guozhang;Ah yes!! Hope we can get KIP-591 by 2.6 :);;;","29/Apr/20 22:35;vvcephei;Just saw this conversation. Thanks for the idea, [~guozhang] . I guess it's moot now, since I've just merged the PR ;)

I agree we should consider that separately, since we need to backport the fix. Plus, to replace the run time vs build time problem, we'd have to _remove_ `build()`, not just _deprecate_ it, so I think even after KIP-591, we'll have to support this code path for a while.

Anyway, I'm resolving this ticket, since the fix is merged to trunk, 2.5, and 2.4. Thanks for the report and review, [~nikuis] ! And sorry again for the trouble.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update examples README,KAFKA-9922,13301195,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,adally,adally,adally,27/Apr/20 08:56,30/Apr/20 02:43,13/Jul/23 09:17,30/Apr/20 02:41,,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,consumer,documentation,,,,0,documentation,patch,,,Class kafka.examples.SimpleConsumerDemo was removed. But the java-simple-consumer-demo.sh was not removed and README was not updated,,adally,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,2020-04-27 08:56:51.0,,,,,,,,,,"0|z0e3ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching is not working properly with WindowStateStore when retaining duplicates,KAFKA-9921,13301051,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,georgi.petkov,georgi.petkov,26/Apr/20 07:15,30/Nov/20 16:45,13/Jul/23 09:17,28/Apr/20 20:06,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,streams,,,,,0,,,,,"I'm using the current latest version 2.5.0 but this is not something new.

I have _WindowStateStore_ configured as following (where _true_ stands for the _retainDuplicates_ paramter):
 _builder.addStateStore(windowStoreBuilder(persistentWindowStore(name, retentionPeriod, windowSize, *true*), keySerde, valueSerde)*.withCachingEnabled()*)_

If I put 4 key-value pairs with the same key and values *1, 2, 3, 4* in that order when reading them through the iterator I'll get the values *4, 2, 3, 4*.
 I've done a bit of investigation myself and the problem is that *the whole caching feature is written without consideration of the case where duplicates are retained*.

The observed behavior is due to having the last value in the cache (and it can have only one since it's not aware of the retain duplicates option) and it is read first (while skipping the first from the RocksDB iterator even though the values are different). This can be observed (for version 2.5.0) in _AbstractMergedSortedCacheStoreIterator#next()_ lines 95-97. Then the next 3 values are read from the RocksDB iterator so they are as expected.

As I said, the whole feature is not considering the _retainDuplicates_ option so there are other examples of incorrect behavior like in _AbstractMergedSortedCacheStoreIterator__#peekNextKey()_ - for each call, you would skip one duplicate entry in the RocksDB iterator for the given key.

In my use case, I want to persist a list of values for a given key without increasing the complexity to linear for a single event (which would be the case if I was always reading the current list appending one value and writing it back). So I go for _List<KeyValuePair<K, V>>_ instead of _KeyValuePair<K, List<V>>_. The whole use case is more complex than that so I use _#transformValues_ and state stores.

So as an impact I can't use caching on my state stores. For others - they'll have incorrect behavior that may take a lot of time to be discovered and even more time to fix the results.",,ableegoldman,bchen225242,cadonna,georgi.petkov,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4887,,,,,,,,,,,,,,,,KAFKA-9923,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 22:14:26 UTC 2020,,,,,,,,,,"0|z0e2xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/20 17:49;bchen225242;Hey [~georgi.petkov], would you mind explaining why you want to turn on caching while still retains duplicates?;;;","27/Apr/20 19:10;georgi.petkov;The same reason you would ever want caching - for the expected performance improvement. I already explained why I need to retain the duplicates in the pre-last paragraph.

Now that I think about it, maybe none of the operations can be optimized as much unless I've already performed read operation on a key and I repeat that later on before flush (with or without changes in between). That is due to the fact that you never know if there are other values for the key you operate on unless you call the underlying store first.

But all that is because I've familiarized myself with the actual implementation. Nobody would ever consider any of this when reading the documentation or looking at the API. Even if you decide that no significant performance gain can be obtained at least you should disable this combination of caching and retaining duplicates since the results are incorrect even in a straightforward scenario like the one provided in the description.;;;","27/Apr/20 20:18;ableegoldman;Hey [~georgi.petkov]

You're right that there seems to be a bug in the store hierarchy that makes duplicates incompatible with caching (and, unfortunately, also with changelogging – see KAFKA-9923). Clearly they were never meant to be used in combination; note that duplicates in the DSL are only used for stream-stream joins, for which you can't enabled caching. Of course this unfortunately left PAPI users vulnerable.

That said, I guess I'm wondering what the expected semantics are? Note that ""duplicates"" in this context doesn't mean ""doubles"", it stores any number of records with the same key. The only thing caching could really be expected to help with is reduce the number of idempotent updates, where the entire record is identical vs just the key. But that's kind of a separate matter. If you're interested you should check out [KIP-557|https://cwiki.apache.org/confluence/display/KAFKA/KIP-557%3A+Add+emit+on+change+support+for+Kafka+Streams] which is currently in progress. But there's really no reason to use caching with duplicates, so yes we should explicitly disallow this combination.;;;","28/Apr/20 05:50;georgi.petkov;[~ableegoldman] WindowStateStores don't really offer updates (or deletes in that matter) at least when using _retainDuplicates_ so `idempotent updates` sounds inappropriate to me. For 2 puts I would expect 2 entries regardless if they accidentally match.

*I was unable to determine the expected bahavior when putting _null_ values in _WindowStateStore_ (from documentation).* It turns out behaving like the _KeyValueStore_ - just delete the existing entry unless using _retainDuplicates_ - then *nothing happens, neither null is persisted nor any entries are deleted*. I've debugged the code and it reaches all the way to calling delete in RocksDB, so I'm not sure this is intended (or at least could be skipped in this case). What do you think? Should I create a separate bug for that? What should be the expected behavior?

Is there some other efficient approach for keeping a list by key? In my case, the store key is not the partition key but a relation between events and I would like to avoid repartitioning. To be honest I had a really hard time finding the appropriate tools for the job. The API is very limited in operations or at least no matter how I turn this around it feels that this is not the most efficient way to do things. *I have already partitioned data by a key that serves as a correlation ID (so data within a partition is self-contained).* The problem could be summarized to ""*I need stream-stream join while avoiding repartitioning*"".
 * If I go with PAPI then I need an efficient retention policy - go with _WindowStateStore_ (and its not that pleasant API when all you need is the retention policy). Then I need an efficient persisting of values by key - retain duplicates (so you only append new values),  but it turns out no optimizations in terms of caching are possible. So far this seems like the best approach and this is what I'm doing but it seems like I'm reimplementing stream-stream join without repartitioning.
 * If I go for stream-stream join this means to repartition both streams first since I have different keys to join by. This means 2 extra topics that won't be reused for the internally used _WindowStateStores_ (and I know that my data is partitioned well enough already). It would have been nice if I had the option to avoid repartitioning with a ""my data is already properly partitioned / I know what I'm doing"" option.
 * If I go with Clients API then I'm basically starting from scratch with API that is hard to use right and there are no state stores available.

*Any advice?* Am I missing something?;;;","28/Apr/20 19:53;ableegoldman;> For 2 puts I would expect 2 entries regardless if they accidentally match

Fair enough. I guess for that reason then caching and inherently incompatible, right?

Regarding putting _null_ values, I think the behavior with _retainDuplicates_ is as expected. The Streams library uses window stores with duplicates for stream-stream joins, for which a null value produces no output and isn't considered a tombstone (see [semantics of stream-stream joins|https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#kstream-kstream-join] section).

I'm starting to get a better sense of what you're trying to do here, but it sounds like the semantics you want might differ slightly from what Streams would consider a stream-stream join. Do you explicitly want a windowed join, or are you just using the window store because the retention policy will keep state from growing without bound? Does your use case require _null_ values to be treated as deletes?

By the way, if the built-in stores don't match your requirements exactly you can always plug in a custom store. You could even just wrap one of the built-in stores to reuse the pieces that work for you, and skip the ones that don't. The rocksdb WindowStore is actually just built out of segments of the rocksdb KeyValueStore, for example.;;;","28/Apr/20 19:58;ableegoldman;I take it you're using rocksdb, by the way? If you are (or can) use the in-memory stores then storing a list and appending should be pretty fast. On that note, I'm actually not sure storing the entire list would be slower than storing individual duplicate records even with rocskdb. I actually have a suspicious that it might even be faster to store as a list, assuming the number and size of duplicates isn't incredibly large (relative to the memtable and block size scale);;;","28/Apr/20 20:05;ableegoldman;I'm resolving the ticket because the PR to disable caching + duplicates and note this in the javadocs was just merged. If you have the chance to take a quick look and let me know if there's anything I missed clarifying in the docs, I can submit a quick followup PR or review one from you if you have something specific in mind;;;","28/Apr/20 22:19;georgi.petkov;[~ableegoldman]

Yeah, I agree that probably not much can be done in terms of caching (compared to the options without _retainDuplicates_).

I totally agree that many of the features like the null value behavior are correct and make perfect sense from point of view of the features implemented with it. Still, it's strange from the perspective where you use it standalone. *1-2 sentences clarifying the behavior with null values in the _WindowStateStore_ documentation could definitely help.* In addition, as I said if this is the desired behavior *you can easily skip calling RocksDB for null values (when using _retainDuplicates)_. This would both make the intention clearer and obviously avoid unnecessary calls.*

I do need exactly stream-stream join but without the repartition part. I want to get matches when there are new events in whichever stream, support duplicate keys in the stream and I also use _WindowStateStore_ only for the retention policy. In fact, due to the lack of many examples, I was looking at the stream-stream join implementation to find out how to correctly use the _WindowStateStores_. I'm building a library for some common yet not trivial at all operations on streams that you may need like topological sorting. Therefore I don't know if the user will provide null values or not. I was curious about the behavior with null values so I know what I'm providing to the user. I've tested it and that's how I found out what is the exact behavior.

*I'm not sure that an in-memory or any custom state store will make it.* Yes, in-memory will help with the efficient append because it avoids any expensive call and serializations/deserializations. Nevertheless, *you will always have the serializations/deserializations somewhere and this is the changelog topic and there you have also bandwidth* (not just precious processing time). Even if the list is fixed to let's say only 5 items you will still have 15 (1 + 2 + 3 + 4 + 5) events recorded instead of 5. Obviously the size grows pretty fast - O(n^2). Combined with the fact that I want to provide a library to many different users (and duplicates count may vary a lot between usages) *to me it's best to implement just as in the stream-stream join - with duplicates*. Still, it was a great discussion and made me more confident in my decisions. Thank you for your assistance.

*Regarding the PR - it adds the same code to both _WindowStoreBuilder.java_ and _TimestampedWindowStoreBuilder.java_ but adds a test for only one of them.*;;;","06/May/20 19:28;georgi.petkov;[~ableegoldman] Did you get to read my last comment? At least the bolded text?;;;","06/May/20 19:53;ableegoldman;Thanks for the reminder, just opened [this small PR|https://github.com/apache/kafka/pull/8626] to add back in the test and hopefully clear up the handling of null values. Please take a look when you get the chance.;;;","06/May/20 21:50;georgi.petkov;Maybe we can add the same information on the WindowStore#put method as well.

It's a personal style preference but if there are short circuit checks in which cases you have trivial or no implementation I would put it at the beginning of the method instead of adding more branching in the rest of the logic. So I would write:
{code:java}
if (some corner case) {
    doSomething();
    return;
}{code}
instead of:
{code:java}
if (some corner case) {
    doSomething();
} else {
    // nested code
    if (...) {
        ...
    } else {
        ...
    }
}{code}
It's kind of hard to explain. See [this|https://softwareengineering.stackexchange.com/questions/18454/should-i-return-from-a-function-early-or-use-an-if-statement] question and InMemoryWindowStore#put.

I haven't checked but I would guess that the behavior with null values and retainDuplicates has no explicit tests. If that it the case you could add some.;;;","08/May/20 02:00;ableegoldman;No need to explain, I actually agree with you in general when it comes to returning early vs if/else. But the general consensus seems to be against returning early, speaking from experience (I've been ""asked"" to switch to if/else in Streams code review before ;)) Anyways the current code base follows the if/else pattern, and consistency is more important than anything particular style issue imho. 

I'm a bit hesitant to add this to the WindowStore#put API, because technically the WindowStore is and should be completely agnostic to the presence of duplicates. Put just guarantees that, for the given key, it will insert the value or delete that key's entry if null. The ""trick"" is that this key gets wrapped up with a unique id in the case of retainDuplicates, so the final #put call is not on the same key. In the case of retainDuplicates and null values we know this will be a no-op so we can just skip the delete, but it's really more of an optimization than a specific or special behavior.

It's not very (or at all) straightforward, and ideally we would not even have this option but a separate store implementation entirely for the duplicates case. That's a much bigger scope of work, but maybe there are some small steps in the right direction we can take now. For example, instead of just hacking the duplicates in as a boolean switch, we could split that out into another store wrapper just like the cache, logging, metrics, etc. Then the DuplicatesWindowStore (any better name suggestions?) could do the key wrapping before delegating to the underlying store, which can then remove the hacky `retainDuplicates` flag. That seems like it would make the duplicate handling logic more explicit, and we can describe it further in the javadocs if necessary. WDYT?

Feel free to comment on the PR directly btw. Thanks for the feedback!;;;","08/May/20 07:03;georgi.petkov;I will do further comments regarding the code in the PR.

Indeed usually boolean flags are a code smell indicating that perhaps another approach should be taken. The idea of adding another decorator sounds good. Still, things will be more explicit in the code but not in the API. Therefore your main concern regarding the fact that #put ??should be completely agnostic to the presence of duplicates?? still remains. IMO it's better to mention it there than not. Since the library is providing it as a feature I guess it's not that much of an implementation detail than clarifying behavior. I can see how a user of the API may be confused that putting null will remove all entries with that key. If it's not written, personally I would either do a google search, check the implementation or test it and then add a test in my own code as a regression test since when not documented and guaranteed by the library this could change and I depend on it.

Regarding the name - _DuplicatesWindowStore_ sounds more like it's storing only the duplicates or at least can be confusing. I would suggest _MultiValueWindowStore_ or something similar as it appears to be the general term for collections like Mutilset and Multimap.;;;","08/May/20 20:16;ableegoldman;Fair enough. I added the clarification to WindowStore#put for now – I was thinking that ultimately it might make sense for the window store with duplicates to use an entirely different interface, in which case the comment on WindowStore#put would not really make sense, but we can always move things around later. 

I think refactoring out the retainDuplicates flag into a separate interface might make sense to do as part of the fix for KAFKA-9923 although it'll require a KIP if we change or add to any of the supplier methods in Stores. It should be fairly straightforward though. If you have any strong opinions on what the duplicates API should look like we'd welcome a KIP/PR.;;;","08/May/20 21:07;georgi.petkov;If this will be handled in a more general and better way (as part of KAFKA-9923) then perhaps you can skip the second commit that applies the optimization. The optimization is not really urgent and would only increase the complexity of the code to be refactored later (and has no explicit test for the handled case). Still - up to you, I'm not strongly opinionated.

I'm not sure that we need any change to the current API. The use of an additional decorator in case of duplicates retention should remain hidden from the user's point of view. Can you be more concrete on the changes in the API that you're thinking of?;;;","08/May/20 21:31;ableegoldman;I just meant deprecating the Stores#xxxWindowStore variants and creating new ones without the `retainDuplicates` flag, ie Stores#xxxwindowStore and Stores#xxxMultiValueWindowStore. It's not really necessary but I think it would be nice. Another unnecessary but potentially nice KIP-requiring change would be to create a separate interface for the retainDuplicates window stores – for example, the normal WindowStore#fetch(key, time) returns just a single value and doesn't really make sense for window stores with duplicates.;;;","08/May/20 21:41;georgi.petkov;+1 for the new methods in Stores.

Currently, fetches are only for a range and since the range can contain more than one window it can return more than one value even when not retaining duplicates. I don't see any fetch by a key and a single timestamp or do you mean to have one in the future?;;;","08/May/20 21:48;ableegoldman;Ah, it's a bit hidden but there actually is a point-lookup fetch method. It's declared on ReadOnlyWindowStore instead of WindowStore. I guess because it's not used in the ""normal"" Streams code anywhere but is still allowed for IQ? ;;;","08/May/20 22:14;georgi.petkov;Oh yeah, my bad.

Well, you can definitely start without new interfaces for the time being (preserving the current behavior even for the case with _WindowStore#fetch(key, time)_). Then you won't need different return types and therefore different methods in _Stores_ - those can be added later (potentially when more information or opinions are present).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SslEngineFactory is NOT closed when channel is closing,KAFKA-9918,13300949,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chia7712,chia7712,chia7712,25/Apr/20 10:56,04/May/20 17:26,13/Jul/23 09:17,04/May/20 17:26,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"the default implementation (DefaultSslEngineFactory) does not have any releasable object so we didn't notice this issue. However, it would be better to fix this issue for the custom engine factory.",,chia7712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-25 10:56:40.0,,,,,,,,,,"0|z0e2b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreQueryIntegrationTest.shouldQueryAllStalePartitionStores,KAFKA-9898,13300101,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ableegoldman,ableegoldman,21/Apr/20 22:15,26/May/20 18:15,13/Jul/23 09:17,26/May/20 18:15,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,unit-test,,,"h3. Stacktrace

java.lang.AssertionError: Expected: is <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQueryAllStalePartitionStores(StoreQueryIntegrationTest.java:233)

 

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5900/testReport/junit/org.apache.kafka.streams.integration/StoreQueryIntegrationTest/shouldQueryAllStalePartitionStores/]",,ableegoldman,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9897,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 18:15:04 UTC 2020,,,,,,,,,,"0|z0dx2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/20 18:15;guozhang;I'd mark it as resolved by https://github.com/apache/kafka/pull/8568/files for now too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StandbyTaskEOSIntegrationTest#surviveWithOneTaskAsStandby[exactly_once_beta],KAFKA-9896,13299860,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ableegoldman,mjsax,mjsax,21/Apr/20 03:47,18/Jun/20 03:27,13/Jul/23 09:17,18/Jun/20 03:27,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1872/testReport/junit/org.apache.kafka.streams.integration/StandbyTaskEOSIntegrationTest/surviveWithOneTaskAsStandby_exactly_once_beta_/]
{quote}java.lang.AssertionError at org.junit.Assert.fail(Assert.java:87) at org.junit.Assert.assertTrue(Assert.java:42) at org.junit.Assert.assertTrue(Assert.java:53) at org.apache.kafka.streams.integration.StandbyTaskEOSIntegrationTest.surviveWithOneTaskAsStandby(StandbyTaskEOSIntegrationTest.java:113){quote}",,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 03:27:35 UTC 2020,,,,,,,,,,"0|z0dvl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/20 01:57;mjsax;[~guozhang] [~bchen225242] I ran this test like 500 times locally and it never failed. And we only had a single failure on Jenkins. Closing this for now as ""cannot reproduce"". If it happens again, we can just reopen.;;;","10/Jun/20 19:50;ableegoldman;[~mjsax] I saw this fail locally, although it was for _exactly_once_ not _exactly_once_beta ._ Not sure if it really exposes an actual bug or is just flaky? The line that timed out was given just 15 seconds;;;","16/Jun/20 20:49;ableegoldman;Saw this fail again locally, will submit a PR;;;","18/Jun/20 03:27;mjsax;Resolving for now. If the fix is not good enough, we can just reopen again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer state snapshot needs to be forced to disk,KAFKA-9892,13299783,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kbrajesh176,junrao,junrao,20/Apr/20 19:05,09/Dec/20 22:14,13/Jul/23 09:17,09/Dec/20 22:14,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,core,,,,,0,,,,,"Currently, ProducerStateManager.writeSnapshot() only calls fileChannel.close(), but not explicitly fileChannel.force(). It seems force() is not guaranteed to be called on close(). ",,iBlackeyes,ijuma,junrao,kbrajesh176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 14:36:40 UTC 2020,,,,,,,,,,"0|z0dv40:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"20/Apr/20 19:53;ijuma;Do we need `force` semantics here? cc [~hachikuji];;;","29/Oct/20 17:21;kbrajesh176;[~junrao]

[~ijuma]

Can I pick this up please? It seems to be a good beginner bug.;;;","29/Oct/20 18:05;junrao;[~kbrajesh176]: Thanks for your interest. Just added you to Kafka contributor list. So, feel free to assign this jira to yourself.;;;","19/Nov/20 14:36;kbrajesh176;[~ijuma]Can you please help me with build on my CR ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid state store content after task migration with exactly_once and standby replicas,KAFKA-9891,13299686,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mjsax,mateuszjadczyk,mateuszjadczyk,20/Apr/20 12:47,19/Jun/20 22:16,13/Jul/23 09:17,19/Jun/20 20:01,2.3.1,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,streams,,,,,0,,,,,"We have a simple command id deduplication mechanism (very similar to the one from Kafka Streams examples) based on Kafka Streams State Stores. It stores command ids from the past hour in _persistentWindowStore_. We encountered a problem with the store if there's an exception thrown later in that topology.
 We run 3 nodes using docker, each with multiple threads set for this particular Streams Application.

The business flow is as follows (performed within a single subtopology):
 *  a valid command is sent with command id (_mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f_). NODE 1 is running an active task 1_2. First node in the topology analyses if this is a duplicate by checking in the state store (_COMMAND_ID_STORE_), if not puts the command id in the state store and processes the command properly.
 * an invalid command is sent with the same key but new command id (_mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc_). Again, check for the duplicated command id is performed, it's not a duplicate, command id is put into the state store. Next node in the topology throws an exception which causes an error on NODE 1 for task 1_2. As a result, transaction is aborted, offsets are not committed. I double checked for the changelog topic - relevant messages are not committed. Therefore, the changelog topic contains only the first command id _mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f,_ and not the one which caused a failure.
 * in the meantime a standby task 1_2 running on NODE 3 replicated _mnl_cmd_31bad7e5-35e7-490e-89f0-616fe967111f_ command id into a local _COMMAND_ID_STORE_
 * standby task 1_2 on NODE 3 Thread-2 takes over the task as an active one. It checks if this command id is a duplicate - no, it isn't - tries to process the faulty command and throws an exception. Again, transaction aborted, all looks fine.
 * NODE 3 Thread-1 takes over. It checks for the duplicate. To our surprise, *it is a duplicate!* Even though the transaction has been aborted and the changelog doesn't contain this command id: _mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc._

 

After digging into the Streams logs and some discussion on ([Stack Overflow|https://stackoverflow.com/questions/61247789/invalid-state-store-content-after-aborted-transaction-with-exactly-once-and-stan]) we concluded it has something to do with checkpoint files. Here are the detailed logs relevant to checkpoint files.

 
{code:java}
NODE_3 2020-04-15 21:06:14.470 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {}
NODE_3 2020-04-15 21:06:19.413 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint null
NODE_3 2020-04-15 21:06:28.470 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets read from checkpoint: {}
NODE_3 2020-04-15 21:06:29.634 TRACE 1 --- [-StreamThread-2] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:06:29.640 TRACE 1 --- [-StreamThread-2] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint
NODE_3 2020-04-15 21:11:15.909 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:11:15.912 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpointNODE_1 log1:2020-04-15 21:11:33.942 DEBUG 1 --- [-StreamThread-2] c.g.f.c.s.validation.CommandIdValidator : CommandId: mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc is not a duplicate.NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Writing tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp
NODE_3 2020-04-15 21:11:47.233 TRACE 1 --- [-StreamThread-1] o.a.k.s.s.internals.OffsetCheckpoint : Swapping tmp checkpoint file /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint.tmp /tmp/kafka-streams/XXXXProcessor/1_2/.checkpoint
NODE_3 2020-04-15 21:11:49.075 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.436 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Found checkpoint 1 from changelog XXXXProcessor-COMMAND_ID_STORE-changelog-2 for store COMMAND_ID_STORE.NODE_3 2020-04-15 21:11:52.023 DEBUG 1 --- [-StreamThread-2] c.g.f.c.s.validation.CommandIdValidator : CommandId: mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc is not a duplicate.
NODE_3 2020-04-15 21:11:53.683 ERROR 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Failed to process stream task 1_2 due to the following error: java.lang.RuntimeExceptionNODE_3 2020-04-15 21:12:05.346 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:12:05.562 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.StoreChangelogReader : stream-thread [XXXXProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Found checkpoint 1 from changelog XXXXProcessor-COMMAND_ID_STORE-changelog-2 for store COMMAND_ID_STORE.NODE_3 2020-04-15 21:12:06.424 WARN 1 --- [-StreamThread-1] c.g.f.c.s.validation.CommandIdValidator : Command duplicate detected. Command id mnl_cmd_9f1752da-45b7-4ef7-9ef8-209d826530bc
{code}
It seems that on NODE_3 there's a standby task 1_2 running on T-2, it replicates a first valid command, thus creating a checkpoint file. Invalid command causes an error on NODE_1, then NODE_3 T-2 takes over the task. It finds the checkpoint file (which is fine), and starts to process the invalid command. It crashes, same node T-1 takes over, finds the checkpoint file (!), thinks state store is clean (apparently it's not as it contains state modified by T-2) and finds a duplicated command id.

 

We use Java 11, kafka clients 4.1 and spring-kafka 2.4.5. We rolled back for a moment to kafka clients 2.3.1 and the problem persists.

*We performed more tests with configuration changes and after changing `num.standby.replicas = 1` to `num.standby.replicas = 0` the problem disappeared. It is also resolved when changing the store to _inMemoryWindowStore._*

In the SO question you can find the relevant java code. I don't have a sample project to share at the moment which replicates the problem, but it is easily repeatable in our project.

Such behaviour can have serious implications on business logic, in our case accidentally skipped messages without properly processing them.",,ableegoldman,bchen225242,mateuszjadczyk,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/20 14:31;mateuszjadczyk;failedtest;https://issues.apache.org/jira/secure/attachment/13003867/failedtest","24/May/20 14:31;mateuszjadczyk;failedtest2;https://issues.apache.org/jira/secure/attachment/13003868/failedtest2","24/May/20 15:44;mateuszjadczyk;failedtest3;https://issues.apache.org/jira/secure/attachment/13003874/failedtest3","24/May/20 15:44;mateuszjadczyk;failedtest3_bug;https://issues.apache.org/jira/secure/attachment/13003873/failedtest3_bug","08/May/20 15:20;mateuszjadczyk;state_store_operations.txt;https://issues.apache.org/jira/secure/attachment/13002422/state_store_operations.txt","08/May/20 15:20;mateuszjadczyk;tasks_assignment.txt;https://issues.apache.org/jira/secure/attachment/13002423/tasks_assignment.txt",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 20:00:14 UTC 2020,,,,,,,,,,"0|z0duig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 19:59;bchen225242;Thanks for the report, will take a look this week.;;;","25/Apr/20 16:56;bchen225242;I made a try to replicate the scenario with integration test, but 2.4 didn't fail. Could you take a look and see if my implementation replicates what you suggested?;;;","28/Apr/20 08:59;mateuszjadczyk;Thanks for looking into it. I looked into the test (see comments in the PR), played a bit with it and enabled some more logging and I may have more insights.
First of all, are you sure your test uses 2.4 clients? We used 2.4.1 clients and this broker image confluentinc/cp-zookeeper:5.3.1. I see that you use StoreQueryParameters in the code which is not available in Streams 2.4.1 and also ProcessorStateManager implementation changed a lot.

I also revised the logs I included in the ticket and may have a new finding. The flow is:
 * NODE 1 T-2 has active task 1_2.
 * NODE 3 *T-1* has standy task 1_2.
 * NODE 1 T-2 crashes
 * NODE 3 *T-2* takes over, T-1 (which had a standby task) is assigned other task, standby task 1_2 is revoked.
 * NODE 2 T1 has standby task 1_2
 * NODE 3 T-2 crashes
 * NODE 3 T-1 takes over
 * NODE2 T-1 standby task 1_2 is revoked.

 

The crucial takeaway here is that if we focus on strictly NODE 3, we can see that the task 1_2 was not taken over by a thread T-1 with standby task, but rather T-2. I guess that's how this version of TaskAssignor works. Digging deeper I checked what exactly happened when standby task was revoked on T-1, and active task was starting on T-2.
So this is T-1 having standby task revoked:
{noformat}
NODE_3 2020-04-15 21:11:47.024  INFO 1 --- [-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] State transition from RUNNING to PARTITIONS_ASSIGNED
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.AssignedStandbyTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Closing revoked standby tasks {1_2=[mnl.xxxx.command-2, xxxx.command-2]}
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing
NODE_3 2020-04-15 21:11:47.027 TRACE 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Committing
NODE_3 2020-04-15 21:11:47.027 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing all stores registered in the state manager
NODE_3 2020-04-15 21:11:47.032 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing store COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:47.194 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Flushing store XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets updated with restored offsets: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Checkpointable offsets updated with active acked offsets: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Writing checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:47.296 TRACE 1 --- [-StreamThread-1] o.a.k.s.processor.internals.StandbyTask  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing state manager
NODE_3 2020-04-15 21:11:47.296 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing its state manager and all the registered state stores
NODE_3 2020-04-15 21:11:47.298 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing storage engine COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:47.388 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Closing storage engine XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:47.455 DEBUG 1 --- [-StreamThread-1] o.a.k.s.p.internals.StateDirectory       : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] Released state dir lock for task 1_2{noformat}
And this is T-2 with active task starting:
{noformat}
NODE_3 2020-04-15 21:11:46.556 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] New active tasks to be created: {1_2=[mnl.xxxx.command-2, xxxx.command-2]}
NODE_3 2020-04-15 21:11:46.697 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}
NODE_3 2020-04-15 21:11:46.703 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Created state store manager for task 1_2
NODE_3 2020-04-15 21:11:46.950  INFO 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Creating producer client for task 1_2
        client.id = XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer
NODE_3 2020-04-15 21:11:47.137  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Instantiated a transactional producer.
NODE_3 2020-04-15 21:11:47.389  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
NODE_3 2020-04-15 21:11:47.403  INFO 1 --- [-StreamThread-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Overriding the default acks to all since idempotence is enabled.
NODE_3 2020-04-15 21:11:47.472 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Starting Kafka producer I/O thread.
NODE_3 2020-04-15 21:11:47.599  INFO 1 --- [-2-1_2-producer] org.apache.kafka.clients.Metadata        : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Cluster ID: UoTd5Q9HQsKwSUpY3eABQA
NODE_3 2020-04-15 21:11:47.670 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Register global stores []
NODE_3 2020-04-15 21:11:47.742 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state UNINITIALIZED to INITIALIZING
NODE_3 2020-04-15 21:11:47.742  INFO 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] ProducerId set to -1 with epoch -1
NODE_3 2020-04-15 21:11:47.742 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request InitProducerIdRequestData(transactionalId='XXXXCommandProcessor-1_2', transactionTimeoutMs=60000)
NODE_3 2020-04-15 21:11:47.744 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request FindCoordinatorRequestData(key='XXXXCommandProcessor-1_2', keyType=1)
NODE_3 2020-04-15 21:11:47.790 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Enqueuing transactional request InitProducerIdRequestData(transactionalId='XXXXCommandProcessor-1_2', transactionTimeoutMs=60000)
NODE_3 2020-04-15 21:11:47.855 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Sending transactional request FindCoordinatorRequestData(key='XXXXCommandProcessor-1_2', keyType=1) to node kafka-3:39092 (id: 3 rack: rack-a)
NODE_3 2020-04-15 21:11:48.000 DEBUG 1 --- [-2-1_2-producer] o.a.k.clients.producer.internals.Sender  : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Sending transactional request InitProducerIdRequestData(transactionalId='XXXXCommandProcessor-1_2', transactionTimeoutMs=60000) to node kafka-1:19092 (id: 1 rack: null)
NODE_3 2020-04-15 21:11:48.133  INFO 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] ProducerId set to 1014 with epoch 3
NODE_3 2020-04-15 21:11:48.146 DEBUG 1 --- [-2-1_2-producer] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state INITIALIZING to READY
NODE_3 2020-04-15 21:11:48.148 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Created task 1_2 with assigned partitions [mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.150  INFO 1 --- [-StreamThread-2] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
NODE_3 2020-04-15 21:11:48.150 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Pausing all active task partitions until the underlying state stores are ready
NODE_3 2020-04-15 21:11:48.152 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.TaskManager  : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Pausing partitions: [zzzzzz_state-1, mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.152 DEBUG 1 --- [-StreamThread-2] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Pausing partitions [zzzzzz_state-1, mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.153  INFO 1 --- [-StreamThread-2] o.a.k.s.p.internals.StreamThread         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] partition assignment took 1599 ms.
NODE_3 2020-04-15 21:11:48.192 DEBUG 1 --- [-StreamThread-2] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Fetching committed offsets for partitions: [mnl.xxxx.command-2, xxxx.command-2]
NODE_3 2020-04-15 21:11:48.316 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Initializing stream tasks [1_2]
NODE_3 2020-04-15 21:11:48.326  INFO 1 --- [-StreamThread-2] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-consumer, groupId=XXXXCommandProcessor] Found no committed offset for partition mnl.xxxx.command-2
NODE_3 2020-04-15 21:11:48.365 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Updating store offset limit with {mnl.xxxx.command-2=0, xxxx.command-2=0}
NODE_3 2020-04-15 21:11:48.371 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] A committed timestamp was detected: setting the partition time of partition xxxx.command-2 to 1586985068274 in stream task 1_2
NODE_3 2020-04-15 21:11:48.371 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] No committed offset for partition mnl.xxxx.command-2, therefore no timestamp can be found for this partition
NODE_3 2020-04-15 21:11:48.393 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing state stores
NODE_3 2020-04-15 21:11:48.410 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.internals.StateDirectory       : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Acquired state dir lock for task 1_2
NODE_3 2020-04-15 21:11:48.410 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing state stores
NODE_3 2020-04-15 21:11:48.410 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing store COMMAND_ID_STORE
NODE_3 2020-04-15 21:11:49.075 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Registering state store COMMAND_ID_STORE to its state manager
NODE_3 2020-04-15 21:11:49.075 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store COMMAND_ID_STORE from changelog topic XXXXCommandProcessor-COMMAND_ID_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.076 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Added restorer for changelog XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2
NODE_3 2020-04-15 21:11:49.076 DEBUG 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing store XXXX_STATE_STORE
NODE_3 2020-04-15 21:11:49.349 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Registering state store XXXX_STATE_STORE to its state manager
NODE_3 2020-04-15 21:11:49.349 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager        : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Restoring state store XXXX_STATE_STORE from changelog topic XXXXCommandProcessor-XXXX_STATE_STORE-changelog at checkpoint 1
NODE_3 2020-04-15 21:11:49.436 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Found checkpoint 1 from changelog XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2 for store COMMAND_ID_STORE.NODE_3 2020-04-15 21:11:50.082 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Stream task 1_2 cannot resume processing yet since some of its changelog partitions have not completed restoring: [XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2]NODE_3 2020-04-15 21:11:50.627 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Transitioning stream task 1_2 to running
NODE_3 2020-04-15 21:11:50.627 TRACE 1 --- [-StreamThread-2] o.a.k.s.processor.internals.StreamTask   : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Initializing processor nodes of the topology
NODE_3 2020-04-15 21:11:51.389 DEBUG 1 --- [-StreamThread-2] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2-1_2-producer, transactionalId=XXXXCommandProcessor-1_2] Transition from state READY to IN_TRANSACTION
NODE_3 2020-04-15 21:11:51.399 DEBUG 1 --- [-StreamThread-2] o.a.k.s.p.i.AssignedStreamsTasks         : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] Stream task 1_2 completed restoration as all its changelog partitions [XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2] have been applied to restore state
{noformat}

Now compare timestamps:
{noformat}
NODE_3 2020-04-15 21:11:46.697 TRACE 1 --- [-StreamThread-2] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-2] task [1_2] Checkpointable offsets read from checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}

NODE_3 2020-04-15 21:11:47.195 TRACE 1 --- [-StreamThread-1] o.a.k.s.p.i.ProcessorStateManager : stream-thread [XXXXCommandProcessor-94f7be8e-beec-411f-b4ec-9031527bccdf-StreamThread-1] standby-task [1_2] Writing checkpoint: {XXXXCommandProcessor-COMMAND_ID_STORE-changelog-2=1, XXXXCommandProcessor-XXXX_STATE_STORE-changelog-2=1}{noformat}
It seems that the standby task on T-1 hasn't finished shutdown yet, and active task on T-2 is already starting. If you look at the code corresponding to these 2 particular logs:
{noformat}
log.trace(""Checkpointable offsets read from checkpoint: {}"", initialLoadedCheckpoints);

if (eosEnabled) {
    // with EOS enabled, there should never be a checkpoint file _during_ processing.
    // delete the checkpoint file after loading its stored offsets.
    checkpointFile.delete();
    checkpointFile = null;
}{noformat}
 
{noformat}
log.trace(""Writing checkpoint: {}"", checkpointFileCache);
try {
    checkpointFile.write(checkpointFileCache);
} catch (final IOException e) {
    log.warn(""Failed to write offset checkpoint file to [{}]"", checkpointFile, e);
}{noformat}

Can it be a race condition, that a new active task reads the file, saves content in memory, then deletes the checkpoint file. 0.5 sec after that standby task when shutting down writes the checkpoint file, and eventually it's there although it shouldn't.
Is this checkpoint file locked in any way? It's also a bit misleading that there are no logs when this files is deleted, so it's hard to debug.

I am aware it may be hard to reproduce in a test due to timing, but maybe with exactly the same topology and this strange rebalancing scenario it could happen. Or some kind of unit test?
I also noticed that the piece of code operating on state store and checkpoint files has been recently rewritten, but it's hard to tell if this scenario can still happen.

Let me know if I can help any other way. I attached detailed logs showing the afore-mentioned failover scenario.;;;","05/May/20 10:00;mateuszjadczyk;[~bchen225242] Hi, is any follow-up planned for this?;;;","05/May/20 15:47;bchen225242;We had some offline discussion, and plan to revise the integration test PR as well. We want to test out on trunk first, and then backport to 2.4 to ensure the regression doesn't carry over.;;;","05/May/20 15:50;bchen225242;[~mateuszjadczyk] on the other hand, if you are willing to work on this ticket, feel free to take over, as I realize that you have better context of this problem in 2.4 than I do.;;;","05/May/20 17:51;mateuszjadczyk;I don't think I have enough knowledge to propose a fix, I'm afraid. One thing is debugging, another one making it work :) ;;;","05/May/20 18:11;bchen225242;[~mateuszjadczyk] Lol, no worry, thanks a lot for the insight you provided so far, will revise the integration test according to them.;;;","08/May/20 05:28;bchen225242;I don't see the full log for 3 nodes you mentioned, where did you attach them? [~mateuszjadczyk];;;","08/May/20 15:22;mateuszjadczyk;[~bchen225242] 
Maybe I didn't. You can now find them in attachments. 
I have also full-full trace logs but would need some time to remove sensitive data from them. So first I need to know that what I provided so far is not enough.;;;","19/May/20 07:59;mateuszjadczyk;[~bchen225242] have you had time to look some more into it?;;;","19/May/20 16:47;bchen225242;Sorry I was oncall last week. Will take another look this week. [~mateuszjadczyk];;;","23/May/20 16:04;bchen225242;I made another attempt to reproduce the issue on local. Basically I started 3 stream instances with 2 threads each, where the instance gets standby task assigned uses thread-1:

[2020-05-23 08:57:55,050] INFO stream-thread [client-2-StreamThread-1] partition assignment took 1 ms.[2020-05-23 08:57:55,050] INFO stream-thread [client-2-StreamThread-1] partition assignment took 1 ms. currently assigned active tasks: [] currently assigned standby tasks: [0_0] revoked active tasks: [] revoked standby tasks: []

After the first instance gets crushed, the same stream thread gets the active task, which is different from what you observed:

 

[2020-05-23 08:58:01,183] INFO stream-thread [client-2-StreamThread-1] partition assignment took 115 ms.[2020-05-23 08:58:01,183] INFO stream-thread [client-2-StreamThread-1] partition assignment took 115 ms. currently assigned active tasks: [0_0] currently assigned standby tasks: [] revoked active tasks: [] revoked standby tasks: [0_0] 

on the other hand, I looked at the codepath and I didn't find the possibility you mentioned, as the state manager creates with the task and there is no shared struct between threads. Right now I think I'm blocked on further investigation until I could find a reliable way to reproduce the scenario. Is this issue still recurring on your side?;;;","24/May/20 01:45;mjsax;Why not create two instances with one thread each? This way, we can control where active/standbys are assigned? And i think the issue is not with any shared stuff between threads: Assume a single partition input topic and a single stateful task:

Instance one holds the active task; instance two holds the standby. Let instance one crash (this leaves a dirty state on local disc; no checkpoint file is present though to indicate that it's dirty). The active is moved to instance two. There is no standby at this point (as there is not enough ""capacity""). Restart instance one; it will get the standby assigned (at this point, the issue occurs: the standby does not wipe out the local state, even if there is no checkpoint file and resumes with the corrupted state – note, that I believe that this issue is is fixed in trunk already, so you need to use 2.3 or 2.4 to reproduce – not sure if the fix is in 2.5). Now let instance two crash (or stop gracefully). The active task is migrated back to instance one and now uses the corrupted state for further processing.;;;","24/May/20 14:33;mateuszjadczyk;I think I reproduce it in [https://github.com/mateuszjadczykDna/kafka/tree/KAFKA-9891-integration-test-2 |https://github.com/mateuszjadczykDna/kafka/tree/KAFKA-9891-integration-test-2]

I branched from an older version, and the setup contains 3 instances, 2 input topics, 2 threads each.

It seems it doesn't happen 100% of the runs. I attached to this ticket logs from 2 failed runs. [^failedtest]

My worry is that if it seems to be fixed in the newest version (2.5), it may be due to changed rebalancing, and not actually fixing the real issue.;;;","24/May/20 15:43;mateuszjadczyk;BTW in this test (_failedtest3_) it also turns out to be happening during the switch between T-1 standby 0_1 to T-2 active 0_1 :
{code:java}
[2020-05-24 17:24:56,442] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO task [0_1] Checkpointable offsets read from checkpoint: {test-app-dedup-store-changelog-1=1} (org.apache.kafka.st
reams.processor.internals.ProcessorStateManager:113)
[2020-05-24 17:24:56,442] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 INFO standby-task [0_1] Writing checkpoint: {test-app-dedup-store-changelog-1=1} (org.apache.kafka.streams.processor.i
nternals.ProcessorStateManager:358)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO task [0_1] eosEnabled checkpointFile.delete() (org.apache.kafka.streams.processor.internals.ProcessorStateManager
:118)
streams.state.internals.OffsetCheckpoint:185)
ssor.internals.ProcessorStateManager:313)
sorStateManager:123)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO delete offset checkpoint /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint, exists: true (org.apache.kafka.s
treams.state.internals.OffsetCheckpoint:185)
[2020-05-24 17:24:56,443] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 TRACE Writing tmp checkpoint file /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint.tmp (org.apache.kafka.streams
.state.internals.OffsetCheckpoint:81)
teManager:317)
        currently assigned active tasks: []
        currently assigned standby tasks: [0_0, 0_2]
        revoked active tasks: []
        revoked standby tasks: []
 (org.apache.kafka.streams.processor.internals.StreamThread:96)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 DEBUG task [0_1] Created state store manager for task 0_1 (org.apache.kafka.streams.processor.internals.ProcessorStateManager:123)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2 INFO stream-thread [test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-2] Creating producer client for task 0_1 (org.apache.kafka.streams.processor.internals.StreamThread:370)
[2020-05-24 17:24:56,444] test-app-4e6ae5d2-8be3-4f65-83e3-5e6e124fe94d-StreamThread-1 TRACE Swapping tmp checkpoint file /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint.tmp /tmp/kafka-streams/instance-2/test-app/0_1/.checkpoint (org.apache.kafka.streams.state.internals.OffsetCheckpoint:97)
{code}
T-2 deletes the checkpoint but T-1 manages to write it back during the shutdown. So it ends up with the file which shouldn't be there. That's probably also the reason why the test is flashing - this timing is quite specific.
Hope you find a fix now :) ;;;","24/May/20 16:30;mateuszjadczyk;[~bchen225242] 
Maybe this could make this test more reliable ([KAFKA-9891-integration-test-2-extra-wait|https://github.com/mateuszjadczykDna/kafka/compare/KAFKA-9891-integration-test-2-extra-wait])
[https://github.com/mateuszjadczykDna/kafka/commit/e0304b48a47a07aa12ab55b7abd1ddf6cfef828b]

I saw your new commits after changing the test on my own, but I use 2 topics (maybe it causes another tasks assignment than in your branch) and I changed failure assertion as the fail() in exception handler didn't seem to work. Let me know if it fails for you.;;;","25/May/20 11:46;mateuszjadczyk;[~mjsax] if this is true 
> _Restart instance one; it will get the standby assigned (at this point, the issue occurs: the standby does not wipe out the local state, even if there is no checkpoint file and resumes with the corrupted state_
then it's another case.

For the case I'm talking about you need 2 threads and a specific rebalancing: T1 with standby task and then T2 taking over even though it wasn't a standby (I don't know the rebalancing algorithm that well but as I understand it has something to do with balances traffic across all instances, that's why you may need 2 topics to reproduce it). *In this scenario T1 hasn't finished shutting down and T2 is already starting up. And both of them operate on the same disk state directory - that's the shared part between threads.* 
Before we feel safe enough to enable standby tasks again, we need to be sure it's fixed, either here or in 2.5;;;","26/May/20 22:40;mjsax;Even if two instances run on the same server and use the same state directory, a task (active or standby) should get a lock on the task directory first. And thus, only after the lock is released by one thread, the other thread can grab it. – Thus, T2 cannot really start before T1 finished it's shut down (or there is another bug in the locking code...).;;;","27/May/20 08:30;mateuszjadczyk;[~mjsax] If I'm not mistaken, these logs indicate a bug, as both threads (same instance) operate on _/tmp/kafka-streams/instance-2/test-app/0_1/_ at the same time
https://issues.apache.org/jira/browse/KAFKA-9891?focusedCommentId=17115352&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17115352;;;","29/May/20 19:58;bchen225242;[~mateuszjadczyk] I ran your example again, and could see that the assignment scenario reproduced with > 1 topic partition. The part I'm confusing is that you used 0 ms as offset commit interval, as this would actually cause the poison record very likely to commit and replicated. Could you elaborate?;;;","30/May/20 13:57;mateuszjadczyk;[~bchen225242] I think I wanted to be sure that keyOne is committed ASAP. If it's set too high, then keyOne is not committed = nothing on changelog topic = nothing to do for a  standby task, right?

Nevertheless, I changed it to default (100ms) on https://github.com/mateuszjadczykDna/kafka/compare/KAFKA-9891-integration-test-2-extra-wait and it still fails.;;;","30/May/20 18:10;mateuszjadczyk;On trunk I can't seem to reproduce this particular partitions reassignment, but I noticed that directory locking before reading checkpoint was introduced in Feb 2020 by [~guozhang] which should prevent such a race condition. [~mjsax] is this the fix you had in mind? Will this be a part of 2.6?;;;","01/Jun/20 16:35;bchen225242;[~mateuszjadczyk] If we commit keyOne, then it *should* be materialized inside the standby state store copy, and the duplicate key check will see it, correct? Will sync with [~guozhang] on whether to backport this checkpoint lock to 2.4.;;;","02/Jun/20 09:09;mateuszjadczyk;[~bchen225242] yes, it should be materialized. Duplicate key will be however performed only once for keyOne (during the very first processing), as this is thrown only for the poisonKey:
{code:java}
throw new IllegalStateException(""Throw on "" + poisonKey + "" to trigger rebalance"");
{code}
 ;;;","02/Jun/20 09:10;mateuszjadczyk;The reason we need at least one materialized key is that we then have something on the changelog topic and some checkpoint files are used which mess things up.;;;","02/Jun/20 20:36;bchen225242;[~mateuszjadczyk] that's right, so we should only check duplicate of `poisonKey` instead for exception, like:



if (storeIterator.hasNext() && key.equals(poisonKey)) {

    throw new IllegalStateException(""Caught a duplicate key "" + key);

}

WDYT?;;;","03/Jun/20 09:14;mateuszjadczyk;[~bchen225242] well, it shouldn't occur for either of the keys, but this will pinpoint the poisonKey.
Btw we're currently not using standby replicas because of this bug and we probably could wait until 2.6. Not sure about all other kafka streams users though..;;;","04/Jun/20 01:26;bchen225242;[~mateuszjadczyk] I see, glad you guys have a workaround. I already synced with [~guozhang] offline to bring in another pair of eye to see if we could find more leads here.;;;","08/Jun/20 15:33;vvcephei;Hi [~mjsax] and [~bchen225242] ,

I see you're just added this as a blocker for 2.5.1. Is it a regression?

Thanks,

-John;;;","08/Jun/20 20:54;mjsax;I am not 100% sure. But even if not, it might be severe enough to treat as blocker?;;;","17/Jun/20 04:25;mjsax;[~mateuszjadczyk] [~bchen225242]: I think I was able to write a test that reproduces the issue in 2.5. Cf [https://github.com/apache/kafka/pull/8886]

Please let me know what you think.;;;","18/Jun/20 01:34;mjsax;PR [https://github.com/apache/kafka/pull/8890] for `trunk` (and `2.6`) seem to confirm that it's already fixed there.;;;","18/Jun/20 20:00;mateuszjadczyk;LGTM thanks for looking into it and making sure that 2.5.1/2.6 should be safe to use;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flaky RocksDBTimestampedStoreTest.shouldVerifyThatMetricsGetMeasurementsFromRocksDB,KAFKA-9889,13299384,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,guozhang,guozhang,18/Apr/20 17:12,20/Apr/20 17:40,13/Jul/23 09:17,20/Apr/20 17:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"Stacktrace
org.apache.kafka.streams.errors.ProcessorStateException: Error opening store db-name at location rocksdb/db-name
	at org.apache.kafka.streams.state.internals.RocksDBStore.openRocksDB(RocksDBStore.java:221)
	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:195)
	at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:231)
	at org.apache.kafka.streams.state.internals.RocksDBStoreTest.shouldVerifyThatMetricsGetMeasurementsFromRocksDB(RocksDBStoreTest.java:633)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.rocksdb.RocksDBException: While lock file: /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/rocksdb/db-name/LOCK: Resource temporarily unavailable
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:286)
	at org.apache.kafka.streams.state.internals.RocksDBStore.openRocksDB(RocksDBStore.java:218)
	... 53 more",,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 19:18:26 UTC 2020,,,,,,,,,,"0|z0dso8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/20 19:18;guozhang;Assigning to [~ableegoldman] since she's started looking into this already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST extensions can mutate connector configs in worker config state snapshot,KAFKA-9888,13299330,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,18/Apr/20 05:07,23/May/20 23:20,13/Jul/23 09:17,23/May/20 23:20,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"The changes made in [KIP-454|https://cwiki.apache.org/confluence/display/KAFKA/KIP-454%3A+Expansion+of+the+ConnectClusterState+interface] involved adding a {{connectorConfig}} method to the [ConnectClusterState|https://github.com/apache/kafka/blob/ecde596180975f8546c0e8e10f77f7eee5f1c4d8/connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterState.java] interface that REST extensions could use to query the worker for the configuration of a given connector. The [implementation for this method|https://github.com/apache/kafka/blob/ecde596180975f8546c0e8e10f77f7eee5f1c4d8/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImpl.java#L86-L89] returns the Java {{Map}} that's stored in the worker's view of the config topic (when running in distributed mode). No copying is performed, which causes mutations of that {{Map}} object to persist across invocations of {{connectorConfig}} and, even worse, propagate to the worker when, e.g., starting a connector.

We should not give REST extensions that original map, but instead a copy of it.",,ChrisEgerton,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 05:31:24 UTC 2020,,,,,,,,,,"0|z0dsc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/20 05:31;githubbot;C0urante commented on pull request #8511: KAFKA-9888: Copy connector configs before passing to REST extensions
URL: https://github.com/apache/kafka/pull/8511
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9888)
   
   The changes made in [KIP-454](https://cwiki.apache.org/confluence/display/KAFKA/KIP-454%3A+Expansion+of+the+ConnectClusterState+interface) involved adding a `connectorConfig` method to the [ConnectClusterState](https://github.com/apache/kafka/blob/ecde596180975f8546c0e8e10f77f7eee5f1c4d8/connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterState.java) interface that REST extensions could use to query the worker for the configuration of a given connector. The implementation for this method returns the Java `Map` that's stored in the worker's view of the config topic (when running in distributed mode). No copying is performed, which causes mutations of that `Map` object to persist across invocations of `connectorConfig` and, even worse, propagate to the worker when, e.g., starting a connector.
   
   The changes here just cause the framework to copy that map before sending it to REST extensions, and alter a comment in `KafkaConfigBackingStore` that addresses the mutability of the snapshots that it provides to warn against changes that may lead to bugs like this one.
   
   An existing unit test is modified to ensure that REST extensions receive a copy of the connector config, not the original.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed-task-count JMX metric not updated if task fails during startup,KAFKA-9887,13299189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,michael_carter,ChrisEgerton,ChrisEgerton,17/Apr/20 17:59,28/Oct/21 22:20,13/Jul/23 09:17,21/Jul/21 13:46,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.1,3.0.1,3.1.0,,,,,KafkaConnect,,,,,0,,,,,"If a task fails on startup (specifically, during [this code section|https://github.com/apache/kafka/blob/00a59b392d92b0d6d3a321ef9a53dae4b3a9d030/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L427-L468]), the {{failed-task-count}} JMX metric is not updated to reflect the task failure, even though the status endpoints in the REST API do report the task as failed.",,ChrisEgerton,michael_carter,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13118,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 28 22:20:34 UTC 2021,,,,,,,,,,"0|z0drt4:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"03/Jun/20 07:18;michael_carter;This similarly applies to connectors that fail during startup. The connector-startup-failure-total metric is not updated because [this code section|https://github.com/apache/kafka/blob/00a59b392d92b0d6d3a321ef9a53dae4b3a9d030/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConnector.java#L103-L121] doesn't propagate the exception back up to the code that would increment the metric. In fact, since it never receives the exception, that code register a successful startup instead.;;;","09/Jun/20 05:08;michael_carter;I'm going to have a go at fixing this one;;;","21/Jul/21 13:46;rhauch;Merged to the following branches:
* `trunk` for inclusion in 3.1.0
* `2.8` for inclusion in the next 2.8.1
* `2.7` for inclusion in the next 2.7.2

Still need to merge it to the `3.0` branch.;;;","21/Jul/21 17:16;rhauch;Unfortunately, the 3.0 code freeze was two weeks ago, so I will create a new issue as a blocker for 3.0.1 to backport this fix to the `3.0` branch after the 3.0.0 release is complete, and will like to this issue.;;;","28/Oct/21 22:20;rhauch;Backported to the `3.0` branch, so it will be included in 3.0.1 when it is released. Updated the FIX VERSION on this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evict last members of a group when the maximum allowed is reached,KAFKA-9885,13299115,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,17/Apr/20 12:08,28/Apr/20 07:56,13/Jul/23 09:17,27/Apr/20 21:44,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"While analysing https://issues.apache.org/jira/browse/KAFKA-7965, we found that multiple members of a group can be evicted from a group if the leader of the consumer offset partition changes before the group is persisted. This happens because the current evection logic always evict the first member which rejoins the group.

We would like to change the evection logic so that the last members to rejoin the group are kicked out instead.

Here is an example of what happens when the leader changes:
{noformat}
// Group is loaded in GroupCoordinator 0
// A rebalance is triggered because the group is over capacity
[2020-04-02 11:14:33,393] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66)
[2020-04-02 11:14:33,406] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:40071 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794)
[2020-04-02 11:14:33,409] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:33,410] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:33,412] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-02 11:14:33,431] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 28 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)

// A first consumer is kicked out of the group while trying to re-join
[2020-04-02 11:14:33,449] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627)
[2020-04-02 11:14:33,451] ERROR [daemon-consumer-assignment-2]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76)
org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members.
[2020-04-02 11:14:33,451] INFO [daemon-consumer-assignment-2]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66)

// Before the rebalance is completed, a preferred replica leader election kicks in and move the leader from 0 to 1
[2020-04-02 11:14:34,155] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2020-04-02 11:14:34,169] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions group-max-size-test-0,group-max-size-test-3,__consumer_offsets-0 triggered by AutoTriggered (kafka.controller.KafkaController:66)

// The group is loaded in GroupCoordinator 1 before completing the rebalance
// Another rebalance is triggered because the group is still over capacity
[2020-04-02 11:14:34,194] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66)
[2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126)
[2020-04-02 11:14:34,201] INFO [GroupCoordinator 1]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-02 11:14:34,202] INFO [GroupCoordinator 1]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-02 11:14:34,203] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)

// Prefered leader election is completed
[2020-04-02 11:14:34,235] INFO [Controller id=0] Partition __consumer_offsets-0 completed preferred replica leader election. New leader is 1 (kafka.controller.KafkaController:66)

// Group is unloaded from GroupCoordinator 0
[2020-04-02 11:14:34,237] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66)
[2020-04-02 11:14:34,237] INFO [GroupCoordinator 0]: Unloading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-02 11:14:34,238] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-0. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager:66)

// A second consumer is kicked out of the group while trying to re-join
[2020-04-02 11:14:34,252] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627)
[2020-04-02 11:14:34,254] ERROR [daemon-consumer-assignment-1]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76)
org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members.
[2020-04-02 11:14:34,254] INFO [daemon-consumer-assignment-1]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66) {noformat}",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7965,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-17 12:08:29.0,,,,,,,,,,"0|z0drco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Connect request to restart task can result in IllegalArgumentError: ""uriTemplate"" parameter is null",KAFKA-9883,13298914,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,rhauch,rhauch,rhauch,16/Apr/20 23:03,23/Apr/20 22:47,13/Jul/23 09:17,23/Apr/20 22:47,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,KafkaConnect,,,,,0,,,,,"When attempting to restart a connector, the following is logged by Connect:



 
{code:java}
ERROR Uncaught exception in REST call to /connectors/my-connector/tasks/0/restart (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
java.lang.IllegalArgumentException: ""uriTemplate"" parameter is null.
    at org.glassfish.jersey.uri.internal.JerseyUriBuilder.uri(JerseyUriBuilder.java:189)
    at org.glassfish.jersey.uri.internal.JerseyUriBuilder.uri(JerseyUriBuilder.java:72)
    at javax.ws.rs.core.UriBuilder.fromUri(UriBuilder.java:96)
    at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.completeOrForwardRequest(ConnectorsResource.java:263)
    at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.completeOrForwardRequest(ConnectorsResource.java:298)
    at org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource.restartTask(ConnectorsResource.java:218)
{code}
Resubmitting the restart REST request will usually resolve the problem.

 ",,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-16 23:03:48.0,,,,,,,,,,"0|z0dq40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while range compacting during bulk loading of FIFO compacted RocksDB Store,KAFKA-9880,13298795,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,Dealus,Dealus,16/Apr/20 12:00,26/Feb/21 19:52,13/Jul/23 09:17,26/Feb/21 19:52,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,," 

When restoring a non empty RocksDB state store, if it is customized to use FIFOCompaction, the following exception is thrown:

 
{code:java}
//
org.apache.kafka.streams.errors.ProcessorStateException: Error while range compacting during restoring  store merge_store
        at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.toggleDbForBulkLoading(RocksDBStore.java:615) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.state.internals.RocksDBStore.toggleDbForBulkLoading(RocksDBStore.java:398) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.state.internals.RocksDBStore$RocksDBBatchingRestoreCallback.onRestoreStart(RocksDBStore.java:644) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.CompositeRestoreListener.onRestoreStart(CompositeRestoreListener.java:59) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StateRestorer.restoreStarted(StateRestorer.java:76) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StoreChangelogReader.startRestoration(StoreChangelogReader.java:211) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StoreChangelogReader.initialize(StoreChangelogReader.java:185) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:81) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:389) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:769) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671) [kafka-stream-router.jar:?]
Caused by: org.rocksdb.RocksDBException: Target level exceeds number of levels
        at org.rocksdb.RocksDB.compactRange(Native Method) ~[kafka-stream-router.jar:?]
        at org.rocksdb.RocksDB.compactRange(RocksDB.java:2636) ~[kafka-stream-router.jar:?]
        at org.apache.kafka.streams.state.internals.RocksDBStore$SingleColumnFamilyAccessor.toggleDbForBulkLoading(RocksDBStore.java:613) ~[kafka-stream-router.jar:?]
        ... 11 more
{code}
 

 

Compaction is configured through an implementation of RocksDBConfigSetter. The exception si gone as soon as I remove:
{code:java}
// 
CompactionOptionsFIFO fifoOptions = new CompactionOptionsFIFO();
fifoOptions.setMaxTableFilesSize(maxSize);
fifoOptions.setAllowCompaction(true);
options.setCompactionOptionsFIFO(fifoOptions);
options.setCompactionStyle(CompactionStyle.FIFO);
{code}
 

 

Bulk loading works fine when the store is non-existent / empty. This occurs only when there are a minimum amount of data in it. I guess it happens when the amount SST layers is increased.

I'm currently using a forked version of Kafka 2.4.1 customizing the RocksDBStore class with this modification as a work around:

 
{code:java}
//
public void toggleDbForBulkLoading() {
      try {
        db.compactRange(columnFamily, true, 1, 0);
      } catch (final RocksDBException e) {
        try {
          if (columnFamily.getDescriptor().getOptions().compactionStyle() != CompactionStyle.FIFO) {
            throw new ProcessorStateException(""Error while range compacting during restoring  store "" + name, e);
          }
          else {
            log.warn(""Compaction of store "" + name + "" for bulk loading failed. Will continue without compacted store, which will be slower."", e);
          }
        } catch (RocksDBException e1) {
          throw new ProcessorStateException(""Error while range compacting during restoring  store "" + name, e);
        }
      }
    }
{code}
 

I'm not very proud of this workaround, but it suits my use cases well.

 

 ",,ableegoldman,cadonna,Dealus,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 19:52:05 UTC 2021,,,,,,,,,,"0|z0dpdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/20 18:43;guozhang;Thanks for reporting this [~Dealus]. I think an appropriate solution would be, inside `toggleDbForBulkLoading` we should not set targetLevel to 1 but use the other overloaded `compactRange` which set it to -1.

WDYT [~cadonna] [~mjsax]?;;;","24/Apr/20 13:00;Dealus;The compactRange method used is marked as Deprecated, thus it should be switched with the new ColumnFamilyHandle columnFamilyHandle, final byte[] begin, final byte[] end) method, which doesn't include target level as parameter. Is that this one you're talking about ?

Documentation also states that:

 
{code:java}
In FIFO compaction, all files are in level 0. When total size of the data exceeds configured size (CompactionOptionsFIFO::max_table_files_size), we delete the oldest table file. This means that write amplification of data is always 1 (in addition to WAL write amplification).
Currently, CompactRange() function just manually triggers the compaction and deletes the old table files if there is need. It ignores the parameters of the function (begin and end keys).
{code}
 So I guess compacting is pretty useless with such configuration

 

 ;;;","26/Feb/21 19:52;ableegoldman;Since we turned off bulk loading in 2.6.0 and removed the offending compactRange call, I think we can go ahead and close this ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test SuppressionDurabilityIntegrationTest.shouldRecoverBufferAfterShutdown[exactly_once],KAFKA-9875,13298623,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,ableegoldman,ableegoldman,15/Apr/20 17:24,29/Apr/20 22:14,13/Jul/23 09:17,29/Apr/20 22:14,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,unit-test,,,"h3. Stacktrace

java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out. at org.apache.kafka.streams.integration.utils.KafkaEmbedded.deleteTopic(KafkaEmbedded.java:211) at org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster.deleteAllTopicsAndWait(EmbeddedKafkaCluster.java:300) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateAfterTest(IntegrationTestUtils.java:148) at org.apache.kafka.streams.integration.SuppressionDurabilityIntegrationTest.shouldRecoverBufferAfterShutdown(SuppressionDurabilityIntegrationTest.java:246)",,ableegoldman,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 22:14:59 UTC 2020,,,,,,,,,,"0|z0dobc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/20 22:14;vvcephei;https://github.com/apache/kafka/pull/8578;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test EOSUncleanShutdownIntegrationTest.shouldWorkWithUncleanShutdownWipeOutStateStore,KAFKA-9868,13298409,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,ableegoldman,ableegoldman,14/Apr/20 22:34,21/Apr/20 02:50,13/Jul/23 09:17,21/Apr/20 02:50,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"h3. Error Message

java.lang.AssertionError: Condition not met within timeout 15000. Expected ERROR state but driver is on RUNNING

 

From what we observed, in unit test with transaction turned on, it takes a long time to bootstrap the test as well as sometimes getting too many open files for system test. To reduce the start time and make tests less flaky, we should set the number of txn log partitions to a much smaller number than 50.",,ableegoldman,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 02:43:41 UTC 2020,,,,,,,,,,"0|z0dmzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/20 18:36;bchen225242;[~ableegoldman]Do we have the failure log?;;;","21/Apr/20 02:43;bchen225242;[https://github.com/apache/kafka/pull/8522];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update the deprecated --zookeeper option in the documentation into --bootstrap-server,KAFKA-9863,13298242,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,showuon,showuon,14/Apr/20 11:20,30/Jun/20 08:20,13/Jul/23 09:17,23/Apr/20 03:25,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.5.1,,,,,,,,docs,documentation,,,,0,,,,,"Since V2.2.0, the -zookeeper option turned into deprecated because Kafka can directly connect to brokers with --bootstrap-server (KIP-377). But in the official documentation, there are many example commands use --zookeeper instead of --bootstrap-server. Follow the command in the documentation, you'll get this warning, which is not good.
{code:java}
Warning: --zookeeper is deprecated and will be removed in a future version of Kafka.
Use --bootstrap-server instead to specify a broker to connect to.{code}",,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 06:56:33 UTC 2020,,,,,,,,,,"0|z0dlyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/20 06:56;showuon;PR: https://github.com/apache/kafka/pull/8482;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-streams-application-reset tool doesn't take into account topics generated by KTable foreign key join operation,KAFKA-9859,13298035,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lkokhreidze,lkokhreidze,lkokhreidze,13/Apr/20 13:50,05/Oct/20 18:05,13/Jul/23 09:17,20/May/20 17:33,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,tools,,,,0,newbie,newbie++,,,"Steps to reproduce:
 * Create Kafka Streams application which uses foreign key join operation (without a Named parameter overload)
 * Stop Kafka streams application
 * Perform `kafka-topics-list` and verify that foreign key operation internal topics are generated
 * Use `kafka-streams-application-reset` to perform the cleanup of your kafka streams application: `kafka-streams-application-reset --application-id <your_app_id> --input-topics <your_input_topic> --bootstrap-servers <your_bootstrap_server> --to-datetime 2019-04-13T00:00:00.000`
 * Perform `kafka-topics-list` again, you'll see that topics generated by the foreign key operation are still there.

[kafka-streams-application-reset|#L679-L680]] uses `-subscription-registration-topic` and `-subscription-response-topic` suffixes to match topics generated by the foreign key operation. While in reality, internal topics are generated in this format:
{code:java}
<application id>-KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-<running number>-topic 
<application id>-KTABLE-FK-JOIN-SUBSCRIPTION-RESPONSE-<running number>-topic{code}
Please note that this problem only happens when `Named` parameter is not used. When named parameter is used, topics are generated with a same pattern as specified in StreamsResetter.",,bchen225242,guozhang,lkokhreidze,mjsax,nikuis,tmau,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10530,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 17:33:27 UTC 2020,,,,,,,,,,"0|z0dko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/20 15:32;bchen225242;Thanks for reporting. From your observation, is there a regular pattern for auto created foreign key join topics?;;;","13/Apr/20 19:42;vvcephei;Yes, the FK join KIP added two new suffixes:
 # *-subscription-registration
 # *-subscription-response

As an implementation note, these topics are registered in org.apache.kafka.streams.kstream.internals.KTableImpl#doJoinOnForeignKey

 

As a workaround, these topics can be deleted ""manually"", but they should absolutely be cleaned up by the application reset tool.

 

[~lkokhreidze], did you want to pick this up? It should be pretty straightforward.;;;","13/Apr/20 19:53;lkokhreidze;Ah sorry, I think this ticket is a bit misleading. Kafka broker I have tested this was 2.3.1 and FK join was implemented in 2.4. Seems like 2.4.0 has proper implementation around topics generated by the FK Join, see `kafka.tools.StreamsResetter#isInternalTopic`. I guess this ticket can be closed as ""not a problem""? Pretty sure streams-resetter on older versions of brokers can't be backward compatible with the FK join feature.;;;","13/Apr/20 21:05;mjsax;Thanks for the follow up [~lkokhreidze] – what you say makes sense – the `2.3.x` reset tool would not consider those topics as internal, for obvious reasons.;;;","24/Apr/20 05:37;nikuis;I have been using 2.4.1 to perform application reset, but there are topics of foreignKey join operation left in broker after reset.

Looks to me the suffix added for foreignKey, and the actual one generated is different.

e.g. topics left behind after reset has suffix of ""-topic"" :

{noformat}
<application id>-KTABLE-FK-JOIN-SUBSCRIPTION-REGISTRATION-<running number>-topic 
<application id>-KTABLE-FK-JOIN-SUBSCRIPTION-RESPONSE-<running number>-topic
{noformat}

Should I raise a new JIRA? ;;;","24/Apr/20 06:09;tmau;Looks like the topic pattern it looking for in `kafka.tools.StreamsResetter#isInternalTopic` does not match the generated internal topic name

[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/tools/StreamsResetter.java#L679-L680]
{code:java}
topicName.endsWith(""-subscription-registration-topic"")
topicName.endsWith(""-subscription-response-topic""){code};;;","24/Apr/20 06:18;lkokhreidze;Thanks for spotting this, haven't paid attention to it. Re-opening the ticket with the new findings.;;;","24/Apr/20 06:31;lkokhreidze;Please also note that this problem only happens when `Named` parameter is not used. When named parameter is used, topics are generated with a same pattern as specified in StreamsResetter.;;;","24/Apr/20 08:03;nikuis;Yes, in my program, I didn't pass in 'Named' parameter, and rely on the default.;;;","04/May/20 18:00;guozhang;[~lkokhreidze] Do you want to send a PR to resolve this ticket?;;;","04/May/20 18:02;lkokhreidze;Hi [~guozhang],

Was meaning to do it this week, will assign ticket to myself and send the PR.;;;","20/May/20 17:33;lkokhreidze;fixed with PR [https://github.com/apache/kafka/pull/8671];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CVE-2016-3189  Use-after-free vulnerability in bzip2recover in bzip2 1.0.6 allows remote attackers to cause a denial of service (crash) via a crafted bzip2 file, related to block ends set to before the start of the block.",KAFKA-9858,13298020,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,sihuan,sihuan,13/Apr/20 12:33,29/Jul/21 18:33,13/Jul/23 09:17,29/Jul/21 18:33,2.2.2,2.3.1,2.4.1,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,security,,,,,0,,,,,"I'm not sure whether  CVE-2016-3189 affects kafka 2.4.1  or not?  This vulnerability  was related to rocksdbjni-5.18.3.jar  which is compiled with *bzip2 .* 

Is there any task or plan to fix it? 

 ",,guozhang,microle.dong,sihuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://nvd.nist.gov/vuln/detail/CVE-2016-3189,,,,,,,,,,,9223372036854775807,,,Thu Jul 29 18:33:37 UTC 2021,,,,,,,,,,"0|z0dkko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 18:08;guozhang;For rocksdbjni, I saw that at the moment even current master is still using bzip version 1.0.6 so 3189 and 12900 would be existed in newest rocksDB version. I'd suggest you post on rocksdb community and see if their community has a better understanding on how to resolve this?;;;","29/Jul/21 05:39;microle.dong;[~guozhang] rocksdbjni has already fixed this issue [https://github.com/facebook/rocksdb/issues/6703 |https://github.com/facebook/rocksdb/issues/6703)]  in rocksdb6.10.

you can find this in [https://github.com/facebook/rocksdb/blob/master/HISTORY.md.]

accroding to this, do we have any task or plan to fix it now ?  ;;;","29/Jul/21 18:33;guozhang;Hi [~sihuan][~microle.dong] in https://issues.apache.org/jira/browse/KAFKA-8897 we've upgraded to rocksDB: ""6.19.3"", so I think we can mark it as resolved in the upcoming 3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-authenticating causes mismatched parse of response,KAFKA-9854,13297895,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,chia7712,chia7712,chia7712,12/Apr/20 16:51,08/Jan/21 11:02,13/Jul/23 09:17,16/Apr/20 12:32,,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,,,,,,0,,,,,"the schema of LIST_OFFSETS consists of
 # throttle_time_ms:INT32 and
 # responses:ARRAY

 
 If throttle_time_ms is zero and size of responses is small enough, its binary is compatible to schema of SASL_HANDSHAKE composed of
 # error_code:INT16 and
 # mechanisms:ARRAY(STRING)

 
 Hence, there is no Schema error when SASL_HANDSHAKE tries to parse response of LIST_OFFSETS but the check of correction id throws IllegalStateException due to mismatched error. The IllegalStateException is NOT caught and the mismatched response is not sent back to Selector so the cascading error happens that all following responses are parsed by incorrect Schema.",,alexx_i,chia7712,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 08 11:02:06 UTC 2021,,,,,,,,,,"0|z0djsw:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,"12/Apr/20 16:53;githubbot;chia7712 commented on pull request #8471: KAFKA-9854 Re-authenticating causes mismatched parse of response
URL: https://github.com/apache/kafka/pull/8471
 
 
   the schema of LIST_OFFSETS consists of 
   
   1. throttle_time_ms:INT32 and 
   1. responses:ARRAY
   
   If both are zero, it’s binary is compatible to schema of SASL_HANDSHAKE which is composed of 
   
   1. error_code:INT16 and 
   1. mechanisms:ARRAY(STRING)
   
   Hence, there is no Schema error when SASL_HANDSHAKE tries to parse resoponse of LIST_OFFSETS but the check of correction id still throw IllegalStateException due to mismatched parse. The IllegalStateException is NOT caught and it is not sent back to Selector so the cascading error happens that all following responses are parsed by incorrect Schema.
   
   https://issues.apache.org/jira/browse/KAFKA-9854
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Apr/20 12:26;githubbot;rajinisivaram commented on pull request #8471: KAFKA-9854 Re-authenticating causes mismatched parse of response
URL: https://github.com/apache/kafka/pull/8471
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","08/Jan/21 09:57;alexx_i;Hello [~chia7712],

Does your PR fix issues like the one bellow?


{code:java}
2021-01-07 10:46:33.751 DEBUG 1 --- [98-202f60784965] i.s.k.oauth.common.OAuthAuthenticator    : loginWithClientSecret() - tokenEndpointUrl: https://sso-copo.apps.uat-ocp.bt.wan/auth/realms/copo/protocol/openid-connect/token, clientId: client-check-adaptor, clientSecret: c9b5**ecdd, scope: null2021-01-07 10:46:33.751 DEBUG 1 --- [98-202f60784965] i.s.k.oauth.common.OAuthAuthenticator    : loginWithClientSecret() - tokenEndpointUrl: https://sso-copo/auth/realms/copo/protocol/openid-connect/token, clientId: client-check-adaptor, clientSecret: c9b5**ecdd, scope: null2021-01-07 10:46:33.788 DEBUG 1 --- [98-202f60784965] jdk.event.security                       :  TLSHandshake: sso-copo:443, TLSv1.2, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, -10307310322021-01-07 10:49:13.920  WARN 1 --- [ntainer#0-1-C-1] o.apache.kafka.common.network.Selector   : [Consumer clientId=consumer-client-check-adaptor-v2-new-8, groupId=client-check-adaptor-v2-new] Unexpected error from copo-kafka-0.copo-kafka-brokers.copo.svc/10.211.2.162; closing connection java.lang.IllegalStateException: Correlation id for response (213994) does not match request (0), request header: RequestHeader(apiKey=SASL_HANDSHAKE, apiVersion=1, clientId=, correlationId=0) at org.apache.kafka.clients.NetworkClient.correlate(NetworkClient.java:943) at org.apache.kafka.clients.NetworkClient.parseStructMaybeUpdateThrottleTimeMetrics(NetworkClient.java:726) at org.apache.kafka.clients.NetworkClient.parseResponse(NetworkClient.java:712) at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator.receiveKafkaResponse(SaslClientAuthenticator.java:523) at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator.authenticate(SaslClientAuthenticator.java:249) at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:177) at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:547) at org.apache.kafka.common.network.Selector.poll(Selector.java:485) at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:549) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1107) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1063) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:988) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.lang.Thread.run(Thread.java:834) 2021-01-07 10:49:13.923 ERROR 1 --- [ntainer#0-1-C-1] essageListenerContainer$ListenerConsumer : Consumer exception java.lang.IllegalStateException: This error handler cannot process 'org.apache.kafka.common.protocol.types.SchemaException's; no record information is available at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:151) at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.handleConsumerException(KafkaMessageListenerContainer.java:1263) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1020) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: org.apache.kafka.common.protocol.types.SchemaException: Error reading field 'responses': Error reading array of size 1431586882, only 5 bytes available at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:110) at org.apache.kafka.common.protocol.ApiKeys.parseResponse(ApiKeys.java:313) at org.apache.kafka.clients.NetworkClient.parseStructMaybeUpdateThrottleTimeMetrics(NetworkClient.java:725) at org.apache.kafka.clients.NetworkClient.handleCompletedReceives(NetworkClient.java:839) at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:558) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1107) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1063) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:988) ... 3 common frames omitted
{code}
Regards,

Alex;;;","08/Jan/21 10:22;chia7712;hi Alex

Do you have sample code to reproduce that error?

(I am off from my laptop. Will take a look at log later);;;","08/Jan/21 10:29;alexx_i;Hello,

I was not able to reproduce this. We have an OpenShift cluster with 3 instance Kafka installed using Strimzi Operator and OAUTHBEARER / SASL_PLAINTEXT security. This issue happens randomly for each consumer once every few hours / days. 

What is strange is that if I have 2 instance of the same application that consumes, the consumer group appears STABLE after this error, but the consumer no longer reads certain partitions.

Any ideas on how to reproduce this or how to investigate further? 

Best regards,
Alex;;;","08/Jan/21 10:44;chia7712;Could you share the Kafka version to me? client-side and server-side.;;;","08/Jan/21 10:51;alexx_i;The latest Strimzi Operator creates brokers using the 2.6.0 version of Kafka.
For the clients we have Spring Boot applications that have Spring Kafka 2.5.4 that include kafka-clients 2.5.0.

I am doing now tests with an overriden version using Spring Kafka 2.6 and kafka-clients 2.5.1 which should have your fix.

Regards,
Alex;;;","08/Jan/21 11:02;chia7712;Oh, I assumed your cluster had included this fix. Thanks for your sharing. I hope this fix can save your life :)
please feel free to open an new jira to report bug if this fix does not work for your case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revoking Connect tasks due to connectivity issues should also clear running assignment,KAFKA-9851,13297650,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,kkonstantine,kkonstantine,10/Apr/20 17:06,11/Jun/20 16:13,13/Jul/23 09:17,06/Jun/20 00:36,,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"https://issues.apache.org/jira/browse/KAFKA-9184 fixed an issue with workers continuing to run tasks even after they'd lose connectivity with the broker coordinator and they'd detect that they are out of the group. 

 

However, because the revocation of tasks in this case is voluntary and does not come with an explicit assignment (containing revoked tasks) from the leader worker, the worker that quits running its tasks due to connectivity issues needs to also clear its running task assignment snapshot. 

This will allow for proper restart of the stopped tasks after the worker rejoins the group when connectivity returns and get assigned the same connectors or tasks. ",,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-10 17:06:26.0,,,,,,,,,,"0|z0diag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issue with worker.unsync.backoff.ms creating zombie workers when incremental cooperative rebalancing is used,KAFKA-9849,13297501,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,kkonstantine,kkonstantine,09/Apr/20 22:47,10/Jun/20 07:17,13/Jul/23 09:17,10/Jun/20 07:17,2.3.1,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"{{worker.unsync.backoff.ms}} is a property that was introduced a while ago when eager (stop-the-world) rebalancing was the only option for Connect workers. The goal of this property is to avoid triggering consecutive rebalances when a worker fails to catch up with the config topic in time and therefore voluntarily leaves the group with a {{LeaveGroupRequest}}.

With incremental cooperative rebalancing this backoff ({{worker.unsync.backoff.ms) }}that has a default value equal to the default value of {{scheduled.rebalance.max.delay.ms}} (5min) might end up turning a worker into a zombie worker that retains its tasks but stays out of the group. This worker, by backing off from rebalancing, leaves not option to the leader of the group but to reassign the missing tasks that were thought as lost to other members of the group if the worker that backs off does not return in time before {{scheduled.rebalance.max.delay.ms}} expires. 

Clearly, {{worker.unsync.backoff.ms}} was introduced to avoid rebalancing storms under the presence of intermittent connectivity issues with eager rebalancing. However when incremental cooperative rebalancing is used this property might inadvertently make workers operate as zombie workers that keep running tasks while they are out of the group.

Of course, a good tradeoff needs to be made between avoiding to make the protocol too eager again and at the same time avoiding to turn workers into zombies when connection is not lost for too long from the broker coordinator.",,ecomar,kkonstantine,MarkC0x,mimaison,xiaotao183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9653,,,,,,KAFKA-9841,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-09 22:47:11.0,,,,,,,,,,"0|z0dhdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid triggering scheduled rebalance delay when task assignment fails but Connect workers remain in the group,KAFKA-9848,13297500,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kkonstantine,kkonstantine,kkonstantine,09/Apr/20 22:46,10/Jun/20 07:16,13/Jul/23 09:17,10/Jun/20 07:16,2.3.1,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"There are cases where a Connect worker does not receive its tasks assignments successfully after a rebalance but will still remain in the group. For example when a SyncGroup response is lost, a worker will not get its expected assignments but will rejoin the group immediately and will trigger another rebalance. 

With incremental cooperative rebalancing, tasks assignments that are computed and sent by the leader but are not received by any of the members are marked as lost assignments in the subsequent rebalance. The presence of lost assignments activates the scheduled rebalance delay (property) and the missing tasks are not assigned until this delay expires.


This situation can be improved in two cases: 
a) When it's the leader that failed to receive the new assignments from the broker coordinator (for example if the SyncGroup request or response was lost). If this worker remains the leader of the group in the subsequent rebalance round, it can detect that the previous assignment was not successfully applied by checking what's the expected generation.

b) If one or more regular members did not receive their assignments successfully, but have joined the latest round of rebalancing, they can be assigned the tasks that remain unassigned from the previous assignment immediately without these tasks being marked as lost. The leader can detect that by checking that some tasks seem lost since the previous assignment but also the number of workers is unchanged between the two rounds of rebalancing. In this case, the leader can go ahead and assign the missing tasks as new tasks immediately.",,ecomar,ivanyu,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-09 22:46:59.0,,,,,,,,,,"0|z0dhd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition can lead to severe lag underestimate for active tasks,KAFKA-9846,13297474,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,ableegoldman,ableegoldman,09/Apr/20 20:19,25/Mar/21 02:39,13/Jul/23 09:17,25/Mar/21 02:39,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"In KIP-535 we added the ability to query still-restoring and standby tasks. To give users control over how out of date the data they fetch can be, we added an API to KafkaStreams that fetches the end offsets for all changelog partitions and computes the lag for each local state store.

During this lag computation, we check whether an active task is in RESTORING and calculate the actual lag if so. If not, we assume it's in RUNNING and return a lag of zero. However, tasks may be in other states besides running and restoring; notably they first pass through the CREATED state before getting to RESTORING. A CREATED task may happen to be caught-up to the end offset, but in many cases it is likely to be lagging or even completely uninitialized.

This introduces a race condition where users may be led to believe that a task has zero lag and is ""safe"" to query even with the strictest correctness guarantees, while the task is actually lagging by some unknown amount.  During transfer of ownership of the task between different threads on the same machine, tasks can actually spend a while in CREATED while the new owner waits to acquire the task directory lock. So, this race condition may not be particularly rare in multi-threaded Streams applications",,ableegoldman,cadonna,githubbot,mjsax,rhauch,vinoth,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 25 02:39:51 UTC 2021,,,,,,,,,,"0|z0dh7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 20:32;vvcephei;Thanks for the report [~ableegoldman] . I'd offer one clarifying comment. The only strict correctness guarantee available is to disable querying stale stores in StoryQueryParameters, which would be unaffected by this race condition, and in fact, such use cases wouldn't even check lags, only the ownership of the active replica.

Additionally, I'd like to add that there's no way to reason strictly about the relationship between the reported lag and the actual lag at the time of a subsequent query. Even if the reported lag is correctly zero, the store may become arbitrarily laggy by the time of a subsequent query.

That said, this bug is clearly a violation of the method's contract, which may overestimate lagginess, but never underestimate it. The fact that we might underestimate by saying that it's fully caught up when it's not even initialized makes it seem that much more egregious.

I've looked into the code base, and I believe that this bug was incidentally fixed by refactoring in trunk, so it would only affect the 2.5 branch.;;;","09/Apr/20 21:31;vinoth;First of all, thanks for flagging this issue [~ableegoldman]! 

 

IMO this need not block the release. It is bad in that sense, that a public API misbehaves under a race condition. However, this is a new feature and the architecture around building lag aware IQ routing around this has looser guarantees,  at least for the application I work on. That said, if any one deems that this evaluation is different from how AK community classifies blockers, please discard my opinion..  

 

I am looking more closely at the issue.. Will respond again in a bit. ;;;","09/Apr/20 23:08;vinoth;>>During transfer of ownership of the task between different threads on the same machine, tasks can actually spend a while in CREATED

Looking at this more closely, there is some consolation since if the caller decides to actually call kafkaStreams.store() based on underestimated lag (0), then we would error out here for CREATED tasks.. But the race does exist for STARTING, PARTITION_ASSIGNED states. but softens the blow quite a bit (IIUC what happens in each transition)

[https://github.com/apache/kafka/blob/2.5/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java#L58]

[https://github.com/apache/kafka/blob/2.5/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L151] 

 

 ;;;","10/Apr/20 00:57;ableegoldman;Hey [~vinoth], just to clarify, the CREATED state this ticket refers to is a task-level state. This is independent from the thread-level states (as in PARTITIONS_ASSIGNED, STARTING);;;","10/Apr/20 15:12;vinoth;hi [~ableegoldman], if you were talking about Task.State, then you are in the future. 
https://github.com/apache/kafka/blame/b02bdd3227f08eef78080ef471c0950a4f77e5fb/streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java

I don't think we had that in 2.5 branch and was one of the harder things to reason about then. IIUC pre 2.5, there is state in only KafkaStreams level and thread level.. Thread level is what we checked to open stores. Let me see how to do a localized fix in 2.5, for this. ;;;","10/Apr/20 17:02;ableegoldman;True, the literal Task.State was introduced for 2.6, but there was always a concept of task state. It was just relatively poorly managed and much more difficult to keep track of – hence the refactoring that introduced Task.State. The TaskManager delegated to the now-removed AssignedStreamTasks class, which kept track of task state by adding/removing tasks from the ""created"", ""restoring"", ""running"", and ""suspended"" maps. Suspended probably also causes problems for IQ, as a task in suspended is basically already revoked;;;","10/Apr/20 21:22;githubbot;vinothchandar commented on pull request #8462: KAFKA-9846: Filter active tasks for running state in KafkaStreams#allLocalStorePartitionLags()
URL: https://github.com/apache/kafka/pull/8462
 
 
   
     - Added check that only treats running active tasks as having 0 lag
     - Tasks that are neither restoring, nor running will report 0 as currentoffset position
     - Fixed LagFetchIntegrationTest to wait till thread/instance reaches RUNNING before checking lag
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Apr/20 21:45;vinoth;Understood, I was always wary of grabbing those maps at will to build this out.. 

Anyways, here's what I found.. 

The original test did not fail for this reason. the test did not wait for the instance to transition into RUNNING, instead just waiting till the lags went down to zero. This only guarantees onRestoreStart() would definitely have been called (otherwise restoration cannot start and lag would not have been zero.). It might happen that the actual lag went to 0 and thread was on its way to RUNNING state by finishing up onRestoreEnd() call. But the test could check for restoreEnd lags before that. and cause the NPE..

 
{code:java}
java.lang.NullPointerException
	at org.apache.kafka.streams.integration.LagFetchIntegrationTest.shouldFetchLagsDuringRestoration(LagFetchIntegrationTest.java:306)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) {code}
 

On the issue reported in this ticket itself, we need to have a scenario where the streamThread.allStreamsTasks() returns a task in `created` and `suspended` lists for e.g.. I wrote a thread to constantly poll for the streamThread.allStreamsTasks() as an active was starting up and restoring and transitioning into RUNNING.  I saw that the first time, I got some tasks back using allStreamsTasks() was only after PARTITIONS_ASSIGNED and it was already a restoring task. 

Do you have suggestions on reproducing this in a test?  [~ableegoldman] you mentioned sometimes the tasks can be in CREATED for a long time? Anyways, I have posted a patch here [https://github.com/apache/kafka/pull/8462/files] 

This issue does not happen on master. So if we are targetting this really for 2.6. We can close this 

 ;;;","19/Jun/20 04:14;ableegoldman;Just realized I never replied [~vinoth]. To answer your question about how to get a task stuck in CREATED during a test, one way would be to start up an instance with two threads and have one of them hang indefinitely. It will drop out of the group and its task will be assigned to the other thread, but since the first thread hasn't released the task state directory lock, this task will be stuck in CREATED.

Anyways, just bringing this up since [~vvcephei] is setting up the 2.5.1 release. It seems like we understand the problem and the fix is quite straightforward, can we get this patched for 2.5.1?;;;","23/Jun/20 17:40;vinoth;Probably don't have time this week.. But if y'all can take a quick pass at the patch above, I can work on this next week.. does that work;;;","23/Jun/20 19:05;ableegoldman;Ok no worries. I don't think it's critical, just wanted to bring it up in case you wanted to get it fixed. Since 2.6 is almost out, we can always point people who want to use this feature to use the latest version;;;","23/Jun/20 20:22;vinoth;Sounds good. Close this as ""Wont fix"" then?;;;","24/Jun/20 01:33;ableegoldman;I think we can just leave it open and maybe someone from the community will pick it up;;;","24/Jun/20 20:03;rhauch;Since this is not a blocker issue, as part of the 2.6.0 release process I'm changing the fix version to `2.7.0`. If this is incorrect, please respond and discuss on the ""[DISCUSS] Apache Kafka 2.6.0 release"" discussion mailing list thread.;;;","24/Jun/20 21:10;ableegoldman;This is definitely a limitation of the current Affects Version/Fix Version system – this actually is fixed in 2.6.0, but has not been fixed in 2.5.0 (hence the ticket is unresolved).

That said, to avoid interfering with the release process I think we can leave it as is for now and then put 2.6.0 back on the fix version once it's released so that users know this doesn't affect 2.6+;;;","25/Mar/21 02:39;ableegoldman;Resolving since this is fixed in 2.6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
plugin.path property does not work with config provider,KAFKA-9845,13297419,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,09/Apr/20 15:31,11/Jun/20 05:07,13/Jul/23 09:17,11/Jun/20 05:07,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,2.7.0,,,,,KafkaConnect,,,,,0,,,,,"The config provider mechanism doesn't work if used for the {{plugin.path}} property of a standalone or distributed Connect worker. This is because the {{Plugins}} instance which performs plugin path scanning is created using the raw worker config, pre-transformation (see [ConnectStandalone|https://github.com/apache/kafka/blob/371ad143a6bb973927c89c0788d048a17ebac91a/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectStandalone.java#L79] and [ConnectDistributed|https://github.com/apache/kafka/blob/371ad143a6bb973927c89c0788d048a17ebac91a/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java#L91]).

Unfortunately, because config providers are loaded as plugins, there's a circular dependency issue here. The {{Plugins}} instance needs to be created _before_ the {{DistributedConfig}}/{{StandaloneConfig}} is created in order for the config providers to be loaded correctly, and the config providers need to be loaded in order to perform their logic on any properties (including the {{plugin.path}} property).

There is no clear fix for this issue in the code base, and the only known workaround is to refrain from using config providers for the {{plugin.path}} property.

A couple improvements could potentially be made to improve the UX when this issue arises:
 #  Alter the config logging performed by the {{DistributedConfig}} and {{StandaloneConfig}} classes to _always_ log the raw value for the {{plugin.path}} property. Right now, the transformed value is logged even though it isn't used, which is likely to cause confusion.
 # Issue a {{WARN}}- or even {{ERROR}}-level log message when it's detected that the user is attempting to use config providers for the {{plugin.path}} property, which states that config providers cannot be used for that specific property, instructs them to change the value for the property accordingly, and/or informs them of the actual value that the framework will use for that property when performing plugin path scanning.

We should _not_ throw an error on startup if this condition is detected, as this could cause previously-functioning, benignly-misconfigured Connect workers to fail to start after an upgrade.",,ChrisEgerton,githubbot,qq619618919,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 05:07:18 UTC 2020,,,,,,,,,,"0|z0dgv4:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"09/Apr/20 15:38;githubbot;C0urante commented on pull request #8455: KAFKA-9845: Fix plugin.path when config provider is used
URL: https://github.com/apache/kafka/pull/8455
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9845)
   
   These changes cause the transformed worker configuration to be passed to the `Plugins` instance which performs plugin path scanning, instead of the raw (pre-transform) worker configuration.
   
   This has the added benefit that worker configuration validation now takes place _before_ plugin path scanning, which in some environments can take a while.
   
   No tests are added as the chance of regression is extremely small and the accuracy of the fix can be easily verified by reading the code changes and/or through local testing with a file-based config provider.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Apr/20 17:25;githubbot;C0urante commented on pull request #8455: KAFKA-9845: Fix plugin.path when config provider is used
URL: https://github.com/apache/kafka/pull/8455
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Apr/20 21:50;githubbot;C0urante commented on pull request #8455: KAFKA-9845: Warn users about using config providers with plugin.path property
URL: https://github.com/apache/kafka/pull/8455
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9845)
   
   These changes cause the transformed worker configuration to be passed to the `Plugins` instance which performs plugin path scanning, instead of the raw (pre-transform) worker configuration. This has the added benefit that worker configuration validation now takes place _before_ plugin path scanning, which in some environments can take a while.
   
   Unfortunately, they may interfere with the loading of properties of type `CLASS` from the `WorkerConfig`. The changes will need to be revisited in order to address this circular dependency issue: `Plugins` needs to be instantiated and `compareAndSwapWithDelegatingLoader` invoked before creating the worker config since it loads classes when instantiated, and the config provider logic specified in the worker config needs to be performed on the `plugin.path` property before instantiating the `Plugins` instance. This is especially complicated by the fact that config providers are loaded as plugins.
   
   No clear workaround is apparent at the moment that wouldn't involve a substantial rewrite, and the scope of the problem is limited enough that simply documenting it as a ""won't fix"" seems the best approach for now.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Jun/20 05:07;rhauch;Merged to `trunk`, and backported to `2.6` (for upcoming 2.6.0), `2.5` (for upcoming 2.5.1), and `2.4` (for future 2.4.2).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maximum number of members within a group is not always enforced due to a race condition in join group,KAFKA-9844,13297365,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,09/Apr/20 12:16,23/Apr/20 20:37,13/Jul/23 09:17,23/Apr/20 20:37,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"While analysing https://issues.apache.org/jira/browse/KAFKA-7965, I found out that the maximum number of members constraints is not always enforced due to a race condition.

When an unknown member joins the group, the group is automatically created if it does not exist. Then, it proceeds with a unknownJoinGroup. On that path, the limit is not enforced because we assumes that the group is empty as this stage because it did not exist. As the lookup and the creation are not protected by a lock, multiple join requests could end up on that path and thus bypass the enforcement.

Here is example of the logs captured while troubleshooting KAFKA-7965. The test setups 3 consumers and use a limit of 2. The logs show that the three members were able to join the group without being evicted.
{noformat}
[2020-04-05 13:29:03,145] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:36449 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794)
[2020-04-05 13:29:03,145] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:36449 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794)
[2020-04-05 13:29:03,151] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:36449 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794)
[2020-04-05 13:29:03,153] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to heartbeat failed since member id ConsumerTestConsumer-764a71ea-f9b3-462c-9986-8e6b2530d6e3 is not valid. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1054)
[2020-04-05 13:29:03,155] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:670)
[2020-04-05 13:29:03,155] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Lost previously assigned partitions group-max-size-test-5, group-max-size-test-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:314)
[2020-04-05 13:29:03,156] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,154] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to heartbeat failed since member id ConsumerTestConsumer-2d2886ad-1244-4ef7-9e07-62282c3547fd is not valid. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1054)
[2020-04-05 13:29:03,156] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to heartbeat failed since member id ConsumerTestConsumer-42d0fa9d-cfbb-458f-afe9-99a75fef8e08 is not valid. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1054)
[2020-04-05 13:29:03,157] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:670)
[2020-04-05 13:29:03,158] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Lost previously assigned partitions group-max-size-test-2, group-max-size-test-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:314)
[2020-04-05 13:29:03,158] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,157] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:670)
[2020-04-05 13:29:03,159] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Lost previously assigned partitions group-max-size-test-1, group-max-size-test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:314)
[2020-04-05 13:29:03,159] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,160] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,161] INFO [GroupCoordinator 2]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member ConsumerTestConsumer-84fd5153-c425-464d-a724-04022a0608f7 with group instanceid None) (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-05 13:29:03,158] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,160] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-04-05 13:29:03,171] INFO [GroupCoordinator 2]: Stabilized group group-max-size-test generation 1 (__consumer_offsets-0) (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-05 13:29:03,605] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Finished assignment for group at generation 1: {ConsumerTestConsumer-84fd5153-c425-464d-a724-04022a0608f7=Assignment(partitions=[group-max-size-test-0, group-max-size-test-1]), ConsumerTestConsumer-e25aedeb-73fd-4fae-b56c-fa929f11a9df=Assignment(partitions=[group-max-size-test-4, group-max-size-test-5]), ConsumerTestConsumer-8ca065a1-2ce4-44d5-881c-c6f01cb0d110=Assignment(partitions=[group-max-size-test-2, group-max-size-test-3])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2020-04-05 13:29:03,606] INFO [GroupCoordinator 2]: Assignment received from leader for group group-max-size-test for generation 1 (kafka.coordinator.group.GroupCoordinator:66)
[2020-04-05 13:29:03,610] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:502)
[2020-04-05 13:29:03,611] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Adding newly assigned partitions: group-max-size-test-1, group-max-size-test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-04-05 13:29:03,612] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,612] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,611] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:502)
[2020-04-05 13:29:03,611] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:502)
[2020-04-05 13:29:03,614] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Adding newly assigned partitions: group-max-size-test-2, group-max-size-test-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-04-05 13:29:03,614] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Adding newly assigned partitions: group-max-size-test-5, group-max-size-test-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-04-05 13:29:03,616] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,617] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,617] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-05 13:29:03,617] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,618] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Found no committed offset for partition group-max-size-test-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1297)
[2020-04-05 13:29:03,619] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-05 13:29:03,619] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-05 13:29:03,645] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-05 13:29:03,646] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383)
[2020-04-05 13:29:03,651] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Resetting offset for partition group-max-size-test-5 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:383){noformat}",,dajac,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 12:31:32 UTC 2020,,,,,,,,,,"0|z0dgj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 12:31;githubbot;dajac commented on pull request #8454: KAFKA-9844; Maximum number of members within a group is not always enforced due to a race condition in join group
URL: https://github.com/apache/kafka/pull/8454
 
 
   This patch fixes a race condition in the join group request handling which sometimes results in not enforcing the maximum number of members allowed in a group. The JIRA provides an example: https://issues.apache.org/jira/browse/KAFKA-9844
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address testing gaps in consumer OffsetsForLeaderEpochs request grouping,KAFKA-9842,13297277,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,09/Apr/20 06:44,14/Apr/20 00:20,13/Jul/23 09:17,14/Apr/20 00:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,KAFKA-9583 identified an issue with the grouping of partitions in OffsetsForLeaderEpoch requests sent by the consumer. We should have test cases which ensure that partitions are grouped correctly.,,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 00:20:07 UTC 2020,,,,,,,,,,"0|z0dfzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 21:01;githubbot;hachikuji commented on pull request #8457: KAFKA-9842; Add test case for OffsetsForLeaderEpoch grouping in Fetcher
URL: https://github.com/apache/kafka/pull/8457
 
 
   This is a follow-up to #8077. The bug exposed a testing gap in how we group partitions. This patch adds a test case which reproduces the reported problem.
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Apr/20 00:20;githubbot;hachikuji commented on pull request #8457: KAFKA-9842; Add test case for OffsetsForLeaderEpoch grouping in Fetcher
URL: https://github.com/apache/kafka/pull/8457
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector and Task duplicated when a worker join with old generation assignment,KAFKA-9841,13297276,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,LucentWong,LucentWong,LucentWong,09/Apr/20 06:32,23/Mar/21 01:29,13/Jul/23 09:17,11/Jun/20 03:18,2.3.1,2.4.0,2.4.1,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,pull-request-available,,,,"When using IncrementalCooperativeAssignor.class to assign connectors and tasks.

Suppose there is a worker 'W' got some connection issue with the coordinator.

During the connection issue, the connectors/tasks on 'W' are assigned to the others worker

When the connection issue disappear, 'W' will join the group with an old generation assignment. Then the group leader will get duplicated connectors/tasks in the metadata sent by the workers. But the duplicated connectors/tasks will not be revoked.

 

Generation 3:

Worker1:

[2020-03-17 04:31:23,481] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-ae2a2c31-fe73-4134-a376-4c4af8f466d0', leaderUrl='http://xxxxxx-2:8083/', offset=514, connectorIds=[], taskIds=[misc-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker2:

[2020-03-17 04:31:23,481] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-ae2a2c31-fe73-4134-a376-4c4af8f466d0', leaderUrl='http://xxxxxx-2:8083/', offset=514, connectorIds=[], taskIds=[misc-4], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker3:

[2020-03-17 04:31:23,481] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-ae2a2c31-fe73-4134-a376-4c4af8f466d0', leaderUrl='http://xxxxxx-2:8083/', offset=514, connectorIds=[], taskIds=[misc-3], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.dist 1480 ributed.DistributedHerder)

Worker4:

[2020-03-17 04:31:23,481] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-ae2a2c31-fe73-4134-a376-4c4af8f466d0', leaderUrl='http://xxxxxx-2:8083/', offset=514, connectorIds=[misc], taskIds=[misc-1], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker5:

[2020-03-17 04:31:23,482] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 3 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-ae2a2c31-fe73-4134-a376-4c4af8f466d0', leaderUrl='http://xxxxxx-2:8083/', offset=514, connectorIds=[], taskIds=[misc-5, misc-2], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

 

Generation 4:

Worker1:

[2020-03-17 04:32:37,165] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker2:

[2020-03-17 04:32:37,165] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-4], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker3:

[2020-03-17 04:32:35,489] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Group coordinator xxxxxx:9092 (id: 2147483631 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2020-03-17 04:32:35,590] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Discovered group coordinator xxxxxx:9092 (id: 2147483631 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2020-03-17 04:32:36,910] INFO WorkerSourceTask\{id=misc-3} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2020-03-17 04:32:36,910] INFO WorkerSourceTask\{id=misc-3} flushing 86 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2020-03-17 04:32:37,164] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[2020-03-17 04:32:37,164] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2020-03-17 04:32:37,164] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

Worker4:

[2020-03-17 04:32:37,165] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[misc], taskIds=[misc-3, misc-1], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker5:

[2020-03-17 04:32:37,165] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 4 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-5, misc-2], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

 

Generation 5:

Worker1:

[2020-03-17 04:32:42,757] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker2:

[2020-03-17 04:32:42,756] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-4], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker3:

[2020-03-17 04:32:42,757] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-3], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker4:

[2020-03-17 04:32:42,756] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[misc], taskIds=[misc-3, misc-1], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Worker5:

[2020-03-17 04:32:42,757] INFO [Worker clientId=connect-1, groupId=xxxxxx_mm2_fb__connect__group] Joined group at generation 5 with protocol version 2 and got assignment: Assignment\{error=0, leader='connect-1-2a332d4a-ef64-4b45-89c4-55f48d58f28c', leaderUrl='http://xxxxxx-4:8083/', offset=515, connectorIds=[], taskIds=[misc-5, misc-2], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

 

 ",,ecomar,giladam,githubbot,kkonstantine,LucentWong,rng,savulchik,vutkin,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10172,,,,,,,,,,KAFKA-9849,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 18 18:02:15 UTC 2020,,,,,,,,,,"0|z0dfzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 07:28;githubbot;Lucent-Wong commented on pull request #8453: KAFKA-9841: Connector and Task duplicated when a worker join with old…
URL: https://github.com/apache/kafka/pull/8453
 
 
   KAFKA-9841:  Connector and Task duplicated when a worker join with old generation assignment
   
   Detect duplicated connectors/tasks in IncrementalCooperativeAssignor then revoke all of them to trigger next round reassignment.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Apr/20 00:01;kkonstantine;Thanks for reporting the issue and submitting a pull-request [~LucentWong] !

Deduplicating once a zombie worker returns to the group might be a bit late since redundant tasks might have been running for some time, but it's definitely worth adding this check as a guard and last line of defense against zombie tasks. ;;;","10/Apr/20 00:03;kkonstantine;I assigned to me to review your PR but feel free to add your Jira Id to the project with a request to the dev mailing list and you can assign this ticket back to yourself. ;;;","10/Apr/20 02:06;LucentWong;Thank you, [~kkonstantine].;;;","29/May/20 21:53;vvcephei;Hello [~LucentWong] and [~kkonstantine] ,

I'm just getting started on the 2.5.1 release. What's the status of this ticket?

Thanks,

-John;;;","29/May/20 21:59;kkonstantine;Thanks for checking [~vvcephei]. I'd like to get this in asap along with a couple of other related bugfixes. 

How much time do we have available for {{2.5.1}} ?;;;","11/Jun/20 03:18;kkonstantine;This fix is now merged. Seems it can make {{2.5.1}} 
Thanks for checking [~vvcephei] and thanks for the contribution [~LucentWong];;;","11/Jun/20 04:08;LucentWong;Thank you for checking [~vvcephei]  and thank you for your help [~kkonstantine].;;;","17/Jul/20 00:13;vutkin;Hi guys, will fix be back-ported to older versions of kafka?;;;","18/Jul/20 18:02;kkonstantine;Hi [~vutkin]. 
This fix has been backported to all the applicable release branches. 

These branches are listed under the {{Fix Versions:}} field in this Jira ticket ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer should not use OffsetForLeaderEpoch without current epoch validation,KAFKA-9840,13297261,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,hachikuji,hachikuji,09/Apr/20 04:49,05/Jun/20 23:00,13/Jul/23 09:17,05/Jun/20 23:00,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,consumer,,,,,0,,,,,"We have observed a case where the consumer attempted to detect truncation with the OffsetsForLeaderEpoch API against a broker which had become a zombie. In this case, the last epoch known to the consumer was higher than the last epoch known to the zombie broker, so the broker returned -1 as both the end offset and epoch in the response. The consumer did not check for this in the response, which resulted in the following message:

{code}
Truncation detected for partition topic-1 at offset FetchPosition{offset=11859, offsetEpoch=Optional[46], currentLeader=LeaderAndEpoch{leader=broker-host (id: 3 rack: null), epoch=-1}}, resetting offset to the first offset known to diverge FetchPosition{offset=-1, offsetEpoch=Optional[-1], currentLeader=LeaderAndEpoch{broker-host (id: 3 rack: null), epoch=-1}} (org.apache.kafka.clients.consumer.internals.SubscriptionState:414)
{code}

There are a couple ways we the consumer can handle this situation better. First, the reason we did not detect the zombie broker is that we did not include the current leader epoch in the OffsetForLeaderEpoch request. This was likely because of KAFKA-9212. Following this patch, we would not initialize the current leader epoch from metadata responses because there are cases that we cannot rely on it. But if the client cannot rely on being able to detect zombies, then the epoch validation is less useful anyway. So the simple solution is to not bother with the validation unless we have a reliable current leader epoch.

Second, the consumer needs to check for the case when the returned offset and epoch are not defined. In this case, we have to treat this as a normal OffsetOutOfRange case and invoke the reset policy. 

",,githubbot,guozhang,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 17:04:00 UTC 2020,,,,,,,,,,"0|z0dfw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 17:18;guozhang;For the second case, when the returned offset / epoch is not defined could the consumer tries to refresh its metadata trying to find the new leader?;;;","13/Apr/20 16:26;hachikuji;[~guozhang] If we have the current epoch validation, it's a bit unclear whether this case should even be possible. There shouldn't be any situation where the epoch we're requesting is larger than the current leader epoch, so the leader should always be able to provide an end offset in the response. I guess we could even treat this as an unexpected error in the client. What do you think?;;;","13/Apr/20 16:43;guozhang;What I'm thinking is that, if we treat it as an unexpected error, either defaulting to reset policy or even throwing are not ideal -- since we are talking to a zombie broker, the consumer should not pay the price as either re-consuming or losing data or even crashing; on the other hand, suppose we do not remove the current validation from broker (since new client can always talk to old brokers), does returning -1 always means that the broker we are talking to is a zombie? If yes, instead of resetting or throwing the consumer can just try to find the current leader and retry.;;;","13/Apr/20 17:19;hachikuji;Other than the zombie case, the only way I could see this happening is some kind of disaster scenario where the cluster had to revert to an older epoch. The current epoch validation in that case would also fail, which would cause the consumer to retry. So perhaps retrying on -1 without the current epoch validation is reasonable as well.;;;","14/Apr/20 17:04;githubbot;abbccdda commented on pull request #8486: KAFKA-9840: Skip End Offset validation when the leader epoch is not reliable
URL: https://github.com/apache/kafka/pull/8486
 
 
   As title suggests.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException on metadata update when broker learns about its new epoch after the controller,KAFKA-9839,13297258,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,apovzner,apovzner,apovzner,09/Apr/20 04:13,10/Jan/22 13:25,13/Jul/23 09:17,27/Apr/20 20:28,2.2.1,2.3.1,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,2.5.1,,,,,,,,controller,core,,,,0,,,,,"Broker throws ""java.lang.IllegalStateException: Epoch XXX larger than current broker epoch YYY""  on UPDATE_METADATA when the controller learns about the broker epoch and sends UPDATE_METADATA before KafkaZkCLient.registerBroker completes (the broker learns about its new epoch).

Here is the scenario we observed in more detail:
1. ZK session expires on broker 1
2. Broker 1 establishes new session to ZK and creates znode
3. Controller learns about broker 1 and assigns epoch
4. Broker 1 receives UPDATE_METADATA from controller, but it does not know about its new epoch yet, so we get an exception:

ERROR [KafkaApi-3] Error when handling request: clientId=1, correlationId=0, api=UPDATE_METADATA, body={
.........
java.lang.IllegalStateException: Epoch XXX larger than current broker epoch YYY at kafka.server.KafkaApis.isBrokerEpochStale(KafkaApis.scala:2725) at kafka.server.KafkaApis.handleUpdateMetadataRequest(KafkaApis.scala:320) at kafka.server.KafkaApis.handle(KafkaApis.scala:139) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69) at java.lang.Thread.run(Thread.java:748)

5. KafkaZkCLient.registerBroker completes on broker 1: ""INFO Stat of the created znode at /brokers/ids/1""

The result is the broker has a stale metadata for some time.

Possible solutions:
1. Broker returns a more specific error and controller retries UPDATE_MEDATA
2. Broker accepts UPDATE_METADATA with larger broker epoch.",,akatona,aklochkov,apovzner,githubbot,junrao,lkokhreidze,sigurdsa,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 19:27:04 UTC 2020,,,,,,,,,,"0|z0dfvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/20 16:36;junrao;Thanks for reporting this, Anna. Accepting UPDATE_METADATA (and other similar requests from the controller) with a larger broker epoch is probably the easier solution.;;;","17/Apr/20 12:37;sigurdsa;We experienced the same issue, adding some logs if it could be helpful.

After this happened the server got stuck in a loop for an hour where ISR was not in sync, and it wasn't fixed until a hard restart was done on the server.
{code:java}
Apr 16 10:01:11 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:11,527] WARN Client session timed out, have not heard from server in 6981ms for sessionid 0x50000484a370021 (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:11 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:11,892] INFO Client session timed out, have not heard from server in 6981ms for sessionid 0x50000484a370021, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:12 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,930] INFO Opening socket connection to server
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,930] INFO Socket connection established to 
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,932] WARN Unable to reconnect to ZooKeeper service, session 0x50000484a370021 has expired (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,932] INFO Unable to reconnect to ZooKeeper service, session 0x50000484a370021 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,932] INFO EventThread shut down for session: 0x50000484a370021 (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,932] INFO [ZooKeeperClient Kafka server] Session expired. (kafka.zookeeper.ZooKeeperClient)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,934] INFO [ZooKeeperClient Kafka server] Initializing a new session to <1><2><3><4><5>. (kafka.zookeepe
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,934] INFO Initiating client connection, connectString=<1><2><3><4><5> sessionTimeout=6000 watcher=kafka
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:12,934] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,042] INFO Opening socket connection to server <1> Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,042] INFO Socket connection established to <1>, initiating session (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,832] INFO Session establishment complete on server <1>, sessionid = 0x10000008d93001b, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,874] ERROR [KafkaApi-0] Error when handling request: clientId=22, correlationId=0, api=UPDATE_METADATA, body={controller_id=22,controller_epoch=83,broker_epoch=25770069286,topic_states=[{topic
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: =47,replicas=[0],offline_replicas=[]},{partition=54,controller_epoch=83,leader=-1,leader_epoch=47,isr=[0],zk_version=47,replicas=[0],offline_replicas=[]},{partition=55,controller_epoch=83,leader=-1,leader_epoch=47
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: java.lang.IllegalStateException: Epoch 25770069286 larger than current broker epoch 25770067254
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.isBrokerEpochStale(KafkaApis.scala:2612)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.handleUpdateMetadataRequest(KafkaApis.scala:242)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.handle(KafkaApis.scala:119)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at java.lang.Thread.run(Thread.java:748)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,885] INFO Stat of the created znode at /brokers/ids/0 is: 25770069286,25770069286,1587024073851,1587024073851,1,0,0,72057596413149211,200,0,25770069286
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:  (kafka.zk.KafkaZkClient)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,885] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(192.168.220.120,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25770069286 (kafka.zk.
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:13,886] ERROR [KafkaApi-0] Error when handling request: clientId=22, correlationId=1, api=LEADER_AND_ISR, body={controller_id=22,controller_epoch=83,broker_epoch=25770069286,topic_states=[{topic=
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: poch=47,isr=[0],zk_version=47,replicas=[0],is_new=false},{partition=85,controller_epoch=83,leader=-1,leader_epoch=47,isr=[0],zk_version=47,replicas=[0],is_new=false},{partition=86,controller_epoch=83,leader=-1,lea
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]: java.lang.IllegalStateException: Epoch 25770069286 larger than current broker epoch 25770067254
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.isBrokerEpochStale(KafkaApis.scala:2612)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:194)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
Apr 16 10:01:13 VM-IWM-APP18 kafka-server-start.sh[9200]:         at java.lang.Thread.run(Thread.java:748)
Apr 16 10:01:14 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:14,867] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(plant_alarms-64, plant_values-146, plant_alarms-8, plant_alarms-0, plant_alarms-80, plant_alarms-128, plant_ala
Apr 16 10:01:14 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:14,867] INFO [Partition plant_alarms-48 broker=0] plant_alarms-48 starts at Leader Epoch 48 from offset 0. Previous Leader Epoch was: 46 (kafka.cluster.Partition)
Apr 16 10:01:15 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:15,119] INFO [Partition plant_alarms-10 broker=0] plant_alarms-10 starts at Leader Epoch 48 from offset 0. Previous Leader Epoch was: 46 (kafka.cluster.Partition)
Apr 16 10:01:15 VM-IWM-APP18 kafka-server-start.sh[9200]: [2020-04-16 10:01:15,356] INFO [Partition plant_values-6 broker=0] plant_values-6 starts at Leader Epoch 48 from offset 76. Previous Leader Epoch was: 46 (kafka.cluster.Partition)
...
...
Apr 16 10:58:15 VM-IWM-APP18 kafka-server-start.sh[32612]: [2020-04-16 10:58:15,130] INFO [Partition demo_plant_values-140 broker=0] demo_plant_values-140 starts at Leader Epoch 98 from offset 30769106. Previous Leader Epoch was: 97 (kafka.cluster.Partition)
Apr 16 10:58:15 VM-IWM-APP18 kafka-server-start.sh[32612]: [2020-04-16 10:58:15,135] INFO [Partition demo_plant_values-102 broker=0] demo_plant_values-102 starts at Leader Epoch 98 from offset 35753967. Previous Leader Epoch was: 97 (kafka.cluster.Partition)


{code}
 ;;;","18/Apr/20 00:47;apovzner;[~junrao] Thanks, yes I agree that accepting controller requests (UpdateMetadataRequest, LeaderAndIsrRequest, StopReplicaRequest) with newer broker epoch is the best approach – I don't see any drawbacks of accepting newer metadata. I am opening a PR with this solution shortly. ;;;","18/Apr/20 02:23;githubbot;apovzner commented on pull request #8509: KAFKA-9839: Broker should accept control requests with newer broker epoch
URL: https://github.com/apache/kafka/pull/8509
 
 
   A broker throws IllegalStateException if the broker epoch in the LeaderAndIsr/UpdateMetadataRequest/StopReplicaRequest is larger than its current broker epoch. However, there is no guarantee that the broker would receive the latest broker epoch before the controller: When the broker registers with ZK, there are few more instructions to process before this broker ""knows"" about its epoch, while the controller may already get notified and send UPDATE_METADATA request (as an example) with the new epoch. This will result in clients getting stale metadata from this broker. 
   
   With this PR, a broker accepts LeaderAndIsr/UpdateMetadataRequest/StopReplicaRequest if the broker epoch is newer than the current epoch.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Aug/20 07:41;viktorsomogyi;Detected this issue in a customer's environment, I've backported it to 2.2.1 and it seemed to fix it for them. I'll publish my solution upstream soon.;;;","13/Aug/20 19:27;akatona;This is in 2.6.0 too, but it's not in [release notes of 2.6.0|https://dist.apache.org/repos/dist/release/kafka/2.6.0/RELEASE_NOTES.html]. Commit:
https://github.com/apache/kafka/commit/bd17085ec10c767bc82e6b19a3016cf5d50dad92 
Shouldn't it be there? It's in 2.5.1 release notes but it was released later, so that confused my colleague. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition with concurrent write allows reads above high watermark,KAFKA-9835,13296992,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,08/Apr/20 06:47,08/Apr/20 19:14,13/Jul/23 09:17,08/Apr/20 18:43,2.2.0,2.3.0,2.4.0,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,,,,,,,,,,,,0,,,,,"Kafka's log implementation serializes all writes using a lock, but allows multiple concurrent reads while that lock is held. The `FileRecords` class contains the core implementation. Reads to the log create logical slices of `FileRecords` which are then passed to the network layer for sending. An abridged version of the implementation of `slice` is provided below:

{code}
    public FileRecords slice(int position, int size) throws IOException {
        int end = this.start + position + size;
        // handle integer overflow or if end is beyond the end of the file
        if (end < 0 || end >= start + sizeInBytes())
            end = start + sizeInBytes();
        return new FileRecords(file, channel, this.start + position, end, true);
    }
{code}

The `size` parameter here is typically derived from the fetch size, but is upper-bounded with respect to the high watermark. The two calls to `sizeInBytes` here are problematic because the size of the file may change in between them. Specifically a concurrent write may increase the size of the file after the first call to `sizeInBytes` but before the second one. In the worst case, when `size` defines the limit of the high watermark, this can lead to a slice containing uncommitted data.",,githubbot,hachikuji,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 18:31:32 UTC 2020,,,,,,,,,,"0|z0de88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 16:02;githubbot;hachikuji commented on pull request #8451: KAFKA-9835; Protect `FileRecords.slice` from concurrent write
URL: https://github.com/apache/kafka/pull/8451
 
 
   If the size of a log changes due to a concurrent write while accessing `FileRecords.slice`, then we may exceed the requested size. In particular, this would allow a read above the high watermark.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","08/Apr/20 18:31;githubbot;hachikuji commented on pull request #8451: KAFKA-9835; Protect `FileRecords.slice` from concurrent write
URL: https://github.com/apache/kafka/pull/8451
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing test: EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState[exactly_once_beta],KAFKA-9831,13296880,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,showuon,vvcephei,vvcephei,07/Apr/20 18:08,09/Apr/21 19:46,13/Jul/23 09:17,09/Apr/21 19:46,,,,,,,,,,,,,,,,,,,,,,,2.7.2,2.8.0,,,,,,,streams,unit tests,,,,0,,,,,"I've seen this fail twice in a row on the same build, but with different errors. Stacktraces follow; stdout is attached.

One:
{noformat}
java.lang.AssertionError: Did not receive all 40 records from topic singlePartitionOutputTopic within 60000 ms
Expected: is a value equal to or greater than <40>
     but: <39> was less than <40>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinKeyValueRecordsReceived$1(IntegrationTestUtils.java:517)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:513)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:491)
	at org.apache.kafka.streams.integration.EosIntegrationTest.readResult(EosIntegrationTest.java:766)
	at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748){noformat}
Two:

 
{noformat}
java.lang.AssertionError: 
Expected: <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45), KeyValue(1, 55), KeyValue(1, 66), KeyValue(1, 78), KeyValue(1, 91), KeyValue(1, 105)]>
     but: was <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45), KeyValue(1, 55), KeyValue(1, 66), KeyValue(1, 78), KeyValue(1, 91), KeyValue(1, 105), KeyValue(1, 55), KeyValue(1, 66), KeyValue(1, 78)]>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.integration.EosIntegrationTest.checkResultPerKey(EosIntegrationTest.java:277)
	at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:478)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748){noformat}
 ",,ableegoldman,bchen225242,guozhang,mimaison,mjsax,showuon,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7544,,,,,,,,,,,,,KAFKA-7544,,,,,,"07/Apr/20 18:07;vvcephei;one.stdout.txt;https://issues.apache.org/jira/secure/attachment/12999259/one.stdout.txt","07/Apr/20 18:07;vvcephei;two.stdout.txt;https://issues.apache.org/jira/secure/attachment/12999260/two.stdout.txt",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 20:46:18 UTC 2021,,,,,,,,,,"0|z0ddjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/20 18:09;vvcephei;Linking possibly related issue.;;;","08/Apr/20 06:07;mjsax;The issue seems to be broker side? \cc [~guozhang] WDYT?;;;","08/Apr/20 18:18;ableegoldman;h3. Error Message

java.lang.AssertionError: Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105), KeyValue(0, 55), KeyValue(0, 66)]>;;;","08/Apr/20 23:48;ableegoldman;Failed twice on the same build:
h3. Error Message

java.lang.AssertionError: Did not receive all 40 records from topic singlePartitionOutputTopic within 60000 ms Expected: is a value equal to or greater than <40> but: <39> was less than <40>
h3. Error Message

java.lang.AssertionError: Expected: <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45)]> but: was <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45), KeyValue(1, 55), KeyValue(1, 66), KeyValue(1, 78)]>;;;","09/Apr/20 01:51;mjsax;I think I found the root cause of this flakieness: [https://github.com/apache/kafka/pull/8443/files#r405910209]

Should be fixed with that PR.;;;","09/Apr/20 04:12;guozhang;Thanks for the report Matthias.;;;","14/Apr/20 05:23;mjsax;Seems the other fix did not resolve the issue. Failed again for eos-alpha on a PR build today: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5762/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_/]

Not sure if [~ableegoldman] latest fixes would address the issue?;;;","14/Apr/20 20:42;ableegoldman;I don't think either of the bugs exposed by soak would cause this test to fail, at least not in the way it did – seems eos is violated in at least some failures, as the ""Expected"" set of records is shorter than the ""Actual"";;;","14/Apr/20 22:40;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5765/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_beta_/]

and

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1747/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_/];;;","15/Apr/20 00:06;mjsax;Thanks [~ableegoldman]. Will investigate this in more details now.;;;","02/May/20 01:59;mjsax;I run this test like 500 times locally but could not reproduce yet...;;;","05/May/20 05:44;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6118/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_/]
{quote}java.lang.AssertionError: Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55)]> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.EosIntegrationTest.checkResultPerKey(EosIntegrationTest.java:280) at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:479){quote};;;","05/May/20 17:31;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2127/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_/]
{quote}java.lang.AssertionError: Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105), KeyValue(0, 55), KeyValue(0, 66)]> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.EosIntegrationTest.checkResultPerKey(EosIntegrationTest.java:280) at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:481){quote};;;","02/Jul/20 18:29;bchen225242;Failed again: 

[https://builds.apache.org/job/kafka-pr-jdk14-scala2.13/1377/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_/]
h3. Error Message

java.lang.AssertionError: Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28)]>
h3. Stacktrace

java.lang.AssertionError: Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78), KeyValue(0, 91), KeyValue(0, 105)]> but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28)]>;;;","06/Oct/20 19:01;ableegoldman;[https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-9373/3/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/Build___JDK_11___shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_beta_/]
h3. Stacktrace

java.lang.AssertionError: Expected: <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45)]> but: was <[KeyValue(1, 0), KeyValue(1, 1), KeyValue(1, 3), KeyValue(1, 6), KeyValue(1, 10), KeyValue(1, 15), KeyValue(1, 21), KeyValue(1, 28), KeyValue(1, 36), KeyValue(1, 45), KeyValue(1, 55)]> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.EosIntegrationTest.checkResultPerKey(EosIntegrationTest.java:281) at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:480);;;","04/Mar/21 02:58;showuon;[~mjsax], I'd like to take over this ticket to help investigate this issue. Is it OK? It failed recent build tests. Thanks.;;;","04/Mar/21 19:38;mjsax;Absolutely! Reassigning.;;;","08/Apr/21 17:22;mimaison;I saw it again today in the 2.7 build:
{noformat}
java.lang.AssertionError: 
Expected: <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45)]>
     but: was <[KeyValue(0, 0), KeyValue(0, 1), KeyValue(0, 3), KeyValue(0, 6), KeyValue(0, 10), KeyValue(0, 15), KeyValue(0, 21), KeyValue(0, 28), KeyValue(0, 36), KeyValue(0, 45), KeyValue(0, 55), KeyValue(0, 66), KeyValue(0, 78)]>
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at org.apache.kafka.streams.integration.EosIntegrationTest.checkResultPerKey(EosIntegrationTest.java:281)
	at org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState(EosIntegrationTest.java:480){noformat}
[https://ci-builds.apache.org/job/Kafka/job/kafka-2.7-jdk8/143/testReport/junit/org.apache.kafka.streams.integration/EosIntegrationTest/shouldNotViolateEosIfOneTaskFailsWithState_exactly_once_beta_/];;;","08/Apr/21 20:46;mjsax;That is weird... the stacktrace does not match the code: [https://github.com/apache/kafka/blob/2.7/streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java#L480-L481]

We don't call `checkResultPerKey` in L480:
{code:java}
final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade = prepareData(15L, 20L, keysFirstClientBeta.toArray(new Long[0]));
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeadLetterQueueReporter leaks KafkaProducer instance,KAFKA-9830,13296874,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,07/Apr/20 17:22,27/May/20 19:27,13/Jul/23 09:17,30/Apr/20 02:42,2.0.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"The DeadLetterQueueReporter (introduced by KAFKA-6738) creates a KafkaProducer to report errors to Kafka, but does not clean up the producer, leaving many idle network threads open after tasks are stopped.

Reproduction steps:
Start a task that has a non-empty DLQ topic name
Stop the task
Observe the list of running threads

Expected result:
There is no thread related to the stopped task's DLQ left running

Actual result:
There is a thread named something like kafka-producer-network-thread | connector-dlq-producer-task-0"" #1234 left running",,gharris1727,githubbot,rng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10045,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 00:18:04 UTC 2020,,,,,,,,,,"0|z0ddi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 00:18;githubbot;gharris1727 commented on pull request #8442: KAFKA-9830: Implement AutoCloseable in ErrorReporter and subclasses
URL: https://github.com/apache/kafka/pull/8442
 
 
   * The DeadLetterQueueReporter has a KafkaProducer that it must close to clean up resources
   * Currently, the producer and it's threads are leaked every time a task is stopped
   * Responsibility for cleaning up ErrorReporters is transitively assigned to the
       ProcessingContext, RetryWithToleranceOperator, and WorkerSinkTask/WorkerSinkTask classes
   * One new unit test in ErrorReporterTest asserts that the producer is closed by the dlq reporter
   
   Signed-off-by: Greg Harris <gregh@confluent.io>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log cleaning repeatedly picks same segment with no effect when first dirty offset is past start of active segment,KAFKA-9826,13296671,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,steverod,steverod,steverod,06/Apr/20 22:36,24/Mar/22 03:54,13/Jul/23 09:17,15/Apr/20 05:28,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,log cleaner,,,,,0,,,,,"Seen on a system where a given partition had a single segment, and for whatever reason (deleteRecords?), the logStartOffset was greater than the base segment of the log, there were a continuous series of 

```

[2020-03-03 16:56:31,374] WARN Resetting first dirty offset of  FOO-3 to log start offset 55649 since the checkpointed offset 0 is invalid. (kafka.log.LogCleanerManager$)

```

messages (partition name changed, it wasn't really FOO). This was expected to be resolved by KAFKA-6266 but clearly wasn't. 

Further investigation revealed that  a few segments were continuously cleaning and generating messages in the `log-cleaner.log` of the form:

```

[2020-03-31 13:34:50,924] INFO Cleaner 1: Beginning cleaning of log FOO-3 (kafka.log.LogCleaner)

[2020-03-31 13:34:50,924] INFO Cleaner 1: Building offset map for FOO-3... (kafka.log.LogCleaner)

[2020-03-31 13:34:50,927] INFO Cleaner 1: Building offset map for log FOO-3 for 0 segments in offset range [55287, 54237). (kafka.log.LogCleaner)

[2020-03-31 13:34:50,927] INFO Cleaner 1: Offset map for log FOO-3 complete. (kafka.log.LogCleaner)

[2020-03-31 13:34:50,927] INFO Cleaner 1: Cleaning log FOO-3 (cleaning prior to Wed Dec 31 19:00:00 EST 1969, discarding tombstones prior to Tue Dec 10 13:39:08 EST 2019)... (kafka.log.LogCleaner)

[2020-03-31 13:34:50,927] INFO [kafka-log-cleaner-thread-1]: Log cleaner thread 1 cleaned log FOO-3 (dirty section = [55287, 55287])

0.0 MB of log processed in 0.0 seconds (0.0 MB/sec).

Indexed 0.0 MB in 0.0 seconds (0.0 Mb/sec, 100.0% of total time)

Buffer utilization: 0.0%

Cleaned 0.0 MB in 0.0 seconds (NaN Mb/sec, 0.0% of total time)

Start size: 0.0 MB (0 messages)

End size: 0.0 MB (0 messages) NaN% size reduction (NaN% fewer messages) (kafka.log.LogCleaner)

```

What seems to have happened here (data determined for a different partition) is:

There exist a number of partitions here which get relatively low traffic, including our friend FOO-5. For whatever reason, LogStartOffset of this partition has moved beyond the baseOffset of the active segment. (Notes in other issues indicate that this is a reasonable scenario.) So there’s one segment, starting at 166266, and a log start of 166301.

grabFilthiestCompactedLog runs and reads the checkpoint file. We see that this topicpartition needs to be cleaned, and call cleanableOffsets on it which returns an OffsetsToClean with firstDirtyOffset == logStartOffset == 166301 and firstUncleanableOffset = max(logStart, activeSegment.baseOffset) = 116301, and forceCheckpoint = true.

The checkpoint file is updated in grabFilthiestCompactedLog (this is the fix for KAFKA-6266). We then create a LogToClean object based on the firstDirtyOffset and firstUncleanableOffset of 166301 (past the active segment’s base offset).

The LogToClean object has cleanBytes = logSegments(-1, firstDirtyOffset).map(_.size).sum → the size of this segment. It has firstUncleanableOffset and cleanableBytes determined by calculateCleanableBytes. calculateCleanableBytes returns:
{{}}
{{val firstUncleanableSegment = log.nonActiveLogSegmentsFrom(uncleanableOffset).headOption.getOrElse(log.activeSegment)}}
{{val firstUncleanableOffset = firstUncleanableSegment.baseOffset}}
{{val cleanableBytes = log.logSegments(firstDirtyOffset, math.max(firstDirtyOffset, firstUncleanableOffset)).map(_.size.toLong).sum

(firstUncleanableOffset, cleanableBytes)}}
firstUncleanableSegment is activeSegment. firstUncleanableOffset is the base offset, 166266. cleanableBytes is looking for logSegments(166301, max(166301, 166266) → which _is the active segment_

So there are “cleanableBytes” > 0.

We then filter out segments with totalbytes (clean + cleanable) > 0. This segment has totalBytes > 0, and it has cleanablebytes, so great! It’s filthiest.

The cleaner picks it, calls cleanLog on it, which then does cleaner.clean, which returns nextDirtyOffset and cleaner stats. cleaner.clean callls doClean, which builds an offsetMap. The offsetMap looks at non-active segments, when building, but there aren’t any. So the endOffset of the offsetMap is lastOffset (default -1) + 1 → 0. We record the stats (including logging to log-cleaner.log). After this we call cleanerManager.doneCleaning, which writes the checkpoint file with the latest value… of 0.

And then the process starts all over.

It appears that there's at least one bug here, where `log.logSegments(from, to)` will return an empty list if from == to and both are segment-aligned, but _not_ if they are in the middle of a segment, and possibly that LogToClean does start=firstDirtyOffset, end = max(firstDirtyOffset, firstUncleanableOffset) – it can move the firstUncleanableOffset even when the firstDirtyOffset is past firstUncleanable.

 ",,dhoard,fekelund,githubbot,junrao,steverod,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 24 03:54:19 UTC 2022,,,,,,,,,,"0|z0dc8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/20 01:44;githubbot;steverod commented on pull request #8469: [KAFKA-9826] Handle an unaligned first dirty offset during log cleaning.
URL: https://github.com/apache/kafka/pull/8469
 
 
   What
   ====
   In KAFKA-9826, a log whose first dirty offset was past the start of the active segment and past the last cleaned point resulted in an endless cycle of picking the segment to clean and discarding it. Though this didn't interfere with cleaning other log segments, it kept the log cleaner thread continuously busy (potentially wasting CPU and impacting other running threads) and filled the logs with lots of extraneous messages.
   
   This was determined to be because the active segment was getting mistakenly picked for cleaning, and because the `logSegments` code handles (start == end) cases only for (start, end) on a segment boundary: the intent is to return a null list, but if they're not on a segment boundary, the routine returns that segment.
   
   This fix has two parts:
   1. It changes logSegments to handle start==end by returning an empty List always.
   2. It changes the definition of calculateCleanableBytes to not consider anything past the UncleanableOffset; previously, it would potentially shift the UncleanableOffset to match the firstDirtyOffset even if the firstDirtyOffset was past the firstUncleanableOffset. This has no real effect now in the context of the fix for (1) but it makes the code read more like the model that the code is attempting to follow.
   
   These changes require modifications to a few test cases that handled this particular test case; they were introduced in the context of KAFKA-8764. Those situations are now handled elsewhere in code, but the tests themselves allowed a DirtyOffset in the active segment, and expected an active segment to be selected for cleaning.
   
   An additional unit test for the logSegments call is added. 
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Apr/20 05:27;githubbot;junrao commented on pull request #8469: [KAFKA-9826] Handle an unaligned first dirty offset during log cleaning.
URL: https://github.com/apache/kafka/pull/8469
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Apr/20 05:28;junrao;merged the PR to trunk;;;","07/May/20 12:57;fekelund;Hello, we have also hit this issue in one of our environments. Is the fix planned to be included in 2.4.2?;;;","07/May/20 15:01;junrao;[~fekelund]: Yes. Updated the fix versions.;;;","11/May/21 13:36;zhangzs;we have same problems，kafka version 2.12_2.4.1
{code:java}
// code placeholder
[2021-05-11 18:31:33,329] WARN Resetting first dirty offset of __consumer_offsets-30 to log start offset 4187979634 since the checkpointed offset 4187569609 is invalid. (kafka.log.LogCleanerManager$)
{code}
 ;;;","11/May/21 22:37;junrao;[~zhangzs] : The issue was fixed in 2.4.2. Could you try that version?;;;","24/Mar/22 03:54;zhangzs;[~junrao]  kafka version from 2.4.1 to 2.8.1，the issue has sloved，tks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer should check equality of the generation for coordinator requests,KAFKA-9823,13296373,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,05/Apr/20 17:26,28/Apr/20 00:10,13/Jul/23 09:17,23/Apr/20 19:25,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,consumer,,,,,0,,,,,"In consumer's requests to group coordinator (heartbeat, join-group, sync-group, commit; leave-group is not considered since consumer do not check its responses anyways), we encoded the generation / member information and the response may indicate that the member.id is invalid or generation is stale, which would cause consumer to reset and re-join group.

However, when the response is sent back it is possible that the consumer has already started re-join due to other channels indicating it out of the group, and hence resetting would introduce unnecessarily more re-joining operations.

We should, instead, remember the generation information that was sent along with the request and upon getting the response compare that with the current generation information. If they do not match it means the responded error indicating stale information has been updated in other places and hence can be handled differently (for example, in heartbeat handling we can just ignore the error).",,githubbot,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 06:30:51 UTC 2020,,,,,,,,,,"0|z0dank:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/20 05:40;githubbot;guozhangwang commented on pull request #8445: KAFKA-9823: Remember the sent generation for the coordinator request
URL: https://github.com/apache/kafka/pull/8445
 
 
   For join / sync / commit / heartbeat request, we would remember the sent generation in the created handler object, and then upon getting the error code, we could check whether the sent generation still matches the current generation. If not, it means that the member has already reset its generation or has participated in a new rebalance already. This means:
   
   1. For join / sync-group request, we do not need to call reset-generation any more for illegal-generation / unknown-member. But we would still set the error since at a given time only one join/sync round-trip would be in flight, and hence we should not be participating in a new rebalance. Also for fenced instance error we still treat it as fatal since we should not be participating in a new rebalance, so this is still not expected.
   
   2. For commit request, we do not set the corresponding error for illegal-generation / unknown-member / fenced-instance but raise rebalance-in-progress. For commit-sync it would be still thrown to user, while for commit-async it would be logged and swallowed.
   
   3. For heartbeat request, we do not treat illegal-generation / unknown-member / fenced-instance errors and just consider it as succeeded since this should be a stale heartbeat which can be ignored.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Apr/20 04:48;guozhang;https://issues.apache.org/jira/browse/KAFKA-9659 seems related to this issue as well.;;;","20/Apr/20 06:26;githubbot;kkonstantine commented on a change in pull request #8445:
URL: https://github.com/apache/kafka/pull/8445#discussion_r411123324



##########
File path: clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
##########
@@ -737,16 +751,21 @@ public void handle(SyncGroupResponse syncResponse,
                     log.debug(""SyncGroup failed because the group began another rebalance"");
                     future.raise(error);
                 } else if (error == Errors.FENCED_INSTANCE_ID) {
-                    log.error(""Received fatal exception: group.instance.id gets fenced"");
+                    // for sync-group request, even if the generation has changed we would not expect the instance id
+                    // gets fenced, and hence we always treat this as a fatal error
+                    log.error(""SyncGroup failed with {} due to group.instance.id {} gets fenced"",
+                        sentGeneration, rebalanceConfig.groupInstanceId);
                     future.raise(error);
                 } else if (error == Errors.UNKNOWN_MEMBER_ID
                         || error == Errors.ILLEGAL_GENERATION) {
-                    log.debug(""SyncGroup failed: {}"", error.message());
-                    resetGenerationOnResponseError(ApiKeys.SYNC_GROUP, error);
+                    log.info(""SyncGroup failed with {}: {}, would request re-join"", sentGeneration, error.message());
+                    if (generationUnchanged())

Review comment:
       Makes sense. Thanks




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Apr/20 06:30;githubbot;kkonstantine commented on a change in pull request #8445:
URL: https://github.com/apache/kafka/pull/8445#discussion_r411125481



##########
File path: clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java
##########
@@ -737,16 +751,21 @@ public void handle(SyncGroupResponse syncResponse,
                     log.debug(""SyncGroup failed because the group began another rebalance"");
                     future.raise(error);
                 } else if (error == Errors.FENCED_INSTANCE_ID) {
-                    log.error(""Received fatal exception: group.instance.id gets fenced"");
+                    // for sync-group request, even if the generation has changed we would not expect the instance id
+                    // gets fenced, and hence we always treat this as a fatal error
+                    log.error(""SyncGroup failed with {} due to group.instance.id {} gets fenced"",
+                        sentGeneration, rebalanceConfig.groupInstanceId);
                     future.raise(error);
                 } else if (error == Errors.UNKNOWN_MEMBER_ID
                         || error == Errors.ILLEGAL_GENERATION) {
-                    log.debug(""SyncGroup failed: {}"", error.message());
-                    resetGenerationOnResponseError(ApiKeys.SYNC_GROUP, error);
+                    log.info(""SyncGroup failed with {}: {}, would request re-join"", sentGeneration, error.message());

Review comment:
       Not sure how this comment was changed, but this still doesn't read well for me. 
   Still seems that the message should say `... will request re-join""`




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream task may skip assignment with static members and incremental rebalances,KAFKA-9821,13296150,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,guozhang,guozhang,04/Apr/20 01:04,01/Jun/20 19:09,13/Jul/23 09:17,01/Jun/20 19:09,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"When static membership (KIP-345) and incremental rebalancing (KIP-429) are turned on at the same time, that upon failure it is possible some partitions are not assigned to anyone. The event sequence is the following:

1. An assignment (task1) from rebalance is sent to an existing static member with owned list (partition1, partition2), hence upon receiving the assignment the static member is supposed to revoke partition2 and then re-join the group to trigger another rebalance.

2. The member crashed before re-join the group, lost all of its assigned partitions. However since this member is static with long session timeout, it was not kicked out of the group yet at the coordinator side.

3. The member resumes and then re-join with a known instance.id. The coordinator would not trigger a rebalance in this case and just give it the previous assignment (partition1), and since the member has forgot about its previous owned partitions it would just take partition1 and not re-join.

4. As a result the partition2 is not owned by this member any more but not re-assigned to anyone; until the next rebalance it would not be fetched by any member of the group.

The key here is that today we are relying on the member's local memory to calculate the added / revoked diff based on (owned, assigned). But if the member crashed and lost all of its owned partition, AND if it is a static member whose re-join would not trigger a new rebalance, this will break. ",,ableegoldman,cadonna,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10078,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 18:03:29 UTC 2020,,,,,,,,,,"0|z0d9tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 01:49;ableegoldman;What do you think about trying to improve the logging/visibility of revoked partitions and followup rebalances? For example, one or both of

1) Log when the assignment is stable, not just when a followup rebalance is required

2) Bump the following log message in ConsumerCoordinator#onJoinComplete from debug to info: ""Need to revoke partitions {} and re-join the group""

It might also be nice to log, at least on debug, which partitions are being left out in order to safely revoke them first. This way users can tell which partitions are experiencing downtime due to transferring ownership (someone specifically asked about this after the Cooperative Rebalancing meetup talk);;;","04/Apr/20 17:26;guozhang;Yup I totally agree with the logging improvements as well.;;;","26/May/20 18:03;ableegoldman;This issue is fixed for Streams in 2.6.0 by using the Streams-specific assignment info. We can consider doing something similar to fix for the consumer client as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validateMessagesAndAssignOffsetsCompressed allocates batch iterator which is not used,KAFKA-9820,13296141,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,lucasbradstreet,lucasbradstreet,04/Apr/20 00:05,04/Apr/20 17:10,13/Jul/23 09:17,04/Apr/20 17:10,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,core,,,,,0,,,,,"KAFKA-8106 added a new skip key/value iterator that reduces allocations [https://github.com/apache/kafka/commit/3e9d1c1411c5268de382f9dfcc95bdf66d0063a0].

Unfortunately in LogValidator it creates that iterator but it never uses it, and this is quite expensive in terms of allocations.",,githubbot,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 04 17:05:56 UTC 2020,,,,,,,,,,"0|z0d9rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 00:06;githubbot;lbradstreet commented on pull request #8422: KAFKA-9820: validateMessagesAndAssignOffsetsCompressed allocates batc…
URL: https://github.com/apache/kafka/pull/8422
 
 
   …h iterator which is not used
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Apr/20 17:05;githubbot;ijuma commented on pull request #8422: KAFKA-9820: validateMessagesAndAssignOffsetsCompressed allocates unused iterator
URL: https://github.com/apache/kafka/pull/8422
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StoreChangelogReaderTest#shouldNotThrowOnUnknownRevokedPartition[0],KAFKA-9819,13296136,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,ableegoldman,ableegoldman,03/Apr/20 22:56,17/Apr/20 14:50,13/Jul/23 09:17,17/Apr/20 14:50,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,flaky-test,,,,"h3. Error Message

java.lang.AssertionError: expected:<[test-reader Changelog partition unknown-0 could not be found, it could be already cleaned up during the handlingof task corruption and never restore again]> but was:<[[AdminClient clientId=adminclient-91] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available., test-reader Changelog partition unknown-0 could not be found, it could be already cleaned up during the handlingof task corruption and never restore again]>
h3. Stacktrace

java.lang.AssertionError: expected:<[test-reader Changelog partition unknown-0 could not be found, it could be already cleaned up during the handlingof task corruption and never restore again]> but was:<[[AdminClient clientId=adminclient-91] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available., test-reader Changelog partition unknown-0 could not be found, it could be already cleaned up during the handlingof task corruption and never restore again]>",,ableegoldman,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 14:49:57 UTC 2020,,,,,,,,,,"0|z0d9q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/20 04:24;mjsax;This might be related to https://issues.apache.org/jira/browse/KAFKA-9818 and we might want to do the same fix for both. Thank to [~ableegoldman] for pointing it out.;;;","15/Apr/20 01:57;githubbot;mjsax commented on pull request #8488: KAFKA-9819: Fix flaky test in StoreChangelogReaderTest
URL: https://github.com/apache/kafka/pull/8488
 
 
   Call for review @ableegoldman @vvcephei 
   
   The test failed with some log message from `AdminClient` however, we don't use any admin client in this test. Thus, we should not get the JVM shared root logger, but only the logger for the test class.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Apr/20 14:49;githubbot;mjsax commented on pull request #8488: KAFKA-9819: Fix flaky test in StoreChangelogReaderTest
URL: https://github.com/apache/kafka/pull/8488
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test RecordCollectorTest.shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContinueExceptionHandler,KAFKA-9818,13296135,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,ableegoldman,ableegoldman,03/Apr/20 22:55,17/Apr/20 21:40,13/Jul/23 09:17,17/Apr/20 21:40,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,flaky-test,,,,"h3. Error Message

java.lang.AssertionError
h3. Stacktrace

java.lang.AssertionError at org.junit.Assert.fail(Assert.java:87) at org.junit.Assert.assertTrue(Assert.java:42) at org.junit.Assert.assertTrue(Assert.java:53) at org.apache.kafka.streams.processor.internals.RecordCollectorTest.shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContinueExceptionHandler(RecordCollectorTest.java:521) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor20.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy5.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:834)
h3. Standard Output

[2020-04-03 21:02:04,472] WARN [AdminClient clientId=adminclient-91] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,479] WARN [AdminClient clientId=adminclient-27] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,481] WARN [AdminClient clientId=adminclient-18] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,483] WARN [AdminClient clientId=adminclient-73] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,484] WARN [AdminClient clientId=adminclient-80] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,485] ERROR test Error encountered sending record to topic topic for task 0_0 due to: java.lang.Exception Exception handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded. (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220) [2020-04-03 21:02:04,485] WARN [AdminClient clientId=adminclient-28] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,489] ERROR test Error encountered sending record to topic topic for task 0_0 due to: org.apache.kafka.common.errors.ProducerFencedException: KABOOM! Written offsets would not be recorded and no more records would be sent since the producer is fenced, indicating the task may be migrated out (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220) [2020-04-03 21:02:04,491] WARN [AdminClient clientId=adminclient-63] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,493] WARN [AdminClient clientId=adminclient-19] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,495] WARN [AdminClient clientId=adminclient-81] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,495] ERROR test Error encountered sending record to topic topic for task 0_0 due to: org.apache.kafka.common.errors.AuthenticationException: KABOOM! Written offsets would not be recorded and no more records would be sent since this is a fatal error. (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220) [2020-04-03 21:02:04,496] WARN [AdminClient clientId=adminclient-29] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,498] ERROR test Error encountered sending record to topic topic for task 0_0 due to: org.apache.kafka.common.KafkaException: KABOOM! Exception handler choose to FAIL the processing, no more records would be sent. (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220) [2020-04-03 21:02:04,502] WARN [AdminClient clientId=adminclient-64] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,511] WARN [AdminClient clientId=adminclient-76] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,511] WARN [AdminClient clientId=adminclient-31] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,518] WARN [AdminClient clientId=adminclient-84] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,518] WARN [AdminClient clientId=adminclient-32] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:02:04,520] WARN [AdminClient clientId=adminclient-44] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762) [2020-04-03 21:04:30,863] ERROR test Error encountered sending record to topic topic for task 0_0 due to: java.lang.Exception Exception handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded. (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220) [2020-04-03 21:04:30,887] ERROR test Error encountered sending record to topic topic for task 0_0 due to: java.lang.Exception Exception handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded. (org.apache.kafka.streams.processor.internals.RecordCollectorImpl:220)",,ableegoldman,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 21:40:07 UTC 2020,,,,,,,,,,"0|z0d9q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/20 02:03;githubbot;mjsax commented on pull request #8423: KAFKA-9818: improve error message to debug test
URL: https://github.com/apache/kafka/pull/8423
 
 
   The error message on the ticket is not helpful. To figure out why the test fails, we need to improve the error message first and wait until it fails again.
   
   \cc @ableegoldman @vvcephei 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Apr/20 04:22;githubbot;mjsax commented on pull request #8423: KAFKA-9818: improve error message to debug test
URL: https://github.com/apache/kafka/pull/8423
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Apr/20 04:24;mjsax;This might be related to https://issues.apache.org/jira/browse/KAFKA-9819 and we might want to do the same fix for both. Thank to [~ableegoldman] for pointing it out.;;;","13/Apr/20 23:06;ableegoldman;[~mjsax] thanks for the improved error logging, saw this fail again locally with

java.lang.AssertionError: Messages received: - [AdminClient clientId=adminclient-74] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. - [AdminClient clientId=adminclient-116] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available. - test Error encountered sending record to topic topic for task 0_0 due to: java.lang.Exception Exception handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded.

 

 ;;;","17/Apr/20 16:18;githubbot;mjsax commented on pull request #8507: KAFKA-9818: Fix flaky test in RecordCollectorTest
URL: https://github.com/apache/kafka/pull/8507
 
 
   Call for review @vvcephei. Follow up of #8488 (own ticket number)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","17/Apr/20 21:40;githubbot;mjsax commented on pull request #8507: KAFKA-9818: Fix flaky test in RecordCollectorTest
URL: https://github.com/apache/kafka/pull/8507
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer may never re-join if inconsistent metadata is received once,KAFKA-9815,13296048,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,03/Apr/20 15:18,07/Apr/20 00:08,13/Jul/23 09:17,07/Apr/20 00:08,,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.0,,,,,,,consumer,,,,,0,,,,,"KAFKA-9797 is the result of an incorrect rolling upgrade test where a new listener is added to brokers and set as the inter-broker listener within the same rolling upgrade. As a result, metadata is inconsistent across brokers until the rolling upgrade completes since interbroker communication is broken until all brokers have the new listener. The test fails due to consumer timeouts and sometimes this is because the upgrade takes longer than consumer timeout. But several logs show an issue with the consumer when one metadata response received during upgrade is different from the consumer's cached `assignmentSnapshot`, triggering rebalance.

In [https://github.com/apache/kafka/blob/7f640f13b4d486477035c3edb28466734f053beb/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L750,] we return true for `rejoinNeededOrPending()` if `assignmentSnapshot` is not the same as the current `metadataSnapshot`. We don't set `rejoinNeeded` in the instance, but we revoke partitions and send JoinGroup request. If the JoinGroup request fails and a subsequent metadata response contains the same snapshot value as the previously cached `assignmentSnapshot`, we never send `JoinGroup` again since snapshots match and `rejoinNeeded=false`. Partitions are not assigned to the consumer after this and the test fails because messages are not received.

Even though this particular system test isn't a valid upgrade scenario, we should fix the consumer, since temporary metadata differences between brokers can result in this scenario.",,githubbot,rsivaram,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 00:00:16 UTC 2020,,,,,,,,,,"0|z0d96o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 19:57;githubbot;rajinisivaram commented on pull request #8420: KAFKA-9815; Ensure consumer always re-joins if JoinGroup fails
URL: https://github.com/apache/kafka/pull/8420
 
 
   On metadata change for assigned topics, we trigger rebalance, revoke partitions and send JoinGroup. If metadata reverts to the original value and JoinGroup fails, we don't resend JoinGroup because we don't set `rejoinNeeded`. This PR sets `rejoinNeeded=true` when rebalance is triggered due to metadata change to ensure that we retry on failure.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Apr/20 00:00;githubbot;hachikuji commented on pull request #8420: KAFKA-9815; Ensure consumer always re-joins if JoinGroup fails
URL: https://github.com/apache/kafka/pull/8420
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests hang and timeout the entire PR build on jenkins ,KAFKA-9812,13295918,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,kkonstantine,kkonstantine,03/Apr/20 00:07,03/Apr/20 16:38,13/Jul/23 09:17,03/Apr/20 16:38,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,flaky,flaky-build,,,"Test {{org.apache.kafka.streams.integration.EosIntegrationTest > shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances[exactly_once_beta]}} and possibly others have been spotted to hang indefinitely without entering a {{FAILED}} state, which resulted in the whole jenkins to time out hours later without a list of failures. 

A suggested fix could possibly involve using a class wide {{Timeout}} rule with reasonable timeouts for integration tests, as described here: [https://github.com/junit-team/junit4/wiki/Timeout-for-tests]

Snippet from the build log [https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1558/console] :
{code:bash}
org.apache.kafka.streams.integration.EosIntegrationTest > shouldBeAbleToRunWithTwoSubtopologies[exactly_once_beta] STARTED 12:46:36 12:46:36 org.apache.kafka.streams.integration.EosIntegrationTest > shouldBeAbleToRunWithTwoSubtopologies[exactly_once_beta] PASSED 12:46:36 12:46:36 org.apache.kafka.streams.integration.EosIntegrationTest > shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances[exactly_once_beta] STARTED 15:12:14 Build timed out (after 270 minutes). Marking the build as aborted. 15:12:15 Build was aborted 15:12:15 [FINDBUGS] Skipping publisher since build result is ABORTED 15:12:15 Recording test results 15:12:15 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:15 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:15 15:12:15 org.apache.kafka.streams.integration.EosIntegrationTest > shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances[exactly_once_beta] SKIPPED 15:12:16 15:12:16 > Task :streams:integrationTest FAILED 15:12:17 The message received from the daemon indicates that the daemon has disappeared. 15:12:17 Build request sent: Build{id=4f5e3086-ab55-48c5-a3a4-6f213565ac61, currentDir=/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12} 15:12:17 Attempting to read last messages from the daemon log... 15:12:17 Daemon pid: 27019 15:12:17 log file: /home/jenkins/.gradle/daemon/5.6.2/daemon-27019.out.log 15:12:17 ----- Last 20 lines from daemon log file - daemon-27019.out.log ----- 15:12:17 at org.gradle.process.internal.DefaultExecHandle.execExceptionFor(DefaultExecHandle.java:237) 15:12:17 at org.gradle.process.internal.DefaultExecHandle.setEndStateInfo(DefaultExecHandle.java:214) 15:12:17 at org.gradle.process.internal.DefaultExecHandle.failed(DefaultExecHandle.java:364) 15:12:17 at org.gradle.process.internal.ExecHandleRunner.run(ExecHandleRunner.java:87) 15:12:17 at org.gradle.internal.operations.CurrentBuildOperationPreservingRunnable.run(CurrentBuildOperationPreservingRunnable.java:42) 15:12:17 at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) 15:12:17 at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) 15:12:17 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 15:12:17 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 15:12:17 at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) 15:12:17 at java.lang.Thread.run(Thread.java:748) 15:12:17 Caused by: java.lang.IllegalStateException: Shutdown in progress 15:12:17 at java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:82) 15:12:17 at java.lang.Runtime.removeShutdownHook(Runtime.java:239) 15:12:17 at org.gradle.process.internal.shutdown.ShutdownHooks.removeShutdownHook(ShutdownHooks.java:33) 15:12:17 at org.gradle.process.internal.DefaultExecHandle.setEndStateInfo(DefaultExecHandle.java:204) 15:12:17 at org.gradle.process.internal.DefaultExecHandle.aborted(DefaultExecHandle.java:360) 15:12:17 at org.gradle.process.internal.ExecHandleRunner.completed(ExecHandleRunner.java:108) 15:12:17 at org.gradle.process.internal.ExecHandleRunner.run(ExecHandleRunner.java:84) 15:12:17 ... 7 more 15:12:17 ----- End of the daemon log ----- 15:12:17 15:12:17 15:12:17 FAILURE: Build failed with an exception. 15:12:17 15:12:17 * What went wrong: 15:12:17 Gradle build daemon disappeared unexpectedly (it may have been killed or may have crashed) 15:12:17 15:12:17 * Try: 15:12:17 Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. 15:12:17 15:12:17 * Get more help at https://help.gradle.org 15:12:17 > Task :tools:integrationTest 15:12:17 Test steps failed 15:12:17 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:17 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:19 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:19 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:19 Adding one-line test results to commit status... 15:12:19 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:19 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:19 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:19 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:19 Setting status of 45ec78a919d6220521b55ca63582d365664b85c1 to FAILURE with url https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1558/ and message: 'FAILURE 15:12:19 13289 tests run, 67 skipped, 0 failed.' 15:12:19 Using context: JDK 8 and Scala 2.12 15:12:19 Setting MAVEN_LATEST__HOME=/home/jenkins/tools/maven/latest/ 15:12:19 Setting GRADLE_4_10_3_HOME=/home/jenkins/tools/gradle/4.10.3 15:12:19 Finished: ABORTED
{code}
 

 ",,githubbot,kkonstantine,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 16:29:39 UTC 2020,,,,,,,,,,"0|z0d8ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 02:54;githubbot;vvcephei commented on pull request #8411: KAFKA-9812: fix infinite loop in test code
URL: https://github.com/apache/kafka/pull/8411
 
 
   If the EosIntegrationTest fails an assertion after setting `gcInjected:=true` but before it gets to set `doGC=false`, then it would never set the flag, and the transformer would become an infinite loop.
   
   There are a couple of ways to tackle this, but I opted to just check inside the loop that we're not currently shutting down.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 16:29;githubbot;vvcephei commented on pull request #8411: KAFKA-9812: fix infinite loop in test code
URL: https://github.com/apache/kafka/pull/8411
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition updating high watermark allows reads above LSO,KAFKA-9807,13295833,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,02/Apr/20 16:24,03/Apr/20 23:13,13/Jul/23 09:17,03/Apr/20 23:13,0.11.0.3,1.0.2,1.1.1,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,,,,,,,,,,,,,,,2.4.2,2.5.0,,,,,,,,,,,,0,,,,,"We had a transaction system test fail with the following error:

{code}
AssertionError: Detected 37 dups in concurrently consumed messages
{code}

After investigation, we found the duplicates were a result of the consumer reading an aborted transaction, which should not be possible with the read_committed isolation level.

We tracked down the fetch request which returned the aborted data:

{code}
[2020-03-24 07:27:58,284] INFO Completed request:RequestHeader(apiKey=FETCH, apiVersion=11, clientId=console-consumer, correlationId=283) -- {replica_id=-1,max_wait_time=500,min_bytes=1,max_bytes=52428800,isolation_level=1,session_id=2043970605,session_epoch=87,topics=[{topic=output-topic,partitions=[{partition=1,current_leader_epoch=3,fetch_offset=48393,log_start_offset=-1,partition_max_bytes=1048576}]}],forgotten_topics_data=[],rack_id=},response:{throttle_time_ms=0,error_code=0,session_id=2043970605,responses=[{topic=output-topic,partition_responses=[{partition_header={partition=1,error_code=0,high_watermark=50646,last_stable_offset=50646,log_start_offset=0,aborted_transactions=[],preferred_read_replica=-1},record_set=FileRecords(size=31582, file=/mnt/kafka/kafka-data-logs-1/output-topic-1/00000000000000045694.log, start=37613, end=69195)}]}]} 
{code}

After correlating with the contents of the log segment 00000000000000045694.log, we found that this fetch response included data which was above the returned LSO which is 50646. In fact, the high watermark matched the LSO in this case, so the data was above the high watermark as well. 

At the same time this request was received, we noted that the high watermark was updated:

{code}
[2020-03-24 07:27:58,284] DEBUG [Partition output-topic-1 broker=3] High watermark updated from (offset=50646 segment=[45694:68690]) to (offset=50683 segment=[45694:69195]) (kafka.cluster.Partition)
{code}

The position of the new high watermark matched the end position from the fetch response, so that led us to believe there was a race condition with the updating of this value. In the code, we have the following (abridged) logic for fetching the LSO:

{code}
    firstUnstableOffsetMetadata match {
      case Some(offsetMetadata) if offsetMetadata.messageOffset < highWatermark => offsetMetadata
      case _ => fetchHighWatermarkMetadata
    }
{code}

If the first unstable offset is less than the high watermark, we should use that; otherwise we use the high watermark. The problem is that the high watermark referenced here could be updated between the range check and the call to `fetchHighWatermarkMetadata`. If that happens, we would end up reading data which is above the first unstable offset.

The solution to fix this problem is to cache the high watermark value so that it is used in both places. We may consider some additional improvements here as well, such as fixing the inconsistency problem in the fetch response which included data above the returned high watermark. We may also consider having the client react more defensively by ignoring fetched data above the high watermark. This would fix this problem for newer clients talking to older brokers which might hit this problem.",,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 23:13:52 UTC 2020,,,,,,,,,,"0|z0d7uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/20 17:23;githubbot;hachikuji commented on pull request #8418: KAFKA-9807; Protect LSO reads from concurrent high-watermark updates
URL: https://github.com/apache/kafka/pull/8418
 
 
   If the high-watermark is updated in the middle of a read with the `read_committed` isolation level, it is possible to return data above the LSO. In the worst case, this can lead to the read of an aborted transaction. The root cause is that the logic depends on reading the high-watermark twice. We fix the problem by reading it once and caching the value.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 20:56;githubbot;hachikuji commented on pull request #8418: KAFKA-9807; Protect LSO reads from concurrent high-watermark updates
URL: https://github.com/apache/kafka/pull/8418
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 23:13;hachikuji;Resolving this. I will likely backport to older branches when I get a chance. I will also open separate jiras for some of the additional improvements suggested above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky system test `TransactionsTest.test_transactions`,KAFKA-9802,13295646,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,01/Apr/20 22:28,28/May/20 03:59,13/Jul/23 09:17,28/May/20 03:59,,,,,,,,,,,,,,,,,,,,,,,2.5.1,,,,,,,,,,,,,0,,,,,"Found when investigating a test failure in `kafkatest.tests.core.transactions_test.TransactionsTest.test_transactions`. The failure was caused by an unexpected transaction timeout. Looking into the logs, I found that the timeout was due to a Produce request which took just over 10s to complete. The test case that failed involved a hard kill of the broker, so it is certainly possible for some produce requests to reach up to the request timeout. 

The problem in this case is that `request.timeout.ms` was set to 30s while the `transaction.timeout.ms` was set to only 10s. There is no benefit to allowing a larger request timeout since the coordinator is just going to abort the transaction. So we should fix this test case to set these timeouts consistently. It might also be worth logging a warning to the user when the request timeout is larger than the transaction timeout.



",,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-01 22:28:41.0,,,,,,,,,,"0|z0d6pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static member could get empty assignment unexpectedly,KAFKA-9801,13295633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,guozhang,guozhang,01/Apr/20 21:52,10/Dec/20 10:08,13/Jul/23 09:17,07/Apr/20 22:30,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,consumer,streams,,,,0,,,,,"Take the following example trace where static members are joining the group:

1. Static member with instance A joined the group with empty member, the coordinator generated member.id 1 for A and added it to the group. The group state is PreparingRebalance.

2. The group is formed and now we move on to CompletingRebalance.

3. Another member joins the group, causing it to transit back to PreparingRebalance, which would potentially send a REBALANCE_IN_PROGRESS to member A as well.

4. Member A gets the REBALANCE_IN_PROGRESS error, trying to re-join (again with an empty member.id)

5. The group is now advanced to CompletingRebalance again.

6. The group get the second join-group from the known instance A with an empty member.id, will generated a new member.id 2 and replace the member.id 1.

7. The group gets the assignment from leader which only includes member.id 1 and not member.id 2.

8. The assignment for member.id 1 is dropped on the broker side while the assignment for member.id 2 is set to an empty byte array.

9. The empty byte array is sent back to the instance A causing it the following error:

{code}
[2020-03-27T21:13:01-05:00] (streams-soak-2-5_soak_i-054b83e98b7ed6285_streamslog) org.apache.kafka.common.protocol.types.SchemaException: Error reading field 'version': java.nio.BufferUnderflowException
	at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:110)
{code}

This error has to be triggered when quite a few cases are aligned together, and hence it was not triggered very frequently.

Personally I think there's a correlation with this error to the observed https://issues.apache.org/jira/browse/KAFKA-9659 as well, which I'd keep investigating (will update in this ticket).",,githubbot,guozhang,zhangzs,zoushengfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10772,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 03:51:20 UTC 2020,,,,,,,,,,"0|z0d6mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/20 01:25;githubbot;guozhangwang commented on pull request #8405: KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase [WIP]
URL: https://github.com/apache/kafka/pull/8405
 
 
   1. Fix the direct cause of the observed issue on the client side: when heartbeat getting errors and resetting generation, we only need to set it to UNJOINED when it was not already in REBALANCING; otherwise, the join-group handler would throw the retriable UnjoinedGroupException to force the consumer to re-send join group unnecessarily.
   
   2. Fix the root cause of the issue on the broker side: we should still trigger rebalance when static member joins in CompletingRebalance phase; otherwise the member.ids would be changed when the assignment is received from the leader, hence causing the new member.id's assignment to be empty.
   
   3. Added log4j entries as a by-product of my investigation.
   
   Testing coverage still in progress.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Apr/20 18:59;githubbot;guozhangwang commented on pull request #8439: KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase
URL: https://github.com/apache/kafka/pull/8439
 
 
   This is a cherry-pick PR from #8405 to trunk (due to the large divergence we cannot do that vie git cherry-pick).
   
   * Fix the direct cause of the observed issue on the client side: when heartbeat getting errors and resetting generation, we only need to set it to UNJOINED when it was not already in REBALANCING; otherwise, the join-group handler would throw the retriable UnjoinedGroupException to force the consumer to re-send join group unnecessarily.
   
   * Fix the root cause of the issue on the broker side: we should still trigger rebalance when static member joins in CompletingRebalance phase; otherwise the member.ids would be changed when the assignment is received from the leader, hence causing the new member.id's assignment to be empty.
   
   * Added log4j entries as a by-product of my investigation.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Apr/20 22:29;githubbot;guozhangwang commented on pull request #8405: KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase
URL: https://github.com/apache/kafka/pull/8405
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","08/Apr/20 03:51;githubbot;guozhangwang commented on pull request #8439: KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase
URL: https://github.com/apache/kafka/pull/8439
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky system test TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener,KAFKA-9797,13295499,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,01/Apr/20 10:50,15/Apr/20 12:06,13/Jul/23 09:17,15/Apr/20 12:06,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,system tests,,,,,0,,,,,"TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener is supposed to test non-disruptive upgrade of inter-broker listener using rolling bounce. But the test updates inter-broker listener without enabling the new listener/security_protocol across all brokers, making this a disruptive upgrade where brokers are in inconsistent state until all brokers have been upgraded. The test must first enable the new listener across all brokers and then update the inter-broker listener to the new listener to ensure that the cluster is functioning during the upgrade.",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 12:04:16 UTC 2020,,,,,,,,,,"0|z0d5so:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"01/Apr/20 11:08;githubbot;rajinisivaram commented on pull request #8403: KAFKA-9797; Fix TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener
URL: https://github.com/apache/kafka/pull/8403
 
 
   In order to perform non-disruptive update of inter-broker listener to a different security protocol, we need to first ensure that the listener is enabled on all brokers. Once the listener is available on all brokers, inter-broker listener can be updated using rolling update.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Apr/20 12:04;githubbot;rajinisivaram commented on pull request #8403: KAFKA-9797; Fix TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener
URL: https://github.com/apache/kafka/pull/8403
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker shutdown could be stuck forever under certain conditions,KAFKA-9796,13295482,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,01/Apr/20 09:36,03/Nov/21 13:08,13/Jul/23 09:17,16/Apr/20 16:15,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"During the broker initialisation, the Acceptor threads are started early to know the bound port and delays starting the processors to the end of the initialisation sequence. We have found out that the shutdown of a broker could be stuck forever under the following conditions:
 - the shutdown procedure is started before the processors are started;
 - the `newConnections` queues of the processors are full; and
 - an extra new connection has been accepted but can't be queued up in a processor.

For instance, this could happen if a `NodeExistsException` is raised when the broker tries to register itself in ZK.

When the above conditions happens, the shutting down triggers the shutdown of the acceptor threads and waits until they are (first thread dump bellow). If an acceptor as a pending connection which can't be queued up in a processor, it ends up waiting until space is made is new queue to accept the new connection (second thread dump bellow). As the processors are not started, the new connection queues are not drained so it never releases the acceptor thread.

*Shutdown wait on acceptor to shutdown*
{noformat}
""main"" #1 prio=5 os_prio=0 cpu=3626.89ms elapsed=106360.56s tid=0x00007f625001c800 nid=0x272 waiting on condition  [0x00007f6257ca4000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.5/Native Method)
	- parking to wait for  <0x0000000689a61800> (a java.util.concurrent.CountDownLatch$Sync)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.5/LockSupport.java:194)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@11.0.5/AbstractQueuedSynchronizer.java:885)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(java.base@11.0.5/AbstractQueuedSynchronizer.java:1039)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(java.base@11.0.5/AbstractQueuedSynchronizer.java:1345)
	at java.util.concurrent.CountDownLatch.await(java.base@11.0.5/CountDownLatch.java:232)
	at kafka.network.AbstractServerThread.shutdown(SocketServer.scala:430)
	at kafka.network.Acceptor.shutdown(SocketServer.scala:521)
	at kafka.network.SocketServer.$anonfun$stopProcessingRequests$2(SocketServer.scala:267)
	at kafka.network.SocketServer.$anonfun$stopProcessingRequests$2$adapted(SocketServer.scala:267)
	at kafka.network.SocketServer$$Lambda$604/0x0000000840540840.apply(Unknown Source)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)
	at kafka.network.SocketServer.stopProcessingRequests(SocketServer.scala:267)
	- locked <0x0000000689a61ac0> (a kafka.network.SocketServer)
	at kafka.server.KafkaServer.$anonfun$shutdown$5(KafkaServer.scala:806)
	at kafka.server.KafkaServer$$Lambda$602/0x000000084052b040.apply$mcV$sp(Unknown Source)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:68)
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:806)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:522)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)
	at kafka.Kafka$.main(Kafka.scala:82)
	at kafka.Kafka.main(Kafka.scala)
{noformat}

*Acceptor waits on processor to accept the new connection*
{noformat}
""data-plane-kafka-socket-acceptor-ListenerName(EXTERNAL)-SASL_SSL-9092"" #54 prio=5 os_prio=0 cpu=16.23ms elapsed=106346.62s tid=0x00007f62523b5000 nid=0x2ca waiting on condition  [0x00007f6157130000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.5/Native Method)
	- parking to wait for  <0x0000000689a7cad8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.5/LockSupport.java:194)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@11.0.5/AbstractQueuedSynchronizer.java:2081)
	at java.util.concurrent.ArrayBlockingQueue.put(java.base@11.0.5/ArrayBlockingQueue.java:367)
	at kafka.network.Processor.accept(SocketServer.scala:1020)
	at kafka.network.Acceptor.assignNewConnection(SocketServer.scala:639)
	at kafka.network.Acceptor.$anonfun$run$1(SocketServer.scala:566)
	at kafka.network.Acceptor.run(SocketServer.scala:550)
	at java.lang.Thread.run(java.base@11.0.5/Thread.java:834)
{noformat}",,dajac,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-13428,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 16:14:08 UTC 2020,,,,,,,,,,"0|z0d5ow:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,,,,,"08/Apr/20 12:24;githubbot;dajac commented on pull request #8448: [WIP] KAFKA-9796; Broker shutdown could be stuck forever under certain conditions
URL: https://github.com/apache/kafka/pull/8448
 
 
   This patch reworks the SocketServer to always start the acceptor threads after the processor threads and to always stop the processor threads before the acceptor threads. It also ensure that the processor threads drain its newConnection queue to unblock acceptors that may be waiting. However, the acceptors still bind during the startup, only the processing is further delayed.
   
   The flow looks like this now:
   
   ```
   val socketServer = ...
   
   socketServer.startup(startProcessingRequests = false)
   // Acceptors are bound.
   
   socketServer.startProcessingRequests(authorizerFutures)
   // Acceptors and Processors process new connections and requests
   
   socketServer.stopProcessingRequests()
   // Acceptors and Processors are stopped
   
   socketServer.shutdown()
   // SocketServer is shutdown (metrics, etc.)
   ```
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Apr/20 16:14;githubbot;rajinisivaram commented on pull request #8448: KAFKA-9796; Broker shutdown could be stuck forever under certain conditions
URL: https://github.com/apache/kafka/pull/8448
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream HandleAssignment should guarantee task close,KAFKA-9793,13295416,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,01/Apr/20 04:44,15/Apr/20 15:14,13/Jul/23 09:17,15/Apr/20 15:14,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"When triggering the `handleAssignment` call, if task preCommit throws, the doom-to-fail task shall not be closed, thus causing a RocksDB metrics recorder re-addition, which is fatal:

 

 

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) [2020-03-31 23:50:42,668] INFO [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] stream-thread [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] Handle new assignment with:

        New active tasks: [1_0, 0_1, 2_0]

        New standby tasks: []

        Existing active tasks: [0_1, 1_0, 2_0, 3_1]

        Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)

 

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) [2020-03-31 23:50:42,671] INFO [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] stream-thread [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] task [3_1] Prepared clean close (org.apache.kafka.streams.processor.internals.StreamTask)

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) [2020-03-31 23:50:42,671] INFO [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] stream-thread [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] task [0_1] Prepared task for committing (org.apache.kafka.streams.processor.internals.StreamTask)

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) [2020-03-31 23:50:42,682] ERROR [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] stream-thread [stream-soak-test-714fba71-3f5c-4418-8613-22d7b085949c-StreamThread-3] task [1_0] Failed to flush state store logData10MinuteFinalCount-store:  (org.apache.kafka.streams.processor.internals.ProcessorStateManager)

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) org.apache.kafka.streams.errors.TaskMigratedException: Error encountered sending record to topic windowed-node-counts for task 1_0 due to:

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.

[2020-03-31T16:50:43-07:00] (streams-soak-trunk-eos_soak_i-022f109d75764a250_streamslog) Written offsets would not be recorded and no more records would be sent since the producer is fenced, indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated.

        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError(RecordCollectorImpl.java:202)

        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.lambda$send$0(RecordCollectorImpl.java:185)

        at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1352)

        at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:231)

        at org.apache.kafka.clients.producer.internals.ProducerBatch.abort(ProducerBatch.java:159)

        at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:768)

        at org.apache.kafka.clients.producer.internals.Sender.maybeAbortBatches(Sender.java:485)

        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:304)

        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240)

        at java.lang.Thread.run(Thread.java:748)

 

The correct solution is to wrap the whole code block by try-catch to avoid unexpected close failure.",,bchen225242,cadonna,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 05:50:31 UTC 2020,,,,,,,,,,"0|z0d5a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/20 05:25;githubbot;abbccdda commented on pull request #8402: KAFKA-9793: Expand the try-catch for task commit in HandleAssignment
URL: https://github.com/apache/kafka/pull/8402
 
 
   As title suggests, we would like to broaden this check so that we don't fail to close a doom-to-cleanup task.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Apr/20 17:15;githubbot;guozhangwang commented on pull request #8402: KAFKA-9793: Expand the try-catch for task commit in HandleAssignment
URL: https://github.com/apache/kafka/pull/8402
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Apr/20 17:15;githubbot;abbccdda commented on pull request #8402: KAFKA-9793: Expand the try-catch for task commit in HandleAssignment
URL: https://github.com/apache/kafka/pull/8402
 
 
   As title suggests, we would like to broaden this check so that we don't fail to close a doom-to-cleanup task.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Apr/20 05:50;githubbot;guozhangwang commented on pull request #8402: KAFKA-9793: Expand the try-catch for task commit in HandleAssignment
URL: https://github.com/apache/kafka/pull/8402
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sensor name collision for group and transaction coordinator load metrics,KAFKA-9788,13295111,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,bob-barrett,bob-barrett,bob-barrett,30/Mar/20 23:11,04/Jun/20 01:01,13/Jul/23 09:17,04/Jun/20 01:00,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"Both the group coordinator and the transaction coordinator create a Sensor object on startup to track the time it takes to load partitions, and both name the Sensor ""PartitionLoadTime"":

[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala#L98]

[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L92]

However, Sensor names are meant to be unique. This name collision means that there is actually only one underlying ""PartitionLoadTime"" Sensor per broker, which is marked for each partition loaded by either coordinator, resulting in the metrics for group and transaction partition loading to be identical, and based the combination of each data set. These should be renamed to allow distinguishing between the two coordinator types.",,bob-barrett,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-30 23:11:11.0,,,,,,,,,,"0|z0d3eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test QueryableStateIntegrationTest#concurrentAccesses,KAFKA-9783,13294765,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,mjsax,mjsax,28/Mar/20 17:42,31/Mar/20 23:42,13/Jul/23 09:17,31/Mar/20 23:42,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1464/consoleFull]
{quote}*02:17:54* org.apache.kafka.streams.integration.QueryableStateIntegrationTest > concurrentAccesses FAILED*02:17:54*     java.lang.AssertionError: Did not receive all 48 records from topic output-concurrent-2 within 120000 ms*02:17:54*     Expected: is a value equal to or greater than <48>*02:17:54*          but: <0> was less than <48>*02:17:54*         at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)*02:17:54*         at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:691)*02:17:54*         at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)*02:17:54*         at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)*02:17:54*         at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:687)*02:17:54*         at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1199)*02:17:54*         at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.concurrentAccesses(QueryableStateIntegrationTest.java:649){quote}",,ableegoldman,guozhang,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 23:42:56 UTC 2020,,,,,,,,,,"0|z0d1a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/20 17:46;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1462/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/]

and

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1462/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses_2/] with different error:
{quote}java.nio.file.DirectoryNotEmptyException: /tmp/state-queryable-state-137107627802392495271/queryable-state-13/1_0 at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:802) at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:772) at java.nio.file.Files.walkFileTree(Files.java:2688) at java.nio.file.Files.walkFileTree(Files.java:2742) at org.apache.kafka.common.utils.Utils.delete(Utils.java:772) at org.apache.kafka.common.utils.Utils.delete(Utils.java:758) at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState(IntegrationTestUtils.java:125) at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shutdown(QueryableStateIntegrationTest.java:228){quote};;;","28/Mar/20 17:53;mjsax;Also: [https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5482/console];;;","28/Mar/20 17:57;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1463/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/]

and

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5483/consoleFull];;;","28/Mar/20 18:00;mjsax;And again:

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1461/testReport/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/]

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5481/consoleFull];;;","30/Mar/20 15:59;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1470/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/]

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5489/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/];;;","30/Mar/20 19:46;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1477/];;;","30/Mar/20 19:47;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1476/];;;","31/Mar/20 01:44;ableegoldman;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5517/console] (Also failed on 1/2 of the previous build for this PR, but lost the link);;;","31/Mar/20 04:53;vvcephei;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5518/]

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1499/]

 

I'm not sure if it's ""flaky"". It looks like it might just be failing.;;;","31/Mar/20 15:33;mjsax;[~vvcephei] Does it fail locally for you?

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1485/testReport/junit/org.apache.kafka.streams.integration/QueryableStateIntegrationTest/concurrentAccesses/];;;","31/Mar/20 16:25;vvcephei;I've just run it 90 times in a row, and it never failed locally. But it's failed in every Jenkins build I ran yesterday,;;;","31/Mar/20 21:37;mjsax;If it passed locally isn't it not flaky be definition? Btw: I had some Jenkins PR builds with no failure. But I agree, the test fails on Jenkins with very high probability (larger than 50% for sure).;;;","31/Mar/20 23:14;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1506/consoleFull];;;","31/Mar/20 23:42;guozhang;I think https://github.com/apache/kafka/pull/8370 should have fixed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Purgatory locking bug can lead to hanging transaction,KAFKA-9777,13294537,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,hachikuji,hachikuji,hachikuji,27/Mar/20 18:50,03/Jun/20 18:00,13/Jul/23 09:17,03/Jun/20 18:00,1.1.1,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"Once a transaction reaches the `PrepareCommit` or `PrepareAbort` state, the transaction coordinator must send markers to all partitions included in the transaction. After all markers have been sent, then the transaction transitions to the corresponding completed state. Until this transition occurs, no additional progress can be made by the producer.

The transaction coordinator uses a purgatory to track completion of the markers that need to be sent. Once all markers have been written, then the `DelayedTxnMarker` task becomes completable. We depend on its completion in order to transition to the completed state.

Related to KAFKA-8334, there is a bug in the locking protocol which is used to check completion of the `DelayedTxnMarker` task. The purgatory attempts to provide a ""happens before"" contract for task completion with `checkAndComplete`. Basically if a task is completed before calling `checkAndComplete`, then it should be given an opportunity to complete as long as there is sufficient time remaining before expiration. 

The bug in the locking protocol is that it expects that the operation lock is exclusive to the operation. See here: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DelayedOperation.scala#L114. The logic assumes that if the lock cannot be acquired, then the other holder of the lock must be attempting completion of the same delayed operation. If that is not the case, then the ""happens before"" contract is broken and a task may not get completed until expiration even if it has been satisfied.

In the case of `DelayedTxnMarker`, the lock in use is the read side of a read-write lock which is used for partition loading: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala#L264. In fact, if the lock cannot be acquired, it means that it is being held in order to complete some loading operation, in which case it will definitely not attempt completion of the delayed operation. If this happens to occur on the last call to `checkAndComplete` after all markers have been written, then the transition to the completing state will never occur.",,githubbot,hachikuji,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8334,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 00:47:11 UTC 2020,,,,,,,,,,"0|z0czvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 22:07;githubbot;hachikuji commented on pull request #8377: KAFKA-9777; Use asynchronous write to log after txn marker completion
URL: https://github.com/apache/kafka/pull/8377
 
 
   This patch addresses a locking issue with `DelayTxnMarker` completion. Because of the reliance on the read lock in `TransactionStateManager`, we cannot guarantee that a call to `checkAndComplete` will offer an opportunity to complete the job. This patch removes the reliance on this lock. Instead when the operation is completed, we write the completion message to the log asynchronously.
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","30/Mar/20 17:44;githubbot;hachikuji commented on pull request #8377: KAFKA-9777; Use asynchronous write to log after txn marker completion
URL: https://github.com/apache/kafka/pull/8377
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","30/Mar/20 17:45;githubbot;hachikuji commented on pull request #8389: KAFKA-9777; Remove txn purgatory to fix race condition on txn completion
URL: https://github.com/apache/kafka/pull/8389
 
 
   This patch addresses a locking issue with DelayTxnMarker completion. Because of the reliance on the shared read lock in TransactionStateManager and the deadlock avoidance algorithm in `DelayedOperation`, we cannot guarantee that a call to checkAndComplete will offer an opportunity to complete the job. This patch removes the reliance on this lock in two ways:
   
   1. We replace the transaction marker purgatory with a map of transaction with pending markers. We were not using purgatory expiration anyway, so this avoids the locking issue and simplifies usage.
   2. We were also relying on the read lock when calling `ReplicaManager.appendRecords`. As far as I can tell, this was not necessary. The lock order is always 1) state read/write lock, 2) txn metadata locks. Since we only call `appendRecords` while holding the read lock, a deadlock does not seem possible. 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","31/Mar/20 00:47;githubbot;hachikuji commented on pull request #8389: KAFKA-9777; Remove txn purgatory to fix race condition on txn completion
URL: https://github.com/apache/kafka/pull/8389
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalFormatConversionException from kafka-consumer-perf-test.sh,KAFKA-9775,13294429,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,tombentley,tombentley,27/Mar/20 11:05,03/Apr/20 17:05,13/Jul/23 09:17,03/Apr/20 17:05,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,tools,,,,,0,,,,,"Exception in thread ""main"" java.util.IllegalFormatConversionException: f != java.lang.Integer
	at java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4426)
	at java.base/java.util.Formatter$FormatSpecifier.printFloat(Formatter.java:2951)
	at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2898)
	at java.base/java.util.Formatter.format(Formatter.java:2673)
	at java.base/java.util.Formatter.format(Formatter.java:2609)
	at java.base/java.lang.String.format(String.java:2897)
	at scala.collection.immutable.StringLike.format(StringLike.scala:354)
	at scala.collection.immutable.StringLike.format$(StringLike.scala:353)
	at scala.collection.immutable.StringOps.format(StringOps.scala:33)
	at kafka.utils.ToolsUtils$.$anonfun$printMetrics$3(ToolsUtils.scala:60)
	at kafka.utils.ToolsUtils$.$anonfun$printMetrics$3$adapted(ToolsUtils.scala:58)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.utils.ToolsUtils$.printMetrics(ToolsUtils.scala:58)
	at kafka.tools.ConsumerPerformance$.main(ConsumerPerformance.scala:82)
	at kafka.tools.ConsumerPerformance.main(ConsumerPerformance.scala)
",,githubbot,omkreddy,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 17:05:25 UTC 2020,,,,,,,,,,"0|z0cz7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/20 11:24;githubbot;tombentley commented on pull request #8373: KAFKA-9775: Fix IllegalFormatConversionException in ToolsUtils
URL: https://github.com/apache/kafka/pull/8373
 
 
   The runtime type of Metric.metricValue() needn't always be a Double,
   for example, if it's a gauge from IntGaugeSuite.
   Since it's impossible to format non-double values with 3 point precision
   IllegalFormatConversionException resulted.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 17:05;githubbot;omkreddy commented on pull request #8373: KAFKA-9775: Fix IllegalFormatConversionException in ToolsUtils
URL: https://github.com/apache/kafka/pull/8373
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 17:05;omkreddy;Issue resolved by pull request 8373
[https://github.com/apache/kafka/pull/8373];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inter-worker SSL is broken for keystores with multiple certificates,KAFKA-9771,13294310,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,26/Mar/20 21:42,27/Mar/20 17:37,13/Jul/23 09:17,27/Mar/20 17:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,,,,,0,,,,,"The recent bump in Jetty version causes inter-worker communication to fail in Connect when SSL is enabled and the keystore for the worker contains multiple certificates (which it might, in the case that SNI is enabled and the worker's REST interface is bound to multiple domain names). This is caused by [changes introduced in Jetty 9.4.23|https://github.com/eclipse/jetty.project/pull/4085], which are later [fixed in Jetty 9.4.25|https://github.com/eclipse/jetty.project/pull/4404].

We recently tried and failed to [upgrade to Jetty 9.4.25|https://github.com/apache/kafka/pull/8183], so upgrading the Jetty version to fix this issue isn't a viable option. Additionally, the [earliest clean version of Jetty|https://www.eclipse.org/jetty/security-reports.html] (at the time of writing) with regards to CVEs is 9.4.24, so reverting to a pre-9.4.23 version is also not a viable option.",,ChrisEgerton,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 17:37:02 UTC 2020,,,,,,,,,,"0|z0cyh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 22:00;githubbot;C0urante commented on pull request #8369: KAFKA-9771: Port patch for inter-worker Connect SSL from Jetty 9.4.25
URL: https://github.com/apache/kafka/pull/8369
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9771)
   
   For reasons outlined in the ticket, we can't upgrade to a version of Jetty with the bug fixed, or one prior to the introduction of the bug. Luckily, the actual fix is pretty straightforward and can be ported over to Connect for use until it's possible to upgrade to a version of Jetty with that bug fixed: https://github.com/eclipse/jetty.project/pull/4404/files#diff-58640db0f8f2cd84b7e653d1c1540913R2188-R2193
   
   The changes here have been verified locally; currently investigating how they can best be tested via unit/integration/system tests.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Mar/20 17:33;githubbot;kkonstantine commented on pull request #8369: KAFKA-9771: Port patch for inter-worker Connect SSL from Jetty 9.4.25
URL: https://github.com/apache/kafka/pull/8369
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Mar/20 17:37;kkonstantine;The fix was merged in `trunk` and the `2.5` release branch in time for the release of `2.5.0`;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caching State Store does not Close Underlying State Store When Exception is Thrown During Flushing,KAFKA-9770,13294309,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,cadonna,cadonna,26/Mar/20 21:35,09/Mar/21 09:13,13/Jul/23 09:17,06/Apr/20 19:45,2.0.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,streams,,,,,0,,,,,When a caching state store is closed it calls its {{flush()}} method. If {{flush()}} throws an exception the underlying state store is not closed.  ,,cadonna,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 19:53:24 UTC 2020,,,,,,,,,,"0|z0cygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 21:53;githubbot;cadonna commented on pull request #8368: KAFKA-9770: Close underlying state store also when flush throws
URL: https://github.com/apache/kafka/pull/8368
 
 
   When a caching state store is closed it calls its flush() method.
   If flush() throws an exception the underlying state store is not closed.
   
   This commit ensures that state stores underlying a caching state store
   are closed even when flush() throws.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Mar/20 15:45;githubbot;vvcephei commented on pull request #8368: KAFKA-9770: Close underlying state store also when flush throws
URL: https://github.com/apache/kafka/pull/8368
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Mar/20 15:46;githubbot;cadonna commented on pull request #8368: KAFKA-9770: Close underlying state store also when flush throws
URL: https://github.com/apache/kafka/pull/8368
 
 
   When a caching state store is closed it calls its flush() method.
   If flush() throws an exception the underlying state store is not closed.
   
   This commit ensures that state stores underlying a caching state store
   are closed even when flush() throws.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","28/Mar/20 01:36;githubbot;vvcephei commented on pull request #8368: KAFKA-9770: Close underlying state store also when flush throws
URL: https://github.com/apache/kafka/pull/8368
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","01/Apr/20 22:35;vvcephei;I can't cherry-pick to the 2.5 branch until the 2.5.0 release is complete, and the older ones don't have a clean cherry-pick, so I'm just going to leave it in for 2.6.0 right now.;;;","02/Apr/20 07:47;cadonna;I will keep this ticket open until you will have tried to cherry-pick the fix to 2.5 after the upcoming release.;;;","06/Apr/20 19:46;mjsax;The commit was cherry-picked to 2.5 and there will be a new RC. Updated the ""fixed version"" to 2.5 and resolved the ticket.;;;","06/Apr/20 19:53;vvcephei;Thanks, [~mjsax] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaManager Partition.makeFollower Increases LeaderEpoch when ZooKeeper disconnect occurs,KAFKA-9769,13294266,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,andrewchoi5,andrewchoi5,andrewchoi5,26/Mar/20 18:06,06/Jul/20 18:27,13/Jul/23 09:17,06/Jul/20 18:27,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,replication,,,,,0,kafka,replica,replication,,"The ZooKeeper Session once expired and got disconnected and the broker received the 1st LeaderAndIsr request simultaneously. As the broker was processing the 1st LeaderAndIsr Request, the ZooKeeper session has not been reestablished just yet.

Within the makeFollowers method, _partition.getOrCreateReplica_ is called before the fetcher begins. _partition.getOrCreateReplica_ needs to fetch information from ZooKeeper but an exception is thrown when calling the ZooKeeper client because the session is invalid, rendering the fetcher start to be skipped.

 

In Partition class's getOrCreateReplica method calls AdminZkClient's fetchEntityConfig(..) which throws an exception if the ZooKeeper session is invalid. 

 
{code:java}
val props = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic){code}
 

When this occurs, the leader epoch should not have been incremented due to ZooKeeper being invalid because once the second LeaderAndIsr request comes in, the leader epoch could be the same between the brokers. 

Few options I can think of for a fix. I think third route could be feasible:

1 - Make LeaderEpoch update and fetch update atomic.

2 - Wait until all individual partitions are successful without problems then process fetch.

3 - Catch the ZooKeeper exception in the caller code block (ReplicaManager.makeFollowers) and simply do not touch the remaining partitions to ensure that the batch of successful partitions up to that point are updated and processed (fetch).

4 - Or make LeaderAndIsr request never arrive at the broker in case of ZooKeeper disconnect, then that would be safe because it is already possible for some replicas to receive the LeaderAndIsr later than the others. However, in that case, the code need to make sure the controller will retry.

 
{code:java}
 else if (requestLeaderEpoch > currentLeaderEpoch) {
 // If the leader epoch is valid record the epoch of the controller that made the leadership decision. 
// This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path 
if (stateInfo.basePartitionState.replicas.contains(localBrokerId))       
partitionState.put(partition, stateInfo)
else 
 
def getOrCreateReplica(replicaId: Int, isNew: Boolean = false): Replica = {          allReplicasMap.getAndMaybePut(replicaId, { 
if (isReplicaLocal(replicaId)) {
val adminZkClient = new AdminZkClient(zkClient) val props = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)




{code}
 

 ",,andrewchoi5,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 18:27:50 UTC 2020,,,,,,,,,,"0|z0cy7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/20 02:38;githubbot;andrewchoi5 commented on pull request #8479: KAFKA-9769: Finish operations for leaderEpoch-updated partitions up to point ZK Exception
URL: https://github.com/apache/kafka/pull/8479
 
 
   KAFKA-9769: Finish operations for leaderEpoch-updated partitions up to point ZK Exception occurs.
   https://issues.apache.org/jira/browse/KAFKA-9769
   
   For example, in such case, we will have the following mechanism :
   1 - P1 and P2 succeeds. leaderEpoch for them are incremented because no ZkException occurs
   2 - while making follower for P3, ZkException occurs and the leaderEpoch is not updated and thus thepartitionsToMakeFollower += partition isn’t executed. We catch this ZkException in line 1498 and log it as an error. No Exception is thrown.
   3 - After catching the exception, makeFollower for P4 is then not executed.
   4 - so the partitionsToMakeFollower only contains P1, P2. And fetchers are added to these partitionsToMakeFollower
   
   Signed-off-by: Andrew Choi <li_andchoi@microsoft.com>
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Jul/20 18:27;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rest.advertised.listener configuration is not handled properly by the worker,KAFKA-9768,13294051,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,26/Mar/20 00:39,07/May/20 01:30,13/Jul/23 09:17,07/May/20 01:30,,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,KafkaConnect,,,,,0,,,,,"The {{rest.advertised.listener}} config can currently be set to either ""http"" or ""https"", and a listener with that protocol should be used when advertising the URL of the worker to other members of the Connect cluster.

For example, someone might configure their worker with a {{listeners}} value of {{[https://localhost:42069,http://localhost:4761|https://localhost:42069%2Chttp//localhost:4761]}} and a {{rest.advertised.listener}} value of {{http}}, which should cause the worker to listen on port {{42069}} with TLS and port {{4761}} with plaintext, and advertise the URL {{[http://localhost:4761|http://localhost:4761/]}} to other workers.

However, the worker instead advertises the URL {{[https://localhost:42069|https://localhost:42069/]}} to other workers. This is because the {{RestServer}} class, which is responsible for determining which URL to advertise to other workers, simply [chooses the first listener whose name begins with the protocol|https://github.com/apache/kafka/blob/0f48446690e42b78a9a6b8c6a9bbab9f01d84cb1/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java#L422] specified in the {{rest.advertised.listener}} config.

This breaks because ""http"" is a prefix of ""https"", so if the advertised listener is ""http"" but the first listener that's found starts with ""https://"", that listener will still be chosen.

This bug has been present since SSL support (and the {{rest.advertised.listener}} config) were added via [KIP-208|https://cwiki.apache.org/confluence/display/KAFKA/KIP-208%3A+Add+SSL+support+to+Kafka+Connect+REST+interface], in release 1.1.0.

This bug should only present in the case where a user has set {{rest.advertised.listener}} to {{http}} but the {{listeners}} list begins with a listener that uses {{https}}. A workaround can be performed by changing the order of the {{listeners}} list to put the desired advertised listener at the beginning.",,ChrisEgerton,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 18:43:40 UTC 2020,,,,,,,,,,"0|z0cwyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 01:18;githubbot;C0urante commented on pull request #8360: KAFKA-9768: Fix handling of rest.advertised.listener config
URL: https://github.com/apache/kafka/pull/8360
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9768)
   
   The `rest.advertised.listener` config is currently broken as setting it to `http` when listeners are configured for both `https` and `http` will cause the framework to choose whichever of the two listeners is listed first. The changes here attempt to fix this by checking not only that `Connector::getName` begins with the specified protocol, but also that that protocol is immediately followed by an underscore, which Jetty appears to use as a delimiter between the protocol and the remainder of the connector name.
   
   An existing unit test for the `RestServer::advertisedUrl` method has been expanded to include a case that fails with the framework in its current state and passes with the changes in this PR.
   
   It's unclear how the name for a Jetty `Connector` is calculated; it would be good to get assurances on that front before merging any changes relying on the name matching a given format. Opening this PR now to hopefully get some visibility into that and also to get review on the changes assuming they align with how Jetty works.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","01/Apr/20 18:36;rhauch;Is it true that one can order the listeners such that the listener that matches the advertised listener comes first? If so, let's document that as a workaround.;;;","01/Apr/20 18:43;ChrisEgerton;[~rhauch] this is already called out in the description:


{quote}A workaround can be performed by changing the order of the {{listeners}} list to put the desired advertised listener at the beginning.
{quote};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recent changes to Connect's InsertField will fail to inject field on key of tombstone record,KAFKA-9763,13293904,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rhauch,rhauch,rhauch,25/Mar/20 18:41,20/Oct/20 07:05,13/Jul/23 09:17,17/Jul/20 16:26,1.0.3,1.1.2,2.0.2,2.1.2,2.2.2,2.3.1,2.4.0,2.5.0,,,,,,,,,,,,,,,1.0.3,1.1.2,2.0.2,2.1.2,2.2.3,2.3.2,2.4.2,2.5.1,KafkaConnect,,,,,0,,,,,"This is a regression due to the changes for KAFKA-8523.

KAFKA-8523 was backported to multiple versions, and was released into 2.2.2, 2.3.1, and 2.4.0, and will soon be released in 2.5.0.

Unfortunately, that fix always makes the `InsertField` SMT skip all tombstone records, even when using the `InsertField$Key`.

Rather than:
{code:java}
    private boolean isTombstoneRecord(R record) {
        return record.value() == null;
    }
{code}
the correct behavior would be:
{code:java}
     private boolean isTombstoneRecord(R record) {
         return operatingValue(record) == null;
     }
{code}
The method no longer detects just tombstone methods, so the code should be refactored to return the record if the operatingValue for the record (which for `InsertField$Key` is the record key and for `InsertField$Value` is the record value) is null.

 ",,ableegoldman,githubbot,rhauch,SeaAndHill,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9707,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 07:05:46 UTC 2020,,,,,,,,,,"0|z0cwmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 19:12;githubbot;rhauch commented on pull request #8351: KAFKA-9763: Correct InsertField to not skip keys of tombstone records
URL: https://github.com/apache/kafka/pull/8351
 
 
   The fix for [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) attempted to skip (immediately returns) tombstone records when InsertField$Value is used. However, it also inadvertently also skips tombstone records when InsertField$Key is used, despite the possibility that the tombstone record’s key is non null.
   
   This commit corrects the behavior so that the InsertField$Value continues to skip tombstone records, but InsertField$Key only skips when the record’s *key* is null.
   
   Added two unit tests, and verified that these tests fail without the proposed fix and pass with the proposed fix. The other unit tests were added with [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) continue to pass.
   
   This should be backported all the way back to the 1.0 branch, since that's how far back [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) was backported.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 19:30;rhauch;This issue is a duplicate of KAFKA-9707.;;;","25/Mar/20 19:31;githubbot;rhauch commented on pull request #8351: KAFKA-9763: Correct InsertField to not skip keys of tombstone records
URL: https://github.com/apache/kafka/pull/8351
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 19:37;rhauch;Duplicate of KAFKA-9707, so closing this issue.;;;","17/Jul/20 16:26;vvcephei;Changed the resolution to ""Fixed"" so that I can run release.py for 2.5.1;;;","20/Oct/20 07:05;SeaAndHill;[~rhauch] where can i down load [1.0.3|https://issues.apache.org/jira/issues/?jql=project+%3D+KAFKA+AND+fixVersion+%3D+1.0.3] version ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer rebalance can be stuck after new member timeout with old JoinGroup version,KAFKA-9752,13293427,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,hachikuji,hachikuji,hachikuji,24/Mar/20 02:38,01/Oct/21 16:24,13/Jul/23 09:17,25/Mar/20 05:42,2.2.2,2.3.1,2.4.1,,,,,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.2,2.5.0,,,,,,,,,,0,,,,,"For older versions of the JoinGroup protocol (v3 and below), there was no way for new consumer group members to get their memberId until the first rebalance completed. If the JoinGroup request timed out and the client disconnected, the member would nevertheless be left in the group until the rebalance completed and the session timeout expired. 

In order to prevent this situation from causing the group size to grow unboundedly, we added logic in KAFKA-7610 to limit the maximum time a new member will be left in the group before it would be kicked out (in spite of rebalance state). 

In KAFKA-9232, we addressed one issue with this solution. Basically the new member expiration logic did not properly get cancelled after the rebalance completed which means that in certain cases, a successfully joined member might get kicked out of the group unnecessarily. 

Unfortunately, this patch introduced a regression in the normal session expiration logic following completion of the initial rebalance. Basically the expiration task fails to get scheduled properly. The issue is in this function:

{code}
  def shouldKeepAlive(deadlineMs: Long): Boolean = {
    if (isNew) {
      // New members are expired after the static join timeout
      latestHeartbeat + GroupCoordinator.NewMemberJoinTimeoutMs > deadlineMs
    } else if (isAwaitingJoin || isAwaitingSync) {
      // Don't remove members as long as they have a request in purgatory
      true
    } else {
      // Otherwise check for session expiration
      latestHeartbeat + sessionTimeoutMs > deadlineMs
    }
  }
{code}

We use this logic in order to check for session expiration. On the surface, there is nothing wrong with it, but it has an odd interaction with the purgatory. When the heartbeat is first scheduled with `tryCompleteElseWatch`, the code relies on `shouldKeepAlive` returning false so that the heartbeat task is not immediately completed. This only works because we update `latestHeartbeat` just prior to calling `tryCompleteElseWatch`, which means that the first or third checks will fail, `shouldKeepAlive` will return false, and the heartbeat expiration task will not be immediately completed. 

The bug in this case has to do with the case when `isNew` is true. When we schedule the session expiration task, the `isNew` flag is still set to true, which means we will hit the first check above. Since in most cases, the session timeout is less than the new member timeout of 5 minutes, the check is very likely to return true. This seems like what we would want, but as noted above, we rely on this function returning false when the expiration task is passed to `tryCompleteElseWatch`. Since it returns true instead, the task completes immediately, which means we cannot rely on its expiration.

The impact of this bug in the worst case is that a consumer group can be left in the `PreparingRebalance` state indefinitely. This state will persist until there is a coordinator change (e.g. as a result of restarting the broker). Note that this is only possible if 1) we have a consumer using an old JoinGroup version, 2) the consumer times out and disconnects from its initial JoinGroup request. 


",,dibbhatt,githubbot,gokul2411s,hachikuji,ijuma,jack_foy,javierholguera,rng,sinitw,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9935,KAFKA-10105,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 08:40:08 UTC 2021,,,,,,,,,,"0|z0cto8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/20 07:21;githubbot;hachikuji commented on pull request #8339: KAFKA-9752; New member timeout can leave group rebalance stuck
URL: https://github.com/apache/kafka/pull/8339
 
 
   Older versions of the JoinGroup rely on a new member timeout to keep the group from growing indefinitely in the case of client disconnects and retrying. The logic for resetting the heartbeat expiration task following completion of the rebalance failed to account for an implicit expectation that `shouldKeepAlive` would return false the first time it is invoked when a heartbeat expiration is scheduled. This patch fixes the issue by making heartbeat satisfaction logic explicit.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 05:16;githubbot;hachikuji commented on pull request #8339: KAFKA-9752; New member timeout can leave group rebalance stuck
URL: https://github.com/apache/kafka/pull/8339
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 20:08;githubbot;rajinisivaram commented on pull request #8354: KAFKA-9752; New member timeout can leave group rebalance stuck (#8339)
URL: https://github.com/apache/kafka/pull/8354
 
 
   Older versions of the JoinGroup rely on a new member timeout to keep the group from growing indefinitely in the case of client disconnects and retrying. The logic for resetting the heartbeat expiration task following completion of the rebalance failed to account for an implicit expectation that shouldKeepAlive would return false the first time it is invoked when a heartbeat expiration is scheduled. This patch fixes the issue by making heartbeat satisfaction logic explicit.
   
   Reviewers:  Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Mar/20 13:29;githubbot;rajinisivaram commented on pull request #8354: KAFKA-9752; New member timeout can leave group rebalance stuck (#8339)
URL: https://github.com/apache/kafka/pull/8354
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Jun/20 08:32;gokul2411s;Would be nice if we could remove 2.3.2 and 2.4.2 from the fixed versions section, since these Kafka versions don't seem to exist.;;;","24/Jun/20 13:29;ijuma;[~gokul2411s] That's to signal that they would be included in such releases if they are created (i.e. they are in the appropriate branch). Otherwise, how would we create the appropriate release notes?;;;","24/Jun/20 17:46;gokul2411s;[~ijuma] thanks for the explanation. makes sense.;;;","09/Aug/20 13:22;dibbhatt;Hi [~ijuma] [~hachikuji], I am seeing a different issue now with this fix . Earlier the ConsumerGroup was stuck in ""PendingRebalance"" state , which is not happening now , but now I see members not able to join the group . I see below logs where members are being removed after session timeout.

 

[2020-08-09 09:29:00,558] INFO [GroupCoordinator 5]: *Pending member* XXX in group YYY  *has been removed after session timeout expiration*. (kafka.coordinator.group.GroupCoordinator)

[2020-08-09 09:29:55,856] INFO [GroupCoordinator 5]: *Pending member* ZZZ in group YYY  *has been removed after session timeout expiration*. (kafka.coordinator.group.GroupCoordinator)

 

As I see the GroupCoridinator code,  when new member tries to join for first time,  GroupCoridinator also schedule a addPendingMemberExpiration (in doUnknownJoinGroup call ) with SessionTimeOut…

{code:}

addPendingMemberExpiration(group, newMemberId, sessionTimeoutMs)

{code:}

 

If for some reason , if addMemberAndRebalance call takes longer, and member still in “Pending” state, the above addPendingMemberExpiration can remove the pending member and they cannot join the group. I think that is what is happening. 

When for new member , Coordinator is already setting a timeout in 

{code:}

completeAndScheduleNextExpiration(group, member, NewMemberJoinTimeoutMs) 

{code:}

 

What the requirement for one more addPendingMemberExpiration task ? 

 ;;;","11/Sep/20 08:16;javierholguera;What version of the Kafka clients (consumer/producer/streams) use JoinGroup v4 that isn’t affected by this bug?

Thx;;;","01/Mar/21 08:40;sinitw;Do you think [~hachikuji] that the problem we described [in this thread|https://lists.apache.org/thread.html/rdf907dbb78092d46e039a9669a40edc7f36bd2eaacfd1a3025db4b6e%40%3Cusers.kafka.apache.org%3E] is caused by the bug you solved in this ticket? Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test kafka.server.ReplicaManagerTest.testFencedErrorCausedByBecomeLeader,KAFKA-9750,13293418,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,chia7712,bob-barrett,bob-barrett,24/Mar/20 01:16,03/Apr/20 19:25,13/Jul/23 09:17,03/Apr/20 19:25,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,core,,,,,0,flaky-test,,,,"When running tests locally, I've seen that 1-2% of the time, testFencedErrorCausedByBecomeLeader fails with
{code:java}
org.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending stateorg.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state
 at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530) at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529) at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389) at org.scalatest.Assertions.fail(Assertions.scala:1091) at org.scalatest.Assertions.fail$(Assertions.scala:1087) at org.scalatest.Assertions$.fail(Assertions.scala:1389) at kafka.server.ReplicaManagerTest.testFencedErrorCausedByBecomeLeader(ReplicaManagerTest.scala:248) at jdk.internal.reflect.GeneratedMethodAccessor25.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:40) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58) {code}",,bob-barrett,chia7712,githubbot,grussell,hachikuji,ijuma,jagsancio,jmeen,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 18:51:10 UTC 2020,,,,,,,,,,"0|z0ctm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/20 10:10;chia7712;{code:scala}
      // change the epoch from 0 to 1 in order to make fenced error
      replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())
      TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),
        s""the partition=$topicPartition should be removed from pending state"")
{code}

The root cause is race condition. The partition is add to the end instead of being removed if the epoch in ReplicaAlterLogDirsThread is increased. This PR includes following changes.
1. controls the lock of ReplicaAlterLogDirsThread to make the fenced error happen almost.
2. wait for the completion of thread;;;","25/Mar/20 10:11;githubbot;chia7712 commented on pull request #8344: KAFKA-9750 Flaky test kafka.server.ReplicaManagerTest.testFencedError…
URL: https://github.com/apache/kafka/pull/8344
 
 
   ```scala
         // change the epoch from 0 to 1 in order to make fenced error
         replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())
         TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),
           s""the partition=$topicPartition should be removed from pending state"")
   ```
   The root cause is race condition. The partition is add to the end instead of being removed if the epoch in ReplicaAlterLogDirsThread is increased. This PR includes following changes.
   1. controls the lock of ReplicaAlterLogDirsThread to make the fenced error happen almost.
   1. wait for the completion of thread
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 17:40;jagsancio;We are fairly consistently getting this failure.

Error Message
{code}
org.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state
{code}

Stacktrace
{code}
org.scalatest.exceptions.TestFailedException: the partition=test-topic-0 should be removed from pending state
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
	at org.scalatest.Assertions.fail(Assertions.scala:1091)
	at org.scalatest.Assertions.fail$(Assertions.scala:1087)
	at org.scalatest.Assertions$.fail(Assertions.scala:1389)
	at kafka.server.ReplicaManagerTest.testFencedErrorCausedByBecomeLeader(ReplicaManagerTest.scala:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.lang.Thread.run(Thread.java:748)
{code}

Standard Output
{code}
[2020-03-24 17:39:19,377] ERROR [ReplicaAlterLogDirsThread-0]: Error due to (kafka.server.ReplicaAlterLogDirsThread:76)
org.apache.kafka.common.errors.ReplicaNotAvailableException: Future log for partition test-topic-0 is not available on broker 0
[2020-03-24 17:39:38,537] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producerId 234 at offset 3 in partition test-topic-0: 13 (incoming seq. number), 2 (current end sequence number)
{code};;;","26/Mar/20 16:51;hachikuji;Elevating this to blocker until we understand the cause. This especially is troubling:
{code}
[2020-03-24 17:39:19,377] ERROR [ReplicaAlterLogDirsThread-0]: Error due to (kafka.server.ReplicaAlterLogDirsThread:76)
org.apache.kafka.common.errors.ReplicaNotAvailableException: Future log for partition test-topic-0 is not available on broker 0
{code};;;","31/Mar/20 19:09;mumrah;[~hachikuji] are we still considering this a blocker for 2.5? ;;;","01/Apr/20 17:08;grussell;Sorry to comment here; couldn't find anywhere else to ask. 

Is there an ETA for 2.5? The Wiki is still showing February; thanks!;;;","01/Apr/20 17:17;mumrah;[~grussell] we're currently working on a few different blockers for 2.5. Once those are resolved, we'll produce RC3 and announce on the mailing list. Thanks!;;;","01/Apr/20 17:24;grussell;[~mumrah] Thanks; any chance the 2.5.0 GA (not RC) will be released before April 22?

I am trying to determine if I will be able to get my code (spring-kafka, depends on kafka-clients) based on 2.5 into the next Spring Boot RC (April 22). If not, we'll have to wait for the next Boot release to get it dependent on 2.5 (they won't accept a version change after their RC).;;;","01/Apr/20 19:53;ijuma;[~grussell] That's the goal, but we can't guarantee it. :);;;","03/Apr/20 03:33;githubbot;hachikuji commented on pull request #8412: KAFKA-9750; Fix race condition with log dir reassign completion
URL: https://github.com/apache/kafka/pull/8412
 
 
   There is a race on receiving a LeaderAndIsr request for a replica with an active log dir reassignment. If the reassignment completes just before the LeaderAndIsr handler updates epoch information, it can lead to an illegal state error since no future log dir exists. This patch fixes the problem by ensuring that the future log dir exists when the fetcher is started. Removal cannot happen concurrently because it requires access the same partition state lock.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 16:02;githubbot;chia7712 commented on pull request #8344: KAFKA-9750 Flaky test kafka.server.ReplicaManagerTest.testFencedError…
URL: https://github.com/apache/kafka/pull/8344
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 18:51;githubbot;hachikuji commented on pull request #8412: KAFKA-9750; Fix race condition with log dir reassign completion
URL: https://github.com/apache/kafka/pull/8412
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TransactionMarkerRequestCompletionHandler should treat storage exceptions as retriable,KAFKA-9749,13293396,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bob-barrett,bob-barrett,bob-barrett,23/Mar/20 21:48,26/Jan/21 20:27,13/Jul/23 09:17,24/Mar/20 21:56,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.0,,,,,,core,,,,,0,,,,,"If `TransactionMarkerRequestCompletionHandler` handles a `KafkaStorageException`, it throws an IllegalStateException rather than retrying. This leaves the corresponding transactional ID in state PendingAbort, where it gets stuck, because any further EndTxn or InitProducerId call will fail with a CONCURRENT_TRANSACTIONS error. We should retry these errors when writing transaction markers.",,bob-barrett,githubbot,guozhang,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8803,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 04:55:44 UTC 2020,,,,,,,,,,"0|z0cthc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/20 21:27;githubbot;hachikuji commented on pull request #8336: KAFKA-9749: TransactionMarkerRequestCompletionHandler should treat KAFKA_STORAGE_…
URL: https://github.com/apache/kafka/pull/8336
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 04:26;guozhang;[~bob-barrett] [~hachikuji] Just for my own understanding on the rationale of retries: if a partition goes offline because its log dir fails, today does Kafka brokers automatically recovers from it? I think it must be since otherwise there's no point retrying, but I cannot find the code path doing so.;;;","25/Mar/20 04:36;ijuma;The broker experiencing the error will relinquish leadership by making the log dir offline (and potentially shutting down if no live log dirs remain). The retry is to attempt to write to the new leader.;;;","25/Mar/20 04:55;guozhang;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No tasks created for a connector,KAFKA-9747,13293325,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,akatona,vko,vko,23/Mar/20 15:34,04/Oct/21 16:17,13/Jul/23 09:17,02/Sep/21 08:10,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.8.1,3.0.1,3.1.0,,,,,,KafkaConnect,,,,,12,,,,,"We are running Kafka Connect in a distributed mode on 3 nodes using Debezium (MongoDB) and Confluent S3 connectors. When adding a new connector via the REST API the connector is created in RUNNING state, but no tasks are created for the connector.

Pausing and resuming the connector does not help. When we stop all workers and then start them again, the tasks are created and everything runs as it should.

The issue does not show up if we run only a single node.

The issue is not caused by the connector plugins, because we see the same behaviour for both Debezium and S3 connectors. Also in debug logs I can see that Debezium is correctly returning a task configuration from the Connector.taskConfigs() method.

Connector configuration examples

Debezium:
{code}
{
  ""name"": ""qa-mongodb-comp-converter-task|1"",
  ""config"": {
    ""connector.class"": ""io.debezium.connector.mongodb.MongoDbConnector"",
    ""mongodb.hosts"": ""mongodb-qa-001:27017,mongodb-qa-002:27017,mongodb-qa-003:27017"",
    ""mongodb.name"": ""qa-debezium-comp"",
    ""mongodb.ssl.enabled"": true,
    ""collection.whitelist"": ""converter[.]task"",
    ""tombstones.on.delete"": true
  }
}
{code}
S3 Connector:
{code}
{
  ""name"": ""qa-s3-sink-task|1"",
  ""config"": {
    ""connector.class"": ""io.confluent.connect.s3.S3SinkConnector"",
    ""topics"": ""qa-debezium-comp.converter.task"",
    ""topics.dir"": ""data/env/qa"",
    ""s3.region"": ""eu-west-1"",
    ""s3.bucket.name"": ""<bucket-name>"",
    ""flush.size"": ""15000"",
    ""rotate.interval.ms"": ""3600000"",
    ""storage.class"": ""io.confluent.connect.s3.storage.S3Storage"",
    ""format.class"": ""custom.kafka.connect.s3.format.plaintext.PlaintextFormat"",
    ""schema.generator.class"": ""io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator"",
    ""partitioner.class"": ""io.confluent.connect.storage.partitioner.DefaultPartitioner"",
    ""schema.compatibility"": ""NONE"",
    ""key.converter"": ""org.apache.kafka.connect.json.JsonConverter"",
    ""value.converter"": ""org.apache.kafka.connect.json.JsonConverter"",
    ""key.converter.schemas.enable"": false,
    ""value.converter.schemas.enable"": false,
    ""transforms"": ""ExtractDocument"",
    ""transforms.ExtractDocument.type"":""custom.kafka.connect.transforms.ExtractDocument$Value""
  }
}
{code}
The connectors are created using curl: {{curl -X POST -H ""Content-Type: application/json"" --data @<json_file> http:/<connect_host>:10083/connectors}}

","OS: Ubuntu 18.04 LTS
Platform: Confluent Platform 5.4
HW: The same behaviour on various AWS instances - from t3.small to c5.xlarge",adw12382,akatona,dalibor.frivaldsky,ddufour1a,duonglt,durban,garrett528,grimsby,hlemon,iskuskov,pawel.wilczynski@fieldaware.com,psz,rhauch,rodrigoabdo,Serganov,vko,yarosman,,,,,,,,,,,,,,,,,,KAFKA-9805,KAFKA-13253,,,,,,,,,,,,KAFKA-9805,,,,,,"23/Mar/20 20:40;vko;connect-distributed.properties;https://issues.apache.org/jira/secure/attachment/12997485/connect-distributed.properties","23/Mar/20 20:56;vko;connect.log;https://issues.apache.org/jira/secure/attachment/12997486/connect.log",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 05 06:39:43 UTC 2021,,,,,,,,,,"0|z0ct1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/20 16:01;psz;Yes, Kafka Connect team, please at least give us a WARN or ERROR as to why the task can't be created or sustained.;;;","20/May/20 08:24;duonglt;I also stuck with the sample problem, task isn't created. Although I get log:
'
{code:java}
[2020-05-20 07:20:33,131] INFO [Worker clientId=connect-1, groupId=xxxxxxxxxxx] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
{code}
 ;;;","21/May/20 17:32;garrett528;I'm seeing similar behavior, getting random tasks that are coming back as blank when checking status through the REST API. Then a second later, these tasks are showing as running on some machine after re-requesting the status. We have 10 Connect pods running in distributed mode with a load balancer pointing to the overarching k8s service for the Connect pods.;;;","27/May/20 19:56;adw12382;+1 encountered similar issue.

running a customized sink conncector in distributed mode on 2 hosts but only 1 host has both connector & task whereas the other one just has connector running.;;;","09/Jun/20 15:05;rodrigoabdo;I have the same issue here... MongoDB (Atlas), MongoDB Connector and AWS MSK.;;;","21/Aug/20 13:01;yarosman;Have the same problem for Debezium (Mysql connector)
  ;;;","28/Sep/20 12:50;Serganov;Same issue for Debezium (SqlServer connector);;;","07/Oct/20 14:01;pawel.wilczynski@fieldaware.com;[https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/] - after making sure that workers can communiate with each other the issue is gone in my case.;;;","20/Oct/20 16:49;rhauch;If anyone has this problem and [~pawel.wilczynski@fieldaware.com]'s suggestion to ensure that workers can communicate with each other does not work, please provide more details of your environment, including:
* the Connect version
* the Connect worker configuration
* a description of how many workers you're using
* debug-level logs showing the startup through the time where the tasks are not being started

Thanks!;;;","04/Aug/21 12:10;akatona;The connect name contains a character which is considered as illegal char via HttpClient::newRequest
{noformat}
java.lang.IllegalArgumentException: Illegal character in path at index ......
	at java.net.URI.create(URI.java:852)
	at org.eclipse.jetty.client.HttpClient.newRequest(HttpClient.java:453)
...
Caused by: java.net.URISyntaxException: Illegal character in path at index .......
	at java.net.URI$Parser.fail(URI.java:2848)
	at java.net.URI$Parser.checkChars(URI.java:3021)
{noformat};;;","05/Aug/21 06:39;durban;To add more details to the issue:
 * When running multiple workers
 * AND the Connector name contains a non-URL compatible character
 * AND a follower worker has the Connector in its assignment
 * Then the follower->leader request sent over Connect REST fails in RestClient (without any error logging, or the corresponding future ever completed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTask could fail to close during HandleNewAssignment,KAFKA-9743,13293142,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,22/Mar/20 06:44,24/Mar/20 22:32,13/Jul/23 09:17,24/Mar/20 16:55,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"We found this particular bug from happening in soak:

[2020-03-20T16:12:02-07:00] (streams-soak-trunk-eos_soak_i-026133a325ea91147_streamslog) [2020-03-20 23:12:01,534] ERROR [stream-soak-test-7ece4c7d-f528-4c92-93e2-9b32f1f722b1-StreamThread-2] stream-thread [stream-soak-test-7ece4c7d-f528-4c92-93e2-9b32f1f722b1-StreamThread-2] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-20T16:12:02-07:00] (streams-soak-trunk-eos_soak_i-026133a325ea91147_streamslog) java.lang.IllegalStateException: RocksDB metrics recorder for store ""KSTREAM-AGGREGATE-STATE-STORE-0000000040"" of task 2_2 has already been added. This is a bug in Kafka Streams.

        at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecordingTrigger.addMetricsRecorder(RocksDBMetricsRecordingTrigger.java:30)

        at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder.addStatistics(RocksDBMetricsRecorder.java:98)

        at org.apache.kafka.streams.state.internals.RocksDBStore.maybeSetUpMetricsRecorder(RocksDBStore.java:207)

        at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:193)

        at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:231)

        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)

        at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init(ChangeLoggingKeyValueBytesStore.java:44)

        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)

        at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init(CachingKeyValueStore.java:58)

        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)

        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0(MeteredKeyValueStore.java:101)

        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:801)

        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init(MeteredKeyValueStore.java:101)

        at org.apache.kafka.streams.processor.internals.StateManagerUtil.registerStateStores(StateManagerUtil.java:81)

        at org.apache.kafka.streams.processor.internals.StreamTask.initializeIfNeeded(StreamTask.java:191)

        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:329)

        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:587)

        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:501)

        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:475)

 

Which could bring the entire instance down. The bug was that if we fail to do the commit during task close section, the actual `closeClean` call could not be triggered.",,ableegoldman,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 16:26:19 UTC 2020,,,,,,,,,,"0|z0crx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/20 07:02;githubbot;abbccdda commented on pull request #8327: KAFKA-9743: Catch commit offset exception to eventually close dirty tasks
URL: https://github.com/apache/kafka/pull/8327
 
 
   This PR tries to close all the dirty tasks during `HandleAssignment` in case the commit call failed. The previous outcome was that all the lost tasks are not properly closed which leads to the RocksDB metric stats not cleared, and eventually blows the application away, since we have an illegal state check for re-adding an existing metrics:
   ```
    public void addMetricsRecorder(final RocksDBMetricsRecorder metricsRecorder) {
           final String metricsRecorderName = metricsRecorderName(metricsRecorder);
           if (metricsRecordersToTrigger.containsKey(metricsRecorderName)) {
               throw new IllegalStateException(""RocksDB metrics recorder for store \"""" + metricsRecorder.storeName() +
                   ""\"" of task "" + metricsRecorder.taskId().toString() + "" has already been added. ""
                   + ""This is a bug in Kafka Streams."");
           }
           metricsRecordersToTrigger.put(metricsRecorderName, metricsRecorder);
       }
   ```
   Unit test will be added shortly.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Mar/20 16:26;githubbot;mjsax commented on pull request #8327: KAFKA-9743: Catch commit offset exception to eventually close dirty tasks
URL: https://github.com/apache/kafka/pull/8327
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandbyTaskEOSIntegrationTest broken,KAFKA-9742,13293117,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,21/Mar/20 18:44,23/Mar/20 23:00,13/Jul/23 09:17,23/Mar/20 23:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"Test failed on a PR build last night:

org.apache.kafka.streams.integration.StandbyTaskEOSIntegrationTest.surviveWithOneTaskAsStandbyFailing
{noformat}
java.lang.AssertionErrorStacktracejava.lang.AssertionError at org.junit.Assert.fail(Assert.java:87)
 at org.junit.Assert.assertTrue(Assert.java:42)
 at org.junit.Assert.assertTrue(Assert.java:53)
 at org.apache.kafka.streams.integration.StandbyTaskEOSIntegrationTest.surviveWithOneTaskAsStandby(StandbyTaskEOSIntegrationTest.java:98)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
 at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
 at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
 at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
 at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
 at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
 at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
 at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
 at java.lang.Thread.run(Thread.java:748) {noformat}
Standard Output
{noformat}
[2020-03-21 00:05:31,344] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$:31)
[2020-03-21 00:05:31,396] INFO Server environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,396] INFO Server environment:host.name=asf937.gq1.ygridcore.net (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,396] INFO Server environment:java.version=1.8.0_241 (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,396] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,396] INFO Server environment:java.home=/usr/local/asfpackages/java/jdk1.8.0_241/jre (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,396] INFO Server environment:java.class.path=/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/classes/java/main:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/resources/main:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/test-utils/build/libs/kafka-streams-test-utils-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/classes/scala/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/libs/kafka-streams-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/libs/kafka_2.12-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/connect/json/build/libs/connect-json-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/connect/api/build/libs/connect-api-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/libs/kafka-clients-2.6.0-SNAPSHOT.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.30/c21f55139d8141d2231214fb1feaf50a1edca95e/slf4j-log4j12-1.7.30.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.12/3.9.2/b1f19bc6774e01debf09bf5f564ad3613687bf49/scala-logging_2.12-3.9.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.5.7/12bdf55ba8be7fc891996319d37f35eaad7e63ea/zookeeper-3.5.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.30/b5a4b6d16ab13e34a88fae84c35cd5d68cac922c/slf4j-api-1.7.30.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/5.18.4/def7af83920ad2c39eb452f6ef9603777d899ea0/rocksdbjni-5.18.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-module-junit4/2.0.5/c922fc29c82664e06466a7ce1face1661d688255/powermock-module-junit4-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-module-junit4-common/2.0.5/d02a42a4cc6d9229a11b1bc5c37a3f5f2c342d0a/powermock-module-junit4-common-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/junit/junit/4.13/e49ccba652b735c93bd6e6f59760d8254cf597dd/junit-4.13.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-api-easymock/2.0.5/a4bca999c461a2787026ce161846affba451fee9/powermock-api-easymock-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.easymock/easymock/4.1/e19506d19d84e8db90d864696282d6981c002e74/easymock-4.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcpkix-jdk15on/1.64/3dac163e20110817d850d17e0444852a6d7d0bd7/bcpkix-jdk15on-1.64.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.4.4-7/f7e9d149c0182968cc2a8706d3ffe82f5c9f01eb/zstd-jni-1.4.4-7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.7.1/c4d931ef8ad2c9c35d65b231a33e61428472d0da/lz4-java-1.7.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.7.3/241bb74a1eb37d68a4e324a4bc3865427de0a62d/snappy-java-1.1.7.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.10.2/dca8c8ab85eaabefe021e2f1ac777f3a6b16a3cb/jackson-datatype-jdk8-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.12/2.10.2/435902f7ac8f01468265c44bd4100b92c6f29663/jackson-module-scala_2.12-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.10.2/b80d499bd4853c784ffd9112aee2ecf5817c28be/jackson-dataformat-csv-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-paranamer/2.10.2/cfd83c1efb7ebfd83aafa5d22fc760a9d94c2a67/jackson-module-paranamer-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.10.2/528de95f198afafbcfb0c09d2e43b6e0ea663ec/jackson-databind-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.12/2.1.3/17ec3eeaba48b3f3e402ecfe22287761fb5c29b7/scala-collection-compat_2.12-2.1.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.12/0.9.0/9525fb6bbf54a9caf0f7e1b65b261215b02fe939/scala-java8-compat_2.12-0.9.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.12.11/7695010d1f4309a9c4b65be33528e382869ab3c4/scala-reflect-2.12.11.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.12.11/1a0634714a956c1aae9abefc83acaf6d4eabfa7d/scala-library-2.12.11.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/1.3/42a25dc3219429f0e5d060061f71acb49bf010a0/hamcrest-core-1.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-api-support/2.0.5/f7e9d65624f55c9c15ebd89a3a8770d1bb21e49c/powermock-api-support-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-core/2.0.5/d5d5ca75413883e00595185d79714e0911c7358e/powermock-core-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-reflect/2.0.5/6bca328201936519e08bb1d8fdf37c0a3d7075d0/powermock-reflect-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.1/48f12deaae83a8dfc3775d830c9fd60ea59bbbca/objenesis-3.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/cglib/cglib-nodep/3.2.9/27ca91ebc2b82f844e62a7ba8c2c1fdf9b84fa80/cglib-nodep-3.2.9.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.64/1467dac1b787b5ad2a18201c0c281df69882259e/bcprov-jdk15on-1.64.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.10.2/3a13b6105946541b8d4181a0506355b5fae63260/jackson-annotations-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.10.2/73d4322a6bda684f676a2b5fe918361c4e5c7cca/jackson-core-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.5.7/1270f80b08904499a6839a2ee1800da687ad96b4/zookeeper-jute-3.5.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.45.Final/51071ba9977cce64e3a58e6f2f6326bbb7e5bc7f/netty-handler-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.45.Final/cf153257db449b6a74adb64fbd2903542af55892/netty-transport-native-epoll-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.45.Final/8c768728a3e82c3cef62a7a2c8f52ae8d777bac9/netty-codec-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.45.Final/49f9fa4b7fe7d3e562666d050049541b86822549/netty-transport-native-unix-common-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.45.Final/b7d8f2645e330bd66cd4f28f155eba605e0c8758/netty-transport-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.45.Final/bac54338074540c4f3241a3d92358fad5df89ba/netty-buffer-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.45.Final/9e77bdc045d33a570dabf9d53192ea954bb195d7/netty-resolver-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.45.Final/5cf5e448d44ddf53d00f2fc4047c2a7aceaa7087/netty-common-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.9.10/211a2b4d3df1eeef2a6cacf78d74a1f725e7a840/byte-buddy-1.9.10.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.9.10/9674aba5ee793e54b864952b001166848da0f26b/byte-buddy-agent-1.9.10.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.javassist/javassist/3.25.0-GA/442dc1f9fd520130bd18da938622f4f9b2e5fba3/javassist-3.25.0-GA.jar (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,397] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,397] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,397] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:os.version=4.15.0-76-generic (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:user.name=jenkins (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:user.home=/home/jenkins (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:user.dir=/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:os.memory.free=209MB (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,398] INFO Server environment:os.memory.max=1820MB (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,399] INFO Server environment:os.memory.total=292MB (org.apache.zookeeper.server.ZooKeeperServer:109)
[2020-03-21 00:05:31,404] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog:115)
[2020-03-21 00:05:31,442] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase:117)
[2020-03-21 00:05:31,444] INFO minSessionTimeout set to 1600 (org.apache.zookeeper.server.ZooKeeperServer:938)
[2020-03-21 00:05:31,445] INFO maxSessionTimeout set to 16000 (org.apache.zookeeper.server.ZooKeeperServer:947)
[2020-03-21 00:05:31,445] INFO Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-1304592028848891536/version-2 snapdir /tmp/kafka-5436111649949570329/version-2 (org.apache.zookeeper.server.ZooKeeperServer:166)
[2020-03-21 00:05:31,465] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 48 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory:673)
[2020-03-21 00:05:31,472] INFO binding to port /127.0.0.1:0 (org.apache.zookeeper.server.NIOServerCnxnFactory:686)
[2020-03-21 00:05:31,485] INFO Snapshotting: 0x0 to /tmp/kafka-5436111649949570329/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:404)
[2020-03-21 00:05:31,491] INFO Snapshotting: 0x0 to /tmp/kafka-5436111649949570329/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog:404)
[2020-03-21 00:05:32,120] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.5-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8727345289613412077/junit1648642492482663308
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.5-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42351
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2020-03-21 00:05:32,159] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util:79)
[2020-03-21 00:05:32,269] INFO starting (kafka.server.KafkaServer:66)
[2020-03-21 00:05:32,270] INFO Connecting to zookeeper on 127.0.0.1:42351 (kafka.server.KafkaServer:66)
[2020-03-21 00:05:32,307] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:42351. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:05:32,318] INFO Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,318] INFO Client environment:host.name=asf937.gq1.ygridcore.net (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,318] INFO Client environment:java.version=1.8.0_241 (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,318] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,318] INFO Client environment:java.home=/usr/local/asfpackages/java/jdk1.8.0_241/jre (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,318] INFO Client environment:java.class.path=/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/classes/java/main:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/resources/main:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/test-utils/build/libs/kafka-streams-test-utils-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/classes/scala/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/resources/test:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams/build/libs/kafka-streams-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/core/build/libs/kafka_2.12-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/connect/json/build/libs/connect-json-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/connect/api/build/libs/connect-api-2.6.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/clients/build/libs/kafka-clients-2.6.0-SNAPSHOT.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.30/c21f55139d8141d2231214fb1feaf50a1edca95e/slf4j-log4j12-1.7.30.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.12/3.9.2/b1f19bc6774e01debf09bf5f564ad3613687bf49/scala-logging_2.12-3.9.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.5.7/12bdf55ba8be7fc891996319d37f35eaad7e63ea/zookeeper-3.5.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.30/b5a4b6d16ab13e34a88fae84c35cd5d68cac922c/slf4j-api-1.7.30.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/5.18.4/def7af83920ad2c39eb452f6ef9603777d899ea0/rocksdbjni-5.18.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-module-junit4/2.0.5/c922fc29c82664e06466a7ce1face1661d688255/powermock-module-junit4-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-module-junit4-common/2.0.5/d02a42a4cc6d9229a11b1bc5c37a3f5f2c342d0a/powermock-module-junit4-common-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/junit/junit/4.13/e49ccba652b735c93bd6e6f59760d8254cf597dd/junit-4.13.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-api-easymock/2.0.5/a4bca999c461a2787026ce161846affba451fee9/powermock-api-easymock-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.easymock/easymock/4.1/e19506d19d84e8db90d864696282d6981c002e74/easymock-4.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcpkix-jdk15on/1.64/3dac163e20110817d850d17e0444852a6d7d0bd7/bcpkix-jdk15on-1.64.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.4.4-7/f7e9d149c0182968cc2a8706d3ffe82f5c9f01eb/zstd-jni-1.4.4-7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.7.1/c4d931ef8ad2c9c35d65b231a33e61428472d0da/lz4-java-1.7.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.7.3/241bb74a1eb37d68a4e324a4bc3865427de0a62d/snappy-java-1.1.7.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.10.2/dca8c8ab85eaabefe021e2f1ac777f3a6b16a3cb/jackson-datatype-jdk8-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.12/2.10.2/435902f7ac8f01468265c44bd4100b92c6f29663/jackson-module-scala_2.12-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.10.2/b80d499bd4853c784ffd9112aee2ecf5817c28be/jackson-dataformat-csv-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-paranamer/2.10.2/cfd83c1efb7ebfd83aafa5d22fc760a9d94c2a67/jackson-module-paranamer-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.10.2/528de95f198afafbcfb0c09d2e43b6e0ea663ec/jackson-databind-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.12/2.1.3/17ec3eeaba48b3f3e402ecfe22287761fb5c29b7/scala-collection-compat_2.12-2.1.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.12/0.9.0/9525fb6bbf54a9caf0f7e1b65b261215b02fe939/scala-java8-compat_2.12-0.9.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.12.11/7695010d1f4309a9c4b65be33528e382869ab3c4/scala-reflect-2.12.11.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.12.11/1a0634714a956c1aae9abefc83acaf6d4eabfa7d/scala-library-2.12.11.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/1.3/42a25dc3219429f0e5d060061f71acb49bf010a0/hamcrest-core-1.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-api-support/2.0.5/f7e9d65624f55c9c15ebd89a3a8770d1bb21e49c/powermock-api-support-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-core/2.0.5/d5d5ca75413883e00595185d79714e0911c7358e/powermock-core-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.powermock/powermock-reflect/2.0.5/6bca328201936519e08bb1d8fdf37c0a3d7075d0/powermock-reflect-2.0.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.1/48f12deaae83a8dfc3775d830c9fd60ea59bbbca/objenesis-3.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/cglib/cglib-nodep/3.2.9/27ca91ebc2b82f844e62a7ba8c2c1fdf9b84fa80/cglib-nodep-3.2.9.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.64/1467dac1b787b5ad2a18201c0c281df69882259e/bcprov-jdk15on-1.64.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.10.2/3a13b6105946541b8d4181a0506355b5fae63260/jackson-annotations-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.10.2/73d4322a6bda684f676a2b5fe918361c4e5c7cca/jackson-core-2.10.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.5.7/1270f80b08904499a6839a2ee1800da687ad96b4/zookeeper-jute-3.5.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.45.Final/51071ba9977cce64e3a58e6f2f6326bbb7e5bc7f/netty-handler-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.45.Final/cf153257db449b6a74adb64fbd2903542af55892/netty-transport-native-epoll-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.45.Final/8c768728a3e82c3cef62a7a2c8f52ae8d777bac9/netty-codec-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.45.Final/49f9fa4b7fe7d3e562666d050049541b86822549/netty-transport-native-unix-common-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.45.Final/b7d8f2645e330bd66cd4f28f155eba605e0c8758/netty-transport-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.45.Final/bac54338074540c4f3241a3d92358fad5df89ba/netty-buffer-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.45.Final/9e77bdc045d33a570dabf9d53192ea954bb195d7/netty-resolver-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.45.Final/5cf5e448d44ddf53d00f2fc4047c2a7aceaa7087/netty-common-4.1.45.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.9.10/211a2b4d3df1eeef2a6cacf78d74a1f725e7a840/byte-buddy-1.9.10.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.9.10/9674aba5ee793e54b864952b001166848da0f26b/byte-buddy-agent-1.9.10.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.javassist/javassist/3.25.0-GA/442dc1f9fd520130bd18da938622f4f9b2e5fba3/javassist-3.25.0-GA.jar (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,319] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:os.version=4.15.0-76-generic (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:user.name=jenkins (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,320] INFO Client environment:user.home=/home/jenkins (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,321] INFO Client environment:user.dir=/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12/streams (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,321] INFO Client environment:os.memory.free=199MB (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,321] INFO Client environment:os.memory.max=1820MB (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,321] INFO Client environment:os.memory.total=292MB (org.apache.zookeeper.ZooKeeper:109)
[2020-03-21 00:05:32,327] INFO Initiating client connection, connectString=127.0.0.1:42351 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@f4c59db (org.apache.zookeeper.ZooKeeper:868)
[2020-03-21 00:05:32,333] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket:237)
[2020-03-21 00:05:32,348] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn:1653)
[2020-03-21 00:05:32,351] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:05:32,362] INFO Opening socket connection to server localhost/127.0.0.1:42351. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn:1112)
[2020-03-21 00:05:32,365] INFO Socket connection established, initiating session, client: /127.0.0.1:43096, server: localhost/127.0.0.1:42351 (org.apache.zookeeper.ClientCnxn:959)
[2020-03-21 00:05:32,383] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog:218)
[2020-03-21 00:05:32,401] INFO Session establishment complete on server localhost/127.0.0.1:42351, sessionid = 0x100fb9d3b070000, negotiated timeout = 10000 (org.apache.zookeeper.ClientCnxn:1394)
[2020-03-21 00:05:32,408] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:05:33,163] INFO Cluster ID = -OYQb-c6TdOzUjNdfG2ngA (kafka.server.KafkaServer:66)
[2020-03-21 00:05:33,182] WARN No meta.properties file under dir /tmp/junit8727345289613412077/junit1648642492482663308/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2020-03-21 00:05:33,299] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.5-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8727345289613412077/junit1648642492482663308
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.5-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42351
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:347)
[2020-03-21 00:05:33,322] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.5-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8727345289613412077/junit1648642492482663308
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.5-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42351
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.se
...[truncated 198295 chars]...
achine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:58)
	at kafka.controller.KafkaController.onReplicasBecomeOffline(KafkaController.scala:450)
	at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:418)
	at kafka.controller.KafkaController.processBrokerChange(KafkaController.scala:1398)
	at kafka.controller.KafkaController.process(KafkaController.scala:1834)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:128)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:131)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2020-03-21 00:05:58,344] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-0 from OfflinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition __consumer_offsets-0 under strategy OfflinePartitionLeaderElectionStrategy(false)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:427)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:236)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:157)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:73)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:58)
	at kafka.controller.KafkaController.onReplicasBecomeOffline(KafkaController.scala:450)
	at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:418)
	at kafka.controller.KafkaController.processBrokerChange(KafkaController.scala:1398)
	at kafka.controller.KafkaController.process(KafkaController.scala:1834)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:128)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:131)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2020-03-21 00:05:58,351] INFO [Controller id=2] Updated broker epochs cache: Map(2 -> 59) (kafka.controller.KafkaController:66)
[2020-03-21 00:05:58,423] INFO Session: 0x100fb9d3b070001 closed (org.apache.zookeeper.ZooKeeper:1422)
[2020-03-21 00:05:58,423] INFO EventThread shut down for session: 0x100fb9d3b070001 (org.apache.zookeeper.ClientCnxn:524)
[2020-03-21 00:05:58,423] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:05:58,424] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:05:59,186] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:05:59,186] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:05:59,187] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:05:59,817] INFO [Controller id=2] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2020-03-21 00:05:59,820] INFO [Controller id=2] Starting replica leader election (PREFERRED) for partitions  triggered by AutoTriggered (kafka.controller.KafkaController:66)
[2020-03-21 00:05:59,821] INFO [Controller id=2] Starting replica leader election (PREFERRED) for partitions  triggered by AutoTriggered (kafka.controller.KafkaController:66)
[2020-03-21 00:06:00,187] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:00,187] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:00,188] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:01,187] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:01,187] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:01,189] INFO [SocketServer brokerId=1] Shutting down socket server (kafka.network.SocketServer:66)
[2020-03-21 00:06:01,229] INFO [SocketServer brokerId=1] Shutdown completed (kafka.network.SocketServer:66)
[2020-03-21 00:06:01,231] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer:66)
[2020-03-21 00:06:01,234] INFO [KafkaServer id=2] shutting down (kafka.server.KafkaServer:66)
[2020-03-21 00:06:01,235] INFO [KafkaServer id=2] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2020-03-21 00:06:01,250] INFO [Controller id=2] Shutting down broker 2 (kafka.controller.KafkaController:66)
[2020-03-21 00:06:01,253] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition input-0 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition input-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:427)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:236)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:157)
	at kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1141)
	at kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1103)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1103)
	at kafka.controller.KafkaController.process(KafkaController.scala:1826)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:128)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:131)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2020-03-21 00:06:01,258] INFO [KafkaServer id=2] Remaining partitions to move: [RemainingPartition(topicName='input', partitionIndex=0)] (kafka.server.KafkaServer:66)
[2020-03-21 00:06:01,259] INFO [KafkaServer id=2] Error from controller: NONE (kafka.server.KafkaServer:66)
[2020-03-21 00:06:05,504] INFO [Log partition=input-0, dir=/tmp/junit8106553923042971486/junit8676271596052198179] Found deletable segments with base offsets [0] due to retention time 604800000ms breach (kafka.log.Log:66)
[2020-03-21 00:06:05,508] INFO [ProducerStateManager partition=input-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager:66)
[2020-03-21 00:06:05,512] INFO [Log partition=input-0, dir=/tmp/junit8106553923042971486/junit8676271596052198179] Rolled new log segment at offset 1 in 0 ms. (kafka.log.Log:66)
[2020-03-21 00:06:05,513] INFO [Log partition=input-0, dir=/tmp/junit8106553923042971486/junit8676271596052198179] Scheduling segments for deletion List(LogSegment(baseOffset=0, size=76, lastModifiedTime=1584749137000, largestTime=10)) (kafka.log.Log:66)
[2020-03-21 00:06:05,515] INFO [Log partition=input-0, dir=/tmp/junit8106553923042971486/junit8676271596052198179] Incrementing log start offset to 1 (kafka.log.Log:66)
[2020-03-21 00:06:06,261] WARN [KafkaServer id=2] Retrying controlled shutdown after the previous attempt failed... (kafka.server.KafkaServer:70)
[2020-03-21 00:06:06,270] INFO [Controller id=2] Shutting down broker 2 (kafka.controller.KafkaController:66)
[2020-03-21 00:06:06,274] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition input-0 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition input-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:427)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:236)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:157)
	at kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1141)
	at kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1103)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1103)
	at kafka.controller.KafkaController.process(KafkaController.scala:1826)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:128)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:131)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2020-03-21 00:06:06,278] INFO [KafkaServer id=2] Remaining partitions to move: [RemainingPartition(topicName='input', partitionIndex=0)] (kafka.server.KafkaServer:66)
[2020-03-21 00:06:06,278] INFO [KafkaServer id=2] Error from controller: NONE (kafka.server.KafkaServer:66)
[2020-03-21 00:06:08,113] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,123] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,123] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,137] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,138] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,140] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,140] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,141] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,142] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,143] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,143] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,145] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,145] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,147] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,147] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,149] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,149] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,151] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,151] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,153] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,153] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,155] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,155] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,157] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,157] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,159] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,160] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,162] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,162] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,164] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,164] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,165] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,166] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,167] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,167] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:08,169] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Discovered group coordinator localhost:42191 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:795)
[2020-03-21 00:06:08,170] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:551)
[2020-03-21 00:06:11,279] WARN [KafkaServer id=2] Retrying controlled shutdown after the previous attempt failed... (kafka.server.KafkaServer:70)
[2020-03-21 00:06:11,287] INFO [Controller id=2] Shutting down broker 2 (kafka.controller.KafkaController:66)
[2020-03-21 00:06:11,292] ERROR [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition input-0 from OnlinePartition to OnlinePartition (state.change.logger:76)
kafka.common.StateChangeFailedException: Failed to elect leader for partition input-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy
	at kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:427)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:424)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:335)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:236)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:157)
	at kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1141)
	at kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1103)
	at kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1103)
	at kafka.controller.KafkaController.process(KafkaController.scala:1826)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:128)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:131)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[2020-03-21 00:06:11,296] INFO [KafkaServer id=2] Remaining partitions to move: [RemainingPartition(topicName='input', partitionIndex=0)] (kafka.server.KafkaServer:66)
[2020-03-21 00:06:11,296] INFO [KafkaServer id=2] Error from controller: NONE (kafka.server.KafkaServer:66)
[2020-03-21 00:06:16,297] WARN [KafkaServer id=2] Retrying controlled shutdown after the previous attempt failed... (kafka.server.KafkaServer:70)
[2020-03-21 00:06:16,307] WARN [KafkaServer id=2] Proceeding to do an unclean shutdown as all the controlled shutdown attempts failed (kafka.server.KafkaServer:70)
[2020-03-21 00:06:16,308] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2020-03-21 00:06:16,309] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2020-03-21 00:06:16,309] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2020-03-21 00:06:16,309] INFO [SocketServer brokerId=2] Stopping socket server request processors (kafka.network.SocketServer:66)
[2020-03-21 00:06:16,312] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,318] INFO [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Group coordinator localhost:42191 (id: 2147483645 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2020-03-21 00:06:16,319] INFO [SocketServer brokerId=2] Stopped socket server request processors (kafka.network.SocketServer:66)
[2020-03-21 00:06:16,319] INFO [data-plane Kafka Request Handler on Broker 2], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2020-03-21 00:06:16,319] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,320] INFO [data-plane Kafka Request Handler on Broker 2], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2020-03-21 00:06:16,321] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,323] INFO [ExpirationReaper-2-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,413] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,414] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,417] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,423] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,424] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,461] INFO [ExpirationReaper-2-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,461] INFO [ExpirationReaper-2-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,462] INFO [KafkaApi-2] Shutdown complete. (kafka.server.KafkaApis:66)
[2020-03-21 00:06:16,462] INFO [ExpirationReaper-2-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,515] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,525] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,526] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,564] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,617] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,627] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,661] INFO [ExpirationReaper-2-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,661] INFO [ExpirationReaper-2-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,662] INFO [TransactionCoordinator id=2] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2020-03-21 00:06:16,662] INFO [ProducerId Manager 2]: Shutdown complete: last producerId assigned 2000 (kafka.coordinator.transaction.ProducerIdManager:66)
[2020-03-21 00:06:16,663] INFO [Transaction State Manager 2]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2020-03-21 00:06:16,663] INFO [Transaction Marker Channel Manager 2]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2020-03-21 00:06:16,664] INFO [Transaction Marker Channel Manager 2]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2020-03-21 00:06:16,664] INFO [Transaction Marker Channel Manager 2]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2020-03-21 00:06:16,664] INFO [TransactionCoordinator id=2] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2020-03-21 00:06:16,665] INFO [GroupCoordinator 2]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2020-03-21 00:06:16,665] INFO [ExpirationReaper-2-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,718] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,728] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,815] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,830] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,838] INFO [ExpirationReaper-2-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,838] INFO [ExpirationReaper-2-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,838] INFO [ExpirationReaper-2-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,917] INFO [ExpirationReaper-2-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,917] INFO [ExpirationReaper-2-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,918] INFO [GroupCoordinator 2]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2020-03-21 00:06:16,918] INFO [ReplicaManager broker=2] Shutting down (kafka.server.ReplicaManager:66)
[2020-03-21 00:06:16,919] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2020-03-21 00:06:16,919] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2020-03-21 00:06:16,919] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2020-03-21 00:06:16,920] INFO [ReplicaFetcherManager on broker 2] shutting down (kafka.server.ReplicaFetcherManager:66)
[2020-03-21 00:06:16,920] INFO [ReplicaFetcherManager on broker 2] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2020-03-21 00:06:16,921] INFO [ReplicaAlterLogDirsManager on broker 2] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2020-03-21 00:06:16,921] INFO [ReplicaAlterLogDirsManager on broker 2] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2020-03-21 00:06:16,921] INFO [ExpirationReaper-2-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,931] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:16,968] INFO [ExpirationReaper-2-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,968] INFO [ExpirationReaper-2-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:16,968] INFO [ExpirationReaper-2-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,061] INFO [ExpirationReaper-2-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,061] INFO [ExpirationReaper-2-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,062] INFO [ExpirationReaper-2-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,120] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,121] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,232] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,261] INFO [ExpirationReaper-2-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,262] INFO [ExpirationReaper-2-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,262] INFO [ExpirationReaper-2-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,268] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,334] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,334] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,462] INFO [ExpirationReaper-2-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,462] INFO [ExpirationReaper-2-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2020-03-21 00:06:17,538] INFO [ReplicaManager broker=2] Shut down completely (kafka.server.ReplicaManager:66)
[2020-03-21 00:06:17,538] INFO Shutting down. (kafka.log.LogManager:66)
[2020-03-21 00:06:17,539] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2020-03-21 00:06:17,540] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2020-03-21 00:06:17,540] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2020-03-21 00:06:17,540] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2020-03-21 00:06:17,673] INFO Shutdown complete. (kafka.log.LogManager:66)
[2020-03-21 00:06:17,673] INFO [ControllerEventThread controllerId=2] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2020-03-21 00:06:17,674] INFO [ControllerEventThread controllerId=2] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2020-03-21 00:06:17,674] INFO [ControllerEventThread controllerId=2] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2020-03-21 00:06:17,675] INFO [PartitionStateMachine controllerId=2] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2020-03-21 00:06:17,676] INFO [ReplicaStateMachine controllerId=2] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2020-03-21 00:06:17,676] INFO [RequestSendThread controllerId=2] Shutting down (kafka.controller.RequestSendThread:66)
[2020-03-21 00:06:17,677] INFO [RequestSendThread controllerId=2] Stopped (kafka.controller.RequestSendThread:66)
[2020-03-21 00:06:17,677] INFO [RequestSendThread controllerId=2] Shutdown completed (kafka.controller.RequestSendThread:66)
[2020-03-21 00:06:17,680] INFO [Controller id=2] Resigned (kafka.controller.KafkaController:66)
[2020-03-21 00:06:17,681] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:06:17,786] INFO Session: 0x100fb9d3b070002 closed (org.apache.zookeeper.ZooKeeper:1422)
[2020-03-21 00:06:17,786] INFO EventThread shut down for session: 0x100fb9d3b070002 (org.apache.zookeeper.ClientCnxn:524)
[2020-03-21 00:06:17,786] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2020-03-21 00:06:17,787] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:17,924] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,925] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,937] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:17,972] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:18,038] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:18,239] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:18,638] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:18,638] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:18,639] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:18,942] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:18,943] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:19,025] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:19,128] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:19,129] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:19,344] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:19,638] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:19,638] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:19,639] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:19,847] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 0 (localhost/127.0.0.1:36215) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,048] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,186] WARN [Consumer clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-StreamThread-1-consumer, groupId=eos-test-app] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,233] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 1 (localhost/127.0.0.1:45421) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,334] WARN [AdminClient clientId=eos-test-app-e1abba99-766c-451f-81f7-f047906b6445-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,550] WARN [AdminClient clientId=eos-test-app-c008d814-f6fd-40d4-a541-5373b56cfd7b-admin] Connection to node 2 (localhost/127.0.0.1:42191) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:762)
[2020-03-21 00:06:20,638] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:20,638] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2020-03-21 00:06:20,639] INFO [SocketServer brokerId=2] Shutting down socket server (kafka.network.SocketServer:66)
[2020-03-21 00:06:20,674] INFO [SocketServer brokerId=2] Shutdown completed (kafka.network.SocketServer:66)
[2020-03-21 00:06:20,675] INFO [KafkaServer id=2] shut down completed (kafka.server.KafkaServer:66)
[2020-03-21 00:06:20,679] INFO ConnnectionExpirerThread interrupted (org.apache.zookeeper.server.NIOServerCnxnFactory:583)
[2020-03-21 00:06:20,681] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2020-03-21 00:06:20,682] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2020-03-21 00:06:20,684] INFO selector thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:420)
[2020-03-21 00:06:20,689] INFO accept thread exitted run method (org.apache.zookeeper.server.NIOServerCnxnFactory:219)
[2020-03-21 00:06:20,692] INFO shutting down (org.apache.zookeeper.server.ZooKeeperServer:558)
[2020-03-21 00:06:20,692] INFO Shutting down (org.apache.zookeeper.server.SessionTrackerImpl:237)
[2020-03-21 00:06:20,692] INFO Shutting down (org.apache.zookeeper.server.PrepRequestProcessor:1007)
[2020-03-21 00:06:20,693] INFO Shutting down (org.apache.zookeeper.server.SyncRequestProcessor:191)
[2020-03-21 00:06:20,693] INFO PrepRequestProcessor exited loop! (org.apache.zookeeper.server.PrepRequestProcessor:155)
[2020-03-21 00:06:20,693] INFO SyncRequestProcessor exited! (org.apache.zookeeper.server.SyncRequestProcessor:169)
[2020-03-21 00:06:20,693] INFO shutdown of request processor complete (org.apache.zookeeper.server.FinalRequestProcessor:514)
{noformat}",,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9727,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 22:58:19 UTC 2020,,,,,,,,,,"0|z0crrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/20 20:43;mjsax;This tests is not flaky but fails 100%.

It seems the issue is that two PRs got merged close to each other and Jenkins passed for each individual PR but in combination the test breaks. Cf. [https://github.com/apache/kafka/pull/8252] and [https://github.com/apache/kafka/pull/8307];;;","22/Mar/20 22:53;githubbot;abbccdda commented on pull request #8329: KAFKA-9742: Fix StandbyTaskEOSIntegrationTest End offset
URL: https://github.com/apache/kafka/pull/8329
 
 
   The `StandbyTaskEOSIntegrationTest` was broken due to the incorrect offset setting in the checkpoint file enforced by https://github.com/apache/kafka/commit/6cf27c9c771900baf43cc47f9b010dbf7a86fa22. The fix is to set the offset to a legitimate value even if the topic doesn't exist.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Mar/20 23:21;githubbot;vvcephei commented on pull request #8330: KAFKA-9742: Fix broken StandbyTaskEOSIntegrationTest
URL: https://github.com/apache/kafka/pull/8330
 
 
   Relax the requirement that tasks' reported offsetSum is less than the endOffsetSum for those
   tasks. This was surfaced by a test for corrupted tasks, but it can happen with real corrupted
   tasks. Rather than throw an exception on the leader, we now de-prioritize the corrupted task.
   Ideally, that instance will not get assigned the task and the stateDirCleaner will make 
   the problem ""go away"". If it does get assigned the task, then it will detect the corruption and
   delete the task directory before recovering the entire changelog. Thus, the estimate we provide
   accurately reflects the amount of lag such a corrupted task would have to recover (the whole log).
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","23/Mar/20 20:55;githubbot;abbccdda commented on pull request #8329: KAFKA-9742: Fix StandbyTaskEOSIntegrationTest End offset
URL: https://github.com/apache/kafka/pull/8329
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","23/Mar/20 22:58;githubbot;vvcephei commented on pull request #8330: KAFKA-9742: Fix broken StandbyTaskEOSIntegrationTest
URL: https://github.com/apache/kafka/pull/8330
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerCoordinator must update ConsumerGroupMetadata before calling onPartitionsRevoked(),KAFKA-9741,13293051,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,mjsax,mjsax,mjsax,21/Mar/20 01:00,21/Mar/20 04:37,13/Jul/23 09:17,21/Mar/20 04:37,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,consumer,,,,,0,,,,,"If partitions are revoked, an application may want to commit the current offsets.

Using transactions, committing offsets would be done via the producer passing in the current `ConsumerGroupMetadata`. If the metadata is not updates before the callback, the call to `commitTransaction(...)` fails as and old generationId would be used.",,ableegoldman,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 04:34:14 UTC 2020,,,,,,,,,,"0|z0crcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/20 01:29;githubbot;mjsax commented on pull request #8325: KAFKA-9741: Update ConsumerGroupMetadata before calling onPartitionsRevoked()
URL: https://github.com/apache/kafka/pull/8325
 
 
   If partitions are revoked, an application may want to commit the current offsets.
   
   Using transactions, committing offsets would be done via the producer passing in the current `ConsumerGroupMetadata`. If the metadata is not updates before the callback, the call to `commitTransaction(...)` fails as and old generationId would be used.
   
   Call for review @ableegoldman @abbccdda @guozhangwang @hachikuji 
   
   \cc @mumrah (not sure if this is a blocker for 2.5 or not)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Mar/20 04:34;githubbot;guozhangwang commented on pull request #8325: KAFKA-9741: Update ConsumerGroupMetadata before calling onPartitionsRevoked()
URL: https://github.com/apache/kafka/pull/8325
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamsBuilder.build fails with StreamsException ""Found a null keyChangingChild node for OptimizableRepartitionNode""",KAFKA-9739,13292958,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,apoli,apoli,20/Mar/20 14:45,15/Apr/20 19:51,13/Jul/23 09:17,03/Apr/20 16:06,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,2.4.1,,,,,,,,,,,,,,,,2.2.3,2.3.2,2.4.2,2.5.1,2.6.0,,,,streams,,,,,1,,,,,"We created a topology using Streams DSL (topology description is available in the attached topology-description.txt, no optimization).

Topology works fine with {{topology.optimization=none}}, however it fails to build with StreamsException ""Found a null keyChangingChild node for OptimizableRepartitionNode"" if we set {{topology.optimization=all}} (exception stack trace is attached streams-exception-log.txt).

We used [https://zz85.github.io/kafka-streams-viz/] to visualize topology and try to guess what might be upsetting optimizer, yet did not manage to figure it out ourselves.",,ableegoldman,apoli,bbejeck,cadonna,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/20 14:35;apoli;streams-exception-log.txt;https://issues.apache.org/jira/secure/attachment/12997228/streams-exception-log.txt","20/Mar/20 16:04;apoli;topology-definition-fragment.java;https://issues.apache.org/jira/secure/attachment/12997237/topology-definition-fragment.java","20/Mar/20 14:35;apoli;topology-description.txt;https://issues.apache.org/jira/secure/attachment/12997229/topology-description.txt",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 19:51:42 UTC 2020,,,,,,,,,,"0|z0cqs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/20 15:46;bbejeck;Hi [~apoli]. 

Thanks for reporting this.  Can you also include the topology code on this ticket?

Thanks;;;","20/Mar/20 16:05;apoli;Sure. [^topology-definition-fragment.java]

I tried to left out majority of irrelevant pieces, but decent amount of business specifics slipped in anyway;;;","21/Mar/20 19:23;bbejeck;Hi Artur,

I've taken a look at the code sample you've sent. I can' fully reproduce the issue at the moment as there is some business code I don't have access to like the `{{mapRecordToLookupEntry}}` method, for example. But by looking at the code sample and the graph visualization, I've been able to get enough information. I can say that you don't need to run the optimizer as there aren't redundant repartition topics in the topology.

While there certainly is an issue, and thanks for reporting it, in this case, I think it's okay for you to run the stream application with the optimizations set to 'none' for now.

The reason I say that the optimizer won't help is that from your topology, as each repartition topic is unique to each sub-topology. There is no shared key-changing operation that causes multiple redundant repartition topics. For example, consider this topology snippet:
{code:java}
mystream = builder.stream(topic).selectKey((k,v) -> ....);
mystream.groupByKey().windowedBy(....).count().toStream().to(....)
mystream.groupByKey().count().toStream().to(....)
{code}
 This topology results in two repartition topics, but since they are key-changing operation, both will have virtually the same data. In this case, the optimizer can replace the two repartition topics with one directly on the initial stream.

However, from looking at the supplied code example, the repartitions occur on separate streams and inside distinct sub topologies.

For example

1. In sub-topology 0, there is a `{{transform}}` call followed by an aggregation, so the resulting repartition topic can't get optimized away.
 2. In sub-topology one there is `{{flatMap}}` call (when building the retryStreom) which triggers another repartition in the resulting join
 3. In sub-topology 2, when creating the `{{recipientIdTable}}`, there is a `flatMap` call followed by a `groupByKey` then an aggregation, which again is a repartition that can't get optimized away.

 
 Thanks again for reporting this issue, as I'll continue to look into it. But for the meantime, for the reasons stated above, IMHO, you should be fine to run with the optimization set to none.

 

EDIT: One more thing I forgot to mention, but there are two {{flatMap}} operators.  If you can switch to {{flatMapValues}}, that would get rid of at least one if not two repartition topics.  But since I can't look into the {{prepareForProcessing}} or  {{mapRecordToLookupEntry}} methods, I can't say for sure if that is possible.

 

Thanks,

Bill

 ;;;","22/Mar/20 16:28;apoli;Hey, Bill!

Thanks for looking into this issue. Both {{prepareForProcessing}} and  {{mapRecordToLookupEntry}} are key changing operations, so we cannot use {{flatMapValues}} there. Sure, turning off optimizer for this topology is not a problem at all. We would still be curious to know what makes this topology special in a way that upsets existing optimization flow.

Regards,
Artur;;;","22/Mar/20 18:26;bbejeck;Hi Artur,

Sure thing, I'm working on it (I have some suspects in mind), and hopefully, I'll have something soon.

EDIT: I've been able to reconstruct the same topology (at least close enough!) without spring and the business logic and I can reproduce the error.

Regards.

Bill;;;","01/Apr/20 00:03;githubbot;bbejeck commented on pull request #8400: KAFKA-9739: Fixes null key changing child node
URL: https://github.com/apache/kafka/pull/8400
 
 
   For some context, when building a streams application, the optimizer keeps track of the key-changing operations and any repartition nodes that are descendants of the key-changer.  During the optimization phase (if enabled), any repartition nodes are logically collapsed into one.  The optimizer updates the graph by inserting the single repartition node between the key-changing node and its first child node.  This graph update process is done by searching for a node that has the key-changing node as one of its direct parents, and the search starts from the repartition node, going up in the parent hierarchy. 
   
   The one exception to this rule is if there is a merge node that is a descendant of the key-changing node, then during the optimization phase, the map tracking key-changers to repartition nodes is updated to have the merge node as the key.  Then the optimization process updates the graph to place the single repartition node between the merge node and its first child node. 
   
   The error in KAFKA-9739 occurred because there was an assumption that the repartition nodes are children of the merge node.  But in the topology from KAFKA-9739, the repartition node was a parent of the merge node.  So when attempting to find the first child of the merge node, nothing was found (obviously) resulting in `StreamException(Found a null keyChangingChild node for..)`
   
   This PR fixes this bug by first checking that all repartition nodes for optimization are children of the merge node.
   
   This PR includes a test with the topology from KAFKA-9739.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","01/Apr/20 00:16;bbejeck;[~apoli] I've submitted a PR with the fix.  A description of what happened is above.;;;","03/Apr/20 16:04;githubbot;bbejeck commented on pull request #8400: KAFKA-9739: Fixes null key changing child node
URL: https://github.com/apache/kafka/pull/8400
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 16:29;githubbot;bbejeck commented on pull request #8416: KAFKA-9739:  Fixes null key changing child node
URL: https://github.com/apache/kafka/pull/8416
 
 
   2.4 port of #8400 since cherry-picking not possible
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 19:14;githubbot;bbejeck commented on pull request #8416: KAFKA-9739:  Fixes null key changing child node
URL: https://github.com/apache/kafka/pull/8416
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","03/Apr/20 19:15;bbejeck;merged to 2.4 via [https://github.com/apache/kafka/pull/8416];;;","03/Apr/20 19:54;githubbot;bbejeck commented on pull request #8419: KAFKA-9739: 2.3 null child node fix
URL: https://github.com/apache/kafka/pull/8419
 
 
   A port of #8400 for 2.3.  The process of sorting source and sink nodes changed in 2.4, so we can't cherry-pick the PR directly as we need to update the expected topology to what it would be in the 2.3 version.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Apr/20 15:46;githubbot;bbejeck commented on pull request #8419: KAFKA-9739: 2.3 null child node fix
URL: https://github.com/apache/kafka/pull/8419
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Apr/20 15:47;bbejeck;merged to 2.3 via [https://github.com/apache/kafka/pull/8419 |https://github.com/apache/kafka/pull/8419]

cherry-picked to 2.2;;;","15/Apr/20 19:51;bbejeck;Merged to 2.5 via [https://github.com/apache/kafka/pull/8492];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams IllegalStateException in trunk during rebalance,KAFKA-9734,13292731,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,19/Mar/20 15:48,20/Mar/20 15:19,13/Jul/23 09:17,20/Mar/20 15:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"I have observed the following exception to kill a thread in the current trunk of Streams:
{noformat}
[2020-03-19 07:04:35,206] ERROR [stream-soak-test-e60443b4-aa2d-4107-abf7-ce90407cb70e-StreamThread-1] stream-thread [stream-soak-test-e60443b4-aa    2d-4107-abf7-ce90407cb70e-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
java.lang.IllegalStateException: The changelog reader is not restoring active tasks while trying to transit to update standby tasks: {}
        at org.apache.kafka.streams.processor.internals.StoreChangelogReader.transitToUpdateStandby(StoreChangelogReader.java:303)                                                                                                         
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:582)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:498)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:472){noformat}
Judging from the fact that the standby tasks are reported as an empty set, I think this is just a missed case in the task manager. PR to follow shortly.",,ableegoldman,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 15:17:56 UTC 2020,,,,,,,,,,"0|z0cpe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 21:59;githubbot;vvcephei commented on pull request #8319: KAFKA-9734: Fix IllegalState in Streams transit to standby
URL: https://github.com/apache/kafka/pull/8319
 
 
   Consolidate ChangelogReader state management inside of StreamThread to avoid having to reason about all execution paths in both StreamThread and TaskManager.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Mar/20 15:17;githubbot;vvcephei commented on pull request #8319: KAFKA-9734: Fix IllegalState in Streams transit to standby
URL: https://github.com/apache/kafka/pull/8319
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky system test StreamsEOSTest.test_failure_and_recovery,KAFKA-9727,13292063,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,16/Mar/20 20:47,21/Mar/20 18:44,13/Jul/23 09:17,20/Mar/20 23:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,system tests,,,,0,,,,,"Hits no lock available exceptions sometime after task revive:

 

[2020-03-13 05:40:50,224] ERROR stream-thread [EosTest-46de8ee5-82a1-4bd4-af23-f4acd8515f0f-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

org.apache.kafka.streams.errors.ProcessorStateException: Error opening store KSTREAM-AGGREGATE-STATE-STORE-0000000003 at location /mnt/streams/EosTest/0_0/rocksdb/KSTREAM-AGGREGATE-STATE-STORE-0000000003

        at org.apache.kafka.streams.state.internals.RocksDBTimestampedStore.openRocksDB(RocksDBTimestampedStore.java:87)

        at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:191)

        at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:230)

        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)

        at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init(ChangeLoggingKeyValueBytesStore.java:44)

        at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)

        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0(MeteredKeyValueStore.java:101)

        at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)

        at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init(MeteredKeyValueStore.java:101)

        at org.apache.kafka.streams.processor.internals.StateManagerUtil.registerStateStores(StateManagerUtil.java:81)

        at org.apache.kafka.streams.processor.internals.StandbyTask.initializeIfNeeded(StandbyTask.java:86)

        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:275) 

        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:583)

        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:498)

        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:472)

Caused by: org.rocksdb.RocksDBException: lock : /mnt/streams/EosTest/0_0/rocksdb/KSTREAM-AGGREGATE-STATE-STORE-0000000003/LOCK: No locks available

        at org.rocksdb.RocksDB.open(Native Method)

        at org.rocksdb.RocksDB.open(RocksDB.java:286)

        at org.apache.kafka.streams.state.internals.RocksDBTimestampedStore.openRocksDB(RocksDBTimestampedStore.java:75)

        ... 14 more",,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9742,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 22:26:19 UTC 2020,,,,,,,,,,"0|z0cl9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/20 04:20;githubbot;abbccdda commented on pull request #8307: KAFKA-9727: cleanup the state store for standby task dirty close and check null for changelogs
URL: https://github.com/apache/kafka/pull/8307
 
 
   This PR fixes two things:
   
   1. the EOS standby task should also wipe out state under dirty close
   2. the changelog reader should check for null as well
   
   The sequence to reproduce the system test failure:
   
   1. Stream job close uncleanly, leaving active task 0_0 no committed offset
   2. The task 0_0 switch from active to standby task, which never logs anything in checkpoint under EOS
   3. Task 0_0 gets illegal state for not finding checkpoints, throwing task corrupted exception
   4. Exception were caught and the task was closed, however the state store was already registered
   5. Next iteration we shall hit lock not available as it never gets released.
   6. We shall also hit a NPE in the changelog removal as well since it never gets registered.
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Mar/20 22:26;githubbot;guozhangwang commented on pull request #8307: KAFKA-9727: cleanup the state store for standby task dirty close and check null for changelogs
URL: https://github.com/apache/kafka/pull/8307
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Consumer wrongly ignores fetched records ""since it no longer has valid position""",KAFKA-9724,13291804,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,o.muravskiy,o.muravskiy,15/Mar/20 12:39,21/Jul/20 08:22,13/Jul/23 09:17,09/Jun/20 13:48,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,clients,consumer,,,,0,,,,,"After upgrading kafka-client to 2.4.0 (while brokers are still at 2.2.0) consumers in a consumer group intermittently stop progressing on assigned partitions, even when there are messages to consume. This is not a permanent condition, as they progress from time to time, but become slower with time, and catch up after restart.

Here is a sample of 3 consecutive ignored fetches:

{noformat}
2020-03-15 12:08:58,440 DEBUG [Thread-6] o.a.k.c.c.i.ConsumerCoordinator - Committed offset 538065584 for partition mrt-rrc10-6
2020-03-15 12:08:58,541 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Skipping validation of fetch offsets for partitions [mrt-rrc10-1, mrt-rrc10-6, mrt-rrc22-7] since the broker does not support the required protocol version (introduced in Kafka 2.3)
2020-03-15 12:08:58,549 DEBUG [Thread-6] org.apache.kafka.clients.Metadata - Updating last seen epoch from null to 62 for partition mrt-rrc10-6
2020-03-15 12:08:58,557 DEBUG [Thread-6] o.a.k.c.c.i.ConsumerCoordinator - Committed offset 538065584 for partition mrt-rrc10-6
2020-03-15 12:08:58,652 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Skipping validation of fetch offsets for partitions [mrt-rrc10-1, mrt-rrc10-6, mrt-rrc22-7] since the broker does not support the required protocol version (introduced in Kafka 2.3)
2020-03-15 12:08:58,659 DEBUG [Thread-6] org.apache.kafka.clients.Metadata - Updating last seen epoch from null to 62 for partition mrt-rrc10-6
2020-03-15 12:08:58,659 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Fetch READ_UNCOMMITTED at offset 538065584 for partition mrt-rrc10-6 returned fetch data (error=NONE, highWaterMark=538065631, lastStableOffset = 538065631, logStartOffset = 485284547, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=16380)
2020-03-15 12:08:58,659 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Ignoring fetched records for partition mrt-rrc10-6 since it no longer has valid position
2020-03-15 12:08:58,665 DEBUG [Thread-6] o.a.k.c.c.i.ConsumerCoordinator - Committed offset 538065584 for partition mrt-rrc10-6
2020-03-15 12:08:58,761 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Skipping validation of fetch offsets for partitions [mrt-rrc10-1, mrt-rrc10-6, mrt-rrc22-7] since the broker does not support the required protocol version (introduced in Kafka 2.3)
2020-03-15 12:08:58,761 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Added READ_UNCOMMITTED fetch request for partition mrt-rrc10-6 at position FetchPosition{offset=538065584, offsetEpoch=Optional[62], currentLeader=LeaderAndEpoch{leader=node03.kafka:9092 (id: 3 rack: null), epoch=-1}} to node node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,761 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(), toForget=(), implied=(mrt-rrc10-6, mrt-rrc22-7, mrt-rrc10-1)) to broker node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,770 DEBUG [Thread-6] org.apache.kafka.clients.Metadata - Updating last seen epoch from null to 62 for partition mrt-rrc10-6
2020-03-15 12:08:58,770 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Fetch READ_UNCOMMITTED at offset 538065584 for partition mrt-rrc10-6 returned fetch data (error=NONE, highWaterMark=538065727, lastStableOffset = 538065727, logStartOffset = 485284547, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=51864)
2020-03-15 12:08:58,770 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Ignoring fetched records for partition mrt-rrc10-6 since it no longer has valid position
2020-03-15 12:08:58,808 DEBUG [Thread-6] o.a.k.c.c.i.ConsumerCoordinator - Committed offset 538065584 for partition mrt-rrc10-6
{noformat}

After which consumer makes progress:

{noformat}
2020-03-15 12:08:58,871 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Skipping validation of fetch offsets for partitions [mrt-rrc10-1, mrt-rrc10-6, mrt-rrc22-7] since the broker does not support the required protocol version (introduced in Kafka 2.3)
2020-03-15 12:08:58,871 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Added READ_UNCOMMITTED fetch request for partition mrt-rrc10-6 at position FetchPosition{offset=538065584, offsetEpoch=Optional[62], currentLeader=LeaderAndEpoch{leader=node03.kafka:9092 (id: 3 rack: null), epoch=-1}} to node node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,871 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(), toForget=(), implied=(mrt-rrc10-6, mrt-rrc22-7, mrt-rrc10-1)) to broker node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,872 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Fetch READ_UNCOMMITTED at offset 538065584 for partition mrt-rrc10-6 returned fetch data (error=NONE, highWaterMark=538065744, lastStableOffset = 538065744, logStartOffset = 485284547, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=58293)
2020-03-15 12:08:58,872 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Added READ_UNCOMMITTED fetch request for partition mrt-rrc10-6 at position FetchPosition{offset=538065744, offsetEpoch=Optional[62], currentLeader=LeaderAndEpoch{leader=node03.kafka:9092 (id: 3 rack: null), epoch=-1}} to node node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,872 DEBUG [Thread-6] o.a.k.c.consumer.internals.Fetcher - Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(mrt-rrc10-6), toForget=(), implied=(mrt-rrc22-7, mrt-rrc10-1)) to broker node03.kafka:9092 (id: 3 rack: null)
2020-03-15 12:08:58,880 DEBUG [Thread-6] org.apache.kafka.clients.Metadata - Updating last seen epoch from null to 62 for partition mrt-rrc10-6
2020-03-15 12:08:58,885 DEBUG [Thread-6] o.a.k.c.c.i.ConsumerCoordinator - Committed offset 538065744 for partition mrt-rrc10-6
{noformat}

But it could be stuck for quite a long time.
",,githubbot,hachikuji,ijuma,mumrah,o.muravskiy,rng,zgw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/20 08:25;o.muravskiy;consumer.log.xz;https://issues.apache.org/jira/secure/attachment/12998307/consumer.log.xz",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 08:22:12 UTC 2020,,,,,,,,,,"0|z0cjo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 16:31;ijuma;cc [~hachikuji] [~bchen225242];;;","19/Mar/20 17:25;hachikuji;[~o.muravskiy] Thanks for the report. Could you include your consumer configuration?;;;","20/Mar/20 12:38;o.muravskiy;Sure [~hachikuji]: 

{noformat}
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        check.crcs = true
        client.dns.lookup = default
        client.id = ris-updates-to-hbase-ristest
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 5000000
        fetch.max.wait.ms = 8000
        fetch.min.bytes = 1
        group.id = ris-updates-to-hbase-ristest
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        isolation.level = read_uncommitted
        max.partition.fetch.bytes = 4000000
        max.poll.interval.ms = 300000
        max.poll.records = 500000
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.StickyAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 65536
        session.timeout.ms = 300000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
{noformat}
;;;","27/Mar/20 20:25;githubbot;mumrah commented on pull request #8376: KAFKA-9724 Newer clients not always sending fetch request to older brokers
URL: https://github.com/apache/kafka/pull/8376
 
 
   We had a similar case previously with KAFKA-8422 (#6806) where we would skip the validation step if the broker was on a version older than 2.3. 
   
   This PR makes a similar change on the `prepareFetchRequest` side. If the broker is older than 2.3, we will skip the transition to AWAITING_VALIDATION but also we will clear that state if it had been set by a call to `seekUnvalidated`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","30/Mar/20 19:33;mumrah;[~o.muravskiy] could you attach a larger log snippet? I'm having trouble reproducing this with 2.4 client and 2.2 broker. I see you have {{enable.auto.commit}} turned off. Can you describe the application a little?;;;","31/Mar/20 01:07;hachikuji;The interesting thing in the log snippet is the frequency of offset commits. Is that expected? I was speculating that we might be entering a loop like the following:

1. user commits offset with `commitSync` (or similar) which updates Metadata.lastSeenLeaderEpochs
2. in prepareFetch, Metadata.currentLeader then would return no leader and the last seen epoch
3. we trigger a metadata update because we have no leader
4. we get the metadata update without epoch information and reset Metadata.lastSeenLeaderEpochs
5. now we can fetch, but if we get another offset commit first, we would go back to 1

I tried to reproduce this issue locally, but can't say I fully succeeded. I did notice some pauses, but they were very brief. I definitely did notice the unnecessary metadata updates from step 3) though, so I think this is worth fixing even if it does not turn out to be the root cause of this issue. A potential fix is to skip updating `lastSeenLeaderEpochs` in step 1 if we have a current leader, but the epoch is not known.;;;","31/Mar/20 08:25;o.muravskiy;Here's the bigger fragment of a log:
 [^consumer.log.xz]  ;;;","31/Mar/20 10:13;o.muravskiy;The algorithm of a consumer is fairly simple:
- subscribe to a number of topics with pattern subscription
- poll
- process the batch (insert to HBase)
- produce a record to another topic (status info)
- async commit consumed offsets
- loop to poll

But looking at [~hachikuji]'s reply, I want to add that I'm using an OffsetCommitCallback which in essence is 

{code:java}
    public synchronized void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
        if (exception instanceof RetriableCommitFailedException) {
            if (++failureCount < ignoranceLevel) {
                log.warn(""Non-fatally failed to commit offsets, will keep going on. "", exception);
            } else {
                try {
                    Thread.sleep((long) (Math.random() * TimeUnit.SECONDS.toMillis(10)));
                    consumer.commitSync(offsets);
                    failureCount = 0;
                } catch (Exception e) {
                    onComplete(offsets, e);
                }
            }
        } else if (exception != null) {
            log.error(""Can't commit offsets, starting shutdown: "", exception);
            System.exit(-1);
        }
    }
{code}
;;;","08/Apr/20 15:53;mumrah;[~o.muravskiy], I have a patch available here https://github.com/apache/kafka/pull/8376. Would you be willing to try it out and see if you continue to see hanging in the consumer? ;;;","15/Apr/20 12:32;ijuma;[~o.muravskiy] any chance you can test the patch above?;;;","15/Apr/20 19:50;o.muravskiy;[~ijuma] Sorry – I deployed it, wanted to run for a longer time and kind of forgot.

I could confirm the issue have disappeared.;;;","15/Apr/20 22:21;ijuma;That's great to hear!;;;","21/Jul/20 08:22;zgw;When merge this to Kafka 2.4.x?  Have any plan?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't log passwords for AlterConfigs requests in request logs,KAFKA-9718,13291529,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,13/Mar/20 10:53,05/May/20 21:23,13/Jul/23 09:17,13/Mar/20 18:24,,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,,,,,,0,,,,,"We currently avoid logging passwords in log files by logging only parsed values were passwords are logged as `[hidden]`. But for AlterConfigs requests in request logs, we log all entries since they just appear as string entries. Since we allow altering password configs like SSL key passwords and JAAS config, we shouldn't include these in log files.",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 18:24:08 UTC 2020,,,,,,,,,,"0|z0chyw:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"13/Mar/20 11:34;githubbot;rajinisivaram commented on pull request #8294: KAFKA-9718; Don't log passwords for AlterConfigs in request logs
URL: https://github.com/apache/kafka/pull/8294
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Mar/20 18:24;githubbot;rajinisivaram commented on pull request #8294: KAFKA-9718; Don't log passwords for AlterConfigs in request logs
URL: https://github.com/apache/kafka/pull/8294
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Values of compression-rate and compression-rate-avg are misleading,KAFKA-9716,13291526,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,RensGroothuijsen,Kosmowski,Kosmowski,13/Mar/20 10:43,09/Jun/20 19:09,13/Jul/23 09:17,09/Jun/20 19:09,2.4.1,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,clients,compression,,,,0,,,,,"The values of the following metrics:

compression-rate and compression-rate-avg and basically every other compression-rate (i.e.) topic compression rate

are confusing.

They are calculated as follows:
{code:java}
if (numRecords == 0L) {
    buffer().position(initialPosition);
    builtRecords = MemoryRecords.EMPTY;
} else {
    if (magic > RecordBatch.MAGIC_VALUE_V1)
        this.actualCompressionRatio = (float) writeDefaultBatchHeader() / this.uncompressedRecordsSizeInBytes;
    else if (compressionType != CompressionType.NONE)
        this.actualCompressionRatio = (float) writeLegacyCompressedWrapperHeader() / this.uncompressedRecordsSizeInBytes;

    ByteBuffer buffer = buffer().duplicate();
    buffer.flip();
    buffer.position(initialPosition);
    builtRecords = MemoryRecords.readableRecords(buffer.slice());
}
{code}
basically the compressed size is divided by the uncompressed size which leads to a value < 1 for high compression (good if you want compression) or > 1 for poor compression (bad if you want compression).

From the name ""compression rate"" i would expect the exact opposite. Apart from the fact that the word ""rate"" usually refers to comparisons based on values of different units (miles per hour) the correct word ""ratio"" would refer to the uncompressed size divided by the compressed size. (In the code this is correct, but not with the metric names)

So if the compressed data takes half the space of the uncompressed data the correct value for compression ratio (or rate) would be 2 and not 0.5 as kafka reports it. That is really confusing and i would AT LEAST expect that this behaviour would be documented somewhere, but it's not all documentation sources just say ""the compression rate"".",,Kosmowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-13 10:43:29.0,,,,,,,,,,"0|z0chy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reflections library 0.9.12 introduced in 2.5 causes regression scanning for plugins on plugin_path,KAFKA-9712,13291395,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nigel.liang,nigel.liang,nigel.liang,12/Mar/20 17:57,06/Jun/20 01:08,13/Jul/23 09:17,16/Mar/20 22:09,,,,,,,,,,,,,,,,,,,,,,,2.5.0,2.6.0,,,,,,,KafkaConnect,,,,,1,,,,,"Reflections v0.9.12 was introduced in 2.5 branch to remove Guava dependency - https://issues.apache.org/jira/browse/KAFKA-3061

This version, however, contains a [bug|https://github.com/ronmamo/reflections/issues/273] that will cause regression when scanning for plugins if the `plugins_path` specified does not contain valid plugins. The case where we were able to repro this, the `plugins_path` was misconfigured to point to `~/.ssh` which contained unrelated files but no plugins. Further testing is needed to figure out if it will repro for other cases such as empty directory, combination of valid and invalid plugins in directory, just some types of plugins and not others in directory, etc.


{code}
[2020-03-12 18:07:01,045] INFO Loading plugin from: /home/ducker/.ssh (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2020-03-12 18:07:01,047] DEBUG Loading plugin urls: [] (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[2020-03-12 18:07:01,062] ERROR Stopping due to error (org.apache.kafka.connect.cli.ConnectDistributed)
org.reflections.ReflectionsException: Scanner SubTypesScanner was not configured
        at org.reflections.Store.get(Store.java:39)
        at org.reflections.Store.get(Store.java:61)
        at org.reflections.Store.get(Store.java:46)
        at org.reflections.Store.getAll(Store.java:93)
        at org.reflections.Reflections.getSubTypesOf(Reflections.java:404)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.getPluginDesc(DelegatingClassLoader.java:342)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:327)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:260)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.registerPlugin(DelegatingClassLoader.java:252)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:221)
        at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:198)
        at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:60)
        at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
        at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:78)
{code}
",,dongjoon,githubbot,kkonstantine,nigel.liang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9476,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 22:09:29 UTC 2020,,,,,,,,,,"0|z0ch54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 19:15;nigel.liang;I have concluded more testing. My conclusion is that any file on the plugin path that is either:
 # not a jar, or
 # a jar containing no classes

will trigger the bug. Consequence is that connect worker will not be able to start up.
  
 Possible workarounds include but are not limited to:
 * Revert back to v0.9.11
 * Switch to reflections8 fork similar to what is done here - [https://github.com/JanusGraph/janusgraph/pull/2029]
 * Try to catch and handle the exception by skipping the problematic file and continuing the scan;;;","12/Mar/20 20:36;nigel.liang;If we were to revert back to v0.9.11, we would bring back dependency on guava. As noted in https://issues.apache.org/jira/browse/KAFKA-3061, this is undesirable since having it in framework leads to potential version conflicts downstream for users of framework that want to use their own Guava. OTOH, this would be the path of lowest risk since no new code would be introduced.

reflections8 claims that they are obsoleted by v0.9.12 of reflections - [https://github.com/aschoerk/reflections8] . This library will likely not be maintained in the future, while reflections library will likely be fixed in a future 0.9.13 release. There is already a PR out for the fix - [https://github.com/ronmamo/reflections/pull/278]

The try...catch workaround seems like it will be most promising. The proposal is to catch the exception [here|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java#L342] and return empty result on exception. The `Reflections` object we build and use here scans over either a single URL at a time expecting to find an Uber jar or an array of URLs representing contents of the plugin directory. This bug is triggered only when the `Reflections` object could find no classes in the URLs provided. In which case, returning empty results is indeed the correct response. Empty result is also essentially the response that you would get from previous versions of reflections that do not contain the bug. So, making the change should be pretty safe in the sense that existing users upgrading to `2.5` would see the same behavior as before (non-jars or jars containing no classes silently skipped). Exception handling comes with a little bit of a performance penalty, but this is in the worker startup path only and it will be only for exceptional situations. The happy path will be unchanged.;;;","12/Mar/20 21:49;githubbot;ncliang commented on pull request #8289: KAFKA-9712: Catch and handle exception thrown by reflections scanner
URL: https://github.com/apache/kafka/pull/8289
 
 
   This commit works around a bug in v0.9.12 in upstream `reflections` library by catching and handling the exception thrown.
   
   New unittests were introduced to test the behavior.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 23:41;kkonstantine;[~nigel.liang] I agree with the direction of the proposed fix, because this seems to correspond to the behavior that was exhibited until recently, according to which any exception due to a class mismatch or a match that was not found during class scanning would result in a {{WARN}} log message from the {{reflections}} library, that was ignored by Kafka Connect. For example, in this logging template: 
[https://github.com/apache/kafka/blob/trunk/config/connect-log4j.properties];;;","16/Mar/20 21:43;githubbot;kkonstantine commented on pull request #8289: KAFKA-9712: Catch and handle exception thrown by reflections scanner
URL: https://github.com/apache/kafka/pull/8289
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Mar/20 22:09;kkonstantine;This fix has now been merged to {{trunk}} and backported to {{2.5}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The authentication failure caused by SSLEngine#beginHandshake is not properly caught and handled,KAFKA-9711,13291327,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,12/Mar/20 14:15,25/Mar/20 01:50,13/Jul/23 09:17,25/Mar/20 01:50,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"{code:java}
    @Override
    public void handshake() throws IOException {
        if (state == State.NOT_INITALIZED)
            startHandshake(); // this line
        if (ready())
            throw renegotiationException();
        if (state == State.CLOSING)
            throw closingException();

{code}

SSLEngine#beginHandshake is possible to throw authentication failures (for example, no suitable cipher suites) so we ought to catch SSLException and then convert it to SslAuthenticationException so as to process authentication failures correctly.",,chia7712,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 01:50:24 UTC 2020,,,,,,,,,,"0|z0cgq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 14:24;githubbot;chia7712 commented on pull request #8287: KAFKA-9711 The authentication failure caused by SSLEngine#beginHandsh…
URL: https://github.com/apache/kafka/pull/8287
 
 
   https://issues.apache.org/jira/browse/KAFKA-9711
   
   ```java
    @Override
       public void handshake() throws IOException {
           if (state == State.NOT_INITALIZED)
               startHandshake(); // this line
           if (ready())
               throw renegotiationException();
           if (state == State.CLOSING)
               throw closingException();
   ```
   
   SSLEngine#beginHandshake is possible to throw authentication failures (for example, no suitable cipher suites) so we ought to catch SSLException and then convert it to SslAuthenticationException so as to process authentication failures correctly.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 01:45;githubbot;junrao commented on pull request #8287: KAFKA-9711 The authentication failure caused by SSLEngine#beginHandsh…
URL: https://github.com/apache/kafka/pull/8287
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 01:50;junrao;Merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector does not prefer to use packaged classes during configuration,KAFKA-9708,13291185,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,11/Mar/20 22:07,01/Feb/23 21:32,13/Jul/23 09:17,19/Aug/20 04:14,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"In connector tasks, classes loaded during configuration are preferentially loaded from the PluginClassLoader since KAFKA-8819 was implemented. This same prioritization is not currently respected in the connector itself, where the delegating classloader is used as the context classloader. This leads to the possibility for different versions of converters to be loaded, or different versions of dependencies to be found when executing code in the connector vs task.

Worker::startConnector should be changed to follow the startTask / KAFKA-8819 prioritization scheme, by activating the PluginClassLoader earlier.",,gharris1727,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-14670,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 04:13:52 UTC 2020,,,,,,,,,,"0|z0cfug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 22:11;githubbot;gharris1727 commented on pull request #8281: KAFKA-9708: Use PluginClassLoader during connector startup
URL: https://github.com/apache/kafka/pull/8281
 
 
   * Use classloading prioritization from Worker::startTask in Worker::startConnector
   
   Signed-off-by: Greg Harris <gregh@confluent.io>
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Aug/20 04:13;kkonstantine;The PR was closed with a note that this was fixed by: 
[https://github.com/apache/kafka/pull/8069] and as part of https://issues.apache.org/jira/browse/KAFKA-9374

Closing this issue as fixed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InsertField.Key transformation should apply to tombstone records,KAFKA-9707,13291183,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,11/Mar/20 21:37,26/Mar/20 22:20,13/Jul/23 09:17,26/Mar/20 22:20,1.0.3,1.1.2,2.0.2,2.1.2,2.2.2,2.3.1,2.4.0,2.4.1,,,,,,,,,,,,,,,1.0.3,1.1.2,2.0.2,2.1.2,2.2.3,2.3.2,2.4.2,2.5.0,KafkaConnect,,,,,0,regression,,,,"*Note: This was an inadvertent regression caused by KAFKA-8523.*

Reproduction steps:
 # Configure an InsertField.Key transformation
 # Pass a tombstone record (with non-null key, but null value) through the transform

Expected behavior:

The key field is inserted, and the value remains null

Observed behavior:

The key field is not inserted, and the value remains null",,gharris1727,githubbot,kkonstantine,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8523,,,KAFKA-9763,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 22:20:25 UTC 2020,,,,,,,,,,"0|z0cfu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 21:49;githubbot;gharris1727 commented on pull request #8280: KAFKA-9707: Fix InsertField.Key not applying to tombstone events
URL: https://github.com/apache/kafka/pull/8280
 
 
   * Fix typo that hardcoded .value() instead of abstract operatingValue
   * Add test for Key transform that was previously not tested
   
   Signed-off-by: Greg Harris <gregh@confluent.io>
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 19:29;rhauch;KAFKA-8523 was backported to multiple versions, and was released into 2.2.2, 2.3.1, and 2.4.0, and will soon be released in 2.5.0. This should be merged into `trunk` and backported to all branches back through 1.0. The fix versions would then be: 1.0.3, 1.1.2, 2.0.2, 2.1.2, 2.2.3, 2.3.2, 2.4.2, 2.5.1.;;;","26/Mar/20 17:21;githubbot;kkonstantine commented on pull request #8280: KAFKA-9707: Fix InsertField.Key should apply to keys of tombstone records
URL: https://github.com/apache/kafka/pull/8280
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Mar/20 22:20;kkonstantine;This fix was merged in time so it will be available in 2.5.0 and 2.4.2, 2.3.2, 2.2.3, 2.1.2, 2.0.2, 1.1.2, 1.0.3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flatten transformation fails when encountering tombstone event,KAFKA-9706,13291176,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gharris1727,gharris1727,gharris1727,11/Mar/20 21:11,30/Mar/20 22:44,13/Jul/23 09:17,30/Mar/20 22:44,2.0.1,2.1.1,2.2.2,2.3.1,2.4.1,,,,,,,,,,,,,,,,,,2.1.2,2.2.3,2.3.2,2.4.2,2.5.0,,,,KafkaConnect,,,,,0,,,,,"When applying the {{Flatten}} transformation to a tombstone event, an exception is raised:
{code:java}
org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [flattening], found: null
{code}
Instead, the transform should pass the tombstone through the transform without throwing an exception.",,gharris1727,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 22:44:33 UTC 2020,,,,,,,,,,"0|z0cfsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 21:26;githubbot;gharris1727 commented on pull request #8279: KAFKA-9706: Handle null keys/values in Flatten transformation
URL: https://github.com/apache/kafka/pull/8279
 
 
   * Fix DataException thrown when handling tombstone events with null value
   * Passes through original record when finding a tombstone record
   * Add tests for schema and schemaless data
   
   Signed-off-by: Greg Harris <gregh@confluent.io>
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","30/Mar/20 22:09;githubbot;kkonstantine commented on pull request #8279: KAFKA-9706: Handle null keys/values in Flatten transformation
URL: https://github.com/apache/kafka/pull/8279
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","30/Mar/20 22:44;kkonstantine;Merged to {{trunk}} and release branches {{2.5, 2.4, 2.3, 2.2 and 2.1}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
z/OS won't let us resize file when mmap,KAFKA-9704,13291032,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,zshuo,zshuo,zshuo,11/Mar/20 09:32,01/Nov/21 08:56,13/Jul/23 09:17,24/Apr/20 14:25,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,log,,,,,0,,,,,"z/OS won't let us resize file when mmap, so we need to force unman like Windows.

 
It was found that if messages were ever published to the Kafka server, the Kafka server will not restart. The issue occurs repeatedly in two of our guest z/OS systems.
 
*java.io.IOException: EDC5121I Invalid argument.*                                                   
 at sun.nio.ch.FileChannelImpl.map0(Native Method)                                                
 at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:946)                                      
 at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:190)                            
 at kafka.log.AbstractIndex$$Lambda$238.00000000D24C4D30.apply$mcZ$sp(Unknown Source)             
 at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)                        
 at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)                                            
 at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)                                       
 at kafka.log.LogSegment.resizeIndexes(LogSegment.scala:77)                                       
 at kafka.log.Log.loadSegments(Log.scala:728)                                                     
 at kafka.log.Log.<init>(Log.scala:298)                                                           
 at kafka.log.Log$.apply(Log.scala:2461)                                                          
 at kafka.log.LogManager.loadLog(LogManager.scala:283)                                            
 at kafka.log.LogManager.$anonfun$loadLogs$12(LogManager.scala:353)                               
 at kafka.log.LogManager$$Lambda$198.00000000C9F511B0.apply$mcV$sp(Unknown Source)                
 at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:65)                                         
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:522)                       
 at java.util.concurrent.FutureTask.run(FutureTask.java:277)                                      
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)               
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)               
 at java.lang.Thread.run(Thread.java:812)                                                         ",,zshuo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-11 09:32:55.0,,,,,,,,,,"0|z0cewg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerBatch.split takes up too many resources if the bigBatch is huge,KAFKA-9703,13291029,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,adally,adally,adally,11/Mar/20 09:20,16/Apr/20 00:11,13/Jul/23 09:17,16/Apr/20 00:11,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"ProducerBatch.split takes up too many resources  and might cause outOfMemory error if the bigBatch is huge. About how I found this issue is in https://lists.apache.org/list.html?users@kafka.apache.org:lte=1M:MESSAGE_TOO_LARGE

Following is the code which takes a lot of resources.

{code:java}
 for (Record record : recordBatch) {
            assert thunkIter.hasNext();
            Thunk thunk = thunkIter.next();
            if (batch == null)
                batch = createBatchOffAccumulatorForRecord(record, splitBatchSize);

            // A newly created batch can always host the first message.
            if (!batch.tryAppendForSplit(record.timestamp(), record.key(), record.value(), record.headers(), thunk)) {
                batches.add(batch);
                batch = createBatchOffAccumulatorForRecord(record, splitBatchSize);
                batch.tryAppendForSplit(record.timestamp(), record.key(), record.value(), record.headers(), thunk);
            }
{code}

Refer to RecordAccumulator#tryAppend, we can call closeForRecordAppends() after a batch is full.

{code:java}
    private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers,
                                         Callback callback, Deque<ProducerBatch> deque, long nowMs) {
        ProducerBatch last = deque.peekLast();
        if (last != null) {
            FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, nowMs);
            if (future == null)
                last.closeForRecordAppends();
            else
                return new RecordAppendResult(future, deque.size() > 1 || last.isFull(), false, false);
        }
        return null;
    }
{code}",,adally,becket_qin,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Thu Apr 16 00:11:17 UTC 2020,,,,,,,,,,"0|z0cevs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 07:13;githubbot;jiameixie commented on pull request #8286: KAFKA-9703:Free up resources when splitting huge batches
URL: https://github.com/apache/kafka/pull/8286
 
 
   Method split takes up too many resources and might
   cause outOfMemory error when the bigBatch is huge.
   Call closeForRecordAppends() to free up resources
   like compression buffers.
   
   Change-Id: Iac6519fcc2e432330b8af2d9f68a8d4d4a07646b
   Signed-off-by: Jiamei Xie <jiamei.xie@arm.com>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 07:19;adally;https://github.com/apache/kafka/pull/8286;;;","16/Apr/20 00:01;githubbot;becketqin commented on pull request #8286: KAFKA-9703:Free up resources when splitting huge batches
URL: https://github.com/apache/kafka/pull/8286
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Apr/20 00:11;becket_qin;Just to make it clear, the root cause of the OOM in the ticket description was due to #KAFKA-9700. This ticket added an optimization to release the compression buffer when splitting a large {{ProducerBatch}}. Because this is just an optimization, we are not cherry-picking it to earlier release branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer could catch InconsistentGroupProtocolException during rebalance,KAFKA-9701,13290966,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bchen225242,bchen225242,bchen225242,11/Mar/20 02:49,11/May/22 13:09,13/Jul/23 09:17,21/Mar/20 04:28,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,,,,,,0,,,,,"The bug was due to an out-of-order handling of the SyncGroupRequest after the LeaveGroupRequest.

The sequence of events are:
 # The stream thread tries to rejoin the group during runOnce#poll
 # The join group call was successful and group was waiting for sync group result
 # Outside the poll, task producer hits FencedException, triggering a partition lost
 # Stream thread unsubscribes and sends out an leave group, and gets the local generation wipe out 
 # The sync group response was processed. Although it is legitimate, the local protocol type becomes null in this case
 # The sync group response hits the protocol type mismatch fatal exception

 

[2020-03-20T*10:40:08-07:00*] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:08,754] INFO [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] [Consumer clientId=stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-consumer, groupId=stream-soak-test] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2020-03-20T*10:40:11-07:00*] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:11,152] ERROR [kafka-producer-network-thread | stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-0_1-producer] stream-thread [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] task [0_1] Error encountered sending record to topic network-id-repartition for task 0_1 due to:

[2020-03-20T10:40:11-07:00] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.

[2020-03-20T10:40:12-07:00] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:12,048] INFO [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] stream-thread [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] at state RUNNING: partitions [logs.json.kafka-1, node-name-repartition-1, logs.json.zookeeper-1, logs.kubernetes-1, windowed-node-counts-1, logs.operator-1, logs.syslog-1] lost due to missed rebalance.

        lost active tasks: []

        lost assigned standby tasks: []

 (org.apache.kafka.streams.processor.internals.StreamThread)

 

[2020-03-20T*10:40:12-07:00*] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:12,048] INFO [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] [Consumer clientId=stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-consumer, groupId=stream-soak-test] Member stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-consumer-34c2198b-5bdd-470b-ae50-30a39873edab sending LeaveGroup request to coordinator ip-172-31-18-29.us-west-2.compute.internal:9092 (id: 2147482644 rack: null) due to the consumer *unsubscribed from all topics* (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2020-03-20T10:40:12-07:00] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:12,048] INFO [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] [Consumer clientId=stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-consumer, groupId=stream-soak-test] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)

[2020-03-20T10:40:17-07:00] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:16,972] ERROR [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] [Consumer clientId=stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1-consumer, groupId=stream-soak-test] SyncGroup failed due to inconsistent Protocol Name, received stream but expected null (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2020-03-20T10:40:17-07:00] (streams-soak-trunk-eos_soak_i-01629239fa39901b4_streamslog) [2020-03-20 17:40:16,973] ERROR [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] stream-thread [stream-soak-test-f7392d33-55d7-484f-8b72-578e22fead96-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

 

----------- Original Exception -------------

INFO log shows that we accidentally hit an unexpected inconsistent group protocol exception:

[2020-03-10T17:16:53-07:00] (streams-soak-2-5-eos-broker-2-5_soak_i-00067445452c82fe8_streamslog) [2020-03-11 *00:16:53,382*] INFO [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1] stream-client [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)

 

[2020-03-10T17:16:53-07:00] (streams-soak-2-5-eos-broker-2-5_soak_i-00067445452c82fe8_streamslog) [2020-03-11 *00:16:53,384*] WARN [kafka-producer-network-thread | stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1-0_1-producer] stream-thread [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1] task [0_1] Error sending record to topic node-name-repartition due to Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.; No more records will be sent and no more offsets will be recorded for this task.

 

 

[2020-03-10T17:16:53-07:00] (streams-soak-2-5-eos-broker-2-5_soak_i-00067445452c82fe8_streamslog) [2020-03-11 *00:16:53,521*] INFO [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1] [Consumer clientId=stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1-consumer, groupId=stream-soak-test] Member stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1-consumer-d1c3c796-0bfb-4c1c-9fb4-5a807d8b53a2 sending LeaveGroup request to coordinator ip-172-31-20-215.us-west-2.compute.internal:9092 (id: 2147482646 rack: null) due to the consumer unsubscribed from all topics (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

 

[2020-03-10T17:16:54-07:00] (streams-soak-2-5-eos-broker-2-5_soak_i-00067445452c82fe8_streamslog) [2020-03-11 *00:16:53,798*] ERROR [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1] stream-thread [stream-soak-test-d3da8597-c371-450e-81d9-72aea6a26949-StreamThread-1] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors: (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-10T17:16:54-07:00] (streams-soak-2-5-eos-broker-2-5_soak_i-00067445452c82fe8_streamslog) org.apache.kafka.common.errors.InconsistentGroupProtocolException: The group member's supported protocols are incompatible with those of existing members or first group member tried to join with empty protocol type or empty protocol list.

 

Potentially needs further log to understand this.",,ableegoldman,bchen225242,cadonna,githubbot,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/20 20:29;vvcephei;cluster.log;https://issues.apache.org/jira/secure/attachment/12997248/cluster.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 04:27:03 UTC 2020,,,,,,,,,,"0|z0cehs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 02:56;githubbot;abbccdda commented on pull request #8272: KAFKA-9701: Add more debug log on client to reproduce the issue
URL: https://github.com/apache/kafka/pull/8272
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Mar/20 04:41;githubbot;iamgd67 commented on pull request #8274: KAFKA-9701: remove -E option from sed in kafka-run-class.sh
URL: https://github.com/apache/kafka/pull/8274
 
 
    remove -E option from sed in kafka-run-class.sh because old version of sed not support -E
   more detail please look at  https://issues.apache.org/jira/browse/KAFKA-9699
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Mar/20 05:57;bchen225242;Reproduced on trunk as well:
```

[2020-03-10T20:05:27-07:00] (streams-soak-2-6-eos_soak_i-053ea65a68df40867_streamslog) [2020-03-11 03:05:27,252] INFO [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5-StreamThread-3] stream-thread [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5-StreamThread-3] State transition from RUNNING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-10T20:05:27-07:00] (streams-soak-2-6-eos_soak_i-053ea65a68df40867_streamslog) [2020-03-11 03:05:27,252] INFO [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5-StreamThread-3] stream-client [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5] State transition from RUNNING to REBALANCING (org.apache.kafka.streams.KafkaStreams)

[2020-03-10T20:05:27-07:00] (streams-soak-2-6-eos_soak_i-053ea65a68df40867_streamslog) [2020-03-11 03:05:27,252] ERROR [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5-StreamThread-1] stream-thread [stream-soak-test-cdaca0fe-8d47-4de5-911e-bb084038d6f5-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-10T20:05:27-07:00] (streams-soak-2-6-eos_soak_i-053ea65a68df40867_streamslog) org.apache.kafka.common.errors.InconsistentGroupProtocolException: The group member's supported protocols are incompatible with those of existing members or first group member tried to join with empty protocol type or empty protocol list.

```;;;","11/Mar/20 22:52;githubbot;guozhangwang commented on pull request #8272: KAFKA-9701: Add more debug log on client to reproduce the issue
URL: https://github.com/apache/kafka/pull/8272
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Mar/20 20:30;vvcephei;Hey [~bchen225242] , I think I've reproduced this issue in a soak test. Attaching the client and broker logs ...[^cluster.log];;;","21/Mar/20 00:31;githubbot;abbccdda commented on pull request #8324: KAFKA-9701 (fix): Only check protocol name when generation is valid
URL: https://github.com/apache/kafka/pull/8324
 
 
   This bug was incurred by https://github.com/apache/kafka/pull/7994 with a too-strong consistency check. It is because a reset generation operation could be called in between the `joinGroupRequest` -> `joinGroupResponse` -> `SyncGroupRequest` -> `SyncGroupResponse` sequence of events, if user calls `unsubscribe` in the middle of consumer#poll().
   
   Proper fix is to avoid the protocol name check when the generation is invalid.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Mar/20 04:27;githubbot;guozhangwang commented on pull request #8324: KAFKA-9701 (fix): Only check protocol name when generation is valid
URL: https://github.com/apache/kafka/pull/8324
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative estimatedCompressionRatio leads to misjudgment about if there is no room,KAFKA-9700,13290956,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,adally,adally,11/Mar/20 02:00,25/Mar/20 03:53,13/Jul/23 09:17,25/Mar/20 03:53,,,,,,,,,,,,,,,,,,,,,,,2.5.0,2.6.0,,,,,,,clients,,,,,0,,,,,"* When I run the following command 
bin/kafka-producer-perf-test.sh --topic test --num-records 50000000 --throughput -1 --record-size 5000 --producer-props bootstrap.servers=server04:9092 acks=1 buffer.memory=67108864 batch.size 65536 compression.type=zstd
There was a warning:
[2020-03-06 17:36:50,216] WARN [Producer clientId=producer-1] Got error produce response in correlation id 3261 on topic-partition test-1, splitting and retrying (2147483647 attempts left). Error: MESSAGE_TOO_LARGE (org.apache.kafka.clients.producer.internals.Sender)

* The batch size(65536) is smaller than max.message.bytes (1048588) .  So it's not the root cause.


* I added some logs in CompressionRatioEstimator.updateEstimation and found there were negative currentEstimation values.  The following were logs I added
public static float updateEstimation(String topic, CompressionType type, float observedRatio) {
    float[] compressionRatioForTopic = getAndCreateEstimationIfAbsent(topic);
    float currentEstimation = compressionRatioForTopic[type.id];
    synchronized (compressionRatioForTopic) {
        if (observedRatio > currentEstimation)
        {
                compressionRatioForTopic[type.id] = Math.max(currentEstimation + COMPRESSION_RATIO_DETERIORATE_STEP, observedRatio);
        }
        else if (observedRatio < currentEstimation) {
                  compressionRatioForTopic[type.id] = currentEstimation - COMPRESSION_RATIO_IMPROVING_STEP;
                  log.warn(""####currentEstimation is {} , COMPRESSION_RATIO_IMPROVING_STEP is {} , compressionRatioForTopic[type.id] is {}, type.id is {}"", currentEstimation, COMPRESSION_RATIO_IMPROVING_STEP,compressionRatioForTopic[type.id], type.id);
        }
    }
     return compressionRatioForTopic[type.id];
}


The observedRatio is smaller than COMPRESSION_RATIO_IMPROVING_STEP in some cases.  Some I think the else if block should be changed into 

else if (observedRatio < currentEstimation) {
                  compressionRatioForTopic[type.id] = Math.max(currentEstimation - COMPRESSION_RATIO_IMPROVING_STEP, observedRatio);
              }

",,adally,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Wed Mar 25 03:48:56 UTC 2020,,,,,,,,,,"0|z0cefk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/20 07:12;githubbot;jiameixie commented on pull request #8285: KAFKA-9700:Fix negative estimatedCompressionRatio issue
URL: https://github.com/apache/kafka/pull/8285
 
 
   There are cases that currentEstimation is smaller than
   COMPRESSION_RATIO_IMPROVING_STEP and it will get negative
   estimatedCompressionRatio,which leads to misjudgment
   about if there is no room and MESSAGE_TOO_LARGE might occur.
   
   Change-Id: I0932a2a6ca669f673ab5d862d3fe7b2bb6d96ff6
   Signed-off-by: Jiamei Xie <jiamei.xie@arm.com>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 07:22;adally;https://github.com/apache/kafka/pull/8285;;;","25/Mar/20 03:48;githubbot;ijuma commented on pull request #8285: KAFKA-9700:Fix negative estimatedCompressionRatio issue
URL: https://github.com/apache/kafka/pull/8285
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AdminClient allows null topic configs, but broker throws NPE",KAFKA-9695,13290923,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,10/Mar/20 22:01,12/Mar/20 10:46,13/Jul/23 09:17,12/Mar/20 10:46,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"Config entries may contain null values, but broker's AdminManager throws NPE resulting in UnknownServerException. We should handle null values in configs.
{code:java}
[2020-03-10 21:56:07,904] ERROR [Admin Manager on Broker 0]: Error processing create topic request CreatableTopic(name='topic', numPartitions=2, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='message.format.version', value=null), CreateableTopicConfig(name='compression.type', value='producer')]) (kafka.server.AdminManager:76)
java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:460)
	at java.util.Properties.setProperty(Properties.java:166)
	at kafka.server.AdminManager.$anonfun$createTopics$3(AdminManager.scala:99)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.server.AdminManager.$anonfun$createTopics$2(AdminManager.scala:98)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.mutable.HashMap$$anon$2.$anonfun$foreach$3(HashMap.scala:158)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:158)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at kafka.server.AdminManager.createTopics(AdminManager.scala:91)
	at kafka.server.KafkaApis.handleCreateTopicsRequest(KafkaApis.scala:1701)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:147)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
 {code}",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 10:46:13 UTC 2020,,,,,,,,,,"0|z0ce88:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"10/Mar/20 22:14;githubbot;rajinisivaram commented on pull request #8266: KAFKA-9695; Handle null config values for createTopics, alterConfigs
URL: https://github.com/apache/kafka/pull/8266
 
 
   1) For createTopics, allow null values since we allow config instances to be created with null values in the map. This just uses default value.
   2) For incrementalAlterConfigs, don't allow null values since we have separate DELETE op for deleting configs. Throw InvalidRequestException if value is null for other ops.
   3) For the old alterConfigs API, AdminClient didn't allow null values. Throw InvalidRequestException similar to 2) if we do get a null value from a non-Java client.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 10:44;githubbot;rajinisivaram commented on pull request #8266: KAFKA-9695; Handle null config values for createTopics, alterConfigs
URL: https://github.com/apache/kafka/pull/8266
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 10:46;rsivaram;Updated broker to throw InvalidRequestException for null config values in CreateTopics, AlterConfigs and IncrementalAlterConfigs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test kafka.admin.TopicCommandWithAdminClientTest#testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress,KAFKA-9691,13290818,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,tombentley,tombentley,10/Mar/20 14:05,09/Apr/20 14:25,13/Jul/23 09:17,09/Apr/20 14:25,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,unit tests,,,,,0,flaky-test,,,,"Stacktrace:

{noformat}
java.lang.NullPointerException
	at kafka.admin.TopicCommandWithAdminClientTest.$anonfun$testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress$3(TopicCommandWithAdminClientTest.scala:673)
	at kafka.admin.TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress(TopicCommandWithAdminClientTest.scala:671)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}

Standard output:

{noformat}
Created topic testCreateWithDefaultReplication-TzaNGcD6mL.
[2020-03-10 10:11:40,529] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-LdzgScMUuy-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:11:40,533] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-LdzgScMUuy-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Created topic testCreateIfItAlreadyExists-tea1UkanNu.
[2020-03-10 10:12:04,086] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition kafka.testTopic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:04,092] ERROR [ReplicaFetcher replicaId=4, leaderId=1, fetcherId=0] Error for partition __consumer_offsets-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,229] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-17 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,229] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-5 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,244] ERROR [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-17 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,245] ERROR [ReplicaFetcher replicaId=2, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-11 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,247] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-14 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,246] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,254] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-13 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,246] ERROR [ReplicaFetcher replicaId=5, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-3 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,254] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-15 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,256] ERROR [ReplicaFetcher replicaId=4, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-16 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Created topic testCreateAlterTopicWithRackAware-wDCgC3q4uS.
[2020-03-10 10:12:23,259] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-11 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,268] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-5 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,253] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-16 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,253] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-7 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,252] ERROR [ReplicaFetcher replicaId=4, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-9 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,252] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-10 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,252] ERROR [ReplicaFetcher replicaId=4, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,252] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-9 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,272] ERROR [ReplicaFetcher replicaId=4, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-7 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,271] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-4 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,269] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-4 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,259] ERROR [ReplicaFetcher replicaId=2, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-14 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,275] ERROR [ReplicaFetcher replicaId=2, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,262] ERROR [ReplicaFetcher replicaId=3, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-8 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,276] ERROR [ReplicaFetcher replicaId=3, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-2 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,256] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-13 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,255] ERROR [ReplicaFetcher replicaId=5, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-15 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,272] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-3 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,430] ERROR [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-29 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,431] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-19 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,431] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-31 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,442] ERROR [ReplicaFetcher replicaId=3, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-32 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,442] ERROR [ReplicaFetcher replicaId=3, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-20 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,448] ERROR [ReplicaFetcher replicaId=2, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-29 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,449] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-33 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,450] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-21 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,450] ERROR [ReplicaFetcher replicaId=2, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-32 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,451] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-25 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,448] ERROR [ReplicaFetcher replicaId=5, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-33 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,451] ERROR [ReplicaFetcher replicaId=2, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-20 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,451] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-22 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,452] ERROR [ReplicaFetcher replicaId=5, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-21 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,452] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-34 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,453] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-23 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,454] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-35 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,453] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-27 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,456] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-23 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,457] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-35 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,457] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-22 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,459] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-34 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:23,460] ERROR [ReplicaFetcher replicaId=4, leaderId=0, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-wDCgC3q4uS-27 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Created topic testTopicDeletion-KFosVGnPLw.
Created topic testCreateWithDefaults-1HjRGXxa1i.
Created topic testDescribeReportOverriddenConfigs-MQmTvVRNrT.
[2020-03-10 10:12:36,102] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition testDescribeReportOverriddenConfigs-MQmTvVRNrT-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:43,622] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition kafka.testTopic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:12:47,813] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testAlterAssignmentWithMoreAssignmentThanPartitions-RyfROWE4t9-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Created topic testCreateWithDefaultPartitions-lmUm4iOhYU.
Created topic testListTopics-js9Zyn4bGs.
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic __consumer_offsets.
Created topic testConfigPreservationAcrossPartitionAlteration-bFfxFd2mQV.
Created topic testCreateWithConfigs-RFZlAnZGsC.
[2020-03-10 10:13:21,139] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-pJH315jRjQ-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:13:21,142] ERROR [ReplicaFetcher replicaId=2, leaderId=4, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-pJH315jRjQ-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:13:21,143] ERROR [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-pJH315jRjQ-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-03-10 10:13:21,143] ERROR [ReplicaFetcher replicaId=3, leaderId=4, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-pJH315jRjQ-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Created topic testDescribeAndListTopicsWithoutInternalTopics-o9HwVMcCsB.
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic __consumer_offsets.
Created topic testCreateWithReplicaAssignment-jnAcY2d2ck.
Created topic testAlterWithInvalidPartitionCount-ojsK73bBtC.
Created topic testCreate-RWpMOPTUM3.
[2020-03-10 10:14:34,288] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition testAlterAssignment-JqE8hyg0UB-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
{noformat}

Standard error:
{noformat}
Option ""[replica-assignment]"" can't be used with option ""[partitions]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Missing required argument ""[partitions]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Option ""[if-exists]"" can't be used with option ""[bootstrap-server]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Option ""[if-not-exists]"" can't be used with option ""[bootstrap-server]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Option ""[replica-assignment]"" can't be used with option ""[replication-factor]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Option combination ""[bootstrap-server],[config]"" can't be used with option ""[alter]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Option ""[delete-config]"" can't be used with option ""[bootstrap-server]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
Missing required argument ""[partitions]""
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                           configuration for the topic.         
--at-min-isr-partitions                  if set when describing topics, only    
                                           show partitions whose isr count is   
                                           equal to the configured minimum. Not 
                                           supported with the --zookeeper       
                                           option.                              
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
  connect to>                              to. In case of providing this, a     
                                           direct Zookeeper connection won't be 
                                           required.                            
--command-config <String: command        Property file containing configs to be 
  config property file>                    passed to Admin Client. This is used 
                                           only with --bootstrap-server option  
                                           for describing and altering broker   
                                           configs.                             
--config <String: name=value>            A topic configuration override for the 
                                           topic being created or altered.The   
                                           following is a list of valid         
                                           configurations:                      
                                         	cleanup.policy                        
                                         	compression.type                      
                                         	delete.retention.ms                   
                                         	file.delete.delay.ms                  
                                         	flush.messages                        
                                         	flush.ms                              
                                         	follower.replication.throttled.       
                                           replicas                             
                                         	index.interval.bytes                  
                                         	leader.replication.throttled.replicas 
                                         	max.compaction.lag.ms                 
                                         	max.message.bytes                     
                                         	message.downconversion.enable         
                                         	message.format.version                
                                         	message.timestamp.difference.max.ms   
                                         	message.timestamp.type                
                                         	min.cleanable.dirty.ratio             
                                         	min.compaction.lag.ms                 
                                         	min.insync.replicas                   
                                         	preallocate                           
                                         	retention.bytes                       
                                         	retention.ms                          
                                         	segment.bytes                         
                                         	segment.index.bytes                   
                                         	segment.jitter.ms                     
                                         	segment.ms                            
                                         	unclean.leader.election.enable        
                                         See the Kafka documentation for full   
                                           details on the topic configs.It is   
                                           supported only in combination with --
                                           create if --bootstrap-server option  
                                           is used.                             
--create                                 Create a new topic.                    
--delete                                 Delete a topic                         
--delete-config <String: name>           A topic configuration override to be   
                                           removed for an existing topic (see   
                                           the list of configurations under the 
                                           --config option). Not supported with 
                                           the --bootstrap-server option.       
--describe                               List details for the given topics.     
--disable-rack-aware                     Disable rack aware replica assignment  
--exclude-internal                       exclude internal topics when running   
                                           list or describe command. The        
                                           internal topics will be listed by    
                                           default                              
--force                                  Suppress console prompts               
--help                                   Print usage information.               
--if-exists                              if set when altering or deleting or    
                                           describing topics, the action will   
                                           only execute if the topic exists.    
                                           Not supported with the --bootstrap-  
                                           server option.                       
--if-not-exists                          if set when creating topics, the       
                                           action will only execute if the      
                                           topic does not already exist. Not    
                                           supported with the --bootstrap-      
                                           server option.                       
--list                                   List all available topics.             
--partitions <Integer: # of partitions>  The number of partitions for the topic 
                                           being created or altered (WARNING:   
                                           If partitions are increased for a    
                                           topic that has a key, the partition  
                                           logic or ordering of the messages    
                                           will be affected). If not supplied   
                                           for create, defaults to the cluster  
                                           default.                             
--replica-assignment <String:            A list of manual partition-to-broker   
  broker_id_for_part1_replica1 :           assignments for the topic being      
  broker_id_for_part1_replica2 ,           created or altered.                  
  broker_id_for_part2_replica1 :                                                
  broker_id_for_part2_replica2 , ...>                                           
--replication-factor <Integer:           The replication factor for each        
  replication factor>                      partition in the topic being         
                                           created. If not supplied, defaults   
                                           to the cluster default.              
--topic <String: topic>                  The topic to create, alter, describe   
                                           or delete. It also accepts a regular 
                                           expression, except for --create      
                                           option. Put topic name in double     
                                           quotes and use the '\' prefix to     
                                           escape regular expression symbols; e.
                                           g. ""test\.topic"".                    
--topics-with-overrides                  if set when describing topics, only    
                                           show topics that have overridden     
                                           configs                              
--unavailable-partitions                 if set when describing topics, only    
                                           show partitions whose leader is not  
                                           available                            
--under-min-isr-partitions               if set when describing topics, only    
                                           show partitions whose isr count is   
                                           less than the configured minimum.    
                                           Not supported with the --zookeeper   
                                           option.                              
--under-replicated-partitions            if set when describing topics, only    
                                           show under replicated partitions     
--version                                Display Kafka version.                 
--zookeeper <String: hosts>              DEPRECATED, The connection string for  
                                           the zookeeper connection in the form 
                                           host:port. Multiple hosts can be     
                                           given to allow fail-over.            
{noformat}",,githubbot,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 14:24:49 UTC 2020,,,,,,,,,,"0|z0cdkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/20 16:49;githubbot;tombentley commented on pull request #8317: KAFKA-9691: Fix NPE by waiting for reassignment request
URL: https://github.com/apache/kafka/pull/8317
 
 
   It seems likely the NPE reported in KAFKA-9691 was due to the call to `alterPartitionReassignments()` returning but the reassignment request not being completed yet, so try to fix it by calling `get()` on the returned `KafkaFuture`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Apr/20 14:24;githubbot;mimaison commented on pull request #8317: KAFKA-9691: Fix NPE by waiting for reassignment request
URL: https://github.com/apache/kafka/pull/8317
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockConsumer#endOffsets should be idempotent,KAFKA-9686,13290506,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,09/Mar/20 11:21,10/Mar/20 17:45,13/Jul/23 09:17,10/Mar/20 16:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"
{code:java}
    private Long getEndOffset(List<Long> offsets) {
        if (offsets == null || offsets.isEmpty()) {
            return null;
        }
        return offsets.size() > 1 ? offsets.remove(0) : offsets.get(0);
    }
{code}

The above code has two issues.
1. It does not return the latest offset since the latest offset is at the end of offsets
1. It removes the element from offsets so MockConsumer#endOffsets gets non-idempotent
",,chia7712,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9682,,,,,,,,,,,,,KAFKA-9682,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 17:45:57 UTC 2020,,,,,,,,,,"0|z0cbnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 11:36;githubbot;chia7712 commented on pull request #8255: KAFKA-9686 MockConsumer#endOffsets should be idempotent
URL: https://github.com/apache/kafka/pull/8255
 
 
   ```scala
       private Long getEndOffset(List<Long> offsets) {
           if (offsets == null || offsets.isEmpty()) {
               return null;
           }
           return offsets.size() > 1 ? offsets.remove(0) : offsets.get(0);
       }
   ```
   The above code has two issues.
   1. It does not return the latest offset since the latest offset is at the end of offsets
   1. It removes the element from offsets so MockConsumer#endOffsets gets non-idempotent
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Mar/20 15:43;githubbot;kkonstantine commented on pull request #8255: KAFKA-9686 MockConsumer#endOffsets should be idempotent
URL: https://github.com/apache/kafka/pull/8255
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Mar/20 17:40;kkonstantine;As a summary here, the PR that was merged for this issue, mainly addressed a bug in a refactoring that was part of [https://github.com/apache/kafka/pull/8220] and was included as additional cleanup for https://issues.apache.org/jira/browse/KAFKA-9645 but was not strictly required by  https://issues.apache.org/jira/browse/KAFKA-9645

 

The fix ended up being a simplification of {{MockConsumer#updateEndOffsets}} and the member variable {{MockConsumer#endOffsets}}. The method {{updateEndOffsets}} now always overwrites any existing end offsets with the ones supplied to this method. 

The fix for this Jira ticket here is also a fix for https://issues.apache.org/jira/browse/KAFKA-9682 that corresponds to the test that broke from the initial refactoring. ;;;","10/Mar/20 17:45;chia7712;[~kkonstantine] thanks for the great summary!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test KafkaBasedLogTest#testSendAndReadToEnd,KAFKA-9682,13290336,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,mjsax,mjsax,08/Mar/20 00:11,10/Mar/20 16:41,13/Jul/23 09:17,10/Mar/20 16:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KafkaConnect,unit tests,,,,0,,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1048/testReport/org.apache.kafka.connect.util/KafkaBasedLogTest/testSendAndReadToEnd/]
{quote}java.lang.AssertionError: expected:<2> but was:<0> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:647) at org.junit.Assert.assertEquals(Assert.java:633) at org.apache.kafka.connect.util.KafkaBasedLogTest.testSendAndReadToEnd(KafkaBasedLogTest.java:355){quote}",,chia7712,hachikuji,kkonstantine,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9686,,,,,,,,,,,,,,,,,,KAFKA-9686,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 16:41:28 UTC 2020,,,,,,,,,,"0|z0cals:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/20 03:24;hachikuji;This fails 100% of the time locally for me. cc [~rhauch] [~kkonstantine];;;","09/Mar/20 16:38;chia7712;KAFKA-9686 is able to fix this issue.;;;","10/Mar/20 02:59;kkonstantine;This test case broke with the consolidation of {{addEndOffsets}} and {{updateEndOffsets}} into a single {{updateEndOffsets}} in: [https://github.com/apache/kafka/pull/8220]

Before this PR, {{addEndOffsets}} was unused. 
 However, instead of consolidating {{innerUpdateEndOffsets}} that was called by both methods to become what the public {{updateEndOffsets}} was doing, it was refactored in a way that was actually executed what the unused {{addEndOffsets}} was performing. 

Refactoring should have resulted into decommissioning {{addEndOffsets}} and keeping the original  {{updateEndOffsets}}, not the other way round. ;;;","10/Mar/20 05:44;mjsax;Do you mean the test is permanently broken?

Have two more failures:

[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5069/testReport/org.apache.kafka.connect.util/KafkaBasedLogTest/testSendAndReadToEnd/]

and

[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1050/testReport/org.apache.kafka.connect.util/KafkaBasedLogTest/testSendAndReadToEnd/];;;","10/Mar/20 06:03;chia7712;I checkout to https://github.com/apache/kafka/commit/fe0b704285ebc916ce5080a5248d91b4dc3c60e0 and then run test ""KafkaBasedLogTest#testSendAndReadToEnd"". It never pass.

BTW, KafkaBasedLogTest#testSendAndReadToEnd pass if fe0b704285ebc916ce5080a5248d91b4dc3c60e0 is reverted ;;;","10/Mar/20 08:24;mjsax;Thanks for clarification!;;;","10/Mar/20 16:41;mjsax;Closing this as ""fixed by"" KAFKA-9686;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Low consume bandwidth quota may cause consumer not being able to fetch data,KAFKA-9677,13290223,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,06/Mar/20 23:22,19/Mar/20 16:28,13/Jul/23 09:17,19/Mar/20 16:28,2.0.1,2.1.1,2.2.2,2.3.1,2.4.0,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,core,,,,,0,,,,,"When we changed quota communication with KIP-219, fetch requests get throttled by returning empty response with the delay in `throttle_time_ms` and Kafka consumer retrying again after the delay. 

With default configs, the maximum fetch size could be as big as 50MB (or 10MB per partition). The default broker config (1-second window, 10 full windows of tracked bandwidth/thread utilization usage) means that < 5MB/s consumer quota (per broker) may stop fetch request from ever being successful.

Or the other way around: 1 MB/s consumer quota (per broker) means that any fetch request that gets >= 10MB of data (10 seconds * 1MB/second) in the response will never get through. From consumer point of view, the behavior will be: Consumer will get an empty response with throttle_time_ms > 0, Kafka consumer will wait for throttle time delay and then send fetch request again, the fetch response is still too big so broker sends another empty response with throttle time, and so on in never ending loop
h3. Proposed fix

Return less data in fetch response in this case: Cap `fetchMaxBytes` passed to replicaManager.fetchMessages() from KafkaApis.handleFetchRequest() to <tracking window> * <consume bandwidth quota>. In the example of default configs and 1MB/s consumer bandwidth quota, fetchMaxBytes will be 10MB.",,apovzner,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 20:45:55 UTC 2020,,,,,,,,,,"0|z0c9wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/20 01:31;junrao;[~apovzner] : Thanks for finding this issue! I agree with your suggested fix by capping fetchMaxBytes.;;;","12/Mar/20 23:36;githubbot;apovzner commented on pull request #8290: KAFKA-9677: Fix consumer fetch with small consume bandwidth quotas
URL: https://github.com/apache/kafka/pull/8290
 
 
   When we changed quota communication with KIP-219, fetch requests get throttled by returning empty response with the delay in `throttle_time_ms` and Kafka consumer retries again after the delay. With default configs, the maximum fetch size could be as big as 50MB (or 10MB per partition). The default broker config (1-second window, 10 full windows of tracked bandwidth/thread utilization usage) means that < 5MB/s consumer quota (per broker) may block consumers from being able to fetch any data.
   
   This PR ensures that consumers cannot get blocked by quota by capping `fetchMaxBytes` in KafkaApis.handleFetchRequest() to <tracking window> * <consume bandwidth quota>. In the example of default configs (10-second quota window) and 1MB/s consumer bandwidth quota, fetchMaxBytes would be capped to 10MB.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Mar/20 20:45;githubbot;rajinisivaram commented on pull request #8290: KAFKA-9677: Fix consumer fetch with small consume bandwidth quotas
URL: https://github.com/apache/kafka/pull/8290
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB metrics reported always at zero,KAFKA-9675,13290161,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,cadonna,mviamari,mviamari,06/Mar/20 18:30,09/Mar/21 09:13,13/Jul/23 09:17,13/Mar/20 12:22,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,streams,,,,,0,,,,,"The rocksdb metrics listed under {{stream-state-metrics}} are reported as zero for all metrics and all rocksdb instances. The metrics are present in JMX, but are always zero.

The streams state is configured with {{MetricsRecordingLevel}} at {{debug}}. I am able to see metrics with appropriate values in the {{stream-rocksdb-window-state-metrics}}, {{stream-record-cache-metrics}}, {{stream-task-metrics}}, and {{stream-processor-node-metrics}}.

Additionally, my DEBUG logs show the appropriate messages for recording events, i.e.

{{org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder [RocksDB Metrics Recorder for agg-store] Recording metrics for store agg-store}}

It happens that all of my rocksdb instances are windowed stores, not key value stores, so I haven't been able to check if this issue is unique to windowed stores.",,ableegoldman,giladam,githubbot,mjsax,mviamari,savulchik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 18:51:39 UTC 2020,,,,,,,,,,"0|z0c9iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 13:18;githubbot;cadonna commented on pull request #8256: KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated
URL: https://github.com/apache/kafka/pull/8256
 
 
   The root cause of the bug is that the statistics object is passed to the RocksDB options after the database is opened. Apparently, the options object is copied during the opening process. The solution is to pass the statistics object before the database is opened.
   
   Additionally, I added some unit tests to check if the Kafka Streams' metrics are updated when the measurements in RocksDB's statistics object are updated.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 18:51;githubbot;vvcephei commented on pull request #8256: KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated
URL: https://github.com/apache/kafka/pull/8256
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task corruption should also close the producer if necessary,KAFKA-9674,13290146,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,06/Mar/20 17:41,10/Mar/20 17:51,13/Jul/23 09:17,10/Mar/20 17:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"The task revive call only transits the task to CREATED mode. It should handle the recreation of task producer as well.

Sequence is like:
 # Task hits out of range exception and throws CorruptedException
 # Task producer closed along with the task
 # Task revived and rebalance triggered
 # Task was assigned back to the same thread
 # Trying to use task producer will throw as it has already been closed.

The full log:

 

[2020-03-03T21:56:29-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:29,070] WARN [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] stream-thread [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] Encountered org.apache.kafka.clients.consumer.OffsetOutOfRangeException fetching records from restore consumer for partitions [stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-0], it is likely that the consumer's position has fallen out of the topic partition offset range because the topic was truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializing it later. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)

[2020-03-03T21:56:29-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:29,071] WARN [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] stream-thread [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] Detected the states of tasks \{1_0=[stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-0]} are corrupted. Will close the task as dirty and re-create and bootstrap from scratch. (org.apache.kafka.streams.processor.internals.StreamThread)

 

[2020-03-03T21:56:30-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:30,010] INFO [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] [Producer clientId=stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3-1_0-producer, transactionalId=stream-soak-test-1_0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)

 

 

[2020-03-03T21:56:30-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:30,017] INFO [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] stream-thread [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] task [1_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)

 

 

[2020-03-03T21:56:22-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:22,827] INFO [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] [Producer clientId=stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3-1_0-producer, transactionalId=stream-soak-test-1_0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)

[2020-03-03T21:56:22-08:00] (streams-soak-trunk-eos_soak_i-0eaa3f3a6a197f876_streamslog) [2020-03-04 05:56:22,829] INFO [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] stream-thread [stream-soak-test-93df69e6-1d85-4b6a-81a1-c6d554693e3f-StreamThread-3] task [1_0] Closed dirty (org.apache.kafka.streams.processor.internals.StreamTask)

 ",,ableegoldman,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 18:37:21 UTC 2020,,,,,,,,,,"0|z0c9fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 18:46;githubbot;abbccdda commented on pull request #8242: KAFKA-9674: corruption should also cleanup producer and recreate
URL: https://github.com/apache/kafka/pull/8242
 
 
   The task producer cleanup doesn't involve handling of task corruption. Adding recreation of task producer to avoid reusing a fatal state producer in next cycle.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Mar/20 18:37;githubbot;abbccdda commented on pull request #8242: KAFKA-9674: corruption should also cleanup producer and recreate
URL: https://github.com/apache/kafka/pull/8242
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead brokers in ISR cause isr-expiration to fail with exception,KAFKA-9672,13290048,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,ivanyu,ivanyu,06/Mar/20 12:12,20/Feb/21 01:08,13/Jul/23 09:17,20/Feb/21 01:08,2.4.0,2.4.1,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,core,,,,,0,,,,,"We're running Kafka 2.4 and facing a pretty strange situation.
 Let's say there were three brokers in the cluster 0, 1, and 2. Then:
 1. Broker 3 was added.
 2. Partitions were reassigned from broker 0 to broker 3.
 3. Broker 0 was shut down (not gracefully) and removed from the cluster.
 4. We see the following state in ZooKeeper:
{code:java}
ls /brokers/ids
[1, 2, 3]

get /brokers/topics/foo
{""version"":2,""partitions"":{""0"":[2,1,3]},""adding_replicas"":{},""removing_replicas"":{}}

get /brokers/topics/foo/partitions/0/state
{""controller_epoch"":123,""leader"":1,""version"":1,""leader_epoch"":42,""isr"":[0,2,3,1]}
{code}
It means, the dead broker 0 remains in the partitions's ISR. A big share of the partitions in the cluster have this issue.

This is actually causing an errors:
{code:java}
Uncaught exception in scheduled task 'isr-expiration' (kafka.utils.KafkaScheduler)
org.apache.kafka.common.errors.ReplicaNotAvailableException: Replica with id 12 is not available on broker 17
{code}
It means that effectively {{isr-expiration}} task is not working any more.

I have a suspicion that this was introduced by [this commit (line selected)|https://github.com/apache/kafka/commit/57baa4079d9fc14103411f790b9a025c9f2146a4#diff-5450baca03f57b9f2030f93a480e6969R856]

Unfortunately, I haven't been able to reproduce this in isolation.

Any hints about how to reproduce (so I can write a patch) or mitigate the issue on a running cluster are welcome.

Generally, I assume that not throwing {{ReplicaNotAvailableException}} on a dead (i.e. non-existent) broker, considering them out-of-sync and removing from the ISR should fix the problem.

 ",,ivanyu,jagsancio,juha.mynttinen,junrao,rndgstn,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 20 01:08:29 UTC 2021,,,,,,,,,,"0|z0c8ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 20:09;jagsancio;I was not able to reproduce this issue but looking at the code and the trace of messages sent by the controller this is what I think it is happening.

Assuming that the initial partition assignment and state is:
{code:java}
Replicas: 0, 1, 2
ISR: 0, 1, 2
Leader: 0
LeaderEpoch: 1{code}
This state is replicated to all of the replicas (0, 1, 2) using the LeaderAndIsr requests.

When the user attempts to perform a reassignment of replacing 0 with 3, the controller bumps the epoch and assignment info
{code:java}
Replicas: 0, 1, 2, 3
Adding: 3
Removing: 0
ISR: 0, 1, 2
Leader: 0
LeaderEpoch: 2{code}
This state is replicated to all of the replicas (0, 1, 2, 3) using the LeaderAndIsr request.

The system roughly stays in this state until the all of the target replicas have join the ISR. When all of the target replicas have join the ISR the controller wants to perform the following flow:

1 - The controller moves the leader if necessary (leader is not in the new replicas set) and stops the leader from letting ""removing"" replicas to join the ISR.

The second requirement (stopping the leader from adding ""removing"" replicas to the ISR) is accomplished by bumping the leader epoch and only sending the new leader epoch to the target replicas (1, 2, 3). Unfortunately, due to how the controller is implemented this is accomplished by deleting the ""removing"" replicas from the in memory state without modifying the ISR state. At this point we have the ZK state:
{code:java}
Replicas: 0, 1, 2, 3
Adding:
Removing: 0
ISR: 0, 1, 2, 3
Leader: 1
LeaderEpoch: 3{code}
but the following LeaderAndIsr requests are sent to replicas 1, 2, 3
{code:java}
Replicas: 1, 2, 3
Adding:
Removing:
ISR: 0, 1, 2, 3
Leader: 1
LeaderEpoch: 3{code}
This works because replica 0 will have an invalid leader epoch which means that it's Fetch request will be ignored by the (new) leader.

2 - The controller removes replica 0 from the ISR by updating ZK and sending the appropriate LeaderAndIsr requests.

3 - The controller removes replica 0 from the replica set by updating ZK and sending the appropriate LeaderAndIsr requests.

 

Conclusion

If this flow executes to completion, everything is okay. The problem is what happens if step 2. and 3. don't get to execute. I am unable to reproduce this with tests or by walking the code but if 2. and 3. don't execute but the controller stays alive there is a flow where the controller persists the following state to ZK
{code:java}
Replicas: 1, 2, 3
Adding:
Removing:
ISR: 0, 1, 2, 3
Leader: 1
LeaderEpoch: 3{code}
Which causes the reassignment flow to terminate with the system staying in this state. This state is persistent at this line in the controller code:

https://github.com/apache/kafka/blob/43fd630d80332f2b3b3512a712100825a8417704/core/src/main/scala/kafka/controller/KafkaController.scala#L728;;;","20/Nov/20 20:13;jagsancio;Based on my observations here: https://issues.apache.org/jira/browse/KAFKA-9672?focusedCommentId=17236416&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17236416

Solution 1: I think the ideal solution is to never allow the ISR to be a superset of the replica set. Unfortunately, this is not easy to with how the controller implementation manages writes to ZK.

Solution 2: Another solution is to allow the ISR to be a superset of the replica set but also allow the Leader to remove replicas from the ISR if they are not in the replica set.;;;","20/Feb/21 01:08;junrao;merged the PR to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka 2.4.0 Chokes on Filebeat 5.6 Produced Data,KAFKA-9669,13289957,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,weichu,weichu,06/Mar/20 05:27,12/May/20 19:08,13/Jul/23 09:17,12/May/20 19:08,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,,,,,,,,,,,,0,,,,,"Hi

In our environment, after upgrading to Kafka 2.4.0, we discovered the broker was not compatible with filebeat 5.

Here is how to reproduce:

1. Startup Kafka 2.4.0, all configurations are vanilla:

{code}
$ kafka_2.13-2.4.0/bin/zookeeper-server-start.sh kafka_2.13-2.4.0/config/zookeeper.properties
$ kafka_2.13-2.4.0/bin/kafka-server-start.sh kafka_2.13-2.4.0/config/server.properties
{code}

2. Startup filebeat 5.6.16 with the following configuration. (downloaded from https://www.elastic.co/jp/downloads/past-releases/filebeat-5-6-16)

{code}
$ cat /tmp/filebeat.yml
name: test

output.kafka:
  enabled: true
  hosts:
    - localhost:9092
  topic: test-3
  version: 0.10.0
  compression: gzip

filebeat:
  prospectors:
    - input_type: log
      paths:
        - /tmp/filebeat-in
      encoding: plain
{code}

{code}
$ filebeat-5.6.16-linux-x86_64/filebeat -e -c /tmp/filebeat.yml
{code}

3. Write some lines to file {{/tmp/filebeat-in}}. Looks like single line won't trigger the issue, but 30 lines are enough.

{code}
seq 30 >> /tmp/filebeat-in
{code}

4. Kafka throws the following error chunk, like, per produced record.

{noformat}
[2020-03-06 05:17:40,129] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=1453875406, CreateTime=1583471854475, key=0 bytes, value=202 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 05:17:40,129] ERROR [KafkaApi-0] Error when handling request: clientId=beats, correlationId=102, api=PRODUCE, version=2, body={acks=1,timeout=10000,partitionSizes=[test-3-0=272]} (kafka.server.KafkaApis)
java.lang.NullPointerException: `field` must be non-null
	at java.base/java.util.Objects.requireNonNull(Objects.java:246)
	at org.apache.kafka.common.protocol.types.Struct.validateField(Struct.java:474)
	at org.apache.kafka.common.protocol.types.Struct.instance(Struct.java:418)
	at org.apache.kafka.common.protocol.types.Struct.instance(Struct.java:436)
	at org.apache.kafka.common.requests.ProduceResponse.toStruct(ProduceResponse.java:281)
	at org.apache.kafka.common.requests.AbstractResponse.toSend(AbstractResponse.java:35)
	at org.apache.kafka.common.requests.RequestContext.buildResponse(RequestContext.java:80)
	at kafka.server.KafkaApis.sendResponse(KafkaApis.scala:2892)
	at kafka.server.KafkaApis.sendResponseCallback$2(KafkaApis.scala:554)
	at kafka.server.KafkaApis.$anonfun$handleProduceRequest$11(KafkaApis.scala:576)
	at kafka.server.KafkaApis.$anonfun$handleProduceRequest$11$adapted(KafkaApis.scala:576)
	at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:546)
	at kafka.server.KafkaApis.handleProduceRequest(KafkaApis.scala:577)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:126)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:835)
{noformat}

",,hachikuji,ijuma,weichu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 23:52:34 UTC 2020,,,,,,,,,,"0|z0c89k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/20 06:20;ijuma;Can you please try 2.4.1 RC0? https://home.apache.org/~bbejeck/kafka-2.4.1-rc0/;;;","06/Mar/20 06:40;weichu;-With 2.4.1 RC0, the Kafka does provider prettier error logs, but still cannot produce:-

Update:
Sorry for the misleading, so I double checked the result. Despite for the ERROR log, the messages were actually stored to the topic.
On the other hand, throwing one ERROR level message per produced record is kinda not acceptable for production usage, because the Kafka log would flood out the disk.

{noformat}
[2020-03-06 06:36:37,159] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=1478844555, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,177] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=4137384030, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,181] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=4137384030, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
[2020-03-06 06:36:37,194] ERROR [ReplicaManager broker=0] Error processing append operation on partition test-3-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.InvalidRecordException: Inner record LegacyRecordBatch(offset=0, Record(magic=1, attributes=0, compression=NONE, crc=3885590418, CreateTime=1583476596257, key=0 bytes, value=199 bytes)) inside the compressed record batch does not have incremental offsets, expected offset is 1 in topic partition test-3-0.
{noformat};;;","11/May/20 23:52;hachikuji;I think this is the result of stricter validation on the broker from KAFKA-8106. The ""inner"" offsets in v1 messages are expected to increase sequentially, but previously we would silently ignore this and rewrite the batch. In 2.4, this become a validation error, which broke compatibility for older clients.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException,KAFKA-9668,13289913,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,BigAndy,BigAndy,BigAndy,05/Mar/20 21:35,06/Mar/20 21:09,13/Jul/23 09:17,06/Mar/20 21:09,0.10.1.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,streams,,,,,0,,,,,"`KafkaStreams.getAllMetadata()` returns `StreamsMetadataState.getAllMetadata()`. All the latter methods is `synchronized` it returns a reference to internal mutable state.  Not only does this break encapsulation, but it means any thread iterating over the returned collection when the metadata gets rebuilt will encounter a `ConcurrentModificationException`.",,ableegoldman,BigAndy,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 21:09:07 UTC 2020,,,,,,,,,,"0|z0c7zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 22:25;githubbot;big-andy-coates commented on pull request #8233: KAFKA-9668: Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException
URL: https://github.com/apache/kafka/pull/8233
 
 
   Fixes [KAFKA-9668](https://issues.apache.org/jira/browse/KAFKA-9668)
   
   `KafkaStreams.getAllMetadata()` returns `StreamsMetadataState.getAllMetadata()`. All the latter methods is `synchronized` it returns a reference to internal mutable state.  Not only does this break encapsulation, but it means any thread iterating over the returned collection when the metadata gets rebuilt will encounter a `ConcurrentModificationException`.
   
   This change:
    * switches from clearing and rebuild `allMetadata` when `onChange` is called to building a new list and swapping this in. This is thread safe and has the benefit that the returned list is not empty during a rebuild: you either get the old or the new list.
    * removes synchronisation from `getAllMetadata` and `getLocalMetadata`. These are returning member variables. Synchronisation adds nothing.
    * changes `getAllMetadata` to wrap its return value in an unmodifiable wrapper to avoid breaking encapsulation.
    * changes the getters in `StreamsMetadata` to wrap their return values in unmodifiable wrapper to avoid breaking encapsulation.
   
   Unit tests have been added to cover both changes classes to ensure encapsulation and thread-safety are maintained.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Mar/20 22:25;BigAndy;Patch available: [https://github.com/apache/kafka/pull/8233];;;","06/Mar/20 21:09;githubbot;guozhangwang commented on pull request #8233: KAFKA-9668: Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException
URL: https://github.com/apache/kafka/pull/8233
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect JSON serde strip trailing zeros,KAFKA-9667,13289876,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,BigAndy,BigAndy,BigAndy,05/Mar/20 17:32,07/May/20 22:43,13/Jul/23 09:17,07/May/20 22:43,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,KafkaConnect,,,,,0,,,,,"The Connect Json serde was recently enhanced to support serializing decimals as standard JSON numbers, (Original work done under -KAFKA-8595)-, e.g. `1.23`.  However, there is a bug in the implementation: it's stripping trailing zeros!  `1.23` is _not_ the same as `1.230`.  The first is a number accurate to 2 decimal places, where as the later is accurate to 3 dp.

It is important that trailing zeros are not dropped when de(serializing) decimals.  For some use-cases it may be acceptable to drop the trailing zeros, but for others it definitely is not.
h3. Current Functionality

If a JSON object was to contain the number `1.230` then the Java JsonDeserializer would correctly deserialize this into a `BigDecimal`. The BigDecimal would have a scale of 3, which is correct.

However, if that same BigDecimal was then serialized back to JSON using the Java JsonSerializer it would incorrectly strip the zeros, serializing to `1.23`. 
h3. Expected Functionality

When serializing, trailing zeros should be maintained.  For example, a BigDecimal such as `1.230`, (3 dp), should be serialized as `1.230`. 
h3. Compatibility

With this bug fix any computation on decimal values that resulted in a number with trailing zeros, would see the number correctly serialized with those trailing zeros.

Both the old serialized number, e.g. `1.23`, and the proposed corrected serialized number, e.g. `1.230`, are valid JSON numbers. Downstream consumers should have no issue deserializing either.  

 

 ",,BigAndy,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 22:43:01 UTC 2020,,,,,,,,,,"0|z0c7rk:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"05/Mar/20 17:40;githubbot;big-andy-coates commented on pull request #8230: KAFKA-9667: Connect JSON serde strip trailing zeros
URL: https://github.com/apache/kafka/pull/8230
 
 
   This change turns on exact decimal processing in Jackson for decimals, meaning trailing zeros are maintained. This means a value of `1.2300` can be deserialized and re-serialized to JSON without any loss of information.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Mar/20 18:53;rhauch;Thanks, [~BigAndy]. We are currently frozen on the 2.4 and 2.5 branches as we're in the final stages of releases (2.4.1 and 2.5.0, respectively). This does not appear to warrant blockers on those releases, so my plan is to merge to the `trunk`, `2.4`, and `2.5` branches after we are no longer frozen on `2.4` and `2.5` branches.;;;","05/Mar/20 22:24;BigAndy;Patch available:  [https://github.com/apache/kafka/pull/8230];;;","05/Mar/20 22:26;BigAndy;Sounds good [~rhauch]. Thanks.

If anything prompts new release candidates being made from those point releases, then it would be handy to get this change in. But no biggie if that's not possible.;;;","07/May/20 22:43;rhauch;Merged to trunk and backported to 2.4.x (for inclusion in 2.4.2) and 2.5.x (for inclusion in 2.5.1).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transactional producer Epoch could not be reset,KAFKA-9666,13289867,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,bob-barrett,bchen225242,bchen225242,05/Mar/20 16:59,15/Jul/20 21:24,13/Jul/23 09:17,15/Jul/20 21:24,,,,,,,,,,,,,,,,,,,,,,,2.6.1,2.7.0,,,,,,,,,,,,0,,,,,"As of today, the producer epoch keeps increasing until it hits Short.Max. The correct behavior at this point should be making another call to re-initialize a new PID, otherwise trying with Short.Max will throw fatal exception which eventually kills the producer.

Stream log:
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_soak_i-00d70680c58afa228_streamslog) [2020-03-05 04:25:41,147] ERROR [stream-soak-test-689bb912-b06c-4c42-88e5-d9578f7ebfdf-StreamThread-3] Thread    StreamsThread threadId: stream-soak-test-689bb912-b06c-4c42-88e5-d9578f7ebfdf-StreamThread-3
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_soak_i-00d70680c58afa228_streamslog) TaskManager
        MetadataState:
                GlobalMetadata: []
                GlobalStores: []
                My HostInfo: HostInfo\{host='unknown', port=-1}
                Cluster(id = null, nodes = [], partitions = [], controller = null)
        Active tasks:
                Running:
                Running Partitions:
                New:
                Restoring:
                Restoring Partitions:
                Restored Partitions:
                Suspended:
        Standby tasks:
                Running:
                Running Partitions:
                New:
 encountered an error processing soak test (org.apache.kafka.streams.StreamsSoakTest)
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_soak_i-00d70680c58afa228_streamslog) org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-689bb912-b06c-4c42-88e5-d9578f7ebfdf-StreamThread-3] Failed to rebalance.
        at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:862)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:749)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:697)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:670)
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_soak_i-00d70680c58afa228_streamslog) Caused by: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The server experienced an unexpected error when processing the request.
        at org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1352)
        at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1260)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:571)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:563)
        at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:414)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:312)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
        at java.lang.Thread.run(Thread.java:748)
 

 

Producer log:
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_broker_i-0d1ef6c9a1a1708f6_server-log) [2020-03-05 04:25:40,885] INFO [Transaction State Manager 1001]: TransactionalId stream-soak-test-1_0 append transaction log for TxnTransitMetadata(producerId=0, producerEpoch=576, txnTimeoutMs=60000, txnState=Ongoing, topicPartitions=Set(stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000013-changelog-0, stream-soak-test-logData10MinuteFinalCount-store-changelog-0, stream-soak-test-logData10MinuteSuppressedCount-store-changelog-0, stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog-0, stream-soak-test-windowed-node-counts-STATE-STORE-0000000030-changelog-0, stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000025-changelog-0, stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000007-changelog-0, windowed-node-counts-0), txnStartTimestamp=1583382340885, txnLastUpdateTimestamp=1583382340885) transition failed due to COORDINATOR_NOT_AVAILABLE, resetting pending state from Some(Ongoing), aborting state transition and returning COORDINATOR_NOT_AVAILABLE in the callback (kafka.coordinator.transaction.TransactionStateManager)
[2020-03-04T20:25:41-08:00] (streams-soak-2-5-eos_broker_i-0d1ef6c9a1a1708f6_stdout) java.lang.IllegalStateException: Cannot fence producer with epoch equal to Short.MaxValue since this would overflow
        at kafka.coordinator.transaction.TransactionMetadata.prepareFenceProducerEpoch(TransactionMetadata.scala:194)
        at kafka.coordinator.transaction.TransactionCoordinator.kafka$coordinator$transaction$TransactionCoordinator$$prepareInitProduceIdTransit(TransactionCoordinator.scala:216)
        at kafka.coordinator.transaction.TransactionCoordinator$$anonfun$2$$anonfun$apply$1.apply(TransactionCoordinator.scala:143)
        at kafka.coordinator.transaction.TransactionCoordinator$$anonfun$2$$anonfun$apply$1.apply(TransactionCoordinator.scala:143)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
        at kafka.coordinator.transaction.TransactionMetadata.inLock(TransactionMetadata.scala:172)
        at kafka.coordinator.transaction.TransactionCoordinator$$anonfun$2.apply(TransactionCoordinator.scala:142)
        at kafka.coordinator.transaction.TransactionCoordinator$$anonfun$2.apply(TransactionCoordinator.scala:138)
        at scala.util.Either$RightProjection.flatMap(Either.scala:522)
        at kafka.coordinator.transaction.TransactionCoordinator.handleInitProducerId(TransactionCoordinator.scala:137)
        at kafka.server.KafkaApis.handleInitProducerIdRequest(KafkaApis.scala:1638)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:135)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
        at java.lang.Thread.run(Thread.java:748)",,bchen225242,githubbot,hachikuji,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 20:03:32 UTC 2020,,,,,,,,,,"0|z0c7pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 17:07;hachikuji;cc [~mumrah] We may need this for 2.5. I am not sure if this is a regression, but KIP-360 makes this case much more likely.;;;","06/Mar/20 16:55;githubbot;bob-barrett commented on pull request #8239: KAFKA-9666: Rotate producer ID when fencing producer with exhausted e…
URL: https://github.com/apache/kafka/pull/8239
 
 
   …poch
   
   When fencing producers, we currently blindly bump the epoch by 1 and write an abort marker to the transaction log. If the log is unavailable (for example, because the number of in-sync replicas is less than min.in.sync.replicas), we will roll back the attempted write of the abort marker, but still increment the epoch in the transaction metadata cache. During periods of prolonged log unavailability, producer retires of InitProducerId calls can cause the epoch to be increased to the point of exhaustion, at which point further InitProducerId calls fail because the producer can no longer be fenced. This patch changes the fencing behavior to rotate the producer ID if the epoch is exhausted, preventing this situation.
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Mar/20 16:56;hachikuji;This seems not to be a regression. This was encountered on an older version of the broker (2.0 to be specific). I've changed priority from ""blocker"" to ""critical."";;;","24/Jun/20 20:03;rhauch;Since this is not a blocker issue, as part of the 2.6.0 release process I'm changing the fix version to `2.7.0`. If this is incorrect, please respond and discuss on the ""[DISCUSS] Apache Kafka 2.6.0 release"" discussion mailing list thread.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaStreams.metadataForKey, queryMetadataForKey docs don't mention null",KAFKA-9663,13289794,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,05/Mar/20 11:57,07/Mar/20 07:47,13/Jul/23 09:17,07/Mar/20 07:47,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,The Javadoc for {{KafkaStreams.metadataForKey}} and {{KafkaStreams.queryMetadataForKey}} don't document the possible null return value.,,ableegoldman,alesj,githubbot,mjsax,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 07:47:37 UTC 2020,,,,,,,,,,"0|z0c79c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 12:00;githubbot;tombentley commented on pull request #8228: KAFKA-9663: Doc some null returns in KafkaStreams
URL: https://github.com/apache/kafka/pull/8228
 
 
   Documents the possibility of null return value in KafkaStreams.getMetadataForKey(), and KafkaStreams.queryMetadataForKey().
   
   This fixes KAFKA-9663.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Mar/20 12:01;tombentley;https://github.com/apache/kafka/pull/8228;;;","05/Mar/20 18:07;githubbot;tombentley commented on pull request #8228: KAFKA-9663: Doc some null returns in KafkaStreams
URL: https://github.com/apache/kafka/pull/8228
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Mar/20 18:10;githubbot;tombentley commented on pull request #8228: KAFKA-9663: Doc some null returns in KafkaStreams
URL: https://github.com/apache/kafka/pull/8228
 
 
   Documents the possibility of null return value in KafkaStreams.getMetadataForKey(), and KafkaStreams.queryMetadataForKey().
   
   This fixes KAFKA-9663.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Mar/20 07:47;githubbot;mjsax commented on pull request #8228: KAFKA-9663: Doc some null returns in KafkaStreams
URL: https://github.com/apache/kafka/pull/8228
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throttling system test fails when messages are produced before consumer starts up,KAFKA-9662,13289783,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,05/Mar/20 10:57,06/Mar/20 20:08,13/Jul/23 09:17,06/Mar/20 20:08,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,system tests,,,,,0,,,,,"The tests produces large records using producer performance tool and then starts another validating produce/consume loop for integer records. Consumer starts consuming from the latest offset to avoid consuming the large records produced earlier by the first producer. If the second producer starts producing records before the consumer has reset its offset to latest offset, then the consumer misses some records produced and the test fails:

{quote}

AssertionError: 762 acked message did not make it to the Consumer. They are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19...plus 742 more. Total Acked: 174330, Total Consumed: 173568. We validated that the first 762 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer.

{quote}",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 19:50:37 UTC 2020,,,,,,,,,,"0|z0c76w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 11:53;githubbot;rajinisivaram commented on pull request #8227: KAFKA-9662: Wait for consumer offset reset in throttle test to avoid losing early messages
URL: https://github.com/apache/kafka/pull/8227
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Mar/20 19:50;githubbot;mumrah commented on pull request #8227: KAFKA-9662: Wait for consumer offset reset in throttle test to avoid losing early messages
URL: https://github.com/apache/kafka/pull/8227
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config synonyms are no longer included in kafka-configs --describe output,KAFKA-9661,13289775,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,05/Mar/20 10:31,05/Mar/20 19:43,13/Jul/23 09:17,05/Mar/20 19:43,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,core,,,,,0,,,,,[https://github.com/apache/kafka/commit/7f35a6713434dd7f2ccd3897aef825d34582beab] accidentally removed `includeSynonyms` from describeConfigs option passed to AdminClient.,,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 16:46:42 UTC 2020,,,,,,,,,,"0|z0c754:",9223372036854775807,,mumrah,,,,,,,,,,,,,,,,,,"05/Mar/20 12:38;githubbot;rajinisivaram commented on pull request #8229: KAFKA-9661: Propagate includeSynonyms option to AdminClient in ConfigCommand
URL: https://github.com/apache/kafka/pull/8229
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Mar/20 16:46;githubbot;mumrah commented on pull request #8229: KAFKA-9661: Propagate includeSynonyms option to AdminClient in ConfigCommand
URL: https://github.com/apache/kafka/pull/8229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kafka Streams / Consumer configured for static membership fails on ""fatal exception: group.instance.id gets fenced""",KAFKA-9659,13289700,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,rohanpd,rohanpd,05/Mar/20 04:04,10/Aug/20 18:44,13/Jul/23 09:17,10/Aug/20 18:44,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"I'm running a KSQL query, which underneath is built into a Kafka Streams application. The application has been running without issue for a few days, until today, when all the streams threads exited with: 

 

 

{{[ERROR] 2020-03-05 00:57:58,776 [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] org.apache.kafka.clients.consumer.internals.AbstractCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5] Received fatal exception: group.instance.id gets fenced}}

{{[ERROR] 2020-03-05 00:57:58,776 [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] org.apache.kafka.clients.consumer.internals.AbstractCoordinator onFailure - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5] Caught fenced group.instance.id Optional[ksql-1-2] error in heartbeat thread}}

{{[ERROR] 2020-03-05 00:57:58,776 [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] org.apache.kafka.streams.processor.internals.StreamThread run - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors:}}
 \{{ org.apache.kafka.common.errors.FencedInstanceIdException: The broker rejected this static consumer since another consumer with the same group.instance.id has registered with a different member.id.}}{{[INFO] 2020-03-05 00:57:58,776 [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] org.apache.kafka.streams.processor.internals.StreamThread setState - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CSAS_RATINGS_WITH_USER_AVERAGE_5-39e8046a-b6e6-44fd-8d6d-37cff78649bf-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN}}

 

I've attached the KSQL and Kafka Streams logs to this ticket. Here's a summary for one of the streams threads (instance id `ksql-1-2`):

 

Around 00:56:36 the coordinator fails over from b11 to b2:

 

{{[INFO] 2020-03-05 00:56:36,258 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.AbstractCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Attempt to heartbeat failed since coordinator b11-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483636 rack: null) is either not started or not valid.}}
 {{ [INFO] 2020-03-05 00:56:36,258 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.AbstractCoordinator markCoordinatorUnknown - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Group coordinator b11-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483636 rack: null) is unavailable or invalid, will attempt rediscovery}}
 {{ [INFO] 2020-03-05 00:56:36,270 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.AbstractCoordinator onSuccess - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Discovered group coordinator b2-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483645 rack: null)}}

 

A few seconds later, offset commits start failing with an error indicating the new coordinator is initializing:

 

{{[WARN] 2020-03-05 00:56:39,048 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Offset commit failed on partition ksql-soak-ratings-json-19 at offset 1825760083: The coordinator is loading and hence can't process requests.}}

 

Looking at ksql-1-2, it looks like it keeps trying to commit on the same partition every half-second or so, which is the retry internal (retry.backoff.ms), so it's probably stuck in the consumer's retry loop, e.g:

{{[WARN] 2020-03-05 00:56:46,616 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Offset commit failed on partition ksql-soak-ratings-json-19 at offset 1825760083: The coordinator is loading and hence can't process requests.}}
{{[WARN] 2020-03-05 00:56:47,175 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Offset commit failed on partition ksql-soak-ratings-json-19 at offset 1825760083: The coordinator is loading and hence can't process requests.}}
{{[WARN] 2020-03-05 00:56:47,742 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Offset commit failed on partition ksql-soak-ratings-json-19 at offset 1825760083: The coordinator is loading and hence can't process requests.}}

 

This goes on until 00:56:50, when the offset commit requests fail because the member ID doesn't match the coordinator expects:

 

{{[ERROR] 2020-03-05 00:56:50,336 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Offset commit failed on partition ksql-soak-ratings-json-19 at offset 1825760083: The broker rejected this static consumer since another consumer with the same group.instance.id has registered with a different member.id.}}
 {{ [ERROR] 2020-03-05 00:56:50,336 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2] org.apache.kafka.clients.consumer.internals.ConsumerCoordinator handle - [Consumer instanceId=ksql-1-2, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-2-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Received fatal exception: group.instance.id gets fenced}}

 

So maybe the offset commit is stuck in a retry loop and not picking up the new member ID?",,ableegoldman,cadonna,githubbot,guozhang,lkokhreidze,mjsax,o0oxid,rohanpd,thebearmayor,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/20 04:17;rohanpd;ksql-1.logs;https://issues.apache.org/jira/secure/attachment/12995680/ksql-1.logs",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 08 11:38:03 UTC 2020,,,,,,,,,,"0|z0c6og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 04:37;rohanpd;Some of the streams threads died because the heartbeat failed (e.g. thread 1 of this query):

 

{{[INFO] 2020-03-05 00:56:36,339 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.internals.AbstractCoordinator handle - [Consumer instanceId=ksql-1-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Attempt to heartbeat failed since coordinator b11-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483636 rack: null) is either not started or not valid.}}
{{[INFO] 2020-03-05 00:56:36,339 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.internals.AbstractCoordinator markCoordinatorUnknown - [Consumer instanceId=ksql-1-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Group coordinator b11-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483636 rack: null) is unavailable or invalid, will attempt rediscovery}}
{{[INFO] 2020-03-05 00:56:36,390 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.internals.AbstractCoordinator onSuccess - [Consumer instanceId=ksql-1-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Discovered group coordinator b2-pkc-lzxjz.us-west-2.aws.devel.cpdev.cloud:9092 (id: 2147483645 rack: null)}}
{{[ERROR] 2020-03-05 00:56:51,857 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.internals.AbstractCoordinator handle - [Consumer instanceId=ksql-1-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Received fatal exception: group.instance.id gets fenced}}
{{[ERROR] 2020-03-05 00:56:51,858 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.internals.AbstractCoordinator onFailure - [Consumer instanceId=ksql-1-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-consumer, groupId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0] Caught fenced group.instance.id Optional[ksql-1-1] error in heartbeat thread}}
{{[ERROR] 2020-03-05 00:56:51,859 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.streams.processor.internals.StreamThread run - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors:}}
{{[INFO] 2020-03-05 00:56:51,859 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.streams.processor.internals.StreamThread setState - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN}}
{{[INFO] 2020-03-05 00:56:51,859 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.streams.processor.internals.StreamThread completeShutdown - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] Shutting down}}
{{[INFO] 2020-03-05 00:56:51,889 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.consumer.KafkaConsumer unsubscribe - [Consumer instanceId=ksql-1, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions}}
{{[INFO] 2020-03-05 00:56:51,889 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.}}
{{[INFO] 2020-03-05 00:56:51,932 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.streams.processor.internals.StreamThread setState - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD}}
{{[INFO] 2020-03-05 00:56:51,932 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] org.apache.kafka.streams.processor.internals.StreamThread completeShutdown - stream-thread [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] Shutdown complete}}
{{[ERROR] 2020-03-05 00:56:51,932 [_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1] io.confluent.ksql.engine.KsqlEngine uncaughtException - Unhandled exception caught in streams thread _confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-c1df9747-f353-47f1-82fd-30b97c20d038-StreamThread-1.}};;;","10/Mar/20 21:48;guozhang;Hi [~rohanpd] is the broker version the same as Streams (2.5.0-snapshot) here?;;;","10/Mar/20 22:20;guozhang;March 5th 2020, 00:55:28.517	ReplicaFetcherThread-0-2	[ReplicaFetcher replicaId=9, leaderId=2, fetcherId=0] Partition __consumer_offsets-35 has an older epoch (375) than the current leader. Will await the new LeaderAndIsr state before resuming fetching.	INFO	kafka-9	kafka.server.ReplicaFetcherThread

March 5th 2020, 00:55:28.517	ReplicaFetcherThread-0-2	[ReplicaFetcher replicaId=9, leaderId=2, fetcherId=0] Partition __consumer_offsets-35 marked as failed	WARN	kafka-9	kafka.server.ReplicaFetcherThread;;;","10/Mar/20 23:09;guozhang;My search in kibana has some good news and some bad news:

1) I can confirm that the blip on leaders cause the topic-partition __consumer_offsets-35, which stores this ksql consumer group, gets migrated from broker-2 to broker-11 around 00:55:22

March 5th 2020, 00:55:22.283	controller-event-thread	[Controller id=11 epoch=157] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=157, leader=11, leaderEpoch=374, isr=[11, 9, 1], zkVersion=639, replicas=[2, 9, 1, 11], observers=[], addingReplicas=[2], removingReplicas=[11], isNew=false) to broker 11 for partition __consumer_offsets-35	TRACE	kafka-11	state.change.logger

And then later gets back to broker-2 about three seconds later:

March 5th 2020, 00:55:25.380	controller-event-thread	[Controller id=11 epoch=157] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=157, leader=2, leaderEpoch=375, isr=[11, 9, 1, 2], zkVersion=641, replicas=[2, 9, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=false) to broker 2 for partition __consumer_offsets-35	TRACE	kafka-11	state.change.logger

The tricky part is that while broker-11 is becoming the new leader, it is also included in the `removingReplicas` which sounds quite weird, I suspect that although the new group generation is written with ack=all and the broker-2 is elected as a clean leader, the previous election of broker-11 was wonky.

2) From all the fenced entries, e.g.:

March 5th 2020, 00:56:50.599	data-plane-kafka-request-handler-3	given member.id ksql-0-3-70cb05ca-4687-4c1e-9c76-7a5a94da9961 is identified as a known static member ksql-0-3,but not matching the expected member.id ksql-0-3-db32cf5a-6d18-435b-bead-82fce848e239

I can find that the ""new"" member.id (ksql-0-3-70cb05ca-4687-4c1e-9c76-7a5a94da9961) was actually used in the past for committing for quite some time, for example:

March 4th 2020, 03:00:42.980	data-plane-kafka-network-thread-11-ListenerName(EXTERNAL)-SASL_SSL-8	Completed request:RequestHeader(apiKey=OFFSET_COMMIT, apiVersion=8, clientId=_confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0-d9c32901-eb60-4165-af3f-2c334ffea2b4-StreamThread-3-consumer, correlationId=7229283) -- {group_id=lkc-19kmj__confluent-ksql-pksqlc-xm6g1query_CTAS_RATINGS_BY_USER_0,generation_id=131,member_id=ksql-0-3-70cb05ca-4687-4c1e-9c76-7a5a94da9961,group_instance_id=ksql-0-3,topics=[{name=lkc-19kmj_ksql-soak-ratings-json,partitions=[{partition_index=2,committed_offset=1549665341,committed_leader_epoch=-1,committed_metadata=AQAAAXCjfPu8,_tagged_fields={}}],_tagged_fields={}}],_tagged_fields={}},response:{throttle_time_ms=0,topics=[{name=lkc-19kmj_ksql-soak-ratings-json,partitions=[{partition_index=2,error_code=0,_tagged_fields={}}],_tagged_fields={}}],_tagged_fields={}} from connection 100.109.28.58:9092-10.13.85.64:33212-567453;totalTime:5.011,requestQueueTime:0.13,localTime:0.402,remoteTime:4.351,throttleTime:0.048,responseQueueTime:0.021,sendTime:0.055,sendIoTime:0.044,securityProtocol:SASL_SSL,principal:MultiTenantPrincipal(tenantMetadata=TenantMetadata(tenantName='lkc-19kmj', clusterId='lkc-19kmj', allowDescribeBrokerConfigs=false, isSuperUser=false), user=67342),listener:EXTERNAL,clientInformation:ClientInformation(softwareName=apache-kafka-java, softwareVersion=5.5.0-ccs-SNAPSHOT)	INFO	kafka-11	kafka.request.logger

Where as the ""old"" member.id cannot be found used anywhere besides the above ERROR logs, and without more logs I cannot conclude where on earth this member.id was introduced to the broker's cache at all.;;;","11/Mar/20 00:58;githubbot;guozhangwang commented on pull request #8269: KAFKA-9659: Add more log4j when updating static member mappings
URL: https://github.com/apache/kafka/pull/8269
 
 
   We could update the mappings when:
   
   1) known static member joins with empty member.id: we will generate a new member.id and put into the mapping, and if there's already an old member id it will be replaced.
   
   2) unknown static member joins with empty member.id: we will generation a new member.id and blindly put into the mapping.
   
   3) new leader loading __consumer_offsets and read consumer group generation messages.
   
   I suspect that there's a bug on broker side such that upon leader migration, the new leader did not load all the way up to the latest entry and hence in 3) above we mistakenly bootstrap the coordinator's cache with an old generation's mapping.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Mar/20 23:41;githubbot;guozhangwang commented on pull request #8269: KAFKA-9659: Add more log4j when updating static member mappings
URL: https://github.com/apache/kafka/pull/8269
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/May/20 17:57;lkokhreidze;Hi [~guozhang] 

Just fyi - we've experienced this error as well. Some of our Kafka streams jobs (with static membership) sometimes die whenever we restart one of the brokers. It affects broker version 2.3.1 and 2.4 as well.

We can reliably reproduce the issue in our ""staging"" environment so if some extra debug logs can be useful from clients/brokers would be happy to help.;;;","13/May/20 22:01;guozhang;Hi [~lkokhreidze] Could you try out the latest trunk for streams client (you do not need to upgrade broker versions) and see if it resolves the issue? I've pushed a couple of commits trying to fix this issue but unfortunately they are not in any released version yet.;;;","08/Aug/20 11:38;lkokhreidze;Hi [~guozhang] just wanted to let you know that after upgrading to 2.6, we no longer see this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing default user quota doesn't take effect until broker restart,KAFKA-9658,13289685,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apovzner,apovzner,apovzner,05/Mar/20 01:28,20/Mar/20 00:48,13/Jul/23 09:17,10/Mar/20 23:08,2.0.1,2.1.1,2.2.2,2.3.1,2.4.0,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.0,,,,,,,,,,,0,,,,,"To reproduce (for any quota type: produce, consume, and request):

Example with consumer quota, assuming no user/client quotas are set initially.
1. Set default user consumer quotas:

{{./kafka-configs.sh --zookeeper <ZK> --alter --add-config 'consumer_byte_rate=100000000' --entity-type users --entity-default}}

{{2. Send some consume load for some user, say user1.}}

{{3. Remove default user consumer quota using:}}
{{./kafka-configs.sh --zookeeper <ZK> --alter --delete-config 'consumer_byte_rate' --entity-type users --entity-default}}

Result: --describe (as below) returns correct result that there is no quota, but quota bound in ClientQuotaManager.metrics does not get updated for users that were sending load, which causes the broker to continue throttling requests with the previously set quota.
 {{/opt/confluent/bin/kafka-configs.sh --zookeeper <ZK>  --describe --entity-type users --entity-default}}
{{}}{{}} ",,apovzner,githubbot,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 19:31:03 UTC 2020,,,,,,,,,,"0|z0c6l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/20 18:15;githubbot;apovzner commented on pull request #8232: KAFKA-9658: Fix removing user quotas
URL: https://github.com/apache/kafka/pull/8232
 
 
   Adding (add-config) default user, user, or <user, client-id> quota and then removing it via delete-config does not update quota bound in ClientQuotaManager.Metrics for existing users or <user,client-id>. This causes brokers to continue to throttle with the previously set quotas until brokers restart (or <user,client> stops sending traffic for sometime and sensor expires). This happens only when removing the user or user,client-id where there are no more quotas  to fall back to. Common example where the issue happens: Initial no quota state --> add default user quota --> remove default user quota. 
   
   The cause of the issue was `DefaultQuotaCallback.quotaLimit` was returning `null` when no default user quota set, which caused ClientQuotaManager.updateQuotaMetricConfigs` to skip updating the appropriate sensor, which left it unchanged with the previous quota. This PR changes `DefaultQuotaCallback.quotaLimit` to return unlimited (Max long) quota in this case.
   
   Added 3 unit tests that failed before the fix in the PR. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Mar/20 19:31;githubbot;hachikuji commented on pull request #8232: KAFKA-9658: Fix removing user quotas
URL: https://github.com/apache/kafka/pull/8232
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TxnOffsetCommit should not return COORDINATOR_LOADING error for old request versions,KAFKA-9656,13289664,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,hachikuji,hachikuji,04/Mar/20 23:56,18/Mar/20 19:44,13/Jul/23 09:17,18/Mar/20 19:44,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"In KAFKA-7296, we fixed a bug which causes the producer to enter a fatal state when the COORDINATOR_LOADING_IN_PROGRESS error is received. The impact of this bug in streams was that the application would crash. Generally we want users to upgrade to a later client version, but in some cases, this takes a long time. I am suggesting here that we revert the behavior change on the broker for older versions of TxnOffsetCommit. For versions older than 2 (which was introduced in 2.3), rather than returning COORDINATOR_LOADING_IN_PROGRESS, we can return COORDINATOR_NOT_AVAILABLE.",,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 19:25:42 UTC 2020,,,,,,,,,,"0|z0c6gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/20 05:57;githubbot;abbccdda commented on pull request #8253: KAFKA-9656: Return COORDINATOR_NOT_AVAILABLE for older producer clients
URL: https://github.com/apache/kafka/pull/8253
 
 
   Txn commit has a bug for lower versions where it would treat `COORDINATOR_LOAD_IN_PROGRESS` as fatal. This PR fixes the handling of older client to return 
   `COORDINATOR_NOT_AVAILABLE` so that it won't crash upon doing txn commit. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Mar/20 19:25;githubbot;hachikuji commented on pull request #8253: KAFKA-9656: Return COORDINATOR_NOT_AVAILABLE for older producer clients
URL: https://github.com/apache/kafka/pull/8253
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaAlterLogDirsThread can't be created again if the previous ReplicaAlterLogDirsThreadmeet encounters leader epoch error,KAFKA-9654,13289635,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,chia7712,chia7712,chia7712,04/Mar/20 21:10,20/Mar/20 22:02,13/Jul/23 09:17,20/Mar/20 22:02,,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,,,,,,,,,,,,0,,,,,"
ReplicaManager does create ReplicaAlterLogDirsThread only if an new future log is created. If the previous ReplicaAlterLogDirsThread encounters error when moving data, the target partition is moved to ""failedPartitions"" and ReplicaAlterLogDirsThread get idle due to empty partitions. The future log is still existent so we CAN'T either create another ReplicaAlterLogDirsThread to handle the parition or update the paritions of the idler ReplicaAlterLogDirsThread.

ReplicaManager should call ReplicaAlterLogDirsManager#addFetcherForPartitions even if there is already a future log since we can create an new ReplicaAlterLogDirsThread to handle the new partitions or update the partitions of existent ReplicaAlterLogDirsThread to make it busy again.",,chia7712,githubbot,MarkC0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8001,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 23:49:41 UTC 2020,,,,,,,,,,"0|z0c6a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 21:11;githubbot;chia7712 commented on pull request #8223: KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the pr…
URL: https://github.com/apache/kafka/pull/8223
 
 
   ReplicaManager does create ReplicaAlterLogDirsThread only if an new future log is created. If the previous ReplicaAlterLogDirsThread encounters error when moving data, the target partition is moved to ""failedPartitions"" and ReplicaAlterLogDirsThread get idle due to empty partitions. The future log is still existent so we CAN'T either create another ReplicaAlterLogDirsThread to handle the parition or update the paritions of the idler ReplicaAlterLogDirsThread.
   
   ReplicaManager should call ReplicaAlterLogDirsManager#addFetcherForPartitions even if there is already a future log since we can create an new ReplicaAlterLogDirsThread to handle the new partitions or update the partitions of existent ReplicaAlterLogDirsThread to make it busy again.
   
   https://issues.apache.org/jira/browse/KAFKA-9654
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Mar/20 23:49;githubbot;hachikuji commented on pull request #8223: KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the pr…
URL: https://github.com/apache/kafka/pull/8223
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throttle time metric needs to be updated for KIP-219,KAFKA-9652,13289620,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,hachikuji,hachikuji,04/Mar/20 19:32,30/Apr/20 03:11,13/Jul/23 09:17,30/Apr/20 03:11,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,KIP-219 changed the throttling logic so that responses are returned immediately. The logic for updating the throttle time in `RequestChannel` appears to have not been updated to reflect this change and instead reflects the old behavior where the timing is based on the time between remote completion and response completion. This means the metric will pretty much always show negligible throttling.,,agam,hachikuji,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 16:17:56 UTC 2020,,,,,,,,,,"0|z0c66o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/20 13:26;ijuma;[~agam] Are you working on this? While looking at another issue, I fixed it in a local branch. I can submit a PR if you haven't started on it.;;;","23/Apr/20 16:17;agam;Not working on this right now, please go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Records could not find corresponding partition/task,KAFKA-9645,13289430,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,04/Mar/20 04:16,07/Mar/20 16:09,13/Jul/23 09:17,07/Mar/20 16:09,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,consumer,,,,,0,,,,,"We could be hitting the illegal state when stream kicks off a rebalance with all tasks closed:

```

[2020-03-03T18:36:09-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,105] WARN [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] Detected that the thread is being fenced. This implies that this thread missed a rebalance and dropped out of the consumer group. Will close out all assigned tasks and rejoin the consumer group. (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-03T18:36:09-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,105] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] [Consumer clientId=stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000049-changelog-2 (org.apache.kafka.clients.consumer.KafkaConsumer)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,286] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] [Producer clientId=stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1-1_1-producer, transactionalId=stream-soak-test-1_1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,287] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] task [1_1] Closed dirty (org.apache.kafka.streams.processor.internals.StreamTask)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,287] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] [Consumer clientId=stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,290] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] [Producer clientId=stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1-3_2-producer, transactionalId=stream-soak-test-3_2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,292] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] task [3_2] Closed dirty (org.apache.kafka.streams.processor.internals.StreamTask)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,293] ERROR [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] Unable to locate active task for received-record partition node-name-repartition-1. Current tasks: TaskManager

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) >      MetadataState:

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) >      Tasks:

 (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,293] ERROR [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) java.lang.NullPointerException: Task was unexpectedly missing for partition node-name-repartition-1

        at org.apache.kafka.streams.processor.internals.StreamThread.addRecordsToTasks(StreamThread.java:984)

        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:820)

```

We should be more careful in this case by avoiding processing data when no corresponding task is found.

        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:751)

        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725)

[2020-03-03T18:36:10-08:00] (streams-soak-trunk-eos_soak_i-050294ea2392cf355_streamslog) [2020-03-04 02:36:09,294] INFO [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] stream-thread [stream-soak-test-e639a3a1-bd9d-49e0-896e-df9fe0cd63db-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)",,bchen225242,cadonna,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 16:08:28 UTC 2020,,,,,,,,,,"0|z0c50g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/20 04:24;githubbot;abbccdda commented on pull request #8220: KAFKA-9645: Remove Illegal State Check for Records Addition
URL: https://github.com/apache/kafka/pull/8220
 
 
   After https://github.com/apache/kafka/pull/7312/, we could still return data during the rebalance phase, which means it could be possible to find records without corresponding tasks. Do not throw illegal state here and just ignore should be safe.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Mar/20 16:08;githubbot;guozhangwang commented on pull request #8220: KAFKA-9645: Fallback to unsubscribe during Task Migrated
URL: https://github.com/apache/kafka/pull/8220
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incrementalAlterConfigs OpType.APPEND on unset property fails with NullPointerException,KAFKA-9644,13289352,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,steverod,steverod,steverod,03/Mar/20 23:06,13/Mar/20 01:23,13/Jul/23 09:17,13/Mar/20 01:20,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,admin,,,,,0,,,,,"Running incrementalAlterConfigs with an OpType.APPEND when the config property doesn't already exist fails with a NullPointerException on the broker.

Attached is a patch to the PlaintextAdminIntegrationTest demonstrating this failure and the test output showing the NPE.",,githubbot,omkreddy,steverod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 23:05;steverod;incrementalAlterTest.patch;https://issues.apache.org/jira/secure/attachment/12995511/incrementalAlterTest.patch","03/Mar/20 23:06;steverod;kafka.api.PlaintextAdminIntegrationTest.testValidIncrementalAlterConfigs.test.stdout;https://issues.apache.org/jira/secure/attachment/12995510/kafka.api.PlaintextAdminIntegrationTest.testValidIncrementalAlterConfigs.test.stdout",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 01:20:54 UTC 2020,,,,,,,,,,"0|z0c4sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/20 23:27;steverod;Made an error while filing: the fix is trivial, the impact is minor.;;;","04/Mar/20 01:47;githubbot;steverod commented on pull request #8216: [KAFKA-9644] Non-existent configs in incrementalAlterConfigs APPEND/SUBTRACT
URL: https://github.com/apache/kafka/pull/8216
 
 
   Problem
   ----
   The `incrementalAlterConfigs` API supports OpType.APPEND and OpType.SUBTRACT for configuration properties of LIST type. If an APPEND or SUBTRACT OpType is submitted for a config property which currently has no value, then the operation fails with a NullPointerException on the broker side (conveyed as an ""unknown server error"" to the client).
   
   This is because the alter code does a `getProperty` of the existing configuration value
   with no concern as to whether or not the property actually exists.
   
   This change handles the case of existing null properties. 
   
   Testing
   -----
   This change includes 2 test cases in the unit test that demonstrate the issue for OpType.SUBTRACT and OpType.APPEND. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Mar/20 01:20;githubbot;omkreddy commented on pull request #8216: [KAFKA-9644] Non-existent configs in incrementalAlterConfigs APPEND/SUBTRACT
URL: https://github.com/apache/kafka/pull/8216
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","13/Mar/20 01:20;omkreddy;Issue resolved by pull request 8216
[https://github.com/apache/kafka/pull/8216];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""BigDecimal(double)"" should not be used",KAFKA-9642,13289340,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,soontaek.lim,soontaek.lim,soontaek.lim,03/Mar/20 21:33,10/Apr/20 21:31,13/Jul/23 09:17,10/Apr/20 21:31,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"I recommend not to use the BigDecimal(double) constructor. Because of floating point imprecision, we're unlikely to get the value we expect from that constructor.

Instead, we should use BigDecimal.valueOf, which uses a string under the covers to eliminate floating-point rounding errors, or the constructor that takes a String argument.

 

From JavaDocs

The results of this constructor can be somewhat unpredictable. One might assume that writing new BigDecimal(0.1) in Java creates a BigDecimal which is exactly equal to 0.1 (an unscaled value of 1, with a scale of 1), but it is actually equal to 0.1000000000000000055511151231257827021181583404541015625. This is because 0.1 cannot be represented exactly as a double (or, for that matter, as a binary fraction of any finite length). Thus, the value that is being passed in to the constructor is not exactly equal to 0.1, appearances notwithstanding.",,githubbot,kkonstantine,soontaek.lim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 17:22:29 UTC 2020,,,,,,,,,,"0|z0c4q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/20 22:06;githubbot;SoontaekLim commented on pull request #8212: KAFKA-9642: Change ""BigDecimal(double)"" constructor to ""BigDecimal.valueOf(double)""
URL: https://github.com/apache/kafka/pull/8212
 
 
   https://issues.apache.org/jira/browse/KAFKA-9642
   for this reason, I modified the code that is using the ""BigDecimal(double)"" constructor.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Apr/20 17:22;githubbot;guozhangwang commented on pull request #8212: KAFKA-9642: Change ""BigDecimal(double)"" constructor to ""BigDecimal.valueOf(double)""
URL: https://github.com/apache/kafka/pull/8212
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigProvider does not document thread safety,KAFKA-9634,13288925,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,02/Mar/20 15:46,23/Mar/20 00:50,13/Jul/23 09:17,23/Mar/20 00:50,,,,,,,,,,,,,,,,,,,,,,,2.0.2,2.1.2,2.2.3,2.3.2,2.4.2,2.5.0,,,,,,,,0,,,,,"In Kafka Connect {{ConfigProvider}} can be used concurrently (e.g. via PUT to {{/{connectorType}/config/validate}}, but there is no mention of concurrent usage in the Javadocs for {{ConfigProvider}}. It's probably worth calling out that implementations need to be thread safe.",,githubbot,kkonstantine,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 00:50:49 UTC 2020,,,,,,,,,,"0|z0c2w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 18:10;tombentley;https://github.com/apache/kafka/pull/8205;;;","22/Mar/20 19:58;githubbot;kkonstantine commented on pull request #8205: KAFKA-9634: Add note about thread safety of ConfigProvider
URL: https://github.com/apache/kafka/pull/8205
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","23/Mar/20 00:50;kkonstantine;Merged to {{trunk}} and all the release branches from {{2.0}} to {{2.5}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigProvider.close() not called,KAFKA-9633,13288919,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tombentley,tombentley,tombentley,02/Mar/20 15:41,23/Oct/20 05:50,13/Jul/23 09:17,01/May/20 18:20,,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,,,,,,0,patch-available,,,,"ConfigProvider extends Closeable, but in the following contexts the {{close()}} method is never called:

1. AbstractConfig
2. WorkerConfigTransformer",,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 18:06:00 UTC 2020,,,,,,,,,,"0|z0c2uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 18:06;tombentley;Patch available: https://github.com/apache/kafka/pull/8204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient test failure: PartitionLockTest.testAppendReplicaFetchWithUpdateIsr,KAFKA-9632,13288897,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,02/Mar/20 14:32,04/Mar/20 09:37,13/Jul/23 09:17,04/Mar/20 09:37,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,core,,,,,0,,,,,"When running this test with _numRecordsPerProducer=500_, the test fails intermittently. The test uses MockTime and runs concurrent log operations. This can cause issues when attempting to roll a segment since Log and MockScheduler don't work well together. MockScheduler currently runs tasks while holding the MockScheduler lock. This can cause a deadlock if a thread attempts to schedule a task while holding a lock which is also acquired within a scheduled task.

The issue in this test occurs when these two operations happen concurrently:

1) LogManager.cleanupLogs is a scheduled task that acquires Log lock. When run with MockScheduler, the thread holds MockScheduler lock and then attempts to acquire Log lock.

2) Partition.appendLogsToLeader holds Log lock and attempts to acquire MockScheduler lock in order to schedule a roll().

Since locking order is reversed in 1) and 2), this causes a deadlock.

The test itself can be easily fixed by avoiding roll() in the test. But it will be good to fix MockScheduler to enable it to be used in this case.

 ",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 09:36:11 UTC 2020,,,,,,,,,,"0|z0c2q0:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"04/Mar/20 09:36;githubbot;rajinisivaram commented on pull request #8209: KAFKA-9632; Fix MockScheduler synchronization for safe use in Log/Partition tests
URL: https://github.com/apache/kafka/pull/8209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to Describe broker configurations that have been set via IncrementalAlterConfigs,KAFKA-9625,13288410,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,skaundinya,cmccabe,cmccabe,28/Feb/20 23:10,05/May/20 21:23,13/Jul/23 09:17,18/Mar/20 07:35,,,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,core,,,,,0,,,,,"There seem to be at least two bugs in the broker configuration APIs and/or logic:

1. Broker throttles are incorrectly marked as sensitive configurations.  This includes leader.replication.throttled.rate, follower.replication.throttled.rate, replica.alter.log.dirs.io.max.bytes.per.second.  This means that their values cannot be read back by DescribeConfigs after they are set.

2. When we clear the broker throttles via incrementalAlterConfigs, DescribeConfigs continues returning the old throttles indefinitely.  In other words, the clearing is not reflected in the Describe API.",,cmccabe,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9139,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 20:34:35 UTC 2020,,,,,,,,,,"0|z0c0s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/20 19:01;githubbot;cmccabe commented on pull request #8206: KAFKA-9625: Broker throttles are incorrectly marked as sensitive configurations
URL: https://github.com/apache/kafka/pull/8206
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Mar/20 01:42;githubbot;skaundinya15 commented on pull request #8260: KAFKA-9625: Fixing IncrementalAlterConfigs with respect to Broker Configs
URL: https://github.com/apache/kafka/pull/8260
 
 
   This PR introduces a bug fix for `IncrementalAlterConfigs` with respect to altering broker configs. Previously broker throttles were incorrectly marked as sensitive configurations. In addition, when trying to delete configs, `DescribeConfigs` would always return the initially deleted configs. This PR fixes both of those issues.
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Mar/20 06:02;githubbot;cmccabe commented on pull request #8260: KAFKA-9625: Fixing IncrementalAlterConfigs with respect to Broker Configs
URL: https://github.com/apache/kafka/pull/8260
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Mar/20 20:34;githubbot;cmccabe commented on pull request #8206: KAFKA-9625: Broker throttles are incorrectly marked as sensitive configurations
URL: https://github.com/apache/kafka/pull/8206
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams will attempt to commit during shutdown if rebalance is in progress,KAFKA-9623,13288175,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,28/Feb/20 01:30,28/Feb/20 22:20,13/Jul/23 09:17,28/Feb/20 22:20,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"This will throw a retriable `RebalanceInProgressException` which Streams does not currently expect. 

A possible fix is to change the condition of while(isRunning()) inside runLoop to sth. like isRunning() || !taskManager.rebalanceInProgress(), and within an iteration after we’ve added the records we will check isRunning() again and if false we would skip processing any records anyways.",,ableegoldman,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 22:16:01 UTC 2020,,,,,,,,,,"0|z0bzdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/20 01:32;githubbot;guozhangwang commented on pull request #8190: KAFKA-9623: Keep polling until the task manager is no longer rebalancing in progress
URL: https://github.com/apache/kafka/pull/8190
 
 
   This bug is found via the flaky SmokeTestDriverIntegrationTest. Without this PR the test fails every 3-4 times, after this issue is fixed we've run the test 20+ locally without error.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","28/Feb/20 22:16;githubbot;guozhangwang commented on pull request #8190: KAFKA-9623: Keep polling until the task manager is no longer rebalancing in progress
URL: https://github.com/apache/kafka/pull/8190
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task revocation failure could introduce remaining unclean tasks,KAFKA-9620,13288144,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,27/Feb/20 20:41,28/Feb/20 20:07,13/Jul/23 09:17,28/Feb/20 20:07,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"The task revocation call should enforce the close of a task, otherwise we could potentially hit the exception during `handleAssignment`.

During revoke we failed:

 
{code:java}
[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) [2020-02-27 19:05:47,321] ERROR [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] [Consumer clientId=stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1-consumer, groupId=stream-soak-test] User provided listener org.apache.kafka.streams.processor.internals.StreamsRebalanceListener failed on invocation of onPartitionsRevoked for partitions [logs.json.kafka-2, logs.json.zookeeper-2, node-name-repartition-1, logs.kubernetes-2, windowed-node-counts-1, logs.operator-2, logs.syslog-2] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) org.apache.kafka.streams.errors.TaskMigratedException: Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated.
        at org.apache.kafka.streams.processor.internals.StreamsProducer.commitTransaction(StreamsProducer.java:172)
        at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.commit(RecordCollectorImpl.java:226)
        at org.apache.kafka.streams.processor.internals.StreamTask.commitState(StreamTask.java:368)
        at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:242)
        at org.apache.kafka.streams.processor.internals.TaskManager.handleRevocation(TaskManager.java:314)
        at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:72)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:297)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:383)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:439)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:477)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1277)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1218)
        at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:920)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:800)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725)
[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
{code}
During assignment we are checking the cleanness of task close and throw fatal:
{code:java}
[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) [2020-02-27 19:05:48,032] ERROR [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] stream-thread [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread) [2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) java.lang.RuntimeException: Unexpected failure to close 1 task(s) [[0_2]]. First exception (for task 0_2) follows.         at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:205)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1176)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:397)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:439)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:477)         at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1277)         at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243)         at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1218)         at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:920)         at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:800)         at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)         at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725) [2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) Caused by: org.apache.kafka.streams.errors.TaskMigratedException: Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated.
{code}
 ",,ableegoldman,bchen225242,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 20:07:03 UTC 2020,,,,,,,,,,"0|z0bz6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 20:43;githubbot;abbccdda commented on pull request #8187: KAFKA-9620: Force close task if suspend fails during revocation
URL: https://github.com/apache/kafka/pull/8187
 
 
   One way of fixing it forward.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Feb/20 21:47;guozhang;I think there are two issues we've observed that needs investigation:

1. The exception thrown from `suspend` is expected to now be swallowed, but by propagated to the stream-thread's runLoop: if it is a task-migrated, it should not be wrapped but be handled as task-migrated; otherwise, it is handled as a fatal one. We need to find out where we mistakenly swallowed the error.

2. In `handleAssignment`, when we are throwing exceptions at the end, we should not always wrap the exception as a RuntimeException, since in that case if the wrapped exception is task-migrated it would not be handled gracefully but be treated as fatal.;;;","28/Feb/20 03:16;ableegoldman;[~guozhang] I agree with your second observation, but w.r.t. your first point I think we don't swallow anything we shouldn't have and will throw TaskMigrated all the way up through poll (or would have, if onAssignment hadn't thrown its own RuntimeException).

If we do hit a TaskMigrated in a rebalance callback there's not much we can do, since the ConsumerCoordinator should still complete all callbacks. We just know they are doomed to also hit TaskMigrated and fail – all we can do is optimize for this case by preventing subsequent callbacks from attempting further close/create operations if any previous callback within the same poll call hit a TaskMigrated (or fatal) exception.;;;","28/Feb/20 17:59;guozhang;I synced with Boyang yesterday over the PR, and I think the root cause of 1) is that we tries to not throw immediately from onPartitionsRevoked, but instead always execute and remember exceptions through the 1) onPartitionsRevoked, 2) onAssignment, 3) onPartitionsAssigned, and then throw. So it is actually not a real problem but by design. The real problem is that, we should not wrap a KafkaException (including TaskMigratedException) again as a RuntimeException which is being fixed inside the PR.;;;","28/Feb/20 20:07;githubbot;guozhangwang commented on pull request #8187: KAFKA-9620: Do not throw in the middle of consumer user callbacks
URL: https://github.com/apache/kafka/pull/8187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed state store deletion could lead to task file not found,KAFKA-9618,13288108,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,27/Feb/20 17:38,10/Mar/20 17:51,13/Jul/23 09:17,10/Mar/20 17:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"A failed deletion of a stream task directory could later lead to the impression that the task state is still there, thus causing file not found exception as the directory was partially deleted.
{code:java}
[2020-02-26T22:08:05-08:00] (streams-soak-trunk-eos_soak_i-04ebd21fd0e0da9bf_streamslog) [2020-02-27 06:08:04,394] WARN [stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-2] stream-thread [stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-2] task [2_2] Failed to wiping state stores for task 2_2 (org.apache.kafka.streams.processor.internals.StreamTask) [2020-02-26T22:08:05-08:00] (streams-soak-trunk-eos_soak_i-04ebd21fd0e0da9bf_streamslog) [2020-02-27 06:08:04,394] INFO [stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-2] [Producer clientId=stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-2-2_2-producer, transactionalId=stream-soak-test-2_2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-26T22:08:05-08:00] (streams-soak-trunk-eos_soak_i-04ebd21fd0e0da9bf_streamslog) [2020-02-27 06:08:04,411] ERROR [stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-1] stream-thread [stream-soak-test-b26adb53-07e2-4013-933a-0f4bcac84c04-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread) [2020-02-26T22:08:05-08:00] (streams-soak-trunk-eos_soak_i-04ebd21fd0e0da9bf_streamslog) org.apache.kafka.streams.errors.ProcessorStateException: Error opening store KSTREAM-AGGREGATE-STATE-STORE-0000000040 at location /mnt/run/streams/state/stream-soak-test/2_2/rocksdb/KSTREAM-AGGREGATE-STATE-STORE-0000000040         at org.apache.kafka.streams.state.internals.RocksDBTimestampedStore.openRocksDB(RocksDBTimestampedStore.java:87)         at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:191)         at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:230)         at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)         at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init(ChangeLoggingKeyValueBytesStore.java:44)         at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)         at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init(CachingKeyValueStore.java:58)         at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
{code}",,ableegoldman,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 05:45:16 UTC 2020,,,,,,,,,,"0|z0byyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/20 19:10;githubbot;abbccdda commented on pull request #8186: KAFKA-9618: Directory deletion failure leading to error task RocksDB open
URL: https://github.com/apache/kafka/pull/8186
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","28/Feb/20 17:23;bchen225242;Reproduced the exception as:
{code:java}
[stream-soak-test-e6003ae8-1331-4b99-97e3-a3229aaa15fd-StreamThread-3] task [1_1] Failed to wiping state stores for task 1_1 (org.apache.kafka.streams.processor.internals.StreamTask)
[2020-02-29T05:50:03-08:00] (streams-soak-trunk-eos_soak_i-0c3043df582d0379a_streamslog) [2020-02-29 13:49:58,304] ERROR [stream-soak-test-e6003ae8-1331-4b99-97e3-a3229aaa15fd-StreamThread-3] stream-thread [stream-soak-test-e6003ae8-1331-4b99-97e3-a3229aaa15fd-StreamThread-3] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-02-29T05:50:03-08:00] (streams-soak-trunk-eos_soak_i-0c3043df582d0379a_streamslog) org.apache.kafka.streams.errors.TaskMigratedException: The deletion of directory for task 1_1failed; it means all tasks belonging to this thread should be migrated.
        at org.apache.kafka.streams.processor.internals.StateManagerUtil.wipeStateStores(StateManagerUtil.java:89)
        at org.apache.kafka.streams.processor.internals.StreamTask.close(StreamTask.java:450)
        at org.apache.kafka.streams.processor.internals.StreamTask.closeDirty(StreamTask.java:392)
        at org.apache.kafka.streams.processor.internals.TaskManager.handleLostAll(TaskManager.java:339)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:766)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725)
[2020-02-29T05:50:03-08:00] (streams-soak-trunk-eos_soak_i-0c3043df582d0379a_streamslog) Caused by: java.nio.file.DirectoryNotEmptyException: /mnt/run/streams/state/stream-soak-test/1_1/KSTREAM-AGGREGATE-STATE-STORE-0000000025/KSTREAM-AGGREGATE-STATE-STORE-0000000025.1582891200000
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:762)
        at org.apache.kafka.common.utils.Utils$2.postVisitDirectory(Utils.java:744)
        at java.nio.file.Files.walkFileTree(Files.java:2688)
        at java.nio.file.Files.walkFileTree(Files.java:2742)
        at org.apache.kafka.common.utils.Utils.delete(Utils.java:744)
        at org.apache.kafka.streams.processor.internals.StateManagerUtil.wipeStateStores(StateManagerUtil.java:86)
        ... 5 more
[2020-02-29T05:50:03-08:00] (streams-soak-trunk-eos_soak_i-0c3043df582d0379a_streamslog) [2020-02-29 13:49:58,305] INFO [stream-soak-test-e6003ae8-1331-4b99-97e3-a3229aaa15fd-StreamThread-3] stream-thread [stream-soak-test-e6003ae8-1331-4b99-97e3-a3229aaa15fd-StreamThread-3] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
{code};;;","01/Mar/20 02:14;bchen225242;The root cause was that we were releasing the state store lock before completing the deletion.
{code:java}
// first close state manager (which is idempotent) then close the record collector (which could throw), 
// if the latter throws and we re-close dirty which would close the state manager again. StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory); 
// if EOS is enabled, we wipe out the whole state store for unclean close 
// since they are invalid to use anymore 
if (!clean && !eosDisabled) { 
  StateManagerUtil.wipeStateStores(log, stateDirectory, stateMgr); 
}
{code}
The sequence of issue was:
1. StreamThread-3 holds task 1_1, and missed a rebalance
2. StreamThread-2 takes the ownership of task 1_1, and tries to open the rocks store but blocked by the lock

3. StreamThread-3 attempted to close tasks to rejoin the group

4. StreamThread-3 releases the lock on the state directory in `closeStateManager`

5. StreamThread-2 acquires the lock and start processing on the state store

6. StreamThread-3 would try to delete all files, however for the Util.delete it could potentially fail:
{code:java}
* <p> On some operating systems it may not be possible to remove a file when * it is open and in use by this Java virtual machine or other programs.
{code}
7. The deletion hits race condition and kills StreamThread-3;;;","04/Mar/20 05:45;githubbot;guozhangwang commented on pull request #8186: KAFKA-9618: Directory deletion failure leading to error task RocksDB open
URL: https://github.com/apache/kafka/pull/8186
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica Fetcher can mark partition as failed when max.message.bytes is changed,KAFKA-9617,13288087,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,enether,enether,27/Feb/20 15:41,16/May/20 17:05,13/Jul/23 09:17,16/May/20 17:05,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,newbie,,,,"There exists a race condition when changing the dynamic max.message.bytes config for a topic. A follower replica can replicate a message that is over that size after it processes the config change. When this happens, the replica fetcher catches the unexpected exception, marks the partition as failed and stops replicating it.
{code:java}
06:38:46.596	Processing override for entityPath: topics/partition-1 with config: Map(max.message.bytes -> 512)

06:38:46.597	 [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Unexpected error occurred while processing data for partition partition-1 at offset 20964
org.apache.kafka.common.errors.RecordTooLargeException: The record batch size in the append to partition-1 is 3349 bytes which exceeds the maximum configured value of 512.
{code}",,chia7712,enether,epinto,guozhang,ijuma,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 03:38:02 UTC 2020,,,,,,,,,,"0|z0byts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/20 04:26;showuon;I can take this if no one else is looking at this issue. Please let me know, and help to assign to me. Thanks.;;;","21/Apr/20 13:32;enether;Feel free to assign yourself [~showuon]!;;;","09/May/20 15:44;chia7712;Should we skip the limit when the fetch is run by follower replica since the ""larger"" batch was accepted by leader ?;;;","09/May/20 17:55;ijuma;Yeah, I think that's reasonable. [~showuon] are you working on this?;;;","10/May/20 12:07;showuon;No, I haven't started to work on this. Go ahead to take it. Thanks.

 ;;;","10/May/20 13:35;chia7712;[~ijuma] Could I take over this ticket?;;;","11/May/20 22:40;guozhang;[~chia7712] sure! I've just assigned the ticket to you, [~showuon] and us can help review it.;;;","12/May/20 17:37;chia7712;[~guozhang] thanks!;;;","14/May/20 03:38;chia7712;[https://github.com/apache/kafka/pull/8659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid initializing the topology twice when resuming stream tasks from suspended state,KAFKA-9614,13287840,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,26/Feb/20 20:10,26/Feb/20 22:03,13/Jul/23 09:17,26/Feb/20 22:03,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"When resuming from a suspended stream task, today it first transit to restoring and then to running. The main motivation is to simplify the state transition FSM of stream tasks. We should only call `initializeTopology` when transiting from restoring to running, but the tech debt cleanup missed one gap that beforehand we transit to running directly and hence also call `initializeTopology` at the first transition.",,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 22:03:21 UTC 2020,,,,,,,,,,"0|z0bxaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/20 22:03;githubbot;guozhangwang commented on pull request #8173: KAFKA-9614: Not initialize topology twice in StreamTask
URL: https://github.com/apache/kafka/pull/8173
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not throw illegal state exception during task revocation,KAFKA-9610,13287630,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,25/Feb/20 23:11,26/Feb/20 22:06,13/Jul/23 09:17,26/Feb/20 22:06,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"In handleRevocation call, the remaining partitions could cause an illegal state exception on task revocation. This should also be fixed as it is expected when the tasks are cleared from the assignor onAssignment callback.",,ableegoldman,bchen225242,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 22:05:47 UTC 2020,,,,,,,,,,"0|z0bw08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 23:17;githubbot;abbccdda commented on pull request #8169: KAFKA-9610: do not throw illegal state when remaining partitions are not empty
URL: https://github.com/apache/kafka/pull/8169
 
 
   For `handleRevocation`, it is possible that previous onAssignment callback has cleaned up the stream tasks, which means no corresponding task could be found for given partitions. We should not throw here as this is expected behavior.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 22:05;githubbot;guozhangwang commented on pull request #8169: KAFKA-9610: do not throw illegal state when remaining partitions are not empty
URL: https://github.com/apache/kafka/pull/8169
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not clear partition queue during task close,KAFKA-9607,13287607,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,25/Feb/20 21:12,27/Feb/20 01:47,13/Jul/23 09:17,27/Feb/20 01:47,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"We detected an issue with a corrupted task failed to revive:
{code:java}
[2020-02-25T08:23:38-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,137] INFO [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] stream-thread [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] Handle new assignment with:
        New active tasks: [0_0, 3_1]
        New standby tasks: []
        Existing active tasks: [0_0]
        Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
[2020-02-25T08:23:38-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,138] INFO [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] [Consumer clientId=stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1-consumer, groupId=stream-soak-test] Adding newly assigned partitions: k8sName-id-repartition-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2020-02-25T08:23:38-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,138] INFO [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] stream-thread [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] State transition from RUNNING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-02-25T08:23:39-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,419] WARN [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] stream-thread [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] Encountered org.apache.kafka.clients.consumer.OffsetOutOfRangeException fetching records from restore consumer for partitions [stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000049-changelog-1], it is likely that the consumer's position has fallen out of the topic partition offset range because the topic was truncated or compacted on the broker, marking the corresponding tasks as corrupted and re-initializingit later. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
[2020-02-25T08:23:38-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,139] INFO [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] [Consumer clientId=stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1-consumer, groupId=stream-soak-test] Setting offset for partition k8sName-id-repartition-1 to the committed offset FetchPosition{offset=3592242, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[ip-172-31-25-115.us-west-2.compute.internal:9092 (id: 1003 rack: null)], epoch=absent}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2020-02-25T08:23:39-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) [2020-02-25 16:23:38,463] ERROR [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] stream-thread [stream-soak-test-8f2124ec-8bd0-410a-8e3d-f202a32ab774-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
[2020-02-25T08:23:39-08:00] (streams-soak-trunk_soak_i-06305ad57801079ae_streamslog) java.lang.IllegalStateException: Partition k8sName-id-repartition-1 not found.
        at org.apache.kafka.streams.processor.internals.PartitionGroup.setPartitionTime(PartitionGroup.java:99)
        at org.apache.kafka.streams.processor.internals.StreamTask.initializeTaskTime(StreamTask.java:651)
        at org.apache.kafka.streams.processor.internals.StreamTask.initializeMetadata(StreamTask.java:631)
        at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:209)
        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:270)
        at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:834)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725)
{code}
The root cause is that we accidentally cleanup the partition group map so that next time we reboot the task it would miss input partitions.

By avoiding clean up the partition group, we may have a slight overhead for GC which is ok. In terms of correctness, currently there is no way to revive the task with partitions reassigned.",,ableegoldman,bchen225242,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 01:47:18 UTC 2020,,,,,,,,,,"0|z0bvv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 21:14;githubbot;abbccdda commented on pull request #8168: KAFKA-9607: Partition group should not be cleared if task will be revived
URL: https://github.com/apache/kafka/pull/8168
 
 
   This PR fixes the illegal state bug where a task gets revived but has no input partition assigned anymore.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Feb/20 01:47;githubbot;guozhangwang commented on pull request #8168: KAFKA-9607: Do not clear partition queues during close
URL: https://github.com/apache/kafka/pull/8168
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOS Producer could throw illegal state if trying to complete a failed batch after fatal error,KAFKA-9605,13287566,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,25/Feb/20 18:12,11/Mar/20 22:24,13/Jul/23 09:17,11/Mar/20 22:24,2.4.0,2.5.0,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"In the Producer we could see network client hits fatal exception while trying to complete the batches after Txn manager hits fatal fenced error:
{code:java}
 
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) [2020-02-24 21:23:28,673] ERROR [kafka-producer-network-thread | stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer] [Producer clientId=stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer, transactionalId=stream-soak-test-1_0] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) [2020-02-24 21:23:28,674] INFO [stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3] [Producer clientId=stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-0_0-producer, transactionalId=stream-soak-test-0_0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) [2020-02-24 21:23:28,684] INFO [kafka-producer-network-thread | stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer] [Producer clientId=stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer, transactionalId=stream-soak-test-1_0] Resetting sequence number of batch with current sequence 354277 for partition windowed-node-counts-0 to 354276 (org.apache.kafka.clients.producer.internals.TransactionManager)
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) [2020-02-24 21:23:28,684] INFO [kafka-producer-network-thread | stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer] Resetting sequence number of batch with current sequence 354277 for partition windowed-node-counts-0 to 354276 (org.apache.kafka.clients.producer.internals.ProducerBatch)
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) [2020-02-24 21:23:28,685] ERROR [kafka-producer-network-thread | stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer] [Producer clientId=stream-soak-test-5e16fa60-12a3-4c4f-9900-c75f7d10859f-StreamThread-3-1_0-producer, transactionalId=stream-soak-test-1_0] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient)
[2020-02-24T13:23:29-08:00] (streams-soak-trunk-eos_soak_i-02ea56d369c55eec2_streamslog) java.lang.IllegalStateException: Should not reopen a batch which is already aborted.
        at org.apache.kafka.common.record.MemoryRecordsBuilder.reopenAndRewriteProducerState(MemoryRecordsBuilder.java:295)
        at org.apache.kafka.clients.producer.internals.ProducerBatch.resetProducerState(ProducerBatch.java:395)
        at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$adjustSequencesDueToFailedBatch$4(TransactionManager.java:770)
        at org.apache.kafka.clients.producer.internals.TransactionManager$TopicPartitionEntry.resetSequenceNumbers(TransactionManager.java:180)
        at org.apache.kafka.clients.producer.internals.TransactionManager.adjustSequencesDueToFailedBatch(TransactionManager.java:760)
        at org.apache.kafka.clients.producer.internals.TransactionManager.handleFailedBatch(TransactionManager.java:735)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:671)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:662)
        at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:620)
        at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:554)
        at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:69)
        at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:745)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:571)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:563)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:304)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
        at java.lang.Thread.run(Thread.java:748)


{code}
The proper fix is to add a check for handle failed batch in txn manager.",,ableegoldman,bchen225242,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 22:24:05 UTC 2020,,,,,,,,,,"0|z0bvm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/20 22:24;githubbot;hachikuji commented on pull request #8177: KAFKA-9605: Do not attempt to abort batches when txn manager is in fatal error
URL: https://github.com/apache/kafka/pull/8177
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of open files keeps increasing in Streams application,KAFKA-9603,13287449,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,biljazovic,biljazovic,25/Feb/20 09:00,09/Mar/21 09:12,13/Jul/23 09:17,20/May/20 22:11,2.1.0,2.2.0,2.3.1,2.4.0,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,3,,,,,"Problem appeared when upgrading from *2.0.1* to *2.3.1*. 

Relevant Kafka Streams code:
{code:java}
KStream<String, Event1> events1 =
    builder.stream(FIRST_TOPIC_NAME, Consumed.with(stringSerde, event1Serde, event1TimestampExtractor(), null))
           .mapValues(...);        

KStream<String, Event2> events2 =
    builder.stream(SECOND_TOPIC_NAME, Consumed.with(stringSerde, event2Serde, event2TimestampExtractor(), null))
           .mapValues(...);        

var joinWindows = JoinWindows.of(Duration.of(1, MINUTES).toMillis())
                             .until(Duration.of(1, HOURS).toMillis());

events2.join(events1, this::join, joinWindows, Joined.with(stringSerde, event2Serde, event1Serde))
               .foreach(...);
{code}
Number of open *.sst files keeps increasing until eventually it hits the os limit (65536) and causes this exception:
{code:java}
Caused by: org.rocksdb.RocksDBException: While open a file for appending: /.../0_8/KSTREAM-JOINOTHER-0000000010-store/KSTREAM-JOINOTHER-0000000010-store.1579435200000/001354.sst: Too many open files
	at org.rocksdb.RocksDB.flush(Native Method)
	at org.rocksdb.RocksDB.flush(RocksDB.java:2394)
{code}
Here are example files that are opened and never closed:
{code:java}
/.../0_27/KSTREAM-JOINTHIS-0000000009-store/KSTREAM-JOINTHIS-0000000009-store.1582459200000/000114.sst
/.../0_27/KSTREAM-JOINOTHER-0000000010-store/KSTREAM-JOINOTHER-0000000010-store.1582459200000/000065.sst
/.../0_29/KSTREAM-JOINTHIS-0000000009-store/KSTREAM-JOINTHIS-0000000009-store.1582156800000/000115.sst
/.../0_29/KSTREAM-JOINTHIS-0000000009-store/KSTREAM-JOINTHIS-0000000009-store.1582459200000/000112.sst
/.../0_31/KSTREAM-JOINTHIS-0000000009-store/KSTREAM-JOINTHIS-0000000009-store.1581854400000/000051.sst
{code}","Spring Boot 2.2.4, OpenJDK 13, Centos image",a6kme,ableegoldman,biljazovic,cadonna,dpoldrugo,guozhang,lkokhreidze,lpandzic,mjsax,RensGroothuijsen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 20:08:33 UTC 2020,,,,,,,,,,"0|z0buw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 19:30;ableegoldman;Hey [~biljazovic], can you tell if this happened during restoration or were there a large number of unclosed files long after it resumed processing?

The reason I ask is that during restoration Streams toggles on ""bulk loading"" mode which involves turning off auto-compaction and dumping all the data from the changelog into (uncompacted) L0 files. These are the first level files and tend to be relatively small, so if you end up with a lot of them. Once restoration is done Streams will issue a manual compaction to merge these into a fewer number of larger files, but of course you can hit system limits before it completes if you have a lot of data to restore;;;","26/Feb/20 10:48;biljazovic;[~ableegoldman] thank you on your reply.

This happens after restoration, sometimes even after a day of processing new messages.;;;","27/Feb/20 05:11;ableegoldman;Admittedly a day does seem like a long time, but the rocksdb background compaction may merge the small L0 files less aggressively once they're under a certain threshold...just a wild guess. Do you mean the large number of files continues to grow up to 24hr after restoration completes, or just that it goes up during restoration and doesn't go back down again for a day?

If you can increase the limit I'd recommend starting there, if you have a large number of stores/task or tasks/instance then the open files can really build up. ;;;","27/Feb/20 05:13;ableegoldman;Do you notice that some of the task directories, or some of the files within a task directory, are not being actively used and haven't been touched since the upgrade? ;;;","27/Feb/20 16:18;biljazovic;Number of open files grows at constant speed from low values - on version 2.0.1 it stays at around 3000, but on 2.3.1 and 2.4.0 it starts at about 3000 and goes up from there slowly, eventually hitting 60k. With that big of a difference I didn't think increasing the OS limit would help. Up until the limit is hit, new versions process messages in the same way as the old version.

More things I noticed today:
 * Number of files is increasing only under odd-numbered tasks
 * numerous files are all under ~10K in size, and in the same directory they are all around the same file size, except for one or two which are >5M
 * Memory usage or CPU usage is not higher than on previous version

I'm not sure if the same files that are opened at the beginning are still opened hours later, or if most of them are closed and new ones are opened. I'll have to check that tomorrow.

 ;;;","12/Mar/20 18:00;ableegoldman;Hey [~biljazovic] any new info or results? It does seem like there may be something odd going on here. Also, just to clarify this occurred when upgrading Streams from 2.0 to 2.3 right? Is the ""2.4"" listed in the affected versions the broker version?;;;","13/Mar/20 11:27;biljazovic;[~ableegoldman] Unfortunately no new info, we are running 2.0 Streams for now. Problems are in both Streams versions 2.3 and 2.4. Broker versions are 2.1.1.0, message format versions 2.1-IV2;;;","30/Mar/20 16:00;biljazovic;Tested with Kafka 2.1 and 2.2 (brokers unchanged), both have similar issues. Number of .sst files is increasing *only in every other* task directory. 

Also, files that are created and opened at the start are never closed until the crash.;;;","09/Apr/20 08:21;lpandzic;Hello, I took this task over from Bruno on the project.

I've tried:
 * upgrading to 2.3
 * upgrading to 2.4.1
 * limiting number of open files with {color:#000000}RocksDBConfigSetterForSeen{color} - options.setMaxOpenFiles() to 1000 and 10000

Nothing helped and results are that one instance of the service keeps opening more files continuously at a relatively steady rate (around 6k per hour).;;;","09/Apr/20 17:28;guozhang;[~lpandzic] Thanks for the update, we will try to reproduce the observed issue first and then try to see if some leads can be found.;;;","10/Apr/20 05:03;lpandzic;Thanks, it seems I've been able to reproduce the issue in our test environment. I'll try to reproduce it locally and isolate the cause as much as possible, ideally I'll be able to share a project that reproduces the issue.;;;","10/Apr/20 19:08;guozhang;Great, thanks! I'd be waiting for you to share the mini project then :);;;","17/Apr/20 08:43;a6kme;[~lpandzic] Potentially related issue with using Confluent ksql. ksql is built using kafka streams. - [https://github.com/confluentinc/ksql/issues/5057];;;","23/Apr/20 14:15;lpandzic;Sadly, I've been unable to reproduce the issue locally.

On production however I was able to isolate the issue to upgrade of kafka-streams from 2.1.1 to 2.2.0.;;;","23/Apr/20 15:56;cadonna;[~lpandzic] Thank you for the information. 

In the description is stated

??Problem appeared when upgrading from 2.0.1 to 2.3.1.??

I guess you investigated more and you could restrict the issue to the upgrade from 2.1.1 to 2.2.0. Is this correct?
  
I checked the RocksDB versions of this two Kafka Streams versions.
Kafka Streams 2.1.1 uses RocksDB 5.14.2.
Kafka Streams 2.2.0 uses RocksDB 5.15.10.
;;;","24/Apr/20 15:03;lpandzic;Yes, that's what I meant but [~biljazovic] warned me about some other circumstances...

Our application is running inside docker with rocksdb stateDir volume mapped to host.

On kafka-streams 2.2.0 with `lsof` the problem is visible from within the container (raising number of file descriptors).

On kafka-streams 2.1.0 and 2.1.1 'lsof' is stable from within the container, but not from host, lsof on host exhibits similar behavior to 2.2.0 from container - rising number of descriptors. Number of files problem is present on both (versions and host/container).;;;","24/Apr/20 16:25;lpandzic;Small update: I've been able to reproduce the last problem even with kafka-streams 2.0.1 and rocksdb 5.14.2 so it seems the problem is down to rocksdb...;;;","27/Apr/20 09:11;lpandzic;I've created a public github repository that reproduces the issue - [https://github.com/lpandzic/kafka-9603].

To reproduce follow the steps in README.md.;;;","27/Apr/20 10:12;biljazovic;To expand on this, it seems that RocksDB version doesn't affect this problem.

Instead, from Streams version 2.1.0 on, when two instances of consumer applications are started, auto compaction in RocksDB is disabled for *standby tasks* on both instances. Furthermore, when one instance is paused, then resumed, then same with the other instance, both instances compact their stores normally.;;;","27/Apr/20 17:07;guozhang;Thanks for uploading the reproduction repo guys! It indeed seems like regression bug.;;;","02/May/20 15:34;RensGroothuijsen;[~biljazovic] It does indeed appear to affect standby tasks only. When I run the reproduction but leave num.standby.replicas at 0, the numbers remain relatively stable.;;;","12/May/20 20:08;cadonna;I think I found the cause of this issue. With segmented state stores (i.e., state stores used to store windowed data), bulk loading mode of RocksDB might be turned on but never off on stand-by tasks. I opened the following PR with a fix:

https://github.com/apache/kafka/pull/8661 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect close of producer instance during partition assignment,KAFKA-9602,13287427,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,25/Feb/20 06:47,26/Feb/20 04:12,13/Jul/23 09:17,26/Feb/20 04:12,2.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,The new StreamProducer instance close doesn't distinguish between an EOS/non-EOS shutdown. The StreamProducer should take care of that.,,ableegoldman,bchen225242,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 04:12:16 UTC 2020,,,,,,,,,,"0|z0bur4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 06:56;githubbot;abbccdda commented on pull request #8166: (WIP) KAFKA-9602: Close the stream producer only in EOS
URL: https://github.com/apache/kafka/pull/8166
 
 
   This bug reproduces through the trunk stream test, the producer was closed unexpectedly when EOS is not turned on.
   
   Will work on adding unit test to guard this logic.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 04:12;githubbot;guozhangwang commented on pull request #8166: KAFKA-9602: Close the stream internal producer only in EOS
URL: https://github.com/apache/kafka/pull/8166
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Workers log raw connector configs, including values",KAFKA-9601,13287421,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,25/Feb/20 06:18,27/Feb/20 05:30,13/Jul/23 09:17,26/Feb/20 23:43,,,,,,,,,,,,,,,,,,,,,,,1.0.3,1.1.2,2.0.2,2.1.2,2.2.3,2.3.2,2.4.1,2.5.0,KafkaConnect,,,,,0,,,,,"[This line right here|https://github.com/apache/kafka/blob/5359b2e3bc1cf13a301f32490a6630802afc4974/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConnector.java#L78] logs all configs (key and value) for a connector, which is bad, since it can lead to secrets (db credentials, cloud storage credentials, etc.) being logged in plaintext.

We can remove this line. Or change it to just log config keys. Or try to do some super-fancy parsing that masks sensitive values. Well, hopefully not that. That sounds like a lot of work.

Affects all versions of Connect back through 0.10.1.

 

*If you are running a version of Connect that contains this vulnerability**, you can set the log level of the* {{org.apache.kafka.connect.runtime.WorkerConnector}} *namespace to* {{INFO}} *or higher in your log4j properties file to prevent raw connector configs from being logged.*",,ChrisEgerton,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 23:43:49 UTC 2020,,,,,,,,,,"0|z0bups:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"25/Feb/20 06:23;githubbot;C0urante commented on pull request #8165: KAFKA-9601: Stop logging raw connector config values
URL: https://github.com/apache/kafka/pull/8165
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9601)
   
   whoopsie daisy
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 21:52;githubbot;rhauch commented on pull request #8165: KAFKA-9601: Stop logging raw connector config values
URL: https://github.com/apache/kafka/pull/8165
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 23:43;rhauch;Thanks for the fix, [~ChrisEgerton]!

Merged to trunk and cherry-picked to the 2.5, 2.4, 2.3, 2.2, 2.1, 2.0, 1.1, and 1.0 branches; I didn't go back farther since it's unlikely we will issue additional patches for earlier branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EndTxn handler should check strict epoch equality,KAFKA-9600,13287289,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,hachikuji,hachikuji,24/Feb/20 18:42,28/Mar/20 17:55,13/Jul/23 09:17,28/Mar/20 17:55,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"The EndTxn path in TransactionCoordinator is shared between direct calls to EndTxn from the client and internal transaction abort logic. To support the latter, the code is written to allow an epoch bump. However, if the client bumps the epoch unexpectedly (e.g. due to a buggy implementation), then the internal invariants are violated which results in a hanging transaction. Specifically, the transaction is left in a pending state because the epoch following append to the log does not match what we expect.

To fix this, we should ensure that an EndTxn from the client checks for strict epoch equality.",,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:55:18 UTC 2020,,,,,,,,,,"0|z0bu4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/20 05:43;githubbot;abbccdda commented on pull request #8164: KAFKA-9600: EndTxn should enforce strict epoch checking if from client
URL: https://github.com/apache/kafka/pull/8164
 
 
   This PR enhances the epoch checking logic for endTransaction call in TransactionCoordinator. Previously it relaxes the checking by allowing a producer epoch bump, which is error-prone since there is no reason to see a producer epoch bump from client.
   
   Since this is purely a server side bug which requires no client side change, we haven't added any integration test to verify this behavior yet.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","28/Mar/20 17:55;githubbot;hachikuji commented on pull request #8164: KAFKA-9600: EndTxn should enforce strict epoch checking if from client
URL: https://github.com/apache/kafka/pull/8164
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create unique sensor to record group rebalance,KAFKA-9599,13287165,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,chia7712,chia7712,chia7712,24/Feb/20 09:30,24/Feb/20 19:40,13/Jul/23 09:17,24/Feb/20 19:40,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,,,,,,0,,,,,"{code:scala}
  val offsetDeletionSensor = metrics.sensor(""OffsetDeletions"")

  ...

  val groupCompletedRebalanceSensor = metrics.sensor(""OffsetDeletions"")
{code}

the ""offset deletion"" and ""group rebalance"" should not be recorded by the same sensor since they are totally different.

the code is introduced by KAFKA-8730",,chia7712,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 16:03:01 UTC 2020,,,,,,,,,,"0|z0btd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/20 16:03;guozhang;Thanks for catching this bug [~chia7712]! Please ping me when you have a PR ready.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing headers causes ConcurrentModificationException,KAFKA-9584,13286608,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mramos,mramos,mramos,21/Feb/20 01:24,30/Sep/20 00:57,13/Jul/23 09:17,30/Sep/20 00:57,2.0.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.2,2.6.1,2.7.0,,,,,streams,,,,,1,,,,,"The consumer record that is used during punctuate is static, this can cause java.util.ConcurrentModificationException when modifying the headers. 

Using a single instance of ConsumerRecord for all punctuates causes other strange behavior:
 # Headers are shared across partitions.
 # A topology that adds a single header could append an infinite number of headers (one per punctuate iteration), causing memory problems in the current topology as well as down stream consumers since the headers are written with the record when it is produced to a topic.  

 

I would expect that each invocation of punctuate would be initialized with a new header object.",,ableegoldman,githubbot,ksbalan2018,mjsax,mramos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 21:04:49 UTC 2020,,,,,,,,,,"0|z0bpxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/20 15:22;mjsax;What `ConsumerRecord` to you refer to? The `punctuate()` call back only has a single long parameter.;;;","21/Feb/20 16:06;mramos;When calling context.headers() from punctuate it returns the headers from this static consumer record [https://github.com/apache/kafka/blob/2.5/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java#L440];;;","23/Feb/20 06:57;mramos;I added an example here [https://github.com/MicahRam/kafka/commit/fddf3aaa38d06b2dc40043cc82239f7a5f705118] These examples sometimes throw null pointer exceptions because of the the same root cause. ;;;","27/Feb/20 06:12;githubbot;MicahRam commented on pull request #8181: KAFKA-9584 Headers ConcurrentModificationException
URL: https://github.com/apache/kafka/pull/8181
 
 
   Prevent exceptions when modifying headers from punctuate by creating a new Headers before punctuate is performed. 
   
   Unit test added
   
   This contribution is my original work and that I license it to the project under the project's open source license.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","27/Feb/20 06:21;mramos;I dont have the necessary permissions to change the status of this ticket to ""Patch Available"";;;","13/Mar/20 21:04;mjsax;Added you to the list of contributors and assigned the ticket to you. You can now modify tickets.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetsForLeaderEpoch requests are sometimes not sent to the leader of partition,KAFKA-9583,13286586,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,andyfangdz,andyfangdz,20/Feb/20 22:52,09/Apr/20 21:07,13/Jul/23 09:17,09/Apr/20 21:07,2.3.1,2.4.0,,,,,,,,,,,,,,,,,,,,,2.5.1,2.6.0,,,,,,,clients,,,,,0,newbie,patch,pull-request-available,,"In [{{validateOffsetsAsync}}|https://github.com/apache/kafka/blob/2.3.1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L737], we group the requests by leader node for efficiency. The list of topic-partitions are grouped from {{partitionsToValidate}} (all partitions) to {{node}} => [{{fetchPostitions}} (partitions by node)|https://github.com/apache/kafka/blob/2.3.1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L739].

However, when actually sending the request with {{OffsetsForLeaderEpochClient}}, we use [{{partitionsToValidate}}|https://github.com/apache/kafka/blob/2.3.1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L765], which is the list of all topic-partitions passed into {{validateOffsetsAsync}}. This results in extra partitions being included in the request sent to brokers that are potentially not the leader for those partitions.

I have submitted a PR, [https://github.com/apache/kafka/pull/8077|https://github.com/apache/kafka/pull/8077], that fixes this issue.",,andyfangdz,githubbot,sujayopensource,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,https://github.com/apache/kafka/pull/8077,,Patch,,,,,,,,,9223372036854775807,,,Thu Apr 09 06:46:51 UTC 2020,,,,,,,,,,"0|z0bpso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/20 11:29;sujayopensource;Hi [~andyfangdz],

 

Will take this up.Let me know if its fine;;;","15/Mar/20 11:56;sujayopensource;I see that a PR has already been raised.

My bad;;;","09/Apr/20 06:46;githubbot;hachikuji commented on pull request #8077: KAFKA-9583: use topic-partitions grouped by node to send OffsetsForLeaderEpoch requests
URL: https://github.com/apache/kafka/pull/8077
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client encountering SASL_HANDSHAKE protocol version errors on 2.5 / trunk,KAFKA-9577,13286362,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,lucasbradstreet,lucasbradstreet,lucasbradstreet,20/Feb/20 01:19,22/Feb/20 05:58,13/Jul/23 09:17,22/Feb/20 05:58,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,core,,,,,0,,,,,"I am trying 2.5.0 with sasl turned on and my consumer and producer clients receive:
{noformat}
org.apache.kafka.common.errors.UnsupportedVersionException: The SASL_HANDSHAKE protocol does not support version 2
{noformat}
I believe this is due to [https://github.com/apache/kafka/commit/0a2569e2b9907a1217dd50ccbc320f8ad0b42fd0] which added flexible version support and bumped the protocol version.

It appears that the SaslClientAuthenticator uses the max version for SASL_HANDSHAKE received in the broker's AP_VERSIONS response, and then uses that version even though it may not support it. See [https://github.com/apache/kafka/blob/eb09efa9ac79efa484307bdcf03ac8eb8a3a94e2/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java#L290]. 

This may make it hard to ever evolve this schema. In the short term I suggest we roll back the version bump and flexible schema until we figure out a path forward.

It appears that this may not have been a problem in the past because the schema versions were the same and maybe we didn't validate the version number [https://github.com/apache/kafka/commit/0cf7708007b01faac5012d939f3c50db274f858d#diff-7f65552a2e23aa7028500f8db06cbb30R47]",,enether,githubbot,ijuma,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 22 05:49:17 UTC 2020,,,,,,,,,,"0|z0boew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/20 01:20;ijuma;cc [~mumrah];;;","20/Feb/20 01:24;lucasbradstreet;We should also fix the client so that it doesn't build requests in this way, but this will not be a sufficient fix until all existing clients are unsupported.;;;","20/Feb/20 14:47;enether;Do we have a system test that would catch this?;;;","20/Feb/20 15:36;lucasbradstreet;[~enether] I will have to check the system test results, but I don't think it's likely unless we have a SASL related test that uses an older client version.;;;","20/Feb/20 19:13;githubbot;lbradstreet commented on pull request #8142: KAFKA-9577: SaslClientAuthenticator incorrectly negotiates supported SaslHandshakeRequest version
URL: https://github.com/apache/kafka/pull/8142
 
 
   The SaslClientAuthenticator incorrectly negotiates supported SaslHandshakeRequest version and  uses the maximum version supported by the broker whether or not the client supports it. This PR rolls back the recent SaslHandshake[Request,Response] bump, fixes the version negotiation, and adds a test to prevent anyone from accidentally bumping the version without a workaround (e.g. a new ApiKey).
   
   Tests:
   - Prevent SASL_HANDSHAKE schema version bump
   - Add test to return ApiVersions unsupported by client
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Feb/20 05:49;githubbot;hachikuji commented on pull request #8142: KAFKA-9577: SaslClientAuthenticator incorrectly negotiates SASL_HANDSHAKE version
URL: https://github.com/apache/kafka/pull/8142
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Contact page links to spam/ads site on search-hadoop[.]com domain,KAFKA-9574,13286216,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gertvdijk,gertvdijk,gertvdijk,19/Feb/20 11:15,20/Feb/20 11:03,13/Jul/23 09:17,20/Feb/20 11:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,website,,,,,0,,,,,"The current live page at [https://kafka.apache.org/contact] displays:
{quote}A searchable archive of the mailing lists is available at search-hadoop[.]com
{quote}
But this is shows as a scam/ads serving site in my browser.

Git history shows me that this link has been present for many years. WHOIS domain info suggests that the domain is transferred 2020-01-22, so I assume this site is now a different site than was intended to link to.

Is there another site that allows to search through all Kafka archives? If not, I suggest to remove the whole paragraph.

I did not find any other links to the domain on the kafka-site repo.",,gertvdijk,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 11:00:00 UTC 2020,,,,,,,,,,"0|z0bnig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 11:20;githubbot;gertvdijk commented on pull request #254: KAFKA-9574: Remove paragraph about searchable archive of the mailing lists
URL: https://github.com/apache/kafka-site/pull/254
 
 
   See [KAFKA-9574](https://issues.apache.org/jira/browse/KAFKA-9574).
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Feb/20 11:00;githubbot;mimaison commented on pull request #254: KAFKA-9574: Remove paragraph about searchable archive of the mailing lists
URL: https://github.com/apache/kafka-site/pull/254
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sum Computation with Exactly-Once Enabled and Injected Failures Misses Some Records,KAFKA-9572,13286173,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,cadonna,cadonna,19/Feb/20 08:30,26/Feb/20 00:11,13/Jul/23 09:17,26/Feb/20 00:11,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"System test {{StreamsEosTest.test_failure_and_recovery}} failed due to a wrongly computed aggregation under exactly-once (EOS). The specific error is:
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Result verification failed for ConsumerRecord(topic = sum, partition = 1, leaderEpoch = 0, offset = 2805, CreateTime = 1580719595164, serialized key size = 4, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = [B@6c779568, value = [B@f381794) expected <6069,17269> but was <6069,10698>
	at org.apache.kafka.streams.tests.EosTestDriver.verifySum(EosTestDriver.java:444)
	at org.apache.kafka.streams.tests.EosTestDriver.verify(EosTestDriver.java:196)
	at org.apache.kafka.streams.tests.StreamsEosTest.main(StreamsEosTest.java:69)
{code} 
That means, the sum computed by the Streams app seems to be wrong for key 6069. I checked the dumps of the log segments of the input topic partition (attached: data-1.txt) and indeed two input records are not considered in the sum. With those two missed records the sum would be correct. More concretely, the input values for key 6069 are:
# 147
# 9250
# 5340 
# 1231
# 1301

The sum of this values is 17269 as stated in the exception above as expected sum. If you subtract values 3 and 4, i.e., 5340 and 1231 from 17269, you get 10698 , which is the actual sum in the exception above. Somehow those two values are missing.

In the log dump of the output topic partition (attached: sum-1.txt), the sum is correct until the 4th value 1231 , i.e. 15968, then it is overwritten with 10698.

In the log dump of the changelog topic of the state store that stores the sum (attached: 7-changelog-1.txt), the sum is also overwritten as in the output topic.

I attached the logs of the three Streams instances involved.",,ableegoldman,apurva,cadonna,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/20 08:21;cadonna;7-changelog-1.txt;https://issues.apache.org/jira/secure/attachment/12993856/7-changelog-1.txt","19/Feb/20 08:16;cadonna;data-1.txt;https://issues.apache.org/jira/secure/attachment/12993858/data-1.txt","19/Feb/20 08:28;cadonna;streams22.log;https://issues.apache.org/jira/secure/attachment/12993853/streams22.log","19/Feb/20 08:27;cadonna;streams23.log;https://issues.apache.org/jira/secure/attachment/12993855/streams23.log","19/Feb/20 08:27;cadonna;streams30.log;https://issues.apache.org/jira/secure/attachment/12993854/streams30.log","19/Feb/20 08:18;cadonna;sum-1.txt;https://issues.apache.org/jira/secure/attachment/12993857/sum-1.txt",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 10:21:13 UTC 2020,,,,,,,,,,"0|z0bn8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/20 05:12;apurva;[~guozhang] can you help take a look at this?;;;","20/Feb/20 19:05;guozhang;Yup I can take a look at it today.;;;","20/Feb/20 22:21;guozhang;It seems that when we injected the error the local state would be wiped since we are closing dirty, and then the tasks got migrated again while the restoration has not completed yet -- in this case we should just update the checkpoint file without committing at all. However in trunk right now this was not done correctly, I will try to piggy-back this fix along with my ongoing PR for handling exceptions: https://github.com/apache/kafka/pull/8058;;;","21/Feb/20 00:17;guozhang;8058 has been merged to trunk, I'm not sure if this is an issue in 2.5.0 though since the refactoring was only on 2.6.0. [~cadonna] did you get this failure in 2.5 or in trunk?;;;","21/Feb/20 07:38;cadonna;No, it was on 2.4. ;;;","25/Feb/20 01:24;guozhang;I looked into the source code of 2.4 and 2.5 and did not find any new issues other than conjectured https://issues.apache.org/jira/browse/KAFKA-8574 – the logs themselves cannot further validates whether it was the root cause, but at least it shows that some restoring tasks are closed before restoration is completed, which could possibly lead to the bug of KAFKA-8574. This bug is fixed as part of the tech debt cleanup as in KAFKA-9113. So I think I have about 60 percent confidence that this issue is no longer there in 2.6 but it would still be in 2.4 and 2.5 since the fix itself incurs a lot of the cleanup it is hard to cherry-pick to older branches.

I'd suggest we close this ticket for 2.6 only and if StreamsEosTest.test_failure_and_recovery failed on trunk again we could look into this once more. WDYT [~cadonna] [~apurva]
;;;","25/Feb/20 10:21;cadonna;Sounds good to me.

It is a pity that we cannot backport the potential fix, but I see that it would be really hard.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker task failing during pool,KAFKA-9571,13286146,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,nitishgoyal13,nitishgoyal13,19/Feb/20 04:30,20/Feb/20 11:12,13/Jul/23 09:17,20/Feb/20 11:12,2.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,consumer,mirrormaker,,,,0,,,,,"I have setup kafka replication between source and target cluster

I am observing Mirror Source task getting killed after certain time with the following error

 

```

[[2020-02-17 22:39:57,344] ERROR Failure during poll. (org.apache.kafka.connect.mirror.MirrorSourceTask:161)
 [2020-02-17 22:39:57,346] ERROR WorkerSourceTask\{id=MirrorSourceConnector-99} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:179)
 [2020-02-17 22:39:57,346] ERROR WorkerSourceTask\{id=MirrorSourceConnector-99} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:180)

```

 

What could be the possible reason for the above?

 ",,nitishgoyal13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 11:11:53 UTC 2020,,,,,,,,,,"0|z0bn2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/20 11:11;nitishgoyal13;It was an issue in our setup

 

Closing the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL cannot be configured for Connect in standalone mode,KAFKA-9570,13286097,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,18/Feb/20 21:25,05/Jun/20 21:47,13/Jul/23 09:17,05/Jun/20 21:47,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.2.0,2.2.1,2.2.2,2.2.3,2.3.0,2.3.1,2.3.2,2.4.0,2.4.1,2.5.0,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,KafkaConnect,,,,,0,,,,,"When Connect is brought up in standalone, if the worker config contains _any_ properties that begin with the {{listeners.https.}} prefix, SSL will not be enabled on the worker.

This is because the relevant SSL configs are only defined in the [distributed worker config|https://github.com/apache/kafka/blob/ebcdcd9fa94efbff80e52b02c85d4a61c09f850b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java#L260] instead of the [superclass worker config|https://github.com/apache/kafka/blob/ebcdcd9fa94efbff80e52b02c85d4a61c09f850b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java]. This, in conjunction with [a call to|https://github.com/apache/kafka/blob/ebcdcd9fa94efbff80e52b02c85d4a61c09f850b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/util/SSLUtils.java#L42] [AbstractConfig::valuesWithPrefixAllOrNothing|https://github.com/apache/kafka/blob/ebcdcd9fa94efbff80e52b02c85d4a61c09f850b/clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java], causes all configs not defined in the {{WorkerConfig}} used by the worker to be silently dropped when the worker configures its REST server if there is at least one config present with the {{listeners.https.}} prefix.

Unfortunately, the workaround of specifying all SSL configs without the {{listeners.https.}} prefix will also fail if any passwords need to be specified. This is because the password values in the {{Map}} returned from {{AbstractConfig::valuesWithPrefixAllOrNothing}} aren't parsed as passwords, but the [framework expects them to be|https://github.com/apache/kafka/blob/ebcdcd9fa94efbff80e52b02c85d4a61c09f850b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/util/SSLUtils.java#L87]. However, if no keystore, truststore, or key passwords need to be configured, then it should be possible to work around the issue by specifying all of those configurations without a prefix (as long as they don't conflict with any other configs in that namespace).",,ChrisEgerton,githubbot,rhauch,wj1918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 21:47:27 UTC 2020,,,,,,,,,,"0|z0bms0:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"18/Feb/20 21:30;githubbot;C0urante commented on pull request #8135: KAFKA-9570: Define SSL configs in all worker config classes, not just distributed
URL: https://github.com/apache/kafka/pull/8135
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9570)
   
   All SSL-related configs are currently defined only in the `DistributedConfig` class, even though they are applicable for standalone mode as well (since standalone mode also supports the Connect REST API). Because of how these configs are parsed by the framework, it's currently impossible to configure Connect in standalone mode to use SSL for the REST API with a password-protected keystore, key, or truststore, and even if no password protection is required, SSL configs will not be picked up correctly by the worker if any of the worker configs start with the `listeners.https.` prefix.
   
   These changes define the relevant SSL-related configs in the parent `WorkerConfig` class, which should fix how they are picked up in standalone mode.
   
   A new unit test is added to verify that the `StandaloneConfig` picks up these configs correctly.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","05/Jun/20 21:47;rhauch;Merged to `trunk` and backported to the `2.6`, `2.5` and `2.4` branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kstreams APPLICATION_SERVER_CONFIG is not updated with static membership,KAFKA-9568,13286029,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,djchuy,djchuy,18/Feb/20 16:36,19/Mar/20 01:53,13/Jul/23 09:17,19/Mar/20 01:53,2.4.0,2.5.0,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"A kstreams application with static membership, and StreamsConfg.APPLICATION_SERVER_CONFIG set, will NOT update old server config upon restart of application on new host.

Steps to reproduce:

 
 # start two kstreams applications (with same consumer group) and enable static membership (and set application server config to <ip-addr>:<port>)
 # kill one of the applications and restart it on a new host(with new ip) before timeout ends (so that rebalancing doesn't occur).
 # the other kstreams application will now have an invalid application_server_config

Possible fix:

If an application restarts with a new host/identity..etc, it could trigger a ""light-rebalance"" where the other applications in the consumer group don't change partition assignments ,but instead just get their configuration updated.",,ableegoldman,bchen225242,djchuy,githubbot,mjsax,o0oxid,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 01:37:43 UTC 2020,,,,,,,,,,"0|z0bmcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/20 22:09;bchen225242;Thanks for reporting this, I'm not sure I fully understood the question here. So let's suppose to have two instances A & B, we first migrate A to another host so that it's APPLICATION_SERVER_CONFIG would change, while instance B should still be using the same ip address and port as it is not restarting. So why would B get an invalid config in this case?;;;","19/Feb/20 03:58;djchuy;B would have an incorrect A mapping.  For example, suppose that there was a state-store with some data on A and other data on B.  Here is some scala that I'm using:

 
{code:java}
val keyHostInfo: Option[IQHost] = Option(streamsRef.metadataForKey(
  storeName,
  docId,
  Implicits.keySerde.serializer)).map(m => IQHost(m.host(), m.port(), thisHost.IQConfig))

//m is of type StreamsMetadata
{code}
StreamsMetadata has mappings that link the partition of docId to a host/port.  Sometimes the host is the current machine, other times its on another machine (as reported by that machine to the controller);;;","19/Feb/20 20:23;bchen225242;I guess the purpose here was to trigger an explicit rebalance. Fortunately, we have an ongoing KIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-568%3A+Explicit+rebalance+triggering+on+the+Consumer]
which could help triggering a rebalance from the client perspective. Feel free to check it out and see if this could help with the situation.;;;","04/Mar/20 18:25;o0oxid;We run Kafka Streams application with _Static Membership_ and uses _Interactive Queries_ functionality. Say we have two instances A and B running on HOST_A and HOST_B correspondingly. Then A migrated to another HOST_C. The APPLICATION_SERVER_CONFIG changes to the IP of HOST_C.  Issue appears when you need to query remote store on the instance A. In that case {{StreamsMetadata}} doesn't get updated and returns ip address of HOST_A, even though A is already migrated to HOST_C.

Let me try to explain the case we are dealing with on an example from kafka-streams-example project.
 # I added static group membership to WordCountInteractiveQueriesExample.java see [https://github.com/o0oxid/kafka-streams-examples/commit/54cb74f40ef3c659588dbedc496bc456123db215] 
 # Started confluent-platform: 
{noformat}
48a1ede4f588        confluentinc/cp-enterprise-kafka:5.4.0   ""/etc/confluent/dock…""   2 hours ago         Up 2 hours          0.0.0.0:9092->9092/tcp, 0.0.0.0:29092->29092/tcp         kafka-streams-examples_kafka_1
bd6cf95c56b7        confluentinc/cp-zookeeper:5.4.0          ""/etc/confluent/dock…""   2 hours ago         Up 2 hours          2181/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:32181->32181/tcp   kafka-streams-examples_zookeeper_1
{noformat}

 # Run first application in static group membership mode - group.instance.id 'client-1' and port '7070'
{noformat}
>  java -cp target/kafka-streams-examples-5.4.0-standalone.jar io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesExample 7070 localhost:9092 client-1
[2020-03-04 10:38:31,071] WARN [main] The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-03-04 10:38:31,072] WARN [main] The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
Mar 04, 2020 10:38:31 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService will be ignored.
[2020-03-04 10:38:49,717] WARN [interactive-queries-example-client-StreamThread-1] [Consumer instanceId=client-1-1, clientId=interactive-queries-example-client-StreamThread-1-consumer, groupId=interactive-queries-example] The following subscribed topics are not assigned to any members: [interactive-queries-example-word-count-repartition]  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
{noformat}

 # Run second application in different terminal - group.instance.id 'client-2' and port '7071'
{noformat}
> java -cp target/kafka-streams-examples-5.4.0-standalone.jar io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesExample 7071 localhost:9092 client-2
[2020-03-04 10:39:05,188] WARN [main] The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-03-04 10:39:05,189] WARN [main] The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
Mar 04, 2020 10:39:05 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService will be ignored.
{noformat}
Querying StreamsMetadata returns two instances 'localhost:7070' and 'localhost:7071':
{noformat}
curl -s localhost:7070/state/instances | jq
[
  {
    ""host"": ""localhost"",
    ""port"": 7070,
    ""storeNames"": [
      ""windowed-word-count""
    ]
  },
  {
    ""host"": ""localhost"",
    ""port"": 7071,
    ""storeNames"": [
      ""word-count""
    ]
  }
]
{noformat}
 

 # Now let's re-run the second application with same group.instance.id 'client-2', but different port '7072'
{noformat}
>java -cp target/kafka-streams-examples-5.4.0-standalone.jar io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesExample 7072 localhost:9092 client-2
[2020-03-04 10:39:05,188] WARN [main] The configuration 'admin.retries' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-03-04 10:39:05,189] WARN [main] The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
Mar 04, 2020 10:39:05 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.examples.streams.interactivequeries.WordCountInteractiveQueriesRestService will be ignored.
{noformat}
Querying instance metadata returns stale data. Note the port is the same '7071', while application is running on '7072':
{noformat}
> curl -s localhost:7070/state/instances | jq
[
  {
    ""host"": ""localhost"",
    ""port"": 7070,
    ""storeNames"": [
      ""windowed-word-count""
    ]
  },
  {
    ""host"": ""localhost"",
    ""port"": 7071,
    ""storeNames"": [
      ""word-count""
    ]
  }
]
{noformat}
So now it's impossible to perform interactive queries against remote machine because metadata points to the old port '7071'.

Issue is not reproducible with dynamic membership.;;;","13/Mar/20 20:54;bchen225242;The current design of static membership will not kick-off rebalance as long as we maintain two members with group.instance.id `client-1` and `client-2`. You have to give another instance id like `client-3` to make it actually rebalance.

There is another open ticket to add the command line support to force remove a static member from admin client:

https://issues.apache.org/jira/browse/KAFKA-9440

That would also help resolve the issue in upcoming 2.6;;;","13/Mar/20 21:17;vvcephei;I think the idea is that the member is still logically the same, but has a new IP address, so the cluster metadata needs to be updated.

The ticket description suggests doing a “light rebalance” so the members can still gossip the new metadata, but the assignment won’t change. It sounds promising to me, but I’m not that deep into static membership. 

Wdyt about that, Boyang?;;;","13/Mar/20 21:38;ableegoldman;In KIP-568/KAFKA-9525 we added a new #enforceRebalance API to the consumer, in part to address the fact that we can't otherwise force static members to rebalance. This was a problem for version probing and the upcoming KIP-441, which rely on triggering rebalances for each member to send their new/updated metadata. This seems like a similar situation, so I think we can leverage #enforceRebalance here as well by having a ""new"" member check the host info in its returned assignment and trigger the group to rebalance if it doesn't match the current expected host info. 

Of course this doesn't help existing apps, anything older than 2.6 would still require manual intervention as Boyang described;;;","14/Mar/20 00:18;githubbot;ableegoldman commented on pull request #8299: KAFKA-9568: enforce rebalance if client endpoint has changed
URL: https://github.com/apache/kafka/pull/8299
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Mar/20 15:44;o0oxid;[~bchen225242] , [~ableegoldman] thanks for explanations. Will be watching the work in scope of KIP-441 which is addressing our case with huge state store. ATM we just use workaround and assign static IP addresses to the static members.;;;","19/Mar/20 01:37;githubbot;guozhangwang commented on pull request #8299: KAFKA-9568: enforce rebalance if client endpoint has changed
URL: https://github.com/apache/kafka/pull/8299
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Kafka connect consumer and producer override documentation,KAFKA-9563,13285510,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,blcksrx,blcksrx,15/Feb/20 13:04,23/Mar/20 03:17,13/Jul/23 09:17,23/Mar/20 03:17,2.3.1,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.0,,,,,,docs,documentation,KafkaConnect,,,0,,,,,"The true parameters for overriding producer config or consumer config in *Kafka-Connect* are  
{code:java}
producer.override.[PRODUCER-CONFIG] {code}
or
{code:java}
consumer.override.[CONSUMER-CONFIG]{code}",,blcksrx,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 03:17:09 UTC 2020,,,,,,,,,,"0|z0bj5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/20 13:29;githubbot;blcksrx commented on pull request #8124: [KAFKA-9563] Fix Kafka connect consumer and producer override docs
URL: https://github.com/apache/kafka/pull/8124
 
 
   The true parameters for overriding producer config or consumer config in **Kafka-Connect** are  
   `producer.override.[PRODUCER-CONFIG] ` or `consumer.override.[CONSUMER-CONFIG]`
   
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","23/Mar/20 03:07;githubbot;kkonstantine commented on pull request #8124: [KAFKA-9563] Fix Kafka connect consumer and producer override docs
URL: https://github.com/apache/kafka/pull/8124
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","23/Mar/20 03:17;kkonstantine;Merged to {{trunk}} and release branches {{2.3, 2.4}} and {{2.5}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams not making progress under heavy failures with EOS enabled on 2.5 branch,KAFKA-9562,13285474,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bchen225242,vvcephei,vvcephei,15/Feb/20 04:13,10/Mar/20 17:52,13/Jul/23 09:17,10/Mar/20 17:52,2.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"During soak testing in preparation for the 2.5.0 release, we have discovered a case in which Streams appears to stop making progress. Specifically, this is a failure-resilience test in which we inject network faults separating the instances from the brokers roughly every twenty minutes.

On 2.4, Streams would obviously spend a lot of time rebalancing under this scenario, but would still make progress. However, on the current 2.5 branch, Streams effectively stops making progress except rarely.

This appears to be a severe regression, so I'm filing this ticket as a 2.5.0 release blocker.",,ableegoldman,bchen225242,cadonna,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 17:52:31 UTC 2020,,,,,,,,,,"0|z0bixk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/20 07:47;mjsax;\cc [~mumrah];;;","20/Feb/20 21:28;githubbot;guozhangwang commented on pull request #8116: KAFKA-9562: part 1: ignore exceptions while flushing stores in close(dirty)
URL: https://github.com/apache/kafka/pull/8116
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Feb/20 17:56;bchen225242;A status update: we have fixed two sub tasks and one flush call try catch. The latest changes are deployed to soak. Will update this ticket once we believe 2.5 is good to go on stream side.;;;","25/Feb/20 17:46;bchen225242;We are soaking the 2.5 with/without EOS for another 2 weeks to make sure the stream side is in good shape.;;;","10/Mar/20 17:52;bchen225242;From the soak result we could close this ticket for now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getListOffsetsCalls doesn't update node in case of leader change,KAFKA-9558,13285405,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,skaundinya,skaundinya,skaundinya,14/Feb/20 19:42,19/Feb/20 17:17,13/Jul/23 09:17,19/Feb/20 17:16,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,admin,,,,,0,,,,,"As seen here:
[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3810]

In handling the response in the `listOffsets` call, if there are errors in the topic partition that require a metadata refresh, it simply passes the call object as `this`. This produces incorrect behavior if there was a leader change, because the call object never gets its leader node updated. This will result in a tight loop of list offsets being called to the same old leader and not resulting in offsets, even though the metadata was correctly updated.",,githubbot,skaundinya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 17:11:50 UTC 2020,,,,,,,,,,"0|z0bii8:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,,,,,"14/Feb/20 22:10;githubbot;skaundinya15 commented on pull request #8119: KAFKA-9558: Fixing retry logic for getListOffsetsCalls
URL: https://github.com/apache/kafka/pull/8119
 
 
   https://issues.apache.org/jira/browse/KAFKA-9558
   
   This PR is to fix the retry logic for `getListOffsetsCalls`. Previously, if there were partitions with errors, it would only pass in the current call object to retry after a metadata refresh. However this is incorrect as if there's a leader change, the call object never gets updated with the correct leader node to query. This PR fixes this by making another call to `getListOffsetsCalls` with only the error topic partitions as the next calls to be made after the metadata refresh. In addition there is an additional test to test the scenario where a leader change occurs.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Feb/20 17:11;githubbot;hachikuji commented on pull request #8119: KAFKA-9558: Fixing retry logic for getListOffsetsCalls
URL: https://github.com/apache/kafka/pull/8119
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Thread-level ""process"" metrics are computed incorrectly",KAFKA-9557,13285394,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,14/Feb/20 17:56,14/Feb/20 21:55,13/Jul/23 09:17,14/Feb/20 21:55,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"Among others, Streams reports the following two thread-level ""process"" metrics:

""process-rate"": The average number of process calls per second.
""process-total"": The total number of process calls across all tasks.

See the docs: https://kafka.apache.org/documentation/#kafka_streams_thread_monitoring

There's some surprising ambiguity in these definitions that has led to Streams actually reporting something different than what most people would probably expect. Specifically, it's not defined what a ""process call"" is.

A reasonable definition of a ""process call"" is processing a record or processing a task (both of which are publicly facing concepts, and both of which are the same, since tasks process records one at a time). However, we currently measure number of invocations to a private, internal `process()` method, which would actually process more than one record at a time. Thus, the current metric is under-counting the throughput, in an esoteric and confusing way.

Instead, we should simply change the rate and total metrics to measure the (rate and total) of _record_ processing.",,ableegoldman,githubbot,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 21:54:45 UTC 2020,,,,,,,,,,"0|z0bifs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/20 21:54;githubbot;vvcephei commented on pull request #8112: KAFKA-9557: correct thread process-rate sensor to measure throughput
URL: https://github.com/apache/kafka/pull/8112
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-558 cannot be fully disabled and when enabled topic reset not working on connector deletion,KAFKA-9556,13285359,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,kkonstantine,rhauch,rhauch,14/Feb/20 15:29,14/Feb/20 22:46,13/Jul/23 09:17,14/Feb/20 22:46,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,KafkaConnect,,,,,0,topic-status,,,,"According to KIP-558 for the new Connect topic tracking feature, Connect should not write the topic status records when the topic is disabled. However, currently that is not the case: when the new topic tracking (KIP-558) feature is disabled, Connect still writes topic status records to the internal status topic. 

Also, according to the KIP, Connect should automatically reset the topic status when a connector is deleted, but that is not happening.

It'd be good to increase test coverage on the new feature.",,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 22:34:39 UTC 2020,,,,,,,,,,"0|z0bi80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/20 15:34;rhauch;[https://github.com/apache/kafka/pull/8085];;;","14/Feb/20 22:34;githubbot;hachikuji commented on pull request #8085: KAFKA-9556: Fix two issues with KIP-558 and expand testing coverage
URL: https://github.com/apache/kafka/pull/8085
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction state loading metric does not count total loading time,KAFKA-9553,13285234,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,agam,hachikuji,hachikuji,14/Feb/20 02:27,20/Mar/20 06:35,13/Jul/23 09:17,20/Mar/20 06:35,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"KIP-484 added a metric to track total loading time for internal topics: https://cwiki.apache.org/confluence/display/KAFKA/KIP-484%3A+Expose+metrics+for+group+and+transaction+metadata+loading+duration. The value seems to be being updated incorrectly in TransactionStateManager. Rather than recording the total loading time, it records the loading separately after every read from the log.",,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 06:35:22 UTC 2020,,,,,,,,,,"0|z0bhg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/20 22:23;githubbot;agam commented on pull request #8155: KAFKA-9553: Improve measurement for loading groups and transactions
URL: https://github.com/apache/kafka/pull/8155
 
 
   Changes:
   - Pull out sensor update to top-level
   - Shifted up the partition-loading sensor measurement for loading transactions
   - Added scheduler-time awareness for loading groups
   
   Tests:
   - Modified test to reflect new scheduling call
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Mar/20 06:35;githubbot;hachikuji commented on pull request #8155: KAFKA-9553: Improve measurement for loading groups and transactions
URL: https://github.com/apache/kafka/pull/8155
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test `RegexSourceIntegrationTest.testRegexMatchesTopicsAWhenDeleted`,KAFKA-9545,13284943,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,hachikuji,hachikuji,13/Feb/20 00:12,17/Feb/20 06:53,13/Jul/23 09:17,17/Feb/20 06:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,,,,,0,,,,,"https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4678/testReport/org.apache.kafka.streams.integration/RegexSourceIntegrationTest/testRegexMatchesTopicsAWhenDeleted/

{code}
java.lang.AssertionError: Condition not met within timeout 15000. Stream tasks not updated
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$5(TestUtils.java:367)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:415)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:383)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:366)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:337)
	at org.apache.kafka.streams.integration.RegexSourceIntegrationTest.testRegexMatchesTopicsAWhenDeleted(RegexSourceIntegrationTest.java:224)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
{code}",,githubbot,hachikuji,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 16 23:55:05 UTC 2020,,,,,,,,,,"0|z0bfnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/20 18:10;githubbot;abbccdda commented on pull request #8109: KAFKA-9545: Fix subscription bugs from Stream refactoring
URL: https://github.com/apache/kafka/pull/8109
 
 
   This PR fixes two bugs related to stream refactoring:
   
   1. The subscribed topics are not updated correctly when topic gets removed from broker. 
   2. The `remainingPartitions` computation doesn't account the condition for one task having a pattern subscription of multiple topics.
   
   The bugs are exposed from `testRegexMatchesTopicsAWhenDeleted` and could be used to verify the fix works.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/Feb/20 17:51;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4734/testReport/org.apache.kafka.streams.integration/RegexSourceIntegrationTest/testRegexMatchesTopicsAWhenDeleted/];;;","15/Feb/20 17:52;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/720/testReport/junit/org.apache.kafka.streams.integration/RegexSourceIntegrationTest/testRegexMatchesTopicsAWhenDeleted/];;;","15/Feb/20 17:53;mjsax;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4733/testReport/junit/org.apache.kafka.streams.integration/RegexSourceIntegrationTest/testRegexMatchesTopicsAWhenDeleted/];;;","16/Feb/20 20:52;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/730/testReport/org.apache.kafka.streams.integration/RegexSourceIntegrationTest/testRegexMatchesTopicsAWhenDeleted/];;;","16/Feb/20 23:55;githubbot;guozhangwang commented on pull request #8109: KAFKA-9545: Fix subscription bugs from Stream refactoring
URL: https://github.com/apache/kafka/pull/8109
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test `KafkaAdminClientTest.testDefaultApiTimeoutOverride`,KAFKA-9544,13284937,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,12/Feb/20 22:43,19/Feb/20 17:26,13/Jul/23 09:17,19/Feb/20 17:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"{code}
org.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1260)
	at org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:594)
	at org.apache.kafka.clients.admin.Admin.close(Admin.java:98)
	at org.apache.kafka.clients.admin.Admin.close(Admin.java:81)
	at org.apache.kafka.clients.admin.AdminClientUnitTestEnv.close(AdminClientUnitTestEnv.java:116)
	at org.apache.kafka.clients.admin.KafkaAdminClientTest.testApiTimeout(KafkaAdminClientTest.java:2642)
	at org.apache.kafka.clients.admin.KafkaAdminClientTest.testDefaultApiTimeoutOverride(KafkaAdminClientTest.java:2595)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}",,githubbot,hachikuji,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 17:24:31 UTC 2020,,,,,,,,,,"0|z0bfm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 22:47;githubbot;hachikuji commented on pull request #8101: KAFKA-9544; Fix flaky test `AdminClientTest.testDefaultApiTimeoutOverride`
URL: https://github.com/apache/kafka/pull/8101
 
 
   There is a race condition with the backoff sleep in the test case and setting the next allowed send time in the AdminClient. To fix it, we allow the test case to do the backoff sleep multiple times if needed.
   
   This was fairly easy to reproduce prior to this fix. With the fix, I could not reproduce the problem after 500 runs.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Feb/20 14:37;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/752/testReport/junit/org.apache.kafka.clients.admin/KafkaAdminClientTest/testDefaultApiTimeoutOverride/];;;","19/Feb/20 17:24;githubbot;hachikuji commented on pull request #8101: KAFKA-9544; Fix flaky test `AdminClientTest.testDefaultApiTimeoutOverride`
URL: https://github.com/apache/kafka/pull/8101
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer offset reset after new segment rolling,KAFKA-9543,13284834,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,boniek,boniek,12/Feb/20 15:10,06/Sep/22 13:32,13/Jul/23 09:17,01/May/20 17:06,2.4.0,2.4.1,2.5.0,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,,,,,,,,,,,,3,,,,,"After upgrade from kafka 2.1.1 to 2.4.0, I'm experiencing unexpected consumer offset resets.

Consumer:
{code:java}
2020-02-12T11:12:58.402+01:00 hostname 4a2a39a35a02 [2020-02-12T11:12:58,402][INFO ][org.apache.kafka.clients.consumer.internals.Fetcher] [Consumer clientId=logstash-1, groupId=logstash] Fetch offset 1632750575 is out of range for partition stats-5, resetting offset
{code}
Broker:
{code:java}
2020-02-12 11:12:58:400 CET INFO  [data-plane-kafka-request-handler-1][kafka.log.Log] [Log partition=stats-5, dir=/kafka4/data] Rolled new log segment at offset 1632750565 in 2 ms.{code}
All resets are perfectly correlated to rolling new segments at the broker - segment is rolled first, then, couple of ms later, reset on the consumer occurs. Attached is grafana graph with consumer lag per partition. All sudden spikes in lag are offset resets due to this bug.",,adupriez,aklochkov,aseigneurin,boniek,brianj,GFriedrich,hachikuji,ijuma,junrao,MarkC0x,nitay.k,noxis,zhangzs,,,,,,,,,,,,,,,,,KAFKA-9838,,,,,KAFKA-9824,,,,,,,,,,,,,,,,,,,"12/Feb/20 15:05;boniek;Untitled.png;https://issues.apache.org/jira/secure/attachment/12993266/Untitled.png","06/Apr/20 21:10;aseigneurin;image-2020-04-06-17-10-32-636.png;https://issues.apache.org/jira/secure/attachment/12999172/image-2020-04-06-17-10-32-636.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 13:32:28 UTC 2022,,,,,,,,,,"0|z0bezc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/20 12:58;adupriez;Interesting. Does the screenshot attached represent consumer lag? Do you have a self-contained reproduction use-case?;;;","18/Feb/20 18:32;boniek;Yes it is consumer lag (every color represents different partition). As you can see before update there was practically none of it (it can be easily deduced when upgrade took place ;)). I will try to reproduce it and let you know how it went.;;;","21/Feb/20 09:55;brianj;I think we've hit the same issue after upgrading from Kafka 2.2.1 to Kafka 2.4.0..

The ""Fetch offset nnn is out of range for partition xxx, resetting offset"" only ever appears in our logs after the upgrade. We've actually downgraded back to 2.2.1 and have seen no more re-occurrences of the issue.

From what we've observed:
 * This does NOT happen on every segment rollover.
 * All of the topics we've seen it occur on are compacted topics.
 * The start offset for the topic partitions in question have often been 0, so the only place the offset can be out of range is at the end.
 * We saw it happen with two different topics, and more frequently on the topic with higher throughput.
 * It only ever occurred in our production environment, where we would have higher throughput.

It is a Kafka Streams application consuming from topic, which resulted in it re-producing a load of downstream messages so we had to revert to the older version. We don't use the ""exactly once"" processing guarantee in this application, but presumably this would've completely broken that guarantee, too.;;;","28/Feb/20 11:23;brianj;I notice the following commit: [https://github.com/apache/kafka/commit/a48b5d900c6b5c9c52a97124a1b51aff3636c32c#diff-ffa8861e850121997a534ebdde2929c6L1436] changed some code and removed the following comment:

{{// If the fetch occurs on the active segment, there might be a race condition where two fetch requests occur after
// the message is appended but before the nextOffsetMetadata is updated. In that case the second fetch may
// cause OffsetOutOfRangeException. To solve that, we cap the reading up to exposed position instead of the log
// end of the active segment.}}

Any chance that this could have re-introduced the race condition described?;;;","28/Feb/20 14:52;boniek;I cannot reproduce it on my development environment.  Couple of facts to add what Brian wrote:
 * This indeed does not happen on every segment rollover, but when it happens it is always on segment rollover
 * We have no compacted topics in our production cluster, so topic type doesn't matter.
 * No topic in our production environment starts at offset 0 - so this doesn't matter as well.
 * Topic where we definetly seen this happen has about 5MB/s traffic (so not that much traffic)
 * Fetch offset ... is out of range for partition is always about offset ""from the future"" (top, not bottom of the log). I assume kafka broker thinks it does not have this offset in log. This assumption is based on low resolution (every 5 minutes) offset data (start and end of the partition log) we gather from jmx. This suggests that maybe offsets are incorrectly cached or cache update has race condition. Also notice that before update client had 0 lag (you can see this in my attached screenshot), so probably this is crucial to reproduce this bug - you have to be reading top of the log all/most of the time to hit this.
 * we tested this in our development environment, where we load generated about 5MB/s  traffic (using kafka-producer-perf-test.sh) and read it back (using identically , as in production environment, configured consumer) at the same time as it was written and cannot reproduce this. Test ran for 3 days non stop - we looked for offset resets and there were none.;;;","06/Apr/20 21:11;aseigneurin;We have been seeing the same issue here with Kafka 2.4.0.

Here is an example: at 2020-04-05 06:54:15.746, the offsets recorded on the consumers were between 623860252 and 623869089, and we received a record with offset 405637478. These offsets and times coincide with logs seen on the brokers (see attached screenshot).

!image-2020-04-06-17-10-32-636.png!;;;","09/Apr/20 00:34;hachikuji;I think this is the same issue as KAFKA-9824. I have been trying to reproduce it in a test case, but no luck so far. I found a case which could result in unexpected out of range errors in KAFKA-9835, but I'm not sure that's what we're looking for here given the coincidence of segment rolling which we have now seen in several independent reports. I guess it's at least theoretically possible that we get a sequence like this:

1. Broker accepts append and rolls segment
2. Data is written to new segment
3. Consumer fetches from previous log end and hits KAFKA-9835 which results in receiving uncommitted data.
4. Consumer fetches again from the new log end offset which results in the out of range error
5. Broker updates new log end offset.

This would require both KAFKA-9835 to be hit (or some similar error) combined with an edge case like the one that [~brianj] mentioned above. I'm having a hard time accepting this though. In my testing I added an explicit sleep between the segment append and the update of the log end offset and I still couldn't manage to reproduce a sequence like the one above. It's possible I'm missing some detail though.

If anyone has a way to reproduce this issue reliably, it would help to have a dump from the segments spanning the log roll. The main thing I want to understand is whether the ""out of range"" data is on the new segment or the old one.
;;;","16/Apr/20 07:11;hachikuji;In KAFKA-9838, I fixed what I thought was a minor race condition with truncation. Since there is no truncation involved with a segment roll, I thought it was unrelated. After thinking a little more about the edge case it uncovered, it seems possible it can explain this issue as well. When we roll a segment, if the log end offset is equal to the high watermark, then both will be updated to point to the new segment. The problem is that the leader also keeps a cache of pointers to offsets which may refer to the older segment. It is possible that a leader update following the roll would leave the high watermark pointing to the older segment. This was not a problem prior to 2.4 because we always looked up the position of the high watermark on every fetch.

As an example, let's say we have a single segment which begins at offset 0 and suppose that we have log end offset = high watermark = 5. The sequence of events is like this:

t1: Initial state
log end offset = (offset = 10, segment = 0)
high watermark = (offset = 5, segment = 0)

t2: Log is rolled
log end offset = (offset = 10, segment = 10)
high watermark = (offset = 5, segment = 0)

t3: Leader calls `maybeIncrementHighWatermark` to update high watermark to 10, but with a reference to the old segment:
log end offset = (offset = 10, segment = 10)
high watermark = (offset = 10, segment = 0)

I verified with a simple test case that the log can get into this state. Prior to the fix in KAFKA-9838, a fetch from the high watermark with the log in this state would result in a read of all the data from segment 10. So that opens the door to a race condition like the following:

1. Say one record is appended and we update the log end offset:
log end offset = (offset = 11, segment = 10)
high watermark = (offset = 10, segment = 0)

2. Now say that one more record is appended to segment 10 at offset 11, but log end offset is not immediately updated
3. A fetch at offset 10 returns the two new record at offset 10 and 11 because of the bug above.
4. A second fetch at offset 12 now returns out of range error because log end offset is still 11
5. Finally log end offset is updated to 12.

This is a devilishly tricky scenario to hit in a test case. I was able to do it, but only by introducing an artificial delay into the append logic. Still I think this is probably on the right track since it explains how it is possible to hit an out of range error with only a segment roll and also why versions older than 2.4 are not affected.

Unfortunately the patch for KAFKA-9838 did not get into 2.5.0, which was just released today. However, I've merged it into the [2.4|https://github.com/apache/kafka/commit/e1f18df7f6615109b6cc77b66d9be37b09256a0a] and [2.5|https://github.com/apache/kafka/commit/3b17fecc9b6af5f896ce2df4b8b6ce23cfd40f17] branches. If anyone who is running into this problem is willing to try one of these patches and let me know, I'd appreciate it. I will think a little bit more whether the patch fixes all problems that can be caused by this case when the high watermark points to the wrong segment (I think it does, but not 100% sure).;;;","17/Apr/20 08:23;brianj;Thanks Jason - that sounds promising. I'll keep an eye for the 2.4.2 / 2.5.1 / 2.6.0 release. We only ever hit this in production, which makes testing tricking, but the fact that you have such a solid explanation of why it might've happened and why it only affected 2.4 is very reassuring.;;;","17/Apr/20 16:40;junrao;[~hachikuji]: Thanks for the analysis. It does seem this issue could be caused by KAFKA-9838.

Also, in Log.read(), we have code like the following. If we get to the _else_ part, the assumption is that maxOffsetMetadata.segmentBaseOffset > segment.baseOffset. Perhaps it's useful to assert that. That may help uncover issues that we may not know yet.

 

 
{code:java}
val maxPosition = {
 // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.
 if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {
 maxOffsetMetadata.relativePositionInSegment
 } else {
 segment.size
 }
}{code}
 ;;;","17/Apr/20 17:09;hachikuji;[~junrao] Good suggestion. I opened https://issues.apache.org/jira/browse/KAFKA-9886 to fix this separately. Note that I'll leave the issue here open until we can confirm whether it is fixed by KAFKA-9838.;;;","01/May/20 17:05;hachikuji;I am going to go ahead and close this issue as fixed by KAFKA-9838. We were able to validate the fix in one environment that was seeing these out of range errors consistently. After investigation, we found a similar problem where the out of range errors coincided with segment rolls. After deploying the fix, these errors have stopped.;;;","14/Aug/20 23:23;aklochkov;This issue is marked as fixed in 2.4.2 and 2.5.1, with the last comment saying that it's fixed by KAFKA-9838, while KAFKA-9838 has a fix version of 2.6.0. Is this defect fixed in 2.5.1?;;;","06/Sep/22 13:32;ijuma;I updated the fix version for KAFKA-9838, the commit was backported to 2.5.x and 2.4.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test DescribeConsumerGroupTest#testDescribeGroupMembersWithShortInitializationTimeout,KAFKA-9541,13284718,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,huxi_2b,huxi_2b,12/Feb/20 04:01,24/Feb/20 08:49,13/Jul/23 09:17,23/Feb/20 18:38,2.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,core,unit tests,,,,0,,,,,"h3. Error Message

java.lang.AssertionError: assertion failed
h3. Stacktrace

java.lang.AssertionError: assertion failed at scala.Predef$.assert(Predef.scala:267) at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupMembersWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:630) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at jdk.internal.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56) at java.base/java.lang.Thread.run(Thread.java:834)",,githubbot,hachikuji,huxi_2b,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9530,,,,,,,,,,,,,,,KAFKA-8110,KAFKA-9530,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 23 18:40:26 UTC 2020,,,,,,,,,,"0|z0be9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 04:04;huxi_2b;Occasionally the captured exception is DisconnectException instead of TimeoutException. That might be due to an unexpected long pause that caused the node disconnection.;;;","12/Feb/20 04:30;githubbot;huxihx commented on pull request #8094: KAFKA-9541:Flaky Test DescribeConsumerGroupTest#testDescribeGroupMembersWithShortInitializationTimeout
URL: https://github.com/apache/kafka/pull/8094
 
 
   https://issues.apache.org/jira/browse/KAFKA-9541
   
   Occasionally the captured exception is DisconnectedException instead of TimeoutException. That might be due to an unexpected long pause that caused the node disconnection.
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Feb/20 20:35;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/659/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupMembersWithShortInitializationTimeout/];;;","15/Feb/20 17:50;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/721/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupMembersWithShortInitializationTimeout/]

Different stack trace:
{quote}java.lang.AssertionError: expected:<class org.apache.kafka.common.errors.TimeoutException> but was:<class org.apache.kafka.common.errors.DisconnectException> at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.failNotEquals(Assert.java:835) at org.junit.Assert.assertEquals(Assert.java:120) at org.junit.Assert.assertEquals(Assert.java:146) at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupMembersWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:630){quote};;;","18/Feb/20 19:28;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/736/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupMembersWithShortInitializationTimeout/]

DisconnectException again instead of Timeout.;;;","23/Feb/20 12:21;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/849/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupMembersWithShortInitializationTimeout/]
 

DisconnectException instead of Timeout.
 ;;;","23/Feb/20 18:37;hachikuji;This is fixed by KAFKA-9530.;;;","23/Feb/20 18:40;githubbot;hachikuji commented on pull request #8094: KAFKA-9541:Flaky Test DescribeConsumerGroupTest#testDescribeGroupMembersWithShortInitializationTimeout
URL: https://github.com/apache/kafka/pull/8094
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Application getting ""Could not find the standby task 0_4 while closing it"" error",KAFKA-9540,13284682,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ableegoldman,badai,badai,11/Feb/20 22:13,15/Apr/20 21:15,13/Jul/23 09:17,18/Feb/20 18:13,2.4.0,2.5.0,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.1,2.6.0,,,,,,streams,,,,,0,,,,,"Because of this the following line, there is a possibility that some standby tasks might not be created:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L436

Then causing this line to not adding the task to standby task list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L299

But this line assumes that all standby tasks are to be created and add it to the standby list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java#L168

This results in user getting this error message on the next PARTITION_ASSIGNMENT state:

{noformat}
Could not find the standby task 0_4 while closing it (org.apache.kafka.streams.processor.internals.AssignedStandbyTasks:74)
{noformat}

But the harm caused by this issue is minimal: No standby task for some partitions. And it is recreated on the next rebalance anyway. So, I suggest lowering this message to WARN. Or probably check to WARN when standby task could not be created.",,ableegoldman,badai,cadonna,githubbot,slachiewicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 08:15:59 UTC 2020,,,,,,,,,,"0|z0be1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/20 22:45;ableegoldman;Hey [~badai], thanks for the ticket. Your analysis is correct that this should not be logged as an error, since the cause is a completely valid situation: a standby task is not created if there are no state stores for it to actually work on. That particular standby task will actually never get created, so whichever thread ends up with this task will always hit this upon closing it. It's probably ok to go all the way down to debug, since warn might still suggest to users that something is wrong.

I think this is actually fixed in trunk already due to some significant refactoring of the task management code. But I can quick together a quick PR to bump down the log level on 2.4/2.5 (won't make it into 2.5.0 but may get into 2.4.1)

Of course the real fix would be for the assignor to be smart enough not to assign these ghost standbys to begin with. We should be able to fix that up as part of KIP-441 ;;;","11/Feb/20 22:56;githubbot;ableegoldman commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: https://github.com/apache/kafka/pull/8092
 
 
   As described in the ticket, this message is logged at the error level but only indicates that a standby task was not created (as is the case if its subtopology is stateless). Moving this to debug level, and clarifying the implications in the log level.
   
   Targeting this PR against 2.4, as the issue is incidentally fixed in trunk as part of the tech debt cleanup. We should also merge this fix to 2.5 but need to wait for the release, since this is obviously not a blocker
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Feb/20 22:59;githubbot;guozhangwang commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: https://github.com/apache/kafka/pull/8092
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Feb/20 08:15;cadonna;[~ableegoldman] [~guozhang] Can we close this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test `testResetOffsetsExportImportPlan`,KAFKA-9538,13284650,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,huxi_2b,hachikuji,hachikuji,11/Feb/20 19:20,13/Feb/20 06:41,13/Jul/23 09:17,13/Feb/20 06:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"{code}
19:44:41 
19:44:41 kafka.admin.ResetConsumerGroupOffsetTest > testResetOffsetsExportImportPlan FAILED
19:44:41     java.lang.AssertionError: expected:<Map(bar2-0 -> 2, bar2-1 -> 2)> but was:<Map()>
19:44:41         at org.junit.Assert.fail(Assert.java:89)
19:44:41         at org.junit.Assert.failNotEquals(Assert.java:835)
19:44:41         at org.junit.Assert.assertEquals(Assert.java:120)
19:44:41         at org.junit.Assert.assertEquals(Assert.java:146)
19:44:41         at kafka.admin.ResetConsumerGroupOffsetTest.testResetOffsetsExportImportPlan(ResetConsumerGroupOffsetTest.scala:429)
{code}",,githubbot,hachikuji,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 06:40:48 UTC 2020,,,,,,,,,,"0|z0bdug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 20:36;mjsax;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/659/testReport/junit/kafka.admin/ResetConsumerGroupOffsetTest/testResetOffsetsAllTopicsAllGroups/];;;","13/Feb/20 06:40;githubbot;hachikuji commented on pull request #6561: KAFKA-9538: Flaky test: testResetOffsetsExportImportPlan
URL: https://github.com/apache/kafka/pull/6561
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abstract transformations in configurations cause unfriendly error message.,KAFKA-9537,13284638,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jcustenborder,jcustenborder,jcustenborder,11/Feb/20 18:07,15/May/20 19:03,13/Jul/23 09:17,15/May/20 19:03,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.2,2.5.1,2.6.0,,,,,,KafkaConnect,,,,,0,,,,,"I was working with a coworker who had a bash script posting a config to connect with
{code:java}org.apache.kafka.connect.transforms.ExtractField.$Key{code} in the script. Bash removed the $Key because it wasn't escaped properly.
{code:java}
org.apache.kafka.connect.transforms.ExtractField.{code}
is made it to the rest interface. A Class<?> was create for the abstract implementation of ExtractField and passed to getConfigDefFromTransformation. It tried to call newInstance which threw an exception. The following gets returned via the rest interface. 

{code}
{
  ""error_code"": 400,
  ""message"": ""Connector configuration is invalid and contains the following 1 error(s):\nInvalid value class org.apache.kafka.connect.transforms.ExtractField for configuration transforms.extractString.type: Error getting config definition from Transformation: null\nYou can also find the above list of errors at the endpoint `/{connectorType}/config/validate`""
}
{code}

It would be a much better user experience if we returned something like 
{code}
{
  ""error_code"": 400,
  ""message"": ""Connector configuration is invalid and contains the following 1 error(s):\nInvalid value class org.apache.kafka.connect.transforms.ExtractField for configuration transforms.extractString.type: Error getting config definition from Transformation: Transformation is abstract and cannot be created.\nYou can also find the above list of errors at the endpoint `/{connectorType}/config/validate`""
}
{code}

or
{code}
{
  ""error_code"": 400,
  ""message"": ""Connector configuration is invalid and contains the following 1 error(s):\nInvalid value class org.apache.kafka.connect.transforms.ExtractField for configuration transforms.extractString.type: Error getting config definition from Transformation: Transformation is abstract and cannot be created. Did you mean ExtractField$Key, ExtractField$Value?\nYou can also find the above list of errors at the endpoint `/{connectorType}/config/validate`""
}
{code}

",,githubbot,jcustenborder,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 19:03:14 UTC 2020,,,,,,,,,,"0|z0bdrs:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"11/Feb/20 18:45;githubbot;jcustenborder commented on pull request #8090: KAFKA-9537 - Cleanup error messages for abstract transformations
URL: https://github.com/apache/kafka/pull/8090
 
 
   Added check if the transformation is abstract. If so throw an error message with guidance for the user. 
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","15/May/20 19:03;rhauch;Merged to `trunk` for inclusion in 2.6.0, and cherry-picked to `2.5` (for future 2.5.1 release) and `2.4` (for future 2.4.2 release).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metadata not updated when consumer encounters FENCED_LEADER_EPOCH,KAFKA-9535,13284434,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bchen225242,bchen225242,bchen225242,10/Feb/20 23:08,16/Feb/20 23:36,13/Jul/23 09:17,16/Feb/20 23:36,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.1,2.5.0,,,,,,,,,,,0,,,,,"Inside the consumer Fetcher's handling of ListOffsetResponse, if we hit `FENCED_LEADER_EPOCH` on partition level, the client will blindly retry without refreshing the metadata, creating a stuck state as the local leader epoch never gets updated and constantly fails the broker check.

The solution is to trigger metadata update upon receiving retriable errors, before we kick off another offset list.",,bchen225242,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 16 20:06:38 UTC 2020,,,,,,,,,,"0|z0bcig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/20 06:51;githubbot;abbccdda commented on pull request #8088: KAFKA-9535: Update metadata upon receiving FENCED_LEADER_EPOCH in ListOffset
URL: https://github.com/apache/kafka/pull/8088
 
 
   Today if we attempt to list offsets with a fenced leader epoch, consumer will infinitely retry without updating the metadata. The fix is to trigger the metadata update call when we see `FENCED_LEADER_EPOCH`, even as partial failure.
   `UNKNOWN_LEADER_EPOCH`, on the other hand, indicates a metadata staleness on broker side, so consumer don't have to update metadata.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","16/Feb/20 20:06;githubbot;hachikuji commented on pull request #8088: KAFKA-9535: Update metadata upon retrying partitions for ListOffset
URL: https://github.com/apache/kafka/pull/8088
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaDocs of KStream#ValueTransform incorrect,KAFKA-9533,13284392,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,mviamari,mviamari,10/Feb/20 19:31,14/Mar/20 18:56,13/Jul/23 09:17,14/Mar/20 18:56,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,streams,,,,,0,,,,,"According to the documentation for `KStream#transformValues`, nulls returned from `ValueTransformer#transform` are not forwarded. (see [KStream#transformValues|https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KStream.html#transformValues-org.apache.kafka.streams.kstream.ValueTransformerSupplier-java.lang.String...-])

However, this does not appear to be the case. In `KStreamTransformValuesProcessor#process` the result of the transform is forwarded directly.
{code:java}
 @Override
 public void process(final K key, final V value) {
     context.forward(key, valueTransformer.transform(key, value));
 }
{code}",,ableegoldman,bbejeck,cadonna,githubbot,mjsax,mviamari,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 18:56:50 UTC 2020,,,,,,,,,,"0|z0bc94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/20 09:38;cadonna;[~mviamari] Thank you for opening this ticket. This seems to be a bug indeed.;;;","12/Feb/20 14:49;cadonna;[~mviamari] Would you be interested in fixing this bug?;;;","13/Feb/20 05:23;mviamari;Sure. I can take a look at it. Before I get started:

1) How should I think about the case where someone might unintentionally (or intentionally) be relying on the buggy behavior? Is that something I should handle in code, or is it handled elsewhere?

2) I noticed that KStream#transform uses an adaptor to conform to KStream#flatTransform. Should I do something similar for KStream#transformValues and KStream#flatTransformValues, since I will be modifying that area of code already?;;;","13/Feb/20 10:07;cadonna;Thank you for picking this up!

Ad 1) We do not support backwards compatibility of bugs. The documentation is clear, we just failed to implement it correctly.

Ad 2) If you want to try to simplify the code with the adapter as in {{transform()}} and {{flatTransform()}} you are very welcome to do so. For a smoother review process, I would like to ask you to create two PRs, one for the refactoring and one for the bug fix.

Are you able to assign the ticket to yourself?;;;","13/Feb/20 15:11;mviamari;Ok. Great. I'll address the adaptor code separately if necessary.

I cannot yet assign this to myself.;;;","13/Feb/20 17:35;mjsax;[~mviamari] Thanks for picking up this ticket – I added you to the list of contributors and you can now self-assign tickets.;;;","13/Feb/20 17:51;githubbot;mviamari commented on pull request #8108: KAFKA-9533: ValueTransform forwards `null` values
URL: https://github.com/apache/kafka/pull/8108
 
 
   Fixes a bug where `KStream#transformValues` would forward null values from the provided `ValueTransform#transform` operation.
   
   A test was added for verification `shouldNotForwardNullTransformValuesWithValueTransformerWithKey`. A parallel test for non-key ValueTransformer was not added, as they share the same code path.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","19/Feb/20 21:20;githubbot;bbejeck commented on pull request #8108: KAFKA-9533: ValueTransform forwards `null` values
URL: https://github.com/apache/kafka/pull/8108
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","20/Feb/20 16:50;bbejeck;cherry-picked to 2.5, 2.4, 2.3 and 2.2;;;","25/Feb/20 15:51;vvcephei;Hi [~mviamari] ,

Since merging this fix into trunk, we've discovered some use cases that actually depend on the prior behavior. In retrospect, I think that we should fix this issue by correcting the docs, not changing the behavior.

 
I bet this started with a simple copy/paste error from the {{transform}} API, in which returning {{null}} (as opposed to a {{KeyValue}} does mean to drop the record). But in that case, it makes sense, since we cannot process a record with no key. The choice is between throwing an NPE and dropping the record, so we drop the record. But this doesn't apply to {{transformValues}}, because the result would have the same key as the input.
 
Really, it's murky territory either way... Since we have {{transorm}} and {{flatTransform}}, as well as the {{xValues}} overloads, it implies that {{transform}} is one-in-to-one-out and {{flatTransform}} is one-in-to-zero-or-more-out. In that case, if you did want to drop an input record, you should use a {{flatTransform}} and return an empty collection. There _should_ be an invariant for {{transform}} and {{transformValues}} that you get exactly the same number of output records as input records, whether they are {{null}} or otherwise.
 
Since it seems like the docs, not the behavior, are wrong, and since this is pre-existing behavior going back a long way in Streams (which breaks some users' code to change), we should go ahead and back out the PR. Sorry for the confusion.
 
As far as the actual fix goes, I'd be in favor of amending the docs to state that:
1. When {{KStream#transformValues}} returns {{null}}, there will simply be an output record with a {{null}} value.
2. When {{KStream#transform}} returns {{null}}, we consider that to be an invalid return, and we will log a warning while dropping the record (similar to other APIs in Streams)
3. When {{KStream#transform}} returns a {{KeyValue(key, null)}}, there will simply be an output record with a null value.
4. When {{KStream#flatTransform}} and {{KStream#flatTransformValues}} return {{null}}, we consider that to be equivalent to returning an empty collection, in which case, we don't forward any results, and also do not log a warning. (I'd also be in favor of logging a warning here for consistency, but it seems like overkill).
 
Accordingly, I'm going to re-open this ticket, and Bill will revert the changes to fix the downstream builds. Sorry again for the trouble.
-John;;;","25/Feb/20 18:16;mviamari;Ok. Thanks for the detailed explanation. I'll take a look at the changes you're proposing here as well.;;;","25/Feb/20 18:25;githubbot;bbejeck commented on pull request #8167: KAFKA-9533: Revert  ValueTransform forwards `null` 
URL: https://github.com/apache/kafka/pull/8167
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Feb/20 18:27;bbejeck;reverted cherry-picks to 2.5, 2.4, 2.3, and 2.2;;;","13/Mar/20 21:29;mjsax;The incorrect JavaDocs were introduced via [https://github.com/apache/kafka/commit/05668e98f531cf4d6ddb0696f0f72675ca128581] – ie, only 2.3.0 and newer is affected.;;;","13/Mar/20 21:30;githubbot;mjsax commented on pull request #8298: KAFKA-9533: Fix JavaDocs of KStream.transformValues
URL: https://github.com/apache/kafka/pull/8298
 
 
   We might want to cherry-pick this back to 2.3.
   
   Call for review @bbejeck 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Mar/20 18:34;githubbot;mjsax commented on pull request #8298: KAFKA-9533: Fix JavaDocs of KStream.transformValues
URL: https://github.com/apache/kafka/pull/8298
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Mar/20 18:56;mjsax;Marking this as fixed version `2.6.0` and `2.5.1` – if we roll a new RC for 2.5, this should be updated to `2.5.0`. \cc [~mumrah];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application Reset Tool Returns NPE when --to-datetime or --by-duration are run on --input-topics with empty partitions ,KAFKA-9527,13284098,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,marcolotz,jbfletch,jbfletch,08/Feb/20 20:39,12/Apr/21 16:29,13/Jul/23 09:17,12/Apr/21 16:28,2.3.0,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,,,,streams,tools,,,,0,,,,,"When running the streams application reset tool with --by-duration or --to-datetime if any partitions for a given input topic are empty a NPE is thrown.  I tested this with a topic with 3 partitions, I received a NPE until all 3 partitions had at least one message.  The behavior was the same for both --to-datetime and --by-duration. 

Error below:

Reset-offsets for input topics [sample-cdc-topic]Reset-offsets for input topics [sample-cdc-topic]Following input topics offsets will be reset to (for consumer group des-demo-stream)ERROR: java.lang.NullPointerExceptionjava.lang.NullPointerException at kafka.tools.StreamsResetter.resetToDatetime(StreamsResetter.java:496) at kafka.tools.StreamsResetter.maybeReset(StreamsResetter.java:426) at kafka.tools.StreamsResetter.maybeResetInputAndSeekToEndIntermediateTopicOffsets(StreamsResetter.java:374) at kafka.tools.StreamsResetter.run(StreamsResetter.java:164) at kafka.tools.StreamsResetter.run(StreamsResetter.java:131) at kafka.tools.StreamsResetter.main(StreamsResetter.java:678)

 

 ",,ableegoldman,cadonna,jbfletch,jeqo,marcolotz,mjsax,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 06 13:08:13 UTC 2021,,,,,,,,,,"0|z0bafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/21 09:39;marcolotz;[~mjsax] what would be the expected behaviour in this scenario? I see three possible behaviours:
 # Throwing a Kafka specific exception
 # Ignoring the empty partition silently
 # Ignoring the empty partition and logging a warn 

I would guess that 2 is the way to go, because an empty partition is technically speaking, reset to any duration or timestamp that can be requested.;;;","03/Feb/21 01:50;mjsax;I guess the problem is, that we don't have any timestamp information from the time index, and thus we cannot get the offset we want to commit. – Silently ignoring seems the worst option from the three you suggested, because in the end, we don't know where the application will effectively start to consume – either there is an exiting committed offset or we might fall back to `auto.offset.reset`. Note, that we cannot make any assumption what the time gap between resetting and restarting is, and thus, new data might be written into the topic in the meantime. Thus, I think we should at least inform the user that no offset was committed for some partitions (and report the corresponding partitions), thus, option 3 might be the best? Option 1 would also be ok IMHO. With one is better seems to depend how ""robust"" the tool should be (don't have strong opinion about it). The point is that both options allow the user to react without the danger of unexpected behavior on restart of the application.

Btw: I would assume that this issue not only affects the ""stream reset tool"" but also the ""consumer group"" tool that offset a similar feature (cf. KIP-122 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-122%3A+Add+Reset+Consumer+Group+Offsets+tooling] and KIP-171 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application])

Not sure what behavior [~jbfletch] had in mind? Maybe [~jeqo] (who proposed the feature originally) has some input?

Btw: the ticket is assigned to [~jbfletch] so she should first agree to hand it off to you before you start working on it.;;;","03/Feb/21 18:34;marcolotz;[~mjsax] I see your point. Indeed in this scenario makes more sense to notify the user about it.

The bug is caused because of this line [here|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L516] - that returns by default a null value on the map.
 The following method
{code:java}
client.seek(topicPartition, topicPartitionsAndOffset.get(topicPartition).offset());{code}
uses it as an argument without optional handling - which indeed causes a NPE for keys with null values on the map.
 [~jbfletch] it think it should be straight forward to fix, do you mind if I assign the bug to me?;;;","03/Feb/21 21:38;jeqo;Hi there!

The NPE is caused by topic partitions where no offset is found after a timestamp, not by consumer offsets related to the topic partition.

This could mean topic-partition that hasn't received records yet, or a topic partition with no records after a retention delete exec (actually I'm not sure about this scenario if it returns null or the latest known offset, but anyway).

At the moment, here is how it's handled in the consumer group's reset-offset: [https://github.com/apache/kafka/blob/70404baffa47b99914d34143e779b0e65522a5ef/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L672-L688]

Where, if offset by timestamp is null, then it fallbacks to the end offset. 

Resetting to the latest offset seem like a sensible default, as resetting by timestamp/duration command allows to go backwards (most cases) or forward (same as to-latest?). Resetting to offset=0 when no records (i.e. as backwards as possible) or offset=latest when all records removed seem to match the semantics.

Also, dry-run could be seen as a kind of ""logging"".

Bringing this into the table to be considered while the NPE is fixed.;;;","03/Feb/21 22:04;mjsax;{quote}Where, if offset by timestamp is null, then it fallbacks to the end offset.
{quote}
Interesting. Was this part of the KIP design or just a PR-scoped decision? Overall, it might be best to align both tools to do the same thing?
{quote}Also, dry-run could be seen as a kind of ""logging"".
{quote}
Well, what would --dry-run print for this case? The offset number of end-offset or a hint that it did not ""seek by time"" but defaulted to ""end offset"". The later seems more reasonable to me? Personally, I would still suggest to also log a WARN for this case during execution, as we should not rely on people to do a dry-run first.;;;","03/Feb/21 22:14;jeqo;> Interesting. Was this part of the KIP design or just a PR-scoped decision? Overall, it might be best to align both tools to do the same thing?

I can't find comments about this in the KIP or the thread, so a PR-scoped decision most probably.

> Well, what would --dry-run print for this case? 

Atm prints the end offset.

> The later seems more reasonable to me? Personally, I would still suggest to also log a WARN for this case during execution, as we should not rely on people to do a dry-run first.

Good point. Will create a ticket to follow this up.;;;","03/Feb/21 22:18;jeqo;https://issues.apache.org/jira/browse/KAFKA-12287;;;","04/Feb/21 13:30;marcolotz;[~jeqo] Understood. About returning the latest offset - if we perform no .seek() operation on empty partitions, the consumer offset will be handled by ""auto.offset.reset"" configuration - and thus by default return latest.

Should I rely on this configuration or you rather have an explicit client.seekToEnd() call?

Since the reporter is currently inactive and is an old ticket, I am taking it over and will forward a PR in a bit.;;;","04/Feb/21 14:03;marcolotz;PR ready.
Please let me know your thoughts and what needs changing (e.g. explicit seekToEnd or the displayed message).
Also, I couldn't find a reason for the CI failing on JDK11 and passing on 8 and 15. Did I make something wrong or is it flaky?;;;","05/Feb/21 19:28;jbfletch;Thanks for taking this on [~marcolotz];;;","05/Feb/21 21:12;mjsax;Thanks everyone. I'll try to review the PR soon – but we have 2.8.0 deadline and we want to do a 2.6.2 release that I need to take care of first. – We can discuss more details on the PR. And yes, we have flaky tests so don't worry about it.;;;","06/Feb/21 13:08;jeqo;[~marcolotz], relying on ""auto.offset.reset"" would be different from seeking to end in the specific scenario where there is new records available in a topic-partition that was empty when offsets were reset.

In this scenario, when consumers resume polling, consumption from this topic-partition will be different depending on ""auto.offset.reset"": latest or earliest.

`seekToEnd` will make the starting point consistent though. To keep this change consistent with consumer-groups reset-offset, `seekToEnd` should be used.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTable Joins Without Materialized Argument Yield Results That Further Joins NPE On,KAFKA-9517,13283769,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,psnively,psnively,06/Feb/20 19:51,23/Sep/20 08:51,13/Jul/23 09:17,10/Feb/20 22:15,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,streams,,,,,0,,,,,"The `KTable` API implemented [[here||#L842-L844]] [https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java#L842-L844] []|#L842-L844]] calls `doJoinOnForeignKey` with an argument of `Materialized.with(null, null)`, as apparently do several other APIs. As the comment spanning [these lines|#L1098-L1099]] makes clear, the result is a `KTable` whose `valueSerde` (as a `KTableImpl`) is `null`. Therefore, attempts to `join` etc. on the resulting `KTable` fail with a `NullPointerException`.

While there is an obvious workaround—explicitly construct the required `Materialized` and use the APIs that take it as an argument—I have to admit I find the existence of public APIs with this sort of bug, particularly when the bug is literally documented as a comment in the source code, astonishing to the point of incredulity. It calls the quality and trustworthiness of Kafka Streams into serious question, and if a resolution is not forthcoming within a week, we will be left with no other option but to consider technical alternatives.",,ableegoldman,cadonna,githubbot,mjsax,psnively,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9248,,,,,,,,,KAFKA-10515,,,,,,"10/Feb/20 17:29;psnively;test.tar.xz;https://issues.apache.org/jira/secure/attachment/12993072/test.tar.xz",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 16:06:52 UTC 2020,,,,,,,,,,"0|z0b8eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 20:23;vvcephei;Hey [~psnively],

Thanks for the report. I think this is the same as KAFKA-9500 . 

I already have a PR to fix it, although I need to address some comments.

If you like, you could take a look at https://github.com/apache/kafka/pull/8015 to see if it fixes the issue for you.;;;","06/Feb/20 20:34;psnively;John,

Having been sharply critical of the project, now let me thank you for the speedy response. I'll look at the related issue and PR immediately.

*Update:* [KAFKA-9500|https://issues.apache.org/jira/browse/KAFKA-9500] looks like an issue a colleague has encountered, so I would say it seems related, but not quite the same: the behavior my colleague and I have seen first is an NPE when attempting to `join` on the result of a `join` constructed without an explicit `Materialized`, because the resulting `KTableImpl`'s `valueSerde` is `null`. However, my colleague has pushed a bit farther by explicitly passing a `Materialized` he's constructed, but then he sees the issue reported in [KAFKA-9500|https://issues.apache.org/jira/browse/KAFKA-9500].;;;","06/Feb/20 21:25;vvcephei;Thanks for the update, [~psnively].

I've been trying to reproduce the issue you've reported (building on the fix already for KAFKA-9500), and I haven't been able to produce an NPE. Do you happen to have the stacktrace handy?

To address the concern you voiced about the comment in the code, the valueSerde is set to `null` to indicate that we do not know the serde a priori. This sets us up to use the following precedence rules at run time.

If an operator needs to serialize some data, it will use:
#1 The serde explicily passed to it, via Materialized, Produced, Grouped, etc.
#2 If no explicit serde is passed, then use the serde passed from the upstream operator, if applicable^
#3 If no explicit or upstream serde is available, use the ""default"" serde provided in config

^ this is where passing a null downstream indicates that no applicable serde is available.

So, what I would expect to see is a ClassCastException if the ""default"" serde isn't for the same type as the foreign key join result. But I think maybe I misunderstood the scenario you provided.

Thanks,
-John;;;","06/Feb/20 23:02;vvcephei;Aha! I think I've found it:

{noformat}
    java.lang.NullPointerException
        at org.apache.kafka.streams.kstream.internals.foreignkeyjoin.SubscriptionWrapperSerde.<init>(SubscriptionWrapperSerde.java:31)
        at org.apache.kafka.streams.kstream.internals.KTableImpl.doJoinOnForeignKey(KTableImpl.java:956)
        at org.apache.kafka.streams.kstream.internals.KTableImpl.join(KTableImpl.java:845)
        at org.apache.kafka.streams.TopologyTestDriverTest.shouldProduceOutputsInTheRightOrder(TopologyTestDriverTest.java:1533)
{noformat}

produced with the following test (that I'm working on for KAFKA-9503):

{noformat}
    public void shouldProduceOutputsInTheRightOrder() {
        final StreamsBuilder builder = new StreamsBuilder();
        final KTable<String, String> aTable = builder.table(""A"");
        final KTable<String, String> bTable = builder.table(""B"");

        final KTable<String, String> fkJoinResult = aTable.join(
            bTable,
            value -> value.split(""-"")[0],
            (aVal, bVal) -> ""("" + aVal + "","" + bVal + "")""
        );

        final KTable<String, String> finalJoinResult = aTable.join(
            fkJoinResult,
            (aVal, fkJoinVal) -> ""("" + aVal + "","" + fkJoinVal + "")""
        );

        finalJoinResult.toStream().to(""output"");

        System.out.println(builder.build().describe());

        final Properties config = new Properties();
        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""dummy"");
        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""dummy"");
        config.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class.getName());
        config.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class.getName());
        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(builder.build(), config)) {
            final TestInputTopic<String, String> aTopic = topologyTestDriver.createInputTopic(""A"", new StringSerializer(), new StringSerializer());
            final TestInputTopic<String, String> bTopic = topologyTestDriver.createInputTopic(""B"", new StringSerializer(), new StringSerializer());
            final TestOutputTopic<String, String> output = topologyTestDriver.createOutputTopic(""output"", new StringDeserializer(), new StringDeserializer());
            aTopic.pipeInput(""a1"", ""b1-alpha"");
            aTopic.pipeInput(""b1"", ""beta"");
            System.out.println(output.readKeyValuesToMap());
        }
    }
{noformat}

;;;","07/Feb/20 14:59;psnively;[~vvcephei], that's great news! First, let me thank you again for your prompt attention. It's done a great deal to restore my confidence in Kafka Streams. Second, and I hate to ask because I know it's a big project with many other customers with various needs, but do you happen to have some idea when a 2.4.1 might be available with fixes for these, and is there anything I can do to help with the process?

Thanks again!;;;","07/Feb/20 16:37;psnively;Speaking of things I can do: I am downloading PR #8015 as a patch, and will apply it locally, build the appropriate `.jar`s, and we will attempt to reproduce the issues we've seen given that PR, and report back.;;;","07/Feb/20 19:30;githubbot;vvcephei commented on pull request #8061: KAFKA-9517: Fix default serdes with FK join
URL: https://github.com/apache/kafka/pull/8061
 
 
   During the KIP-213 implementation and verification, we neglected to test the code path for falling back to default serdes if none are given in the topology.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","07/Feb/20 20:00;vvcephei;Hi [~psnively], thanks for your kind words.

Actually, the 2.4.1 release is currently in progress. Fortunately, we caught these issues early enough to have them included as blockers for the release. The release plan is here: https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+2.4.1
Of course, it cannot progress until the blockers are resolved, which is the biggest wild-card in the timeline. As soon as the blockers are cleared, Bill (who volunteered to drive the release) will be able to give a better estimate about the timeline.

As you said, the biggest thing you can do to help is to check out the PRs and test them. I think you'll need both #8015 and #8061. As per Apache Kafka policies, the PRs are actually based on trunk, so you'll want to squash them and cherry-pick them onto 2.4 to get an accurate proxy for 2.4.1 . This is actually a huge help, since even extensive testing can have subtle but important gaps (which is how we wound up with these bugs to begin with).

The other bug thing you can do if you have time is review the PRs. You've already become familiar enough with the code to identify the root cause even before I saw it, and a fresh perspective is always helpful.

Thanks again,
-John;;;","10/Feb/20 17:30;psnively;Thanks for the suggestion to squash and cherry-pick #8015 and #8061. I've done that, and `.gradlew test` is giving me four errors that seem related to the cherry-picked PRs. I'm attaching the test report for others to perhaps analyze. My colleague and I will also attempt to reproduce the issues we specifically encountered in using 2.4.0. I'm reasonably confident we can also take some time to review these two PRs, but that seems somewhat unlikely to happen today.

[^test.tar.xz];;;","10/Feb/20 21:30;vvcephei;Hi Paul,

Ah, those failures shouldn't affect your testing. #8015 depends on a fix I added for the TopologyTestDriver (https://github.com/apache/kafka/pull/8065). If you want to clear the error, you can cherry-pick that one also, or you can just skip the tests and build the artifacts directly with `installAll`.

Sorry for neglecting to mention that initially; both changes were part of 8015 to begin with, but the reviewers rightly suggested I should pull out the TopologyTestDriver fix into a separately verified PR.

Thanks,
-John;;;","10/Feb/20 21:33;githubbot;vvcephei commented on pull request #8061: KAFKA-9517: Fix default serdes with FK join
URL: https://github.com/apache/kafka/pull/8061
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Feb/20 22:14;vvcephei;Quick update, I've merged this fix to trunk, 2.5, and 2.4. I'll go ahead and mark this ticket resolved so that it doesn't prevent the creation of 2.4.1 or 2.5.0 release candidates.

I'd still very much appreciate it if you can test it to make sure it resolves the issue for your use case. ;;;","12/Feb/20 16:40;vvcephei;Hi [~psnively],

Just another quick update: now all the reported bugs for foreign-key joins are fixed and merged to the branches: trunk, 2.5, and 2.4 . If you want to test the fixes, you can now just check out and build the 2.4 branch without any cherry-picking.

There is still a blocker for 2.4.1, so [~bbejeck] hasn't created an actual release candidate yet.

Thanks,
-John;;;","13/Feb/20 16:06;psnively;John,

Thanks so much for your prompt attention, attention to detail, and pointers to the relevant issues and PRs. I'm glad to hear they've been merged and look forward to the 2.4.1 release when the last blocker is resolved. We'll continue pushing on our specific use-cases for the remainder of the week and let you know ASAP if anything continues to arise.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test PlaintextProducerSendTest#testNonBlockingProducer,KAFKA-9516,13283752,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rsivaram,mjsax,mjsax,06/Feb/20 17:14,16/Aug/20 10:08,13/Jul/23 09:17,16/Aug/20 10:08,,,,,,,,,,,,,,,,,,,,,,,2.5.2,2.6.1,2.7.0,,,,,,core,producer ,tools,unit tests,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4521/testReport/junit/kafka.api/PlaintextProducerSendTest/testNonBlockingProducer/]
{quote}java.util.concurrent.TimeoutException: Timeout after waiting for 10000 ms. at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:78) at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:30) at kafka.api.PlaintextProducerSendTest.verifySendSuccess$1(PlaintextProducerSendTest.scala:148) at kafka.api.PlaintextProducerSendTest.testNonBlockingProducer(PlaintextProducerSendTest.scala:172){quote}
{quote}
h3. Standard Output
[2020-02-06 03:35:27,912] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:35:50,812] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:35:51,015] ERROR [ReplicaManager broker=0] Error processing append operation on partition topic-0 (kafka.server.ReplicaManager:76) org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp [2020-02-06 03:35:51,027] ERROR [ReplicaManager broker=0] Error processing append operation on partition topic-0 (kafka.server.ReplicaManager:76) org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp [2020-02-06 03:35:53,127] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:35:58,617] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:01,843] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:05,111] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:08,383] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:08,383] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:12,582] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:12,582] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:15,902] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:19,111] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:22,399] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:27,675] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:31,918] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-02-06 03:36:37,997] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{quote}",,ableegoldman,bbejeck,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 03:24:45 UTC 2020,,,,,,,,,,"0|z0b8ao:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"01/Apr/20 16:36;bbejeck;[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1537/testReport/junit/kafka.api/PlaintextProducerSendTest/testNonBlockingProducer/]
{noformat}
Error Messagejava.util.concurrent.TimeoutException: Timeout after waiting for 10000 ms.


Standard Output
[2020-04-01 14:48:45,777] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:08,071] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:08,275] ERROR [ReplicaManager broker=1] Error processing append operation on partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp
[2020-04-01 14:49:08,287] ERROR [ReplicaManager broker=1] Error processing append operation on partition topic-0 (kafka.server.ReplicaManager:76)
org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp
[2020-04-01 14:49:11,232] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:18,575] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:21,869] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:25,160] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:32,200] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:35,600] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:39,774] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:48,477] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:52,717] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-04-01 14:49:58,804] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition topic-1 at offset 0 (kafka.server.ReplicaFetcherThread:76)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{noformat};;;","03/Jun/20 03:24;ableegoldman;[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/6706/testReport/junit/kafka.api/PlaintextProducerSendTest/testNonBlockingProducer/]
h3. Stacktrace

java.util.concurrent.TimeoutException: Timeout after waiting for 10000 ms. at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:78) at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:30) at kafka.api.PlaintextProducerSendTest.verifySendSuccess$1(PlaintextProducerSendTest.scala:147) at kafka.api.PlaintextProducerSendTest.testNonBlockingProducer(PlaintextProducerSendTest.scala:176);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The protocol generator generated useless condition when a field is made nullable and flexible version is used,KAFKA-9514,13283658,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,06/Feb/20 09:24,29/Sep/20 15:29,13/Jul/23 09:17,29/Sep/20 15:29,,,,,,,,,,,,,,,,,,,,,,,2.7.0,,,,,,,,,,,,,0,,,,,"The protocol generator generates useless conditions when a field of type string is made nullable after the request has been converted to using optional fields.

As an example, we have make the field `ProtocolName` nullable in the `JoinGroupResponse`. The `JoinGroupResponse` supports optional fields since version 6 and the field is nullable since version 7. Under these conditions, the generator generates the following code:

{code:java}
if (protocolName == null) {
 if (_version >= 7) {
   if (_version >= 6) {
     _writable.writeUnsignedVarint(0);
   } else {
     _writable.writeShort((short) -1);
  }
 } else {
   throw new NullPointerException();
 }
}
{code}

spotbugs raises an `UC_USELESS_CONDITION` because `_version >= 6` is always true.  

We could optimise the generator to handle this.
 ",,dajac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-06 09:24:48.0,,,,,,,,,,"0|z0b7ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test LagFetchIntegrationTest.shouldFetchLagsDuringRestoration,KAFKA-9512,13283592,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,vinoth,mjsax,mjsax,05/Feb/20 23:17,23/Apr/20 00:15,13/Jul/23 09:17,18/Feb/20 18:07,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/497/testReport/junit/org.apache.kafka.streams.integration/LagFetchIntegrationTest/shouldFetchLagsDuringRestoration/]
{quote}java.lang.NullPointerException at org.apache.kafka.streams.integration.LagFetchIntegrationTest.shouldFetchLagsDuringRestoration(LagFetchIntegrationTest.java:306){quote}",,cadonna,githubbot,guozhang,mjsax,vinoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 18:07:58 UTC 2020,,,,,,,,,,"0|z0b7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 00:48;vinoth;{code:java}
restartedStreams.setGlobalStateRestoreListener(new StateRestoreListener() {
    @Override
    public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) {
        restoreStartLagInfo.putAll(restartedStreams.allLocalStorePartitionLags());
    }

    @Override
    public void onBatchRestored(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored) {
    }

    @Override
    public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {
        restoreEndLagInfo.putAll(restartedStreams.allLocalStorePartitionLags());
    }
});

restartedStreams.start();
TestUtils.waitForCondition(() -> restartedStreams.allLocalStorePartitionLags().get(stateStoreName).get(0).offsetLag() == 0,
    ""Standby should eventually catchup and have zero lag."");
final LagInfo fullLagInfo = restoreStartLagInfo.get(stateStoreName).get(0);
assertThat(fullLagInfo.currentOffsetPosition(), equalTo(0L));
assertThat(fullLagInfo.endOffsetPosition(), equalTo(5L));
assertThat(fullLagInfo.offsetLag(), equalTo(5L));

assertThat(restoreEndLagInfo.get(stateStoreName).get(0), equalTo(zeroLagInfo)); <-- NPE line {code}
 

NPE can happen only if an empty lag map is added to `onRestoreEnd` i.e when restoration finishes there is no entry for changelog topic? This is not possible esp for standby, since there should be a standby task . The test clearly waits till we reach zero lag, using the same stateStoreName.. and that seems to be working.. 

 

So I wonder if there is some race between restoration ending and the standy task creation? In any case, the problematic line seems redundant anyway, since it just checks for the same thing as the waitForCondition() 

 

 

 ;;;","06/Feb/20 00:49;vinoth;I will try to reproduce this locally and investigate.. 

If I cannot reproduce this, will send a PR to remove the redundant assert at the end. ;;;","06/Feb/20 01:11;vinoth;I have run this locally over 15 times.. without luck.. 

[~guozhang] Quick confirmation from you.. For a standby changelog partition, during restoration, the StandbyTask would already be created? Or is that created in parallel while the the restoration goes on.. This can be the only cause of such a race.. [~mjsax] as well, please chime in if you know ;;;","06/Feb/20 01:31;guozhang;Did you see it in Scala 2.13 or other Scala versions? I also found some test failures are quite consistent with the Scala 2.13 jenkins jobs.. and locally even if I enabled parallel runs I cannot reproduce it (I'm on 2.11 locally).;;;","06/Feb/20 17:16;vinoth;IIUC [~mjsax] saw this couple times and since this one is on 2.12, its probably not the same as what you are facing.. 

 

But can you confirm this understanding for me? It will help me formulate a fix here 

> For a standby changelog partition, during restoration, the StandbyTask would already be created? Or is that created in parallel while the the restoration goes on..?;;;","07/Feb/20 01:15;vinoth;Repro-ed with some tips from guozhang.. Appears as though the fetch of the endoffsets fails to the broker, leaving us with an empty map.. (the test has no retries now) .. 

Looking at the fix ;;;","10/Feb/20 02:02;githubbot;vinothchandar commented on pull request #8076: KAFKA-9512: Flaky Test LagFetchIntegrationTest.shouldFetchLagsDuringRestoration
URL: https://github.com/apache/kafka/pull/8076
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
    - Added additional synchronization and increased timeouts to handle flakiness
    - Added some pre-cautionary retries when trying to obtain lag map
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   Ran locally the entire suite 100+ times (2x as much as it took to reproduce the original issue) 
   ![Screen Shot 2020-02-09 at 5 57 31 PM](https://user-images.githubusercontent.com/1179324/74115829-c4937680-4b65-11ea-9a97-6c657e40f4a9.png)
   
   P.S: Also ran the two tests I added in `QueryableStateIntegrationTest`. They seem solid (standby test ran 1900 times without flaking out)
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","14/Feb/20 16:49;githubbot;guozhangwang commented on pull request #8076: KAFKA-9512: Flaky Test LagFetchIntegrationTest.shouldFetchLagsDuringRestoration
URL: https://github.com/apache/kafka/pull/8076
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Feb/20 08:18;cadonna;[~vinoth][~guozhang] Can we close this ticket?;;;","18/Feb/20 18:07;vinoth;Closing since the PR is now landed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdminClient should check for missing committed offsets,KAFKA-9507,13283571,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,david.mao,hachikuji,hachikuji,05/Feb/20 20:41,08/Feb/20 00:54,13/Jul/23 09:17,08/Feb/20 00:54,,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.1,2.5.0,,,,,,,,,,,0,newbie,,,,"I noticed this exception getting raised:

{code}
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
	at org.apache.kafka.clients.consumer.OffsetAndMetadata.<init>(OffsetAndMetadata.java:50)
	at org.apache.kafka.clients.admin.KafkaAdminClient$24$1.handleResponse(KafkaAdminClient.java:2832)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.handleResponses(KafkaAdminClient.java:1032)
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1160)
{code}

The AdminClient should check for negative offsets in OffsetFetch responses in the api `listConsumerGroupOffsets`.",,david.mao,epinto,githubbot,hachikuji,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 08 00:43:57 UTC 2020,,,,,,,,,,"0|z0b76g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/20 23:48;githubbot;splett2 commented on pull request #8057: KAFKA-9507 AdminClient should check for missing committed offsets
URL: https://github.com/apache/kafka/pull/8057
 
 
   JIRA: https://issues.apache.org/jira/browse/KAFKA-9507
   
   Addresses exception being thrown by AdminClient when listConsumerGroupOffsets returns a negative offset.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Feb/20 23:48;david.mao;[https://github.com/apache/kafka/pull/8057]

Pull request to address this issue.;;;","08/Feb/20 00:43;githubbot;hachikuji commented on pull request #8057: KAFKA-9507 AdminClient should check for missing committed offsets
URL: https://github.com/apache/kafka/pull/8057
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalTopicManager may falls into infinite loop with partially created topics,KAFKA-9505,13283318,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,04/Feb/20 20:55,10/Feb/20 21:00,13/Jul/23 09:17,10/Feb/20 21:00,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,streams,,,,,0,,,,,"In {{InternalTopicManager#validateTopics(topicsNotReady, topics)}}, the topics map (second) does not change while the first topicsNotReady may change if some topics have been validated while others do not, however inside that function we still loop of the second map which may never completes then.",,ableegoldman,cadonna,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 20:59:20 UTC 2020,,,,,,,,,,"0|z0b5m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/20 00:08;githubbot;guozhangwang commented on pull request #8039: KAFKA-9505: Only loop over topics-to-validate in retries
URL: https://github.com/apache/kafka/pull/8039
 
 
   Also added a unit test.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Feb/20 20:59;githubbot;guozhangwang commented on pull request #8039: KAFKA-9505: Only loop over topics-to-validate in retries
URL: https://github.com/apache/kafka/pull/8039
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopologyTestDriver processes intermediate results in the wrong order,KAFKA-9503,13283070,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vvcephei,vvcephei,vvcephei,03/Feb/20 23:38,12/Feb/20 05:49,13/Jul/23 09:17,12/Feb/20 04:09,,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,streams-test-utils,,,,,0,,,,,"TopologyTestDriver has the feature that it processes each input synchronously, resolving one of the most significant challenges with verifying the correctness of streaming applications.

When processing an input, it feeds that record to the source node, which then synchronously (it's always synchronous within a task) gets passed through the subtopology via Context#forward calls. Ultimately, outputs from that input are forwarded into the RecordCollector, which converts it to Producer.send calls. In TopologyTestDriver, this Producer is a special one that actually just captures the records.

Some output topics from one subtopology are inputs to another subtopology. For example, repartition topics. Immediately after the synchronous subtopology process() invocation, TopologyTestDriver iterates over the collected outputs from the special Producer. If they are purely output records, it just enqueues them for later retrieval by testing code. If they are records for internal topics, though, TopologyTestDriver immediately processes them as inputs  for the relevant subtopology.

The problem, and this is very subtle, is that TopologyTestDriver does this recursively, which with some (apparently rare) programs can cause the output to be observed in an invalid order.

One such program is the one I wrote to test the fix for KAFKA-9487 . It involves a foreign-key join whose result is joined back to one of its inputs.

{noformat}
Here's a simplified version:
// foreign key join
J = A.join(B, (extractor) a -> a.b, (joiner) (a,b) -> new Pair(a, b))
// equi-join
OUT = A.join(J, (joiner) (a, j) -> new Pair(a, j))

Let's say we have the following initial condition:
A:
a1 = {v: X, b: b1}
B:
b1 = {v: Y}
J:
a1 = Pair({v: X}, b: b1}, {v: Y})
OUT:
a1 = Pair({v: X}, b: b1}, Pair({v: X}, b: b1}, {v: Y}))

Now, piping an update:
a1: {v: Z, b: b1}
results immediately in two buffered results in the Producer:
(FK join subscription): b1: {a1}
(OUT): a1 = Pair({v: Z}, b: b1}, Pair({v: X}, b: b1}, {v: Y}))
Note that the FK join result isn't updated synchronously, since it's an async operation, so the RHS lookup is temporarily incorrect, yielding the nonsense intermediate result where the outer pair has the updated value for a1, but the inner (fk result) one still has the old value for a1.

However! We don't buffer that output record for consumption by testing code yet, we leave it in the internal Producer while we process the first intermediate record (the FK subscription).
Processing that internal record means that we have a new internal record to process:
(FK join subscription response): a1: {b1: {v: Y}}

so right now, our internal-records-to-process stack looks like:
(FK join subscription response): a1: {b1: {v: Y}}
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: X}, b: b1}, {v: Y}))

Again, we start by processing the first thing, the FK join response, which results in an updated FK join result:
(J) a1: Pair({v: Z}, b: b1}, {v: Y})
and output:
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: Z}, b: b1}, {v: Y}))
and, we still haven't handled the earlier output, so now our internal-records-to-process stack looks like:

(J) a1: Pair({v: Z}, b: b1}, {v: Y})
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: Z}, b: b1}, {v: Y}))
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: X}, b: b1}, {v: Y}))

At this point, there's nothing else to process in internal topics, so we just copy the records one by one to the ""output"" collection for later handling by testing code, but this yields the wrong final state of:
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: X}, b: b1}, {v: Y}))

That was an incorrect intermediate result, but because we're processing internal records recursively (as a stack), it winds up emitted at the end instead of in the middle.

If we change the processing model from a stack to a queue, the correct order is preserved, and the final state is:
(OUT) a1: Pair({v: Z}, b: b1}, Pair({v: Z}, b: b1}, {v: Y}))

{noformat}

This is what I did in https://github.com/apache/kafka/pull/8015",,ableegoldman,cadonna,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9500,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-03 23:38:42.0,,,,,,,,,,"0|z0b4ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote Standby tasks to active tasks without closing them,KAFKA-9501,13283051,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ableegoldman,mjsax,mjsax,03/Feb/20 21:36,01/Jun/20 18:43,13/Jul/23 09:17,01/Jun/20 18:43,2.6.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"The purpose of StandbyTasks in Kafka Streams is fast failover. However, atm we close StandbyTasks (and create a new active task) if they are promoted to active.

While this works ok for persistent state stores, it renders hot standbys for in-memory state stores useless, because we drop the in-memory state when we close the StandbyTask and thus the new active tasks needs to reread the changelog topic to recreate the in-memory state.

Hence, we should promote StandbyTasks to active tasks without closing them. This will not only fix the issue for in-memory stores, but will make rebalancing faster for persistent state stores, too, because closing and reopening RocksDB has significant overhead.",,ableegoldman,cadonna,githubbot,johannz,mjsax,NaviBrar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 05:47:49 UTC 2020,,,,,,,,,,"0|z0b49k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/20 05:47;githubbot;ableegoldman commented on pull request #8248: KAFKA-9501: convert between active and standby without closing stores
URL: https://github.com/apache/kafka/pull/8248
 
 
   WIP -- need to add tests
   
   Add methods to the active/standby task creators that reuse the state manager to avoid closing the stores (and losing all data in the case of in-memory stores)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Foreign-Key Join creates an invalid topology,KAFKA-9500,13283050,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,vvcephei,vvcephei,03/Feb/20 21:25,18/Mar/20 15:20,13/Jul/23 09:17,12/Feb/20 05:00,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,streams,,,,,0,,,,,"Foreign-Key Join results are not required to be materialized by default, but they might be needed if downstream operators need to perform lookups on the result (such as when the join result participates in an equi-join).

Currently, if the result is explicitly materialized (via Materialized), this works correctly, but if the result is _not_ materialized explicitly, but _is_ needed, the topology builder throws an exception that the result store isn't added to the topology. This was an oversight in testing and review and needs to be fixed ASAP.",,abellemare,ableegoldman,cadonna,githubbot,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,KAFKA-9503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 15:20:22 UTC 2020,,,,,,,,,,"0|z0b49c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/20 04:38;githubbot;vvcephei commented on pull request #8015: KAFKA-9500: Fix topology bug in foreign key joins
URL: https://github.com/apache/kafka/pull/8015
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","18/Mar/20 15:20;abellemare;[~vvcephei]

Hey John - I ran across this while trying to port my Scala joiners to 2.4. I don't know enough about expectations of what serdes should be present in a KTable created via stream+groupByKey+reduce, but it doesn't seem to be what I expect. If it's normal for a table created that way to be missing the valueSerde, can you advise me and I can keep looking into it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic validation during the creation trigger unnecessary TopicChange events ,KAFKA-9498,13282984,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,03/Feb/20 14:05,26/Feb/20 01:48,13/Jul/23 09:17,26/Feb/20 01:48,,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,,,,,,0,,,,,"I have found out that the topic validation logic, which is executed when CreateTopicPolicy or when validateOnly is set, triggers unnecessary ChangeTopic events in the controller. In the worst case, it can trigger up to one event per created topic and leads to overloading the controller.

This happens because the validation logic reads all the topics from ZK using the method getAllTopicsInCluster provided by the KafkaZKClient. This method registers a watch every time the topics are read from Zookeeper.

I think that we should make the watch registration optional for this call in oder to avoid this unwanted behaviour.

 ",,dajac,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 01:48:47 UTC 2020,,,,,,,,,,"0|z0b3uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/20 20:55;githubbot;dajac commented on pull request #8062: KAFKA-9498; Topic validation during the creation trigger unnecessary TopicChange events
URL: https://github.com/apache/kafka/pull/8062
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 01:45;githubbot;junrao commented on pull request #8062: KAFKA-9498; Topic validation during the topic creation triggers unnecessary TopicChange events
URL: https://github.com/apache/kafka/pull/8062
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Feb/20 01:48;junrao;Merged the PR to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerResponse with record-level errors throw NPE with older client version,KAFKA-9492,13282777,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,01/Feb/20 16:54,04/Feb/20 17:20,13/Jul/23 09:17,04/Feb/20 17:20,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,clients,,,,,0,,,,,ProduceResponse.toStruct(version) throws NullPointerException if the response contains record errors and version < 8 (before record errors were aded to ProduceResponse). The response can't be serialized to send to older clients as a result.,,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 17:01:14 UTC 2020,,,,,,,,,,"0|z0b2ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/20 16:57;githubbot;rajinisivaram commented on pull request #8030: KAFKA-9492; Ignore record errors in ProduceResponse for older versions
URL: https://github.com/apache/kafka/pull/8030
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Feb/20 17:01;githubbot;rajinisivaram commented on pull request #8030: KAFKA-9492; Ignore record errors in ProduceResponse for older versions
URL: https://github.com/apache/kafka/pull/8030
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fast election during reassignment can lead to replica fetcher failures,KAFKA-9491,13282748,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,01/Feb/20 08:39,04/Feb/20 23:23,13/Jul/23 09:17,04/Feb/20 23:21,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,,,,,,0,,,,,"We have observed an unusual case in which a new replica became leader before it had received an initial high watermark from the previous leader. This resulted in an OffsetOutOfRangeException being raised while looking up the segment position of the uninitialized high watermark, since it was lower than the log start offset. The error was raised while handle the fetch request from one of the followers and prevented it from making progress.

{code}
org.apache.kafka.common.errors.OffsetOutOfRangeException: Received request for offset 0 for partition foo-0, but we only have log segments in the range 20 to 20.
{code}

Here is what we have observed from the logs. The initial state of the partition for the relevant sequence of events is the following:

Initial state: replicas=[4,1,2,3], leader=1, isr=[1,2,3], adding=[4], removing=[1], epoch=5, logStartOffset=20, logEndOffset=20

We see the following events:

t0: Replica 4 becomes follower and initializes log with hw=0, logStartOffset=0
t1: Replica 4 begins fetching from offset 0 and receives an out of range error
t2: After a ListOffset request to the leader, replica 4 initializes logStartOffset to 20.
t3: Replica 4 sends fetch request to the leader at start offset 20
t4: Upon receiving the fetch request, the leader adds 4 to the ISR (i.e. isr=[1,2,3,4])
t5: The controller notices the ISR addition and makes 4 the leader since 1 is to be removed and 4 is the new preferred leader
t6: Replica 4 stops fetchers and becomes leader
t7: We begin seeing the out of range errors as the other replicas begin fetching from 4.

We know from analysis of a heap dump from broker 4, that the high watermark was still set to 0 some time after it had become leader. We also know that broker 1 was under significant load. The time between events t4 and t6 was less than 10ms. We don't know when the fetch response sent at t3 returned to broker 4, but we speculate that it happened after t6 due to the heavy load on the leader, which is why broker 4 had an uninitialized high watermark.

A more mundane possibility is that there is a bug in the fetch session logic and the partition was simply not included in the fetch response. However, the code appears to anticipate this case. When a partition has an error, we set the cached high watermark to -1 to ensure that it gets updated as soon as the error clears.

Regardless how we got there, the fix should be straightforward. When a broker becomes leader, it should ensure its high watermark is at least as large as the log start offset.",,githubbot,hachikuji,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 23:23:13 UTC 2020,,,,,,,,,,"0|z0b2e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/20 06:19;githubbot;hachikuji commented on pull request #8037: KAFKA-9491; Increment high watermark after full log truncation
URL: https://github.com/apache/kafka/pull/8037
 
 
   When a follower's fetch offset is behind the leader's log start offset, the follower will do a full log truncation. When it does so, it must update both its log start offset and high watermark. Failure to do so can lead to out of range errors if the follower becomes leader before getting the latest high watermark from the previous leader. The out of range errors occur when we attempt to resolve the log position of the high watermark in `DelayedFetch` in order to determine if a fetch is satisfied.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Feb/20 19:22;githubbot;ijuma commented on pull request #8037: KAFKA-9491; Increment high watermark after full log truncation
URL: https://github.com/apache/kafka/pull/8037
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Feb/20 19:40;ijuma;Reopening as we are still considering a cherry-pick to 2.3.;;;","04/Feb/20 23:23;hachikuji;After reviewing the previous logic, it looks like this was a regression in 2.4.0. It came from the new logic in KIP-392, which required followers to maintain the high watermark more strictly than before.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some factory methods in Grouped are missing generic parameters,KAFKA-9490,13282711,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mjsax,kordzik,kordzik,31/Jan/20 22:30,04/Feb/20 01:03,13/Jul/23 09:17,04/Feb/20 01:03,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,,,,,0,,,,,"The following methods in {{Grouped}} class seem to be missing generic parameters {{<K,V>}} in the declared return type:
{code:java}
public static <K> Grouped keySerde(final Serde<K> keySerde) { return new       Grouped<>(null, keySerde, null); 
}

public static <V> Grouped valueSerde(final Serde<V> valueSerde) { return new Grouped<>(null, null, valueSerde); 
} {code}
I think it both cases it should be:
{code:java}
public static <K,V> Grouped<K,V> ...() {code}
This causes ""unchecked call"" compiler warnings when called by clients.",,githubbot,kordzik,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 01:02:37 UTC 2020,,,,,,,,,,"0|z0b260:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/20 23:57;githubbot;mjsax commented on pull request #8028: KAFKA-9490: Fix generics for Grouped
URL: https://github.com/apache/kafka/pull/8028
 
 
   Call for review @guozhangwang @vvcephei 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","04/Feb/20 01:02;githubbot;mjsax commented on pull request #8028: KAFKA-9490: Fix generics for Grouped
URL: https://github.com/apache/kafka/pull/8028
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Value for Task-level Metric process-rate is Constant Zero ,KAFKA-9480,13282083,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,cadonna,cadonna,28/Jan/20 23:42,10/Feb/20 21:08,13/Jul/23 09:17,10/Feb/20 21:08,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,streams,,,,,0,,,,,"The value for task-level metric process-rate is constant zero. The value should reflect the number of calls to {{process()}}  on source processors which clearly cannot be constant zero. 
This behavior applies to built-in metrics version {{latest}}. ",,ableegoldman,cadonna,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 21:06:11 UTC 2020,,,,,,,,,,"0|z0ayag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/20 04:31;githubbot;cadonna commented on pull request #8018: KAFKA-9480: Fix bug that prevented to measure task-level process-rate
URL: https://github.com/apache/kafka/pull/8018
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Feb/20 21:06;githubbot;guozhangwang commented on pull request #8018: KAFKA-9480: Fix bug that prevented to measure task-level process-rate
URL: https://github.com/apache/kafka/pull/8018
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A large number of core system tests failing due to Kafka server failed to start on trunk,KAFKA-9473,13281477,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,bchen225242,bchen225242,24/Jan/20 19:47,19/Feb/20 17:47,13/Jul/23 09:17,19/Feb/20 17:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"By running a full set of core system tests, we detected 38/166 test failures which are due to 

`FAIL: Kafka server didn't finish startup in 60 seconds`

need further investigation on this.

[https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3701/]",,ableegoldman,bchen225242,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 23:58:45 UTC 2020,,,,,,,,,,"0|z0aujs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 22:01;bchen225242;```

[2020-01-24 07:53:17,594] ERROR [KafkaServer id=1] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)

org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Connection refused (Connection refused)

        at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:172)

        at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:157)

        at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:97)

        at kafka.network.Processor.<init>(SocketServer.scala:724)

        at kafka.network.SocketServer.newProcessor(SocketServer.scala:367)

        at kafka.network.SocketServer.$anonfun$addDataPlaneProcessors$1(SocketServer.scala:252)

        at kafka.network.SocketServer.addDataPlaneProcessors(SocketServer.scala:251)

        at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1(SocketServer.scala:214)

        at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1$adapted(SocketServer.scala:211)

        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

        at kafka.network.SocketServer.createDataPlaneAcceptorsAndProcessors(SocketServer.scala:211)

        at kafka.network.SocketServer.startup(SocketServer.scala:122)

        at kafka.server.KafkaServer.startup(KafkaServer.scala:242)

        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)

        at kafka.Kafka$.main(Kafka.scala:84)

        at kafka.Kafka.main(Kafka.scala)

Caused by: javax.security.auth.login.LoginException: Connection refused (Connection refused)

        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:808)

        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:498)

        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)```;;;","24/Jan/20 23:58;ijuma;I think [https://github.com/apache/kafka/commit/a3509c0870230bcc1af4efbfafcf9f69d7cf55fd] may have fixed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reducing number of tasks for connector causes deleted tasks to show as UNASSIGNED,KAFKA-9472,13281271,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,23/Jan/20 19:07,27/May/20 03:20,13/Jul/23 09:17,25/May/20 22:18,2.0.0,2.0.1,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,,,,,,,,,,,,,2.3.2,2.4.2,2.5.1,2.6.0,,,,,KafkaConnect,,,,,0,,,,,"If a connector is successfully created with {{t1}} running tasks and then reconfigured to use {{t1 - n}} tasks (where {{t1}} and {{n}} are both whole numbers and {{n}} is strictly less than {{t1}}), the connector should then list {{t1 - n}} total tasks in its status (which can be queried via the {{/connectors/:name:/status}} endpoint or the {{/connectors}} endpoint with the {{expand}} URL query parameter set to {{status}}).

However, the connector will instead continue to list {{t1}} total tasks in its status, with {{n}} of them being listed as {{UNASSIGNED}} and the remaining {{t1 - n}} of them being listed as {{STARTED}}.

This is because the only time a task status is removed from the status backing store (as opposed to simply being updated to {{UNASSIGNED}}) is when its connector is deleted. See relevant code snippets from the [AbstractHerder|https://github.com/apache/kafka/blob/df13fc93d0aebfe0ecc40dd4af3c5fb19b35f710/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java#L187-L192] and [DistributedHerder|https://github.com/apache/kafka/blob/df13fc93d0aebfe0ecc40dd4af3c5fb19b35f710/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1511-L1520] classes.",,ChrisEgerton,githubbot,kkonstantine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 20:58:20 UTC 2020,,,,,,,,,,"0|z0ata0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/20 20:50;kkonstantine;Thanks for reporting [~ChrisEgerton]
I’m suspecting this ticket might be a duplicate of: 
https://issues.apache.org/jira/browse/KAFKA-8869;;;","24/Jan/20 21:05;ChrisEgerton;[~kkonstantine] the issues may be similar but I don't see how this is a duplicate. KAFKA-8869 appears to be related to the config backing store whereas this issue is related to the status backing store. Do you suspect a shared root cause that is not explicitly mentioned in either ticket?;;;","24/Jan/20 21:44;kkonstantine;I overlooked the backing stores. But, yes, I'm suspecting a common root cause, which has to do with the fact that we don't remove the task from the config backing store, which would be reflected to the status backing store. More investigation might be granted, that's why I didn't close the ticket. But I wanted to bring up that this sounded relevant to KAFKA-8869. ;;;","24/Jan/20 22:08;ChrisEgerton;After looking into this a bit I suspect the root cause is different... it looks like KAFKA-8869 is caused by not removing task configurations for deleted connectors from the task config map, whereas this issue is caused by not removing the statuses for no-longer-needed tasks from the status backing store when the number of tasks for a connector is reduced, but the connector is still running.

From what I can tell, this issue does not arise when a connector is deleted (the task statuses are correctly removed from the backing store), and KAFKA-8869 does not arise when the number of tasks for a connector is reduced (the task reduction is correctly reflected in the values for the task config map).;;;","24/Jan/20 23:15;kkonstantine;Sounds good. Thanks for taking a closer look;;;","14/Feb/20 20:58;githubbot;C0urante commented on pull request #8118: KAFKA-9472: Remove deleted tasks from status store
URL: https://github.com/apache/kafka/pull/8118
 
 
   [Jira](https://issues.apache.org/jira/browse/KAFKA-9472)
   
   Although the statuses for tasks are removed from the status store when their _connector_ is deleted, their status are not removed when only the task is deleted, which happens in the case that the number of tasks for a connector is reduced.
   
   These changes add logic for deleting the statuses for those tasks from the status store whenever a rebalance has completed and the leader of a distributed cluster has detected that there are recently-deleted tasks. Standalone is also updated to accomplish this.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct exception message in DistributedHerder,KAFKA-9462,13280863,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,yuzhihong@gmail.com,yuzhihong@gmail.com,yuzhihong@gmail.com,22/Jan/20 00:09,24/Jan/20 19:59,13/Jul/23 09:17,24/Jan/20 19:59,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,KafkaConnect,,,,,0,,,,,"There are a few exception messages in DistributedHerder which were copied from other exception message.

This task corrects the messages to reflect actual condition",,githubbot,rhauch,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 19:59:03 UTC 2020,,,,,,,,,,"0|z0aqrk:",9223372036854775807,,rhauch,,,,,,,,,,,,,,,,,,"22/Jan/20 00:10;githubbot;tedyu commented on pull request #7995: KAFKA-9462: Correct exception message in DistributedHerder
URL: https://github.com/apache/kafka/pull/7995
 
 
   There are a few exception messages in DistributedHerder which were copied from other exception message.
   
   This PR corrects the messages to reflect actual condition
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Jan/20 19:56;githubbot;rhauch commented on pull request #7995: KAFKA-9462: Correct exception message in DistributedHerder
URL: https://github.com/apache/kafka/pull/7995
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","24/Jan/20 19:59;rhauch;Thanks, [~yuzhihong@gmail.com]. Merged to the `trunk` branch. IMO backporting is not really warranted this since this is a very minor change to an exception message.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test org.apache.kafka.common.network.SelectorTest.testGracefulClose,KAFKA-9457,13280729,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,21/Jan/20 10:46,21/Jan/20 15:40,13/Jul/23 09:17,21/Jan/20 15:40,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,network,,,,,0,,,,,"org.apache.kafka.common.network.SelectorTest.testGracefulClose has been failing a lot in PR builds:

{{java.lang.AssertionError: expected:<1> but was:<0>}}
{{ at org.junit.Assert.fail(Assert.java:89)}}
{{ at org.junit.Assert.failNotEquals(Assert.java:835)}}
{{ at org.junit.Assert.assertEquals(Assert.java:647)}}
{{ at org.junit.Assert.assertEquals(Assert.java:633)}}
{{ at org.apache.kafka.common.network.SelectorTest.testGracefulClose(SelectorTest.java:588)}}",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 15:40:39 UTC 2020,,,,,,,,,,"0|z0apxs:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"21/Jan/20 11:02;githubbot;rajinisivaram commented on pull request #7989: KAFKA-9457; Fix flaky test org.apache.kafka.common.network.SelectorTest.testGracefulClose
URL: https://github.com/apache/kafka/pull/7989
 
 
   Test currently assumes that exactly one receive is completed within 1 second. We cannot guarantee that. Updated to increase timeout to 5 seconds and allow one or more pending receives to complete.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Jan/20 15:40;githubbot;rajinisivaram commented on pull request #7989: KAFKA-9457; Fix flaky test org.apache.kafka.common.network.SelectorTest.testGracefulClose
URL: https://github.com/apache/kafka/pull/7989
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer's BufferPool may block the producer from closing.,KAFKA-9449,13280184,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbyrne,bbyrne,bbyrne,17/Jan/20 16:49,17/Jan/20 22:32,13/Jul/23 09:17,17/Jan/20 22:32,,,,,,,,,,,,,,,,,,,,,,,2.4.1,,,,,,,,,,,,,0,,,,,"The producer's BufferPool may block allocations if its memory limit has hit capacity. If the producer is closed, it's possible for the allocation waiters to wait for max.block.ms if progress cannot be made, even when force-closed (immediate), which can cause indefinite blocking if max.block.ms is particularly high.

The BufferPool should be made close-able, which should immediate wake up any waiters that are pending allocations and throw a ""producer is closing"" exception.",,bbyrne,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 22:27:36 UTC 2020,,,,,,,,,,"0|z0amko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/20 22:27;githubbot;hachikuji commented on pull request #7967: KAFKA-9449: Adds support for closing the producer's BufferPool.
URL: https://github.com/apache/kafka/pull/7967
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test BranchedMultiLevelRepartitionConnectedTopologyTest.testTopologyBuild,KAFKA-9399,13278797,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,mjsax,mjsax,10/Jan/20 21:23,28/Feb/20 17:42,13/Jul/23 09:17,28/Feb/20 17:42,2.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/172/testReport/junit/org.apache.kafka.streams.integration/BranchedMultiLevelRepartitionConnectedTopologyTest/testTopologyBuild/]
{quote}java.lang.AssertionError: Condition not met within timeout 15000. Failed to observe stream transits to RUNNING at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:369) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:339) at org.apache.kafka.streams.integration.BranchedMultiLevelRepartitionConnectedTopologyTest.testTopologyBuild(BranchedMultiLevelRepartitionConnectedTopologyTest.java:146){quote}
STDOUT
{quote}[2020-01-10 20:54:59,190] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-consumer, groupId=branched-repartition-topic-test] Connection to node 0 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756) [2020-01-10 20:54:59,264] WARN [Producer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-producer] Connection to node 0 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756) [2020-01-10 20:54:59,314] WARN [AdminClient clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-admin] Connection to node 0 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756)
...
[2020-01-10 20:54:59,356] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-restore-consumer, groupId=null] Connection to node -1 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756) [2020-01-10 20:54:59,356] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-restore-consumer, groupId=null] Bootstrap broker localhost:38720 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1024) [2020-01-10 20:54:59,391] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-consumer, groupId=branched-repartition-topic-test] Connection to node 0 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756) [2020-01-10 20:54:59,457] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-restore-consumer, groupId=null] Connection to node -1 (localhost/127.0.0.1:38720) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:756) [2020-01-10 20:54:59,457] WARN [Consumer clientId=branched-repartition-topic-test-7bb4acef-c1c1-401f-adb9-a67a448cae02-StreamThread-1-restore-consumer, groupId=null] Bootstrap broker localhost:38720 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1024)
...
and some more of those...{quote}",,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-10 21:23:41.0,,,,,,,,,,"0|z0aeew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeleteRecords may cause extreme lock contention for large partition directories,KAFKA-9393,13278495,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gardnervickers,lucasbradstreet,lucasbradstreet,09/Jan/20 17:00,15/Dec/20 17:55,13/Jul/23 09:17,09/Oct/20 21:26,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.8.0,,,,,,,,,,,,,0,,,,,"DeleteRecords, frequently used by KStreams triggers a Log.maybeIncrementLogStartOffset call, calling kafka.log.ProducerStateManager.listSnapshotFiles which calls java.io.File.listFiles on the partition dir. The time taken to list this directory can be extreme for partitions with many small segments (e.g 20000) taking multiple seconds to finish. This causes lock contention for the log, and if produce requests are also occurring for the same log can cause a majority of request handler threads to become blocked waiting for the DeleteRecords call to finish.

I believe this is a problem going back to the initial implementation of the transactional producer, but I need to confirm how far back it goes.

One possible solution is to maintain a producer state snapshot aligned to the log segment, and simply delete it whenever we delete a segment. This would ensure that we never have to perform a directory scan.",,ableegoldman,githubbot,guozhang,junrao,lucasbradstreet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 21:26:06 UTC 2020,,,,,,,,,,"0|z0acjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 19:28;guozhang;Thanks for filing this Lucas. This is good to know.

Regarding the fix what you've proposed looks good to me -- and at the moment I think another ""workaround"" is to let streams app to be less frequently deleting records on changelogs. cc [~ableegoldman] [~mjsax];;;","10/Jan/20 19:03;githubbot;gardnervickers commented on pull request #7929: KAFKA-9393: Establish a 1:1 mapping between producer state snapshot files and segment files.
URL: https://github.com/apache/kafka/pull/7929
 
 
   https://issues.apache.org/jira/browse/KAFKA-9393
   This PR avoids a performance issue with `DeleteRecords` when a partition directory contains high numbers of files. Previously, `DeleteRecords` would iterate the partition directory searching for producer state snapshot files. With this change, the iteration is removed in favor of keeping a 1:1 mapping between producer state snapshot file and segment file. A segment files corresponding producer state snapshot file is now deleted when the segment file is deleted. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Oct/20 21:26;junrao;merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-key joining of KTable not compatible with confluent avro serdes,KAFKA-9390,13278361,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vvcephei,kiwiandy,kiwiandy,09/Jan/20 05:20,12/Feb/20 00:30,13/Jul/23 09:17,12/Feb/20 00:30,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.4.1,2.5.0,,,,,,,streams,,,,,1,,,,,"I was trying out the new one-to-many KTable joins against some CDC data in Avro format and kept getting serialisation errors.

 
{code:java}
org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {""type"":""record"",""name"":""Key"",""namespace"":""dbserver1.inventory.orders"",""fields"":[

{""name"":""order_number"",""type"":""int""}

],""connect.name"":""dbserver1.inventory.orders.Key""}
 Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Schema being registered is incompatible with an earlier schema; error code: 409
  
{code}
Both tables have avro keys of different types (one is an order key, the other a customer key).

This looks like it will cause issues.

[https://github.com/apache/kafka/blob/2.4/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySchema.java#L57-L60]

 They will both attempt to register schemas with the same subject to the schema registry which will fail a backward compatibility check.

I also noticed in the schema registry there were some subjects that didn't have the application id prefix. This is probably caused by this...

 [https://github.com/apache/kafka/blob/2.4/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionSendProcessorSupplier.java#L88]

Where here {{repartitionTopicName}} doesn't have the application prefix.

 

 

 

 

 ",,ableegoldman,bwittwer,cadonna,githubbot,guozhang,kiwiandy,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3705,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 11 23:34:06 UTC 2020,,,,,,,,,,"0|z0abpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 05:25;kiwiandy;Here's the code snippet. I'm joining the orders table to the customers table using the customer key as the foreign key.
{code:java}
final KTable<dbserver1.inventory.customers.Key, dbserver1.inventory.customers.Value> customersTable =
    CdcHelpers.valueTable(streams.customers(), ""customersTable"");
final KTable<dbserver1.inventory.orders.Key, dbserver1.inventory.orders.Value> ordersTable =
    CdcHelpers.valueTable(streams.orders(), ""ordersTable"");
final KTable<dbserver1.inventory.orders.Key, OrderView> enrichedOrders =
    ordersTable.join(
      customersTable,
      order -> new dbserver1.inventory.customers.Key(order.getPurchaser()),
      (order, customer) ->
          OrderView.newBuilder()
            .setCustomerName(customer.getFirstName() + ' ' + customer.getLastName())
            .setOrderId(order.getOrderNumber())
            .setOrderDate(LocalDate.fromDateFields(Date.valueOf(java.time.LocalDate.ofEpochDay(order.getOrderDate()))))
            .setOrderQuantity(order.getQuantity())
            .setProductName(""unknown"")
            .setProductWeight(0.0d)
            .build(),
        Named.as(""wtf""),
        AvroSerdes.materializedAs(""ordersWithCustomerxx""));
{code};;;","09/Jan/20 05:26;kiwiandy;Also I'm using the 2.4 version of Kafka Streams with 5.3.2 version of confluent serdes (since 5.4 isn't out);;;","09/Jan/20 19:26;guozhang;There is a known issue I think since Confluent's SR do not allow nulls and hence we use a not-null dummy value. Not sure if there are still missing pieces out there. [~bbejeck] [~mjsax] could you take a look?;;;","06/Feb/20 20:57;githubbot;vvcephei commented on pull request #8054: KAFKA-9390: Make serde pseudo-topics unique
URL: https://github.com/apache/kafka/pull/8054
 
 
   During the discussion for KIP-213, we decided to pass ""pseudo-topics""
   to the internal serdes we use to construct the wrapper serdes for
   CombinedKey and hashing the left-hand-side value. However, during
   the implementation, this strategy wasn't fully implemented, and we wound
   up using the same topic name for a few different data types.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","11/Feb/20 23:34;githubbot;vvcephei commented on pull request #8054: KAFKA-9390: Make serde pseudo-topics unique
URL: https://github.com/apache/kafka/pull/8054
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test StandbyTaskCreationIntegrationTest.shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables,KAFKA-9388,13278265,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,mjsax,mjsax,08/Jan/20 23:27,22/Apr/20 21:29,13/Jul/23 09:17,22/Apr/20 21:29,2.5.0,,,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,streams,unit tests,,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/122/testReport/junit/org.apache.kafka.streams.integration/StandbyTaskCreationIntegrationTest/shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables/]
{quote}java.lang.AssertionError: Condition not met within timeout 30000. At least one client did not reach state RUNNING with active tasks and stand-by tasks: Client 1 is NOT OK, client 2 is NOT OK. at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:24) at org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:369) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:368) at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:356) at org.apache.kafka.streams.integration.StandbyTaskCreationIntegrationTest.waitUntilBothClientAreOK(StandbyTaskCreationIntegrationTest.java:178) at org.apache.kafka.streams.integration.StandbyTaskCreationIntegrationTest.shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables(StandbyTaskCreationIntegrationTest.java:141){quote}",,cadonna,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-08 23:27:14.0,,,,,,,,,,"0|z0abg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test AclAuthorizerTest.testHighConcurrencyDeletionOfResourceAcls,KAFKA-9386,13278196,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,08/Jan/20 15:48,09/Jan/20 14:51,13/Jul/23 09:17,09/Jan/20 14:51,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,security,,,,,0,,,,,"Failure:

org.scalatest.exceptions.TestFailedException: expected acls:
 
but got:
 (principal=User:alice, host=*, operation=ALL, permissionType=ALLOW)

at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
 at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
 at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)
 at org.scalatest.Assertions.fail(Assertions.scala:1091)
 at org.scalatest.Assertions.fail$(Assertions.scala:1087)
 at org.scalatest.Assertions$.fail(Assertions.scala:1389)
 at kafka.utils.TestUtils$.waitAndVerifyAcls(TestUtils.scala:843)
 at kafka.security.authorizer.AclAuthorizerTest.testHighConcurrencyDeletionOfResourceAcls(AclAuthorizerTest.scala:552)",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 14:48:07 UTC 2020,,,,,,,,,,"0|z0ab0w:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,"08/Jan/20 18:45;githubbot;rajinisivaram commented on pull request #7911: KAFKA-9386; Apply delete ACL filters to resources from filter even if not in cache
URL: https://github.com/apache/kafka/pull/7911
 
 
   With the old SimpleAclAuthorizer, we were handling delete filters that matched a single resource by looking up that resource directly, even if it wasn't in the cache. AclAuthorizerTest.testHighConcurrencyDeletionOfResourceAcls relies on this behaviour and fails intermittently when the cache is not up-to-date. This PR includes the resource from non-matching filters even if it is not in the cache to retain the old behaviour.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Jan/20 14:48;githubbot;rajinisivaram commented on pull request #7911: KAFKA-9386; Apply delete ACL filters to resources from filter even if not in cache
URL: https://github.com/apache/kafka/pull/7911
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadocs + Scaladocs not published on maven central,KAFKA-9381,13278021,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,bbejeck,sirocchj,sirocchj,07/Jan/20 22:26,27/Oct/20 00:48,13/Jul/23 09:17,27/Oct/20 00:48,1.0.0,1.0.1,1.0.2,1.1.0,1.1.1,2.0.0,2.0.1,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,2.4.1,2.5.0,,,,,,2.7.0,,,,,,,,documentation,streams,,,,0,,,,,"As per title, empty (aside for MANIFEST, LICENCE and NOTICE) javadocs/scaladocs jars on central for any version (kafka nor scala), e.g.

[http://repo1.maven.org/maven2/org/apache/kafka/kafka-streams-scala_2.12/2.3.1/]",,ableegoldman,bbejeck,mjsax,mumrah,rhauch,sirocchj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 00:48:21 UTC 2020,,,,,,,,,,"0|z0a9y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/20 00:23;mjsax;Thanks for picking this one up [~mumrah]! I marked it as blocker for 2.5 release.;;;","13/Mar/20 18:39;mumrah;Got the streams-scala scaladoc jar to build, but now getting {{java.net.MalformedURLException}} from the core scaladocs. Seems like a known issue for Gradle and Scala 2.13 https://github.com/gradle/gradle/issues/9855;;;","13/Mar/20 19:06;mumrah;Also, looks like this affects all scaladoc jars, not just streams-scala, e.g. https://repository.apache.org/content/groups/staging/org/apache/kafka/kafka_2.13/2.5.0/kafka_2.13-2.5.0-scaladoc.jar;;;","13/Mar/20 23:03;mumrah;kafka-streams-scala has published an empty scaladoc jar since 2.0.0, e.g. https://repo1.maven.org/maven2/org/apache/kafka/kafka-streams-scala_2.12/2.0.0/. The core kafka module has published an empty scaladoc since 1.0.0. So, this has been a long running packaging bug.

It looks like the easiest fix is to upgrade to Gradle 6.0+. Filed KAFKA-9720 for a future release.;;;","08/Jun/20 17:57;mjsax;[~rhauch] – can you maybe help us resolving this issue for 2.6.0, as you are the RM?;;;","24/Jun/20 22:57;rhauch;[~mumrah] can you share the changes you made to get the javadoc and scaladoc to build? We're running Gradle 6.5, which is supposed to have the fix for the {{MalformedURLException}}.;;;","21/Oct/20 17:47;bbejeck;Since this is a long-standing issue, I'm going to remove the blocker tag. I'm taking a look at getting this fixed in this release, so I have picked up the ticket.;;;","27/Oct/20 00:48;bbejeck;Resolved via [https://github.com/apache/kafka/pull/9486.]

 

Merged to trunk and cherry-picked to 2.7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test TopicCommandWithAdminClientTest.testCreateAlterTopicWithRackAware,KAFKA-9379,13278010,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,rsivaram,mjsax,mjsax,07/Jan/20 20:51,09/Jan/20 14:52,13/Jul/23 09:17,09/Jan/20 14:52,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,admin,core,unit tests,,,0,flaky-test,,,,"[https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/116/testReport/junit/kafka.admin/TopicCommandWithAdminClientTest/testCreateAlterTopicWithRackAware/]
{quote}java.lang.IllegalArgumentException: Topic 'testCreateAlterTopicWithRackAware-1Ski7jYwdP' does not exist as expected at kafka.admin.TopicCommand$.kafka$admin$TopicCommand$$ensureTopicExists(TopicCommand.scala:503) at kafka.admin.TopicCommand$AdminClientTopicService.alterTopic(TopicCommand.scala:257) at kafka.admin.TopicCommandWithAdminClientTest.testCreateAlterTopicWithRackAware(TopicCommandWithAdminClientTest.scala:476){quote}
STDOUT
{quote}[2020-01-07 17:50:17,384] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition testAlterPartitionCount-MixWA3fYA3-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:17,631] ERROR [ReplicaFetcher replicaId=4, leaderId=1, fetcherId=0] Error for partition testAlterPartitionCount-MixWA3fYA3-2 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:24,498] ERROR [ReplicaFetcher replicaId=0, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-7B6hJOHCF7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:24,506] ERROR [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-7B6hJOHCF7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:24,506] ERROR [ReplicaFetcher replicaId=5, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-7B6hJOHCF7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:24,506] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-7B6hJOHCF7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:50:24,507] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition testDescribeAtMinIsrPartitions-7B6hJOHCF7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:51:29,090] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition kafka.testTopic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:51:29,204] ERROR [ReplicaFetcher replicaId=1, leaderId=5, fetcherId=0] Error for partition __consumer_offsets-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:51:29,255] ERROR [ReplicaFetcher replicaId=4, leaderId=0, fetcherId=0] Error for partition __consumer_offsets-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,546] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-2 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,547] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-14 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,550] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-13 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,551] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-7 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,615] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-11 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,651] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-4 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,762] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-15 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,766] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-9 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,835] ERROR [ReplicaFetcher replicaId=5, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-3 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,850] ERROR [ReplicaFetcher replicaId=4, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-7 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,862] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-13 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,863] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,894] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-14 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,895] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-8 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,898] ERROR [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-15 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,899] ERROR [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-3 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,918] ERROR [ReplicaFetcher replicaId=4, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-4 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,919] ERROR [ReplicaFetcher replicaId=4, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-16 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,964] ERROR [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,970] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-9 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,981] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-10 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:24,990] ERROR [ReplicaFetcher replicaId=1, leaderId=5, fetcherId=0] Error for partition testCreateAlterTopicWithRackAware-1Ski7jYwdP-5 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:52:55,508] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testDescribeReportOverriddenConfigs-kJmNGKXCWp-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:20,208] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition kafka.testTopic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:20,272] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition kafka.testTopic1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:20,339] ERROR [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Error for partition kafka.testTopic2-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:20,345] ERROR [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error for partition kafka.testTopic2-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:20,502] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition oooof.testTopic1-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:34,538] ERROR [ReplicaFetcher replicaId=5, leaderId=1, fetcherId=0] Error for partition testAlterAssignmentWithMoreAssignmentThanPartitions-6CrQCdg5uJ-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:34,546] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testAlterAssignmentWithMoreAssignmentThanPartitions-6CrQCdg5uJ-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:53:47,842] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testCreateWithDefaultPartitions-DOrrkPW5SS-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. [2020-01-07 17:54:48,082] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition testAlterAssignmentWithMorePartitionsThanAssignment-FhXH31oqrP-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:09,738] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition testCreateWithConfigs-9TbAdbpoBC-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:20,755] ERROR [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-8a0EQsSDy7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:20,795] ERROR [ReplicaFetcher replicaId=4, leaderId=0, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-8a0EQsSDy7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:20,798] ERROR [ReplicaFetcher replicaId=3, leaderId=0, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-8a0EQsSDy7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:20,814] ERROR [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error for partition testDescribeUnderReplicatedPartitions-8a0EQsSDy7-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:37,873] ERROR [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error for partition testDescribeUnderMinIsrPartitions-q3HCY3lYQ1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:37,902] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition testDescribeUnderMinIsrPartitions-q3HCY3lYQ1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:55:37,906] ERROR [ReplicaFetcher replicaId=0, leaderId=3, fetcherId=0] Error for partition testDescribeUnderMinIsrPartitions-q3HCY3lYQ1-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. [2020-01-07 17:57:52,186] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition testCreateWithReplicaAssignment-HNp5UMNPYL-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:57:52,238] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition testCreateWithReplicaAssignment-HNp5UMNPYL-2 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,198] ERROR [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error for partition under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,201] ERROR [ReplicaFetcher replicaId=4, leaderId=2, fetcherId=0] Error for partition under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,200] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,204] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error for partition under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,204] ERROR [ReplicaFetcher replicaId=5, leaderId=2, fetcherId=0] Error for partition under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,320] ERROR [ReplicaFetcher replicaId=4, leaderId=3, fetcherId=0] Error for partition not-under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,319] ERROR [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Error for partition fully-replicated-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,319] ERROR [ReplicaFetcher replicaId=5, leaderId=3, fetcherId=0] Error for partition not-under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,320] ERROR [ReplicaFetcher replicaId=0, leaderId=3, fetcherId=0] Error for partition not-under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,352] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition fully-replicated-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:10,364] ERROR [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Error for partition not-under-min-isr-topic-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:27,798] ERROR [ReplicaFetcher replicaId=5, leaderId=3, fetcherId=0] Error for partition testDescribe-xHbIEW9DxM-0 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:27,800] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition testDescribe-xHbIEW9DxM-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. [2020-01-07 17:58:31,646] ERROR [ReplicaFetcher replicaId=0, leaderId=5, fetcherId=0] Error for partition testAlterAssignment-uub2jm6g0o-1 at offset 0 (kafka.server.ReplicaFetcherThread:76) org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.{quote}
STDERR
{quote}Option ""[replica-assignment]"" can't be used with option ""[partitions]""
...

Missing required argument ""[partitions]""
...

Option ""[if-exists]"" can't be used with option ""[bootstrap-server]""
...

Option ""[replica-assignment]"" can't be used with option ""[replication-factor]"" 
...

Option combination ""[bootstrap-server],[config]"" can't be used with option ""[alter]"" 
...

Option ""[delete-config]"" can't be used with option ""[bootstrap-server]"" 
...{quote}",,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 14:17:47 UTC 2020,,,,,,,,,,"0|z0a9vk:",9223372036854775807,,ijuma,,,,,,,,,,,,,,,,,,"09/Jan/20 11:27;githubbot;rajinisivaram commented on pull request #7917: KAFKA-9379; Fix flaky test TopicCommandWithAdminClientTest.testCreateAlterTopicWithRackAware
URL: https://github.com/apache/kafka/pull/7917
 
 
   Test creates a topic, waits for `adminClient.listTopics` to return the topic and then performs `alterTopic()`, which also uses `adminClient.listTopics` to verify that the topic exists. This may fail if the topic hadn't yet been propagated to the broker to which the second `listTopics` is sent. This PR updates the test to wait for topic to be in the metadata cache of all brokers before calling `alterTopic()`.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","09/Jan/20 14:17;githubbot;rajinisivaram commented on pull request #7917: KAFKA-9379; Fix flaky test TopicCommandWithAdminClientTest.testCreateAlterTopicWithRackAware
URL: https://github.com/apache/kafka/pull/7917
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker can be disabled by blocked connectors,KAFKA-9374,13277872,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ChrisEgerton,ChrisEgerton,ChrisEgerton,07/Jan/20 07:15,01/Feb/23 21:32,13/Jul/23 09:17,11/Jun/20 15:25,1.0.0,1.0.1,1.0.2,1.1.0,1.1.1,2.0.0,2.0.1,2.1.0,2.1.1,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.4.0,,,,,,,,2.6.0,,,,,,,,KafkaConnect,,,,,0,,,,,"If a connector hangs during any of its {{{}initialize{}}}, {{{}start{}}}, {{{}stop{}}}, {{{}taskConfigs{}}}, {{{}taskClass{}}}, {{{}version{}}}, {{{}config{}}}, or {{validate}} methods, the worker will be disabled for some types of requests thereafter, including connector creation, connector reconfiguration, and connector deletion.
-This only occurs in distributed mode and is due to the threading model used by the [DistributedHerder|https://github.com/apache/kafka/blob/03f763df8a8d9482d8c099806336f00cf2521465/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java] class.- This affects both distributed and standalone mode. Distributed herders perform some connector work synchronously in their {{tick}} thread, which also handles group membership and some REST requests. The majority of the herder methods for the standalone herder are {{{}synchronized{}}}, including those for creating, updating, and deleting connectors; as long as one of those methods blocks, all subsequent calls to any of these methods will also be blocked.

 

One potential solution could be to treat connectors that fail to start, stop, etc. in time similarly to tasks that fail to stop within the [task graceful shutdown timeout period|https://github.com/apache/kafka/blob/03f763df8a8d9482d8c099806336f00cf2521465/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java#L121-L126] by handling all connector interactions on a separate thread, waiting for them to complete within a timeout, and abandoning the thread (and transitioning the connector to the {{FAILED}} state, if it has been created at all) if that timeout expires.",,ChrisEgerton,githubbot,rhauch,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-12904,,,KAFKA-9975,KAFKA-14670,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 07 16:34:58 UTC 2021,,,,,,,,,,"0|z0a90w:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,,,,,"07/Jan/20 08:56;tombentley;""Abandoning"" the thread isn't really a solution. You cannot be sure that thread will ever die. Abandon enough threads and it starts to look like a resource leak. Also, any connector which hangs in one of those methods is buggy and that bug will come to light (and potentially be fixed) sooner if the connector fails in a noticeable way which the end user is likely to notice. By papering over the cracks like this isn't the end user more likely to just try recreating the connector (which maybe works some of the time)? Thus potentially letting the bug live longer?;;;","07/Jan/20 19:02;ChrisEgerton;[~tombentley] yeah, it's not great if a lot threads are created and then abandoned. However, I'd like to make the following observations:
 * We already use this approach with connector tasks and it seems to be working well enough; the only time in the wild I've seen it be a problem is due to an issue in the Connect framework, specifically KAFKA-9051
 * A single poorly-behaved connector should not be able to block the worker's REST API; users could have dozens if not hundreds of connectors running on their worker
 * If a connector config has already been written to the config topic but the connector blocks in its {{start}} method, removing it can be extremely difficult if the worker that's blocked by that connector is the leader or if the cluster only consists of a single worker; it may even be necessary to directly write to the internal config topic to do so

Because we want the worker to remain available even when connectors hang, I think sacrificing a thread in this situation is preferable to sacrificing the entire worker. If there's a way to avoid continually creating and then abandoning new threads when poorly-behaved connectors are created and then block indefinitely _and_ keep the worker going with no impact on the REST API or other connectors and tasks, we should definitely consider it. However, as far as I know, once a Java thread is blocked there is no guaranteed, safe way of blocking or terminating it (well, besides [this|https://stackoverflow.com/a/32909191/12417563]).

 

As far as failing the connector in a noticeable way–I completely agree. I mentioned that we could transition connectors that have at least been created (i.e., written to the config topic) to a failed state should they block for too long, which would be a start as far as alerting the user that their connector is buggy goes. Additionally, if a connector blocks in something like its {{validate}} or {{config}} method, we could also fail the REST request that led to that method invocation.;;;","08/Jan/20 09:52;tombentley;[~ChrisEgerton] yeah, I agree there's no tenable alternative for recovering the thread, and protecting the worker is a worthy aim. I like the idea of using an error response (for those cases associated with a request) in addition to transitioning the connector to failed.

Would the timeout be configurable (not arguing that it should, merely asking)?;;;","08/Feb/20 02:36;githubbot;C0urante commented on pull request #8069: KAFKA-9374: Make connector interactions asynchronous
URL: https://github.com/apache/kafka/pull/8069
 
 
   [Jira ticket](https://issues.apache.org/jira/browse/KAFKA-9374)
   
   These changes allow herders to continue to function even when a connector they
   are running hangs in its start, stop, initialize, validate, and/or config
   methods.
   
   The main idea is to make these connector interactions asynchronous and accept
   a callback that can be invoked upon the completion (successful or otherwise) of
   these interactions. The distributed herder handles any follow-up logic by adding
   a new herder request to its queue in that callback, which helps preserve some
   synchronization and ordering guarantees provided by the current tick model.
   
   There are several items that still need to be addressed:
   
   1)  The standalone herder has not been updated to utilize the new asynchronous
       connector wrapper API provided by the Worker and AbstractHerder classes
   
   2)  There is a minor TODO in the DistributedHerderTest class regarding the need
       to migrate some testing logic into the AbstractHerderTest class
   
   3)  More significantly, since connector shutdown is now handled asynchronously,
       there are two problems with the current changes:
   
       a - It is possible (even likely) that a new instance of a connector will be
           created before an older instance has been shut down. This is especially
           problematic if a connector claims a shared resource such as a port
           number and could potentially lead to unnecessary connector failure on
           startup.
   
       b - There is no time allocated during shutdown of the herder for its
           connectors to shutdown, which may lead to improper resource cleanup.
   
   Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","10/Feb/20 23:12;ChrisEgerton;Hi [~tombentley], sorry for the delay in reply.

 

After some thought, it seems that setting a timeout on interactions with connectors but keeping those actions synchronous within the herder's tick method isn't really a viable approach. There doesn't seem to be a good value to use for that timeout; if it's too conservative it may be impossible to start some connectors that have to do heavy-duty initialization on startup, and if it's too liberal there will still be the original problem (for however long the timeout is) of the worker being effectively disabled during that period, and potentially even dropping out of the group due.

Instead, in [https://github.com/apache/kafka/pull/8069], I've made changes to make most connector interactions (specifically, calls to the start, {{stop}}, {{config}}, {{validate}}, and {{initialize}} methods) completely asynchronous and handle any follow-up logic via callback. In the {{DistributedHerder}} class, this callback adds a new herder request to the queue, which helps keep the class thread-safe and preserves some of the guarantees provided by the current {{tick}} model.

Unfortunately, this means that status tracking for connectors becomes... difficult. If we don't establish a timeout for any of our connector interactions, we also then don't have a good metric for know if/when to update the status of a connector to {{FAILED}}. At this point, the best we may be able to do is include log messages detailing when certain connector interactions are scheduled, and when those interactions are complete. That should at least provide a decent method for diagnosing via log files whether a connector is blocking and effectively a zombie. In the future, a KIP may be warranted for adding a new metric to track the number and types of zombie connectors/tasks.

This also still leaves the door open for zombie thread creation; any connector that blocks in any of the aforementioned methods will still be taking up a thread until/unless it returns control to the framework.;;;","11/Jun/20 15:25;rhauch;Merged to `trunk` and backported to the `2.6` branch for inclusion in 2.6.0.;;;","07/Jun/21 16:34;rhauch;Please ignore link to [GitHub Pull Request #10833|https://github.com/apache/kafka/pull/10833]. I created it with the wrong KAFKA issue number.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve shutdown performance via lazy accessing the offset and time indices.,KAFKA-9373,13277832,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,agencer,agencer,agencer,07/Jan/20 02:43,26/Mar/20 11:31,13/Jul/23 09:17,26/Mar/20 11:31,2.3.0,2.3.1,2.4.0,,,,,,,,,,,,,,,,,,,,2.6.0,,,,,,,,log,,,,,1,,,,,"KAFKA-7283 enabled lazy mmap on index files by initializing indices on-demand rather than performing costly disk/memory operations when creating all indices on broker startup. This helped reducing the startup time of brokers. However, segment indices are still created on closing segments, regardless of whether they need to be closed or not.
 
Ideally we should:
 * Improve shutdown performance via lazy accessing the offset and time indices.
 * Eliminate redundant disk accesses and memory mapped operations while deleting or renaming files that back segment indices.
 * Prevent illegal accesses to underlying indices of a closed segment, which would lead to memory leaks due to recreation of the underlying memory mapped objects.",,agencer,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 11:30:03 UTC 2020,,,,,,,,,,"0|z0a8s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 02:48;githubbot;efeg commented on pull request #7900: KAFKA-9373: Improve shutdown performance via lazy accessing the offset and time indices
URL: https://github.com/apache/kafka/pull/7900
 
 
   KAFKA-7283 enabled lazy mmap on index files by initializing indices on-demand rather than performing costly disk/memory operations when creating all indices on broker startup. This helped reducing the startup time of brokers. However, segment indices are still created on closing segments, regardless of whether they need to be closed or not.
   
   This patch:
   * Improves shutdown performance via lazy accessing the offset and time indices.
   * Eliminates redundant disk accesses and memory mapped operations while deleting or renaming files that back segment indices.
   * Prevents illegal accesses to underlying indices of a closed segment, which would lead to memory leaks due to recreation of the underlying memory mapped objects.
   
   In our evaluations in a cluster with 31 brokers, where each broker has 13K to 20K segments, we observed up to 2 orders of magnitude faster LogManager shutdown times with this patch -- i.e. dropping the LogManager shutdown time of each broker from 10s of seconds to 100s of milliseconds.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","25/Mar/20 14:48;githubbot;ijuma commented on pull request #8346: KAFKA-9373: Reduce shutdown time by avoiding unnecessary loading of indexes
URL: https://github.com/apache/kafka/pull/8346
 
 
   KAFKA-7283 enabled lazy mmap on index files by initializing indices
   on-demand rather than performing costly disk/memory operations when
   creating all indices on broker startup. This helped reducing the startup
   time of brokers. However, segment indices are still created on closing
   segments, regardless of whether they need to be closed or not.
   
   This is a cleaned up version of #7900, which was submitted by @efeg. It
   eliminates unnecessary disk accesses and memory map operations while
   deleting, renaming or closing offset and time indexes.
   
   In a cluster with 31 brokers, where each broker has 13K to 20K segments,
   @efeg and team observed up to 2 orders of magnitude faster LogManager
   shutdown times - i.e. dropping the LogManager shutdown time of each
   broker from 10s of seconds to 100s of milliseconds.
   
   To avoid confusion between `renameTo` and `setFile`, I replaced the
   latter with the more restricted updateParentDir` (it turns out that's
   all we need).
   
   Co-authored-by: Adem Efe Gencer <agencer@linkedin.com>
   Co-authored-by: Ismael Juma <ismael@juma.me.uk>
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Mar/20 11:26;githubbot;ijuma commented on pull request #8346: KAFKA-9373: Reduce shutdown time by avoiding unnecessary loading of indexes
URL: https://github.com/apache/kafka/pull/8346
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","26/Mar/20 11:30;githubbot;ijuma commented on pull request #7900: KAFKA-9373: Improve shutdown performance via lazy accessing the offset and time indices
URL: https://github.com/apache/kafka/pull/7900
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix misleading consumer logs on throttling,KAFKA-9364,13277419,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,cmccabe,cmccabe,cmccabe,03/Jan/20 19:44,12/Mar/20 18:40,13/Jul/23 09:17,12/Mar/20 18:40,,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,,,,,,0,,,,,Fix misleading consumer logs on throttling,,cmccabe,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 18:30:48 UTC 2020,,,,,,,,,,"0|z0a68g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/20 19:46;githubbot;cmccabe commented on pull request #7894: KAFKA-9364: Fix misleading consumer logs on throttling
URL: https://github.com/apache/kafka/pull/7894
 
 
   When the consumer's fetch request is throttled by the KIP-219 mechanism,
   it receives an empty fetch response.  The consumer should not log this
   as an error.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","06/Jan/20 18:30;githubbot;cmccabe commented on pull request #7894: KAFKA-9364: Fix misleading consumer logs on throttling
URL: https://github.com/apache/kafka/pull/7894
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Admin Script ""kafka-topics"" doesn't confirm on successful creation",KAFKA-9363,13277384,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,manmedia@gmail.com,manmedia@gmail.com,manmedia@gmail.com,03/Jan/20 15:07,12/Jan/20 06:27,13/Jul/23 09:17,12/Jan/20 06:27,2.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,admin,,,,,0,,,,,"When a topic is created from admin console, no confirmation is provided if --bootstrap-server is chosen. 

 

How to reproduce:

1) Get 2.4.0 distro

2) Download and extract code

3) Run ""kafka-topics --create --topic ""dummy"" --partition 1 --replication-factor 1 --bootstrap-server localhost:9092

4) Observe that no confirmation e.g. ""Successfully created dummy"" was provided.


We should, at least, provide a confirmation or restore the confirmation message which was annunciated before using --zookeeper argument. We all must use --describe flag to do a follow-up, but a confirmation message is a nice addition.

 

 ",,githubbot,manmedia@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 12 06:27:28 UTC 2020,,,,,,,,,,"0|z0a60o:",9223372036854775807,,mjsax,,,,,,,,,,,,,,,,,,"03/Jan/20 17:09;githubbot;mmanna-sapfgl commented on pull request #7893: KAFKA-9363 Confirms successful topic creation for JAdminClient
URL: https://github.com/apache/kafka/pull/7893
 
 
   When admin client script is used with --zookeeper, it's printing a confirmation message. The same doesn't occur when using JAdminClient. Since KIP 500 is going to replace ZK in the future, it would be nice to have this confirmation message retained.
   
   No unit testing is requires as this message is simply a confirmation on the console upon a successful return from the AdminClient API call.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","12/Jan/20 06:27;githubbot;hachikuji commented on pull request #7893: KAFKA-9363 Confirms successful topic creation for JAdminClient
URL: https://github.com/apache/kafka/pull/7893
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB statistics are removed from JMX when EOS enabled and empty local state dir,KAFKA-9355,13277169,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cadonna,savulchik,savulchik,02/Jan/20 09:36,09/Mar/21 09:12,13/Jul/23 09:17,12/Feb/20 01:32,2.4.0,,,,,,,,,,,,,,,,,,,,,,2.5.0,,,,,,,,metrics,streams,,,,0,,,,,"*Steps to Reproduce*

Set processing.guarantee = exactly_once and remove local state dir in order to force state restoration from changelog topics that have to be non empty.

*Expected Behavior*

There are registered MBeans like kafka.streams:type=stream-state-metrics,client-id=<application.id>-678e4b25-8fc7-4266-85a0-7a5fe52a4060-StreamThread-1,task-id=0_0,rocksdb-state-id=<state.store> for persistent RocksDB KeyValueStore-s after streams task state restoration.

*Actual Behavior*

There are no registered MBeans like above after streams task state restoration.

*Details*

I managed to inject custom MetricsReporter in order to log metricChange and metricRemoval calls. According to the logs at some point the missing metrics are removed and never restored later. Here is an excerpt for number-open-files metric:
{noformat}
16:33:40.403 DEBUG c.m.r.LoggingMetricsReporter - metricChange MetricName [name=number-open-files, group=stream-state-metrics, description=Number of currently open files, tags={client-id=morpheus.conversion-678e4b25-8fc7-4266-85a0-7a5fe52a4060-StreamThread-1, task-id=0_0, rocksdb-state-id=buffered-event}]
16:33:40.403 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Adding metrics recorder of task 0_0 to metrics recording trigger
16:33:40.403 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Adding statistics for store buffered-event of task 0_0
16:33:40.610 INFO  o.a.k.s.p.i.StoreChangelogReader - stream-thread [morpheus.conversion-678e4b25-8fc7-4266-85a0-7a5fe52a4060-StreamThread-1] No checkpoint found for task 0_0 state store buffered-event changelog morpheus.conversion-buffered-event-changelog-0 with EOS turned on. Reinitializing the task and restore its state from the beginning.
16:33:40.610 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Removing statistics for store buffered-event of task 0_0
16:33:40.610 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Removing metrics recorder for store buffered-event of task 0_0 from metrics recording trigger
16:33
16:33:40.611 DEBUG c.m.r.LoggingMetricsReporter - metricRemoval MetricName [name=number-open-files, group=stream-state-metrics, description=Number of currently open files, tags={client-id=morpheus.conversion-678e4b25-8fc7-4266-85a0-7a5fe52a4060-StreamThread-1, task-id=0_0, rocksdb-state-id=buffered-event}]
16:33:40.625 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Adding metrics recorder of task 0_0 to metrics recording trigger
16:33:40.625 DEBUG o.a.k.s.s.i.m.RocksDBMetricsRecorder - [RocksDB Metrics Recorder for buffered-event] Adding statistics for store buffered-event of task 0_0
...
(no more calls to metricChange for the removed number-open-files metric){noformat}
Also a complete log is attached [^metric-removal.log]

Metric removal happens along this call stack:
{noformat}
19:27:35.509 DEBUG c.m.r.LoggingMetricsReporter - metricRemoval MetricName [name=number-open-files, group=stream-state-metrics, description=Number of currently open files, tags={client-id=morpheus.conversion-9b76f302-7149-47de-b17b-362d642e05d5-StreamThread-1, task-id=0_0, rocksdb-state-id=buffered-event}]
java.lang.Exception: null
   at casino.morpheus.reporter.LoggingMetricsReporter.metricRemoval(LoggingMetricsReporter.scala:24)
   at org.apache.kafka.common.metrics.Metrics.removeMetric(Metrics.java:534)
   at org.apache.kafka.common.metrics.Metrics.removeSensor(Metrics.java:448)
   at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.removeAllStoreLevelSensors(StreamsMetricsImpl.java:440)
   at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.close(MeteredKeyValueStore.java:345)
   at org.apache.kafka.streams.processor.internals.StateManagerUtil.reinitializeStateStoresForPartitions(StateManagerUtil.java:93)
   at org.apache.kafka.streams.processor.internals.ProcessorStateManager.reinitializeStateStoresForPartitions(ProcessorStateManager.java:190)
   at org.apache.kafka.streams.processor.internals.AbstractTask.reinitializeStateStoresForPartitions(AbstractTask.java:215)
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.startRestoration(StoreChangelogReader.java:234)
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.initialize(StoreChangelogReader.java:185)
   at org.apache.kafka.streams.processor.internals.StoreChangelogReader.restore(StoreChangelogReader.java:81)
   at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:389)
   at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:769)
   at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698)
   at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671){noformat}",,ableegoldman,bchen225242,cadonna,githubbot,mjsax,savulchik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6498,,,,,,"02/Jan/20 10:20;savulchik;metric-removal.log;https://issues.apache.org/jira/secure/attachment/12989809/metric-removal.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 12 01:31:18 UTC 2020,,,,,,,,,,"0|z0a4ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/20 10:28;savulchik;[~cadonna] could you please take a look at this case? Thanks in advance!;;;","02/Jan/20 17:45;bchen225242;Thanks for the report. Does all RocksDB metrics vanish or some of them? ;;;","03/Jan/20 05:16;savulchik;All RocksDB metrics vanished for all restoring state stores that have non empty changelog topics. There are no stream-state-metrics MBeans for that state stores in JMX.   

I believe it happens due to StreamsMetricsImpl.removeAllStoreLevelSensors call before state store restoration and missing sensors reinitialization in RocksDBMetricsRecorder afterwards.;;;","07/Jan/20 08:20;cadonna;[~savulchik] Thank you for reporting this. I will take a look.;;;","22/Jan/20 02:43;githubbot;cadonna commented on pull request #7996: KAFKA-9355: Fix bug that removed RocksDB metrics after failure in EOS
URL: https://github.com/apache/kafka/pull/7996
 
 
   - Added `init()` method to `RocksDBMetricsRecorder`
   - Added call to `init()` of `RocksDBMetricsRecorder` to `init()` of RocksDB store
   - Added call to `init()` of `RocksDBMetricsRecorder` to `openExisting()` of segmented state stores
   - Adapted unit tests
   - Added integration test that reproduces the situation in which the bug occurred
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","22/Jan/20 03:01;cadonna;The cause of the bug is the following. In EOS when the a task is not shutdown gracefully (i.e., no checkpoint found, which happens when the local state directory is wiped out), the state stores are reinitialized. During reinitialization all metrics on state store level are removed when the state store is closed on `Metered*Store` level. Afterwards when the store is initialised again the RocksDB metrics recorder does not know that the RocksDB metrics were removed by `Metered*Store` and does not reinitialize them. The above PR fixes this issue. ;;;","12/Feb/20 01:31;githubbot;guozhangwang commented on pull request #7996: KAFKA-9355: Fix bug that removed RocksDB metrics after failure in EOS
URL: https://github.com/apache/kafka/pull/7996
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
